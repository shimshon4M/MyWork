    \documentclass[english]{jnlp_1.4}

\usepackage{jnlpbbl_1.1}
\usepackage[dvips]{graphicx}
\usepackage{hangcaption_jnlp}
\usepackage{amsmath}
\usepackage{tabularx}
\usepackage{udline}
\setulminsep{1.2ex}{0.2ex}
\let\underline

\Volume{15}
\Number{5}
\Month{October}
\Year{2008}

\received{2007}{10}{9}
\revised{2008}{4}{30}
\accepted{2008}{6}{2}

\setcounter{page}{3}

\etitle{A Study on Cross Transformation of Mongolian Language}
\eauthor{Idomucogiin Dawa\affiref{Author_1} \and Satoshi Nakamura\affiref{Author_2}} 
\eabstract{
This paper discusses a segmentation approach of Mongolian for Cyrillic text 
for machine translation. Using this method, the processing of one-to-one 
word permutation between the variations of Mongolian and other languages, 
especially Altaic family languages like Japanese, becomes easier. 
Furthermore, it can be used for two-way conversion between texts of 
Mongolian used in different regions and counties, such as Mongolia and 
China. Our system has been implemented based on DP (dynamic programming) 
matching supported by knowledge-based sequence matching, referred to as a 
multilingual dictionary and linguistic rule bank (LRB), and a data-driven 
approach of the target language corpus (TLC). For convenience, NM (New 
Mongolian) is treated as the source language, and TM (Traditional Mongolian) 
and Todo as the target language in this test. Our application was tested 
using manually transcribed texts with sizes of 5,000 sentences paralleled 
from NM to TM and Todo. We found that our method could achieve 91.9{\%} of 
the transformation accuracy for ``NM'' to ``TM'' and 94.3{\%} for ``NM'' to 
``Todo''.
}
\ekeywords{Mongolian text processing, Machine translation, Text corpus, Linguistic rule bank, DP matching}

\headauthor{Dawa and Nakamura}
\headtitle{A Study on Cross Transformation of Mongolian Language}


\affilabel{Author_1}{}{独立行政法人情報通信研究機構 (NICT)}
\affilabel{Author_2}{}{国際電気通信基礎技術研究所 (ATR)}


\begin{document}

\maketitle


\section{Introduction}

The Mongolian language, together with its many varieties, is used in many 
countries, across from Asia and Europe. However, there are considerable 
differences concerning the written languages in some of these countries and 
regions, such as in China and Mongolia. In addition the, in some areas, 
Mongolian people have no other way to communicate except by using other 
languages, thus endangering their traditional culture (Ts. Shagdarsun 1992, 
田中克彦 2002).

The problem of how to communicate in areas such as these, and how to rescue 
and maintain these endangered languages has become an important research 
topic for Mongolians. Therefore, an effective transformation system between 
the versions of Mongolian, as well as from Mongolian to other languages is 
essential.

Mongolian texts and the word structure differ by region. Some examples of 
current digital text versions of Mongolian are shown in Figure 1, here New 
Mongolian (NM), Traditional Mongolian (TM), Mongolian Todo (Todo), Kalmyk 
and Buryat respectively are the official writing systems used in 
publications and for communication over the Internet. NM, TM, Todo, Kalmyk 
and Buryat are used in Mongolia, Inner Mongolia and XinJiang in China, 
Kalmyk and Buryat in Russia, respectively.

\begin{figure}[t]
\centerline{\includegraphics{15-5ia1f1.eps}}
\caption{Digital texts of Mongolian}
\label{fig1}
\end{figure}

At present time, research related to the Mongolian language and its 
technical processing for communication is regarded as rare.

Ishikawa and Mandla, recently reported their experimental results for 
converting one NM text to a TM text and vice versa, using a fundamental 
linguistic rules and a character processing unit. However, they also pointed 
out that it was very hard to use their approach when the source language was 
different and out-of-rule words occurred frequently (Ishikawa et al. 2005).

Y. Namsurai's group reported a transformation method between two Mongolian 
scripts based on linguistic rules. There are some problems, of course, for 
transforming to other texts like TM or Todo. Their approach cannot be used 
in the case of unlisted words in a limited database (Namsurai 2006).

Our system has been implemented based on DP (dynamic programming) matching 
supported by knowledge-based sequence matching, referred to as a 
multilingual dictionary (Dict.MED), and a linguistic rule bank (LRB), using 
a data-driven approach of the target language corpus (TLC).

In this paper, we first analyze the features and rules of the linguistic and 
language structures of various Mongolian texts. Then, we explain how our 
system was implemented. Finally, we present the results of tests and 
evaluation.


\section{Language structure and features}


\subsection{Differences between written and spoken Mongolian}

Mongolian uses both vertical and horizontal writing systems, as shown in 
Figure 1 above. Some words, for example, such as \textit{jargal} in Figure 2, have the same 
meaning in all writing systems, but there are differences in writing style, 
pronunciation (indicated by ``//''), and Unicode encoding (indicated by 
``()''). Moreover, as illustrated, a vowel is used in some cases but omitted 
in other cases.

\begin{figure}[b]
\centerline{\includegraphics{15-5ia1f2.eps}}
\caption{Differences among source languages for written, spoken, and Unicode forms}
\label{fig2}
\end{figure}



\subsection{Differences in grammar}

As seen in the NM and TM rows shown in Figure 3, a sentence in Mongolian 
consists of a sequence of entries, separated from each other by spaces, like 
in English, while its SOV grammar, like Japanese (Jp), is in the order of 
subject, predicate, object and verb. In NM, an entry generally consists of 
two or three parts: named root (\textit{Yapon}), inflection suffix and affix (\textit{oor}). Refer to 
``\textit{Yoponoor}'', the second string of row 4 in Figure 3, as an example. In the case of 
TM, there is no fixed symbol to spell long vowels in a word. The dash lines 
and dotted circle marks highlight case particles and suffixes. The case 
particle and suffix are usually attached with the root word (stem) in NM but 
they are separated from the root words in TM, just as they are in Japanese 
and English. However, both are allowed in Todo.

In order to create word to word alignments, some rule based and statistical 
natural language processing is used, such as segmentation of the suffixes 
and syntactic analysis of root word in the case of NM, and conversion of the 
long-vowel expressions of TM, remains important for conversion and for 
Mongolian to other language processing, as seen in machine translation 
(Ehara et al. 2007).

\begin{figure}[t]
\centerline{\includegraphics{15-5ia1f3.eps}}
\caption{Grammar comparisons between Mongolian and other languages}
\label{fig3}
\end{figure}



\section{Approach}


\subsection{System overview}

The block diagram in Figure 4 shows the major components of our system, 
which transforms the NM text (source language) to another Mongolian texts 
(target language).

While there are many blocks in the figure, the main process of our system 
uses three main steps as follows:

\begin{itemize}
\item
step-1: The entries in NM (\textit{e.g}, \textit{Mongol}, \textit{Yaponoor} and \textit{bolj} in Figure 4) are searched with 
Dict.MED, and if they are found in Dict.MED a pair is formed (Dawa and 
Isahara 2006). An entry, for example, ``\textit{Mongol}'' was transcribed to ``\textit{mongol}'' in the 
target language.

\item
step-2: If there is no such entry in Dict.MED, the entry is checked to 
determine whether or not it is an item appended with a suffix according to 
the LRB. If so, the entry is segmented into two parts of root and suffix. 
The root is then checked whether it is a long vowel syllable by referring to 
the LRB again. Finally, DP matching is performed (described in 3.2) between 
the root and the entries in Dict.MED. If a better match is found, the 
corresponding pairs of both root and suffix from Dict.MED are produced. An 
example ``\textit{Yaponoor}'' is turned into ``\textit{yapon}'' and ``\textit{oor}'' and to ``\textit{yapon yer}'' shown in Figure 4.

\begin{figure}[t]
\centerline{\includegraphics{15-5ia1f4.eps}}
\caption{Block diagram of our system}
\vspace{-1\baselineskip}
\end{figure}

\item
step-3: If step-2 fails, the entry is first converted by a character unit 
by referring to a Bi-lingual phoneme set. DP matching is then conducted 
between the converted entries to the TLC. The closest possible match is 
produced. Figure 5 shows some examples for test string ``\textit{bolj}''. There were four 
candidates such as tests (t$_{1}$), (t$_{2}$), (t$_{3}$) and (t$_{4}$) in 
Figure 5, but, the best performance was (t$_{2}$) and (t$_{3})$ because the 
minimized overall distance was $\min (n,m)=0.111$.
\end{itemize}

\begin{figure}[t]
\centerline{\includegraphics{15-5ia1f5.eps}}
\caption{example by DP matching for test string ``\textit{bolj}''}
\end{figure}



\subsection{Dynamic programming (DP) matching}

DP, also known as dynamic time warping (DTW), was introduced for non-linear 
time alignment of two continuing patterns (Sakae and Chiba 1971). DP can 
effectively minimize errors that occur during the time alignment of the two 
patterns. Compared with conventional methods of matching two strings such as 
edit distance and longest common subsequence, DP is more effective because 
in DP, a character can correspond to more than one character during the 
matching. Hence we use DP to find similar strings for Mongolian but we 
compare other approaches with DP in the later section.

Consider two strings A and B with arbitrary length, say, $n$ and $m$ respectively 
in equation (1).
\begin{equation}
 \begin{cases}
	A=a_{1}, a_{2}, … a_{n}\\
	B=b_{1}, b_{2}, … b_{m}
\end{cases}
\end{equation}
Taking distance $d_{n}(i,j)$ between the characters, we initialize them as follows:
\begin{gather}
 d_{n}=
  \begin{cases}
	1 & \text{if}\  a_{i}=b_{j} \\
	0 & \text{Otherwise}
  \end{cases}, \\
 g_{n}[0][0]=
  \begin{cases}
	0 & \text{if} \ A[0]=B[0] \\
	1 & Otherwise
  \end{cases}\\
 \begin{aligned}[t]
	{} & \text{For} \  j=1 \ \text{to} |B|, g_{n}[0][j]=g_{n}[0][j-1]+d_{n}[0][j]  \\
	{} & \text{For} \  i=1 \ \text{to} |A|, g_{n}[i][0]=g_{n}[i-1][0]+d_{n}[i][0]
 \end{aligned}
\end{gather}
Then, the matching between strings A and B is regarded as a temporal 
alignment in a two-dimensional plane (see Figure 6). Suppose that the 
sequence of matched pairs $c_{k}(i_{k},j_{k})$ of A and B forms a time warping function F expressed as,
\begin{figure}[t]
\centerline{\includegraphics{15-5ia1f6.eps}}
\caption{an example of DP matching path}
\vspace{-1\baselineskip}
\end{figure}
\begin{equation}
 F=c_{1}, c_{2}, \cdots, c_{k}, \cdots, c_{K}. 
\end{equation}
Let $g_{k}(c_{k})$ denote the minimized overall distance representing the explicitly 
accumulated distance from $c_{1}(1,1)$ to $c_{k}(i,j)$. 
Then, $g_{n}(c_{k})=g_{n}(i,j)$ can be expressed by equation (6) when the initializations are given by 
equations (3) and (4) (here, $n$ is the size of the dictionary).
\begin{equation}
 g_{n}(i,j)= \min
  \begin{cases}
	g_{n}(i,j-1)+d_{n}(i,j) \\
	g_{n}(i-1,j-1)+2 \times d_{n}(i,j) \\
	g_{n}(i-1,j)+d_{n}(i,j)
  \end{cases}
\end{equation}
Now, if, for example, there are $q$ candidate words to be select and the distance is given 
by $D^{q}_{\min} (A,B)=1/(n+m)g_{q}(j-1, m-1)$, 
then the word will finally be selected by equation (7).
\begin{equation}
 D_{x}=\min{D^{q}_{\min}s(A,B)}
\end{equation}
Note that, the implementation of equation (6) runs in O($n,m)$ time.



\section{Experiments and results}


\subsection{Data}

Text segmentation of NM and transformation from NM to TM and Todo was 
performed using the data sets shown in Table 1.
\pagebreak

\begin{table}[b]
\caption{Test data set}
\begin{center}
\input{01table01.txt}
\end{center}
\end{table}
\begin{figure}[b]
\centerline{\includegraphics{15-5ia1f7.eps}}
\caption{Cyrillic ``H'' to ``$n$'' or ``\textit{ng}'' of TM and Todo}
\label{fig17}
\end{figure}


For the goal, we first created a large vocabulary dictionary (called 
Dict{\_}MED) with the size of 100 million words consisting of parallel 
entries from multiple languages and manually input POS tags (Dawa and 
Nakamura 2006a). Second, a linguistic rule bank (LRB) which was defined by a 
human expert was created. Examples of LRB are shown in the appendix List. 
(Dawa and Isahara 2006).


\subsection{Pre-processing of data}

(1) The texts of Mongolian were first converted into Latin text using a 
global alphabet set (Dawa et al. 2006b).

(2) In many cases, the first character of an NM string was written in 
uppercase. Since there is no distinction between uppercase or lowercase 
characters in TM and Todo, the initial capital was replaced by a lowercase 
character.

(3) One Cyrillic script ``H'' in NM usually corresponds to two phonemes 
(``$n$'' or ``$ng$'') in TM and Todo shown in Figure 7, and these are encoded 
differently in graphics and Unicode.


\subsection{Converting the Cyrillic ``H'' into ``$n$''or ``$ng$''}

Statistics using a TM lexicon of 50,000 words show that words including the 
phoneme ``$n$'' and ``\textit{ng}'' is 16 percent of lexicon. Therefore, to get the 
correct word in TM or Todo, it is necessary to first decide whether to 
convert the ``H'' scripts to an ``$n$'' or an ``\textit{ng}'' phoneme.

First, the maximum likelihood of phoneme $ph^{k}$ following $ph^{j}$ is investigated using an amount of words from Mongolian Todo by equation (8) 
in case of phoneme \textit{ng} or $n$ followed by vowels, and followed by consonants. The 
results are given in Figure 8 and Figure 9.
\begin{equation}
 P(ph^{k}|ph^{j})=\frac{C(ph^{j},ph^{k})}{C(ph^{j})}
\end{equation}
According to this statistics and LRB, we defined a rule changing Cyrillic 
``H'' to \textit{ng} or $n$ of TM or Todo as shown in Table 2.

\begin{figure}[b]
\centerline{\includegraphics{15-5ia1f8.eps}}
\caption{Distributions of vowels followed by (\textit{ng or n})}
\label{fig21}
\end{figure}
\begin{figure}[b]
\centerline{\includegraphics{15-5ia1f9.eps}}
\caption{Distributions of (\textit{ng or n}.) followed by consonants}
\label{fig22}
\end{figure}


Then, as in test, a set of 18,498 words including words containing the 
``$n$'' or ``$ng$'' phonemes only were extracted from Todo word set (seen in Table 
1). Test process is as follows:

1) the phoneme ``$n$'' or ``$ng$'' of Todo word set is replaced by a mark ``H'', 
and created a new word list.

2) the symbol ``H'' in words in the new word list is changed into phoneme 
``$n$'' or ``$ng$'' according to Table 2. Results transcribed by the general 
linguistics rule and Table 2 using equation (9) are shown in Table 3 and 
Table 4 respectively.
\begin{equation}
 Acc=\frac{C}{C+R} \times 100[\%]
\end{equation}
Where, C is the number of correctly transcribed words, and R is the number 
of words incorrectly replaced by (``$n$'' or ``$ng$''). Results show that the 
proposed method summarized in Table 4 is more effective than linguistic 
rules (Galasan 2001) summarized in Table 3.

\begin{table}[b]
\caption{Changing Cyrillic ``H'' to $n$ or \textit{ng} of TM (Todo)}
\begin{center}
\input{01table02.txt}
\end{center}
\end{table}
\begin{table}[b]
\begin{minipage}[t]{0.5\textwidth}
\setlength{\captionwidth}{\linewidth}
\hangcaption{``H'' to ($n$ or \textit{ng}) changing by linguistics rule}
\begin{center}
\input{01table03.txt}
\end{center}
\end{minipage}
\hfill
\begin{minipage}[t]{0.45\textwidth}
\caption{``H'' to $n$ or \textit{ng} changing by Table 2}
\begin{center}
\input{01table04.txt}
\end{center}
\end{minipage}
\end{table}
\begin{table}[b]
\caption{Common suffixes in NM text}
\begin{center}
\includegraphics{15-5ia1t5.eps}
\end{center}
\end{table}



\subsection{Inflection suffix investigation}

With regards to NM, an inflection suffix segmented from or added to the root 
will in general have different forms according to the root's gender 
(positive, negative, or neutral)\footnote{positive (a, o, u), 
negative (e, \"{u}, \"{o}, and neutral (i).} (Erden 1991). A statistical 
result of sets of inflection suffixes with 50,000 words is shown on Table 5.


\subsection{Transformation of NM text}

The transformation tests from NM to TM and Todo are made according to the 
following three steps:

\begin{itemize}
\item
step-(1) segmentation and transformation of NM sequence

We used a NM textbook of 5,000 sentences which paralleled sentences from NM 
to TM (we added Todo references) for Test (Tserenpil and Kullmann 2005).

First, the system picks out a number of sequences, which may be appended 
suffixes in NM, and they were segmented based on LRB. The results were 
matched by TM and Todo references using the textbook. In this test, the 
accuracy checked manually was 32.1{\%}. Second, the strings were matched 
with Dict.MED. Finally, the closest possible matches between the root and 
suffix were produced. Thus, we were able to calculate the F-measure 
expressed by equation (10) automatically. In this test, the calculated 
F-measure was improved to 77.1{\%} when only Dict.MED matching was used, and 
it was further improved to 88.7{\%} by combining Dict.MED and DP matching 
(see Table 6).
\begin{gather}
 \begin{split}
  P(\mathrm{precision}) & =\frac{\# \text{of words cheked manually}}{\# \text{of out words by proposed method}} \\
  R(\mathrm{recall}) & =\frac{\# \text{of words cheked manually}}{\# \text{of all entries}}
 \end{split}\nonumber\\
 F=2 \times P \times R/(P+R) \times 100[\%]
\end{gather}

\begin{table}[b]
\caption{Test results of step-(1) whh}
\begin{center}
\input{01table06.txt}
\end{center}
\end{table}


\item
step-(2) Changing of NM long-vowels to that of TM or Todo

First, the system picks out several entries that may be the long vowel 
syllable entries in NM text referring to LRB, and then changed them to TM 
(Todo) strings. For example, a long---vowel entry 
``\textit{\underline{uu}}$l$'' in NM is turned 
into ``\textit{\underline{\mbox{agu}}}\textit{la}'' of TM. 
The accuracy was then checked referring to pairs of TM and Todo. In this 
case, the accuracy is 37.6{\%} (see Table 7). Second, the selected out 
entries were changed again using both Dict.MED and LRB. In this case, the 
calculated F-measure was improved to 88.4{\%} for TM, and it was further 
improved to 91.9{\%} by combining Dict.MED with TLC matching, as shown in 
Table 7. It was further improved to 89.4{\%} and 94.3{\%} in the case of 
Todo.


\item
step-(3) Conversion of character unit

\begin{table}[t]
\caption{Test results for step-(2)}
\begin{center}
\input{01table07.txt}
\end{center}
\end{table}

Otherwise, except step-(1) and step-(2), the entries were converted by using 
a character unit referring to the Bi-lingual phoneme list, then, The closest 
possible match between the converted one and target language word is 
produced.

We conformed that the proposed method improved Ishikawa's methods (82{\%} 
and 84{\%}). We found the F-measure values were better for Todo than for TM. 
The reason is presumed to be that the NM word grammar structure is similar 
to that of Todo.
\end{itemize}



\section{Evaluation}

We used DP for the transformation in the previous experiments. There are 
some other conventional methods for matching of two strings with arbitrary 
length, such as edit distance (ED) and longest common subsequence (LCS). 
This section evaluates these approaches' effect by comparing with the 
results from human producing.


\subsection{Edit distance (ED) criterion}

ED is a criterion based on the concept of edits distance (Levenshtein 1966). 
It gives an indication of how `close' or `similar' two strings are. Suppose 
two strings A and B are given in equation (1), and the edit distance is d(A, 
B). Then, the normalized edit distance is defined as follows:
\begin{equation}
 Ed(A,B)=1-2 \times \frac{d(A,B)}{n+m}
\end{equation}


\subsection{Longest common subsequence (LCS) criterion}

LCS is useful for searching for the \textit{longest common subsequence} LCS (A,B) when matching the free 
lengths of two strings A and B when LCS (A,B) was computed (Landau et al. 
2004). Let $g(i,j)$ denote the LCS value when the sub-rows are given by 
$A(1:i)$, $B(1:j)$. Then, the algorithm is expressed as
\pagebreak
\begin{equation}
 g(i,j)=
 \begin{cases}
	g(i-1,j-1)+1 & a_{i}=b_{j} \\
	\mathrm{max}(g(i-1,j), g(i,j-1)) & a_{i} \neq b_{j}
 \end{cases}
\end{equation}

Initialization, $g(i,0)=0$, $g(0,j)=0$.

Note that, the point is that LCS's implementation is completed in two steps. 
First, LCS(A,B) of two sequences A and B are found using equation (12) and 
(2). In other words, DP was calculated once for LCS(A,B), so LCS(A,B)'s 
implementation is completed in O($n,m)$ time (DP time). Second, LCS(cA,B) is 
calculated for arbitrary character $c$ at O(L) time when LCS(A,B) is found.

Here, $\mathrm{L}=\mathrm{LCS(A,B)}{\_}=O(n,m) \le \min(n,m)$. Consequently, LCS's implementation 
runs in $\mathrm{O}(n,m)+\mathrm{O(L)}$ time (Yuske Ishida \textit{et al}. 2005).

The cosine distance measure, which calculates the similarity between two 
vectors X and Y as defined in equation (13) and (14) when the data size 
(number of correctly transformed data) is $r$, is used in the evaluation (Wood and Fletcher 1986).
\begin{gather}
 X= \begin{bmatrix}
	x_{1} \\ x_{2} \\ \cdots \\ 2x_{r}
  \end{bmatrix}
 Y_{k}= \begin{bmatrix}
	y_{1} \\ y_{2} \\ \cdots \\ 2y_{r}
  \end{bmatrix}, \\
 sim(X,Y)= \cos \theta =\frac{Cov(X,Y)}{\sqrt{1D(X)} \sqrt{1D(Y)}}
\end{gather}
Where Cov(X,Y) is the covariance defined by
\[
 \mathrm{Cov}(X,Y)=E\{[X-E(X)][Y-E(Y)]\}
\]
and $\sqrt{D(X)}$, $\sqrt{D(Y)}$ are sample standard deviations of variables X and Y, and E is the sample 
mean of variable X(Y).

5 task data (website text, journal, publication, newspaper and textbook) 
were used for the evaluation test. For a larger value of \textit{sim}, the measured 
value obtained for each data task and each criterion (ED, LCS, and DP) is 
closer to that obtained manually. From Figure 10 in this case the manual 
check is a baseline method, we can see that the performances obtained from 
using ED, LCS, and DP correlate well with the human (manual) evaluation 
results. Moreover, the DP method is better than the ED method. The DP method 
provides similar results to the LCS method, but the LCS method has a larger 
computational cost as described in subsection 5.2 above. Figure 11 and 
Figure 12 illustrate the demonstrations of transforming NM text to Todo by 
character unit and by proposed method respectively.

\begin{figure}[t]
\centerline{\includegraphics{15-5ia1f10.eps}}
\caption{Comparison of human and by different criteria}
\label{fig37}
\end{figure}
\begin{figure}[t]
\centerline{\includegraphics{15-5ia1f11.eps}}
\caption{Demonstration of NM text to Todo by character unit only}
\label{fig38}
\vspace{-1\baselineskip}
\end{figure}

As seen from Figure 11, many non-orthographic words were occurred in the 
case of one to one character unit conversion, and these were improved 
efficiently when using the proposed method as shown in Figure 12.
\pagebreak

In Figure 12, there were still a few words produced incorrectly because the 
entry was not found in the target language corpus and the system gave a 
result from the character unit conversion, like the word $\Rightarrow$, 
but this doesn't become a detriment to 
comprehension.

\begin{figure}[t]
\centerline{\includegraphics{15-5ia1f12.eps}}
\caption{Demonstration of NM text to Todo by proposed method}
\label{fig39}
\end{figure}



\section{Conclusion}

In this paper, we discussed a conversion method for variations of Mongolian. 
Our system was implemented based on DP matching, synthesized LRB, and a 
multilingual corpus. For the segmentation and conversion of NM text to TM 
and Todo, we obtained mean F-measures of 91.9{\%} and 94.3{\%} respectively.

We compared the results of applied DP and conventional methods such as ED 
and LCS respectively with the results of the transformation directly, and 
found that proposed method gave closer and more accurate results.

The Mongolian languages have complex variations in terms of scripts and 
dialects. There are many problems relating to standardization, conversion, 
and natural language processing which must be overcome.

We are working continually to challenge two - way transformation and for 
improving our system accuracy.


\acknowledgment

We would like to sincerely thank anyone who cooperated during the collection 
of data and supported by software, especially the Mongolian Information 
Technology and MENKLsoft Co. Ltd, Inner Mongolia in China. We appreciate the 
help, valuable comments and helpful advice of the members of the spoken 
language communication group at ATR and computational linguistics group at 
NICT.

\begin{thebibliography}{}

\item
Dawa, I. and Isahara, H. (2006). ``Analysis of Mongolian texts.'' 
\textit{Proceeding of the twelfth annual meeting}.

\item
Dawa, I. and Nakamura, S. (2006a).`` Multilingual Text-Speech Corpus Of 
Mongolian.'' \textit{Chinese Spoken Language Processing}, pp. 759-770.

\item
Dawa, I. et al. (2006b). ``Processing of Mongolian by Computer.'' \textit{Journal of Chinese Information Processing}, 
\textbf{20}(4), pp. 56-62.

\item
Ehara, T. et al. (2007). ``Mongolian to Japanese Machine Translation system 
Focused on Translation Selection.'' \textit{Second International Symposium on Information and Language Processing}, August 24-25, Urumqi, Xinjiang 
University pp. 27-33, of the ANLP, 2006. 3, Japan, pp. 588-591.

\item
Erden, B. (1991). ``The grammar of the traditional Mongolian.'' Inner 
Mongolian educational publisher, China.

\item
Galasan, P. (2001). ``Cyrillic Grammar of Mongolian.'' Inner Mongolian 
publishing house, Hohhot China.

\item
Ishikawa, T. et al. (2005). ``A Bidirectional Translation Method for the 
Traditional and Modern.

\item
Landau, G. M. et al. (2004). ``Two algorithms for LCS consecutive suffix 
alignment.'' In Proc. 15th Ann. Simp. On combinatorial Pattern Matching, 
LNCS 3109, pp. 173-193.

\item
Levenshtein, V. (1966). ``Binary codes capable of correcting deletions, 
insertions, and reversals.'' \textit{Sovie Physis-Doklady} 10, \textbf{10} (1966), pp. 707-710.

\item
Mongolian Scripts''. \textit{Proceeding of the Eleventh Annual Meeting of The Association for Natural Language Processing}, pp. 360-363, Japan.

\item
Namsurai, Y. et al. (2006). ``The database Structure for BI-Directional 
Textual Transformation Between Two Mongolian Scripts'', ICEIC2006, June 
27-28, Ulaanbaatar, Mongolia, pp. 265-268.

\item
Sakae, H. and Chiba, S. (1971). ``Recognition of Continuously Spoken Word 
Based on Time-Normalization by Dynamic Programming.'' J. ASJ, 27, 9, pp. 
483-490.

\item
Ts. Shagdarsun (1992). ``Mongolyn utga soyolyn tovchoo.'' 1992 Ulaanbaatar.

\item
田中克彦 (2002)，``法廷にたつ言語（言葉を失った民族は行き残るのか）''．岩波書店，東京．

\item
Tserenpil, D. and Kullmann, R. (2005). ``Mongolian Grammar.'' School of 
Mongolian Language and Culture, National University of Mongolia and 
Institute of Language and Literature, \textit{Academy of Science}, Mongolia, Ulaanbaatar.

\item
Wood, A. and Fletcher, P. (1986). ``Statistics in Language Studies.'' 
Cambridge Textbooks in Linguistics, Cambridge University Press.

\item
Yusuke Ishida, et al. (2005). ``Incremental Longest Common Subsequence 
Problem.'',JSIAM2005.

\end{thebibliography}



\clearpage
{\small\textbf{List} Linguistic Rule bank of Mongolian suffix to others}
\vspace{1ex}

\centerline{\includegraphics{15-5ia1f13.eps}}
\label{fig40}


\clearpage
\begin{biography}

\bioauthor[:]{Idomucogiin Dawa}{
Idomucogiin Dawa graduated from department of electrical engineering, Xinjiang university in China. He received the ME., and Infor. Ph.D degrees in Waseda University, Japan, in 1998 and 2001 respectively. He worked as a full-time lecturer in the department of engineering and science of Waseda university from 2001 to 2004.  He is a senior researcher of the national institute of information and communications technology, Kyoto, Japan from 2004.  His research interests included natural language processing, machine translation and spoken language processing.  He is a member of the acoustical society, the information processing society of Japan, and oriental-cocosda workshop.
}

\bioauthor[:]{Satoshi Nakamura}{
Satoshi Nakamura is ATR fellow and the director of ATR Spoken Language Communication Research Laboratories. He is also Executive Researcher and the project leader of MASTAR Project, National Institute of Information and Communications Technology. He also serves as an honorary professor of University Karlsruhe, Germany since 2004. He received Awaya Award from the Acoustical Society of Japan, the Interaction Best Paper Award, Yamashita Research Award from the Information Processing Society of Japan, Telecom System Award, Docomo Mobile Science Award, AAMT Nagao Award, IPSJ Industrial Achievement Award, and ASJ Award for Distinguished Achievements in Acoustics.
}
\end{biography}

\biodate


\end{document}

