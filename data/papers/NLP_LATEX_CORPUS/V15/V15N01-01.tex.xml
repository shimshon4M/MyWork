<?xml version="1.0" ?>
<root>
  <section title="Introduction">Questionclassificationisthetaskofidentifyingthetypeofagivenquestionamongapredefinedsetofquestiontypes.Thetypeofaquestioncanbeusedasacluetonarrowdownthesearchspacetoextracttheanswer,andusedforquerygenerationinaquestion-answering(QA)system.Therefore,ithasasignificantimpactontheoverallperformanceofQAsystems.Therehavebeenseveralstudiestosolvethisproblemfocusingonsupervisedlearning~.However,thecostofmakinglabeled(training)dataishigh,andalargetrainingdatasetisneededtomakesignificantimpactontheperformance.Alsotheabovemethodsdonotuseunlabeledquestions,whicharereadilyavailabletoimprovetheperformanceofclassification.Inordertoutilizebothlabeledandunlabeleddata,weproposetousesemi-supervisedlearning.Forthesemi-supervisedlearningalgorithm,weadoptedtheTri-training~,sinceithasasimplebutefficientmethodofdecidinghowtolabelanunlabeledinstance~.Tri-trainingusesthreeclassifiersofthesamealgorithm,andifanytwoclassifiersofthethreeclassifierspredictthesamelabelforanunlabeledinstance,whiletheconfidenceofthelabellingoftheclassifiersarenotneededtobeexplicitlymeasured,thenthatinstanceisusedforfurthertrainingtheotherclassifier.SuchsimplicitygivesTri-trainingadvantagesoverotherCo-trainingalgorithms,suchastheCo-trainingalgorithmpresentedby,whichfrequentlyuses10-foldcrossvalidationonthelabeledsettodeterminehowtolabeltheunlabeledinstancesandhowtoproducethefinalhypothesis.Iftheoriginallabeledsetisrathersmall,crossvalidationwillgivehighvarianceandisnotusefulformodelselection.ThesimplicityalsomakesTri-trainingfasterthanthealgorithmofGoldman,inwhichthefrequentuseofcrossvalidationmakesthelearningprocesstime-consuming.Atthebeginning,Tri-trainingbootstrap-samplesthelabeleddatatogeneratedifferenttrainingsetsforthreeclassifiersinordertomakethethreeclassifiersdiverseenoughsothattheTri-trainingalgorithmdoesnotdegenerateintoself-training~withasingleclassifier.However,questiondataissparseandimbalanced.Aquestionclassmayincludeonlyafewquestionsinacorpus,soifthebootstrap-samplingprocedureduplicatessomequestionswhileomittingsomequestionsintheclasseswithfewquestions,thenclassifiersbeingtrainedonthesebootstrap-sampledsetshavehighererrorratesthanthoseofclassifiersbeingtrainedonthelabeledset.Inordertoavoidthisdrawback,whilestillkeepingclassifiersdiverse,weproposetousemorethanoneclassifierwithdifferentalgorithms.Theoriginaltrainingsetisinitiallyusedbythethreeclassifierswithoutbootstrap-sampling.Anotherproposalistoapplymorethanoneviews(featurespaces)inthelearningprocess.Thisallowsthethreeclassifierstoinitiallybetrainedfromthelabeledsetwithdifferentfeaturespacesandstillhavediversity.Inthesecondproposal,forthesakeofsimplicity,intheexperiments,weusedtwodifferentclassificationalgorithms:SupportVectorMachines~andMaximumEntropyModels~incombinationwithtwoviews:bag-of-wordandbag-of-pos&amp;wordfeatures.Twoclassifierswhichusethefirstalgorithmareassigneddifferentviews,i.e.,thefirstclassifiergetsbag-of-wordandtheothergetsbag-of-pos&amp;wordfeatures.Thethirdclassifierusesthesecondalgorithmwithbag-of-wordfeatures.Withthisstrategy,threeclassifiershaveinitiallydifferenthypotheses.Ourexperimentsshowpromisingresults.Theremainderofthepaperisorganizedasfollows:Section2givessummariesofrelatedwork;Section3givesdetailsabouttheTri-trainingalgorithmandourmodifications.Section4describesdatasetsandfeatureselection.TheexperimentalresultsaregiveninSection5andconclusionsaregiveninSection6.</section>
  <section title="Related work">Therearetwobroadclassesofapproachestoquestionclassification:rule-basedandstatistical.Inrule-basedapproaches,anexpertmanuallyconstructsanumberofregularexpressionsandkeywordscorrespondingtoeachtypeofquestion.Meanwhile,instatisticalapproaches,amodelisassumedandtrainedonasufficientlylargesetoflabelledquestionsinordertoautomaticallyfindoutusefulpatternsforclassification.Statisticalapproachhaveadvantagesoverrule-basedapproach,becausetheyrequirelessexpertlaborandareeasilyportabletootherdomains.Thus,recentworkhasconcentratedontheapproach,especiallyonthesupervisedlearningapproachwhichisabranchofthestatisticalapproach.andexploreddifferenttypesoffeaturesforimprovingtheclassificationaccuracy.ZhangandLeeconsideredbag-of-word,bag-of-ngram(allcontinuouswordsequencesinaquestion)features.Especially,theyproposedakernelfunctioncalledtreekerneltoenablesupportvectormachine(SVM)totakeadvantageofthesyntacticstructuresofquestions.LiandRothfocusedonseveralfeatures:words,postags,chunks(nonoverlappingphrases),namedentities,headchunks(e.g.,thefirstnounchunkinaquestion)andsemanticallyrelatedwords(wordsthatoftenoccurinaspecificquestiontype).Theyalsousedhierarchicalclassifiers,inwhichaquestionisclassifiedbytwoclassifiers:thefirstoneclassifiesitintoacoarsecategory;theseconddeterminesthefinecategoryfromtheresultproducedbythefirstclassifier.employederrorcorrectingcodesincombinationwithsupportvectormachinetoimprovetheresultsofclassification.</section>
  <section title="Tri-training semi-supervised learning and its modifications">Inthissection,wedescribetheoriginalTri-trainingalgorithmandgivetwoproposalstoimproveit.</section>
  <subsection title="Semi-supervised Tri-training algorithm">IntheTri-trainingalgorithm~,threeclassifiers:h_1,h_2andh_3areinitiallytrainedfromasetbybootstrap-samplingthelabeledsetL.Foranyclassifier,anunlabeledinstancecanbelabeledaslongastheothertwoclassifierspredictthesamelabel.Forexample,ifh_1andh_2agreeonthelabellingofaninstancexintheunlabeledsetU,thenxcanbelabeledforh_3.Obviously,inthisscheme,ifthepredictionofh_1andh_2onxiscorrect,thenh_3willreceiveavalidnewinstanceforfurthertraining;otherwise,h_3willgetaninstancewithanoisylabel.Nonetheless,asclaimedin,evenintheworsecase,theincreaseintheclassificationnoiseratecanbecompensatedfor,ifthenumberofnewlylabeledinstancesissufficient.Alsointhealgorithm,eachclassifierisinitiallytrainedfromadatasetgeneratedbybootstrap-samplingtheoriginallabeledset,inordertomakeclassifiersdiverse.Ifalltheclassifiersareidentical,thenforanyofthreeclassifiers,theunlabeledinstanceslabeledbytheothertwoclassifierswillbethesameasthoselabeledbyitself,thus,Tri-trainingbecomesself-trainingwithasingleclassifier.Thepseudo-codeofthealgorithmisdescribedinFig.~a,whereLearnisaclassificationalgorithm;S_iisalabeledsetbootstrap-sampledfromthelabeledsetL.e'_iistheerrorrateofh_iinthe(t-1)^thround.Withtheassumptionthatthebeginningerrorrateislessthan0.5,thereforee'_iisinitiallysetto0.5;e_iistheerrorrateofh_iinthet^thround;L_iisthesetofinstancesthatarelabeledforh_iinthet^thround;l'_iisthesizeofL_iat(t-1)^thround,andinthefirstrounditisestimatedbye_ie'_i-e_i+1;Subsample(L_i,s)functionrandomlyremoves&quot;|&quot;L_i&quot;|&quot;-snumberofinstancesfromL_iinordertomakecurrentroundhavebetterperformancethanthatofthepreviousround,asprovedin;MeasureError(h_j&amp;h_k)functionattemptstoestimatetheclassificationerrorrateofthehypothesisderivedfromthecombinationofh_jandh_k.Becauseitisdifficulttoestimatetheclassificationerrorrateontheunlabeledinstances,thealgorithmonlyestimatesonthelabeledsetwiththeassumptionthatboththelabeledandunlabeledinstancesetshavethesamedistribution.Ineachiteration,L_iisnotmergedwiththeoriginallabeledsetL.ItisputintotheunlabeledsetUasunlabeledinstances.TheinterestingpointintheTri-trainingalgorithmisthat,inordertoensurethatthecurrentroundoftraininghasbetterperformancethanthatofthepreviousround,thesizeofeachnewlylabelledsetL_imustnotbegreaterthane'_il'_ie_i-1.Ifitisgreaterthanthisvalue,thefunctionSubsample(L_i,s)isusedtorandomlyremoveredundantinstances.Thethreeclassifiersarerefinedinthetrainingprocess,andthefinalhypothesisisproducedviamajorityvoting.Forthesakeofsavingspace,otherdetailscanbeseenin~.</subsection>
  <subsection title="Modified versions of Tri-training">Duetoitsnature,questiondatatypeisverysparseandimbalancedasshowninTable~.Asstatedin,textdatatype,whenrepresentedinthevectorspacemodel,isverysparse.Foreachdocument,thecorrespondingdocumentvectorcontainsonlyafewentrieswhicharenon-zero.Aquestioncontainsquiteafewwordsincomparisonwithadocument,soquestiondataisevenmoresparsethantextdata.Becauseoftheimbalance,afterbootstrap-sampling,eachnewlycreatedlabeledsetmissesanumberofquestionsascomparedtotheoriginallabeledset.Ifthemissedquestionsareinaclasswhichcontainsonlyfewquestions,thentheinitialerrorrateofeachclassifierincreaseswhenbeingtrainedfromthesedatasets.Thefinalimprovementafterlearningsometimesdoesnotcompensateforthisproblem.Inordertoavoidthisdrawback,weproposetousemorethanonealgorithmforthethreeclassifiers.Eachclassifierisinitiallytrainedonthelabeledset.Ourexperimentsshowedthat,iftheperformanceofoneofthethreeclassifiersismuchbetter(orworse)thanthatoftheothers,thefinalresultisnotimproved.Forthisreason,aconstraintonthreeclassifiersisthattheirperformancesaresimilar.ThemodifiedversionisdepictedinFig.~b,whereLearn_istandsfordifferentalgorithms.WeomitotherlinesthatareidenticaltothoseoftheoriginalalgorithminFig.~a.Anotherproposaltoavoidbootstrap-samplingistousemorethanoneviews,suchastwoorthreeviewsinthelearningprocess,sothateachclassifiercanbetrainedfromtheoriginallabeledsetwithdifferentfeaturespaceswhilestillmakingsurethattheyarediverseenough.ThemodifiedalgorithmseemstohavethestandardCo-trainingstyleintheframeworkofTri-training.ThemodifiedversionaccordingtothisproposalisgiveninFig.~c,whereview_i(L)isthei^thviewofthedatasetL.OtherlinesthatarethesameasthoseinFig.~aareignored.OneimportantaspectofTri-trainingalgorithmistheneedlessofredundantviews,soitcanbeappliedtoproblemswhichhaveonlyoneview.Inthisdomain,itiseasytogetredundantviews,thatisthereasonofthisproposal.</subsection>
  <section title="Question data sets and feature selection"/>
  <subsection title="Question data sets">TheQuestionAnsweringTrackinTextRetrievalConference(TREC)definessixquestionclasses,namely,abbreviation,description,entity,human,locationandnumeric.However,foraquestionansweringsysteminanopendomain,sixclassesarenotsufficientenough.Thelargerthenumberofquestionclasses,thebetteraQAsystemlocatesandextractsanswerstoquestions~.Hence,fromsixcoarseclassesdefinedbyTREC,proposedtodividequestionsinto50fine-grainedclasses.Wefollowthisproposaltoclassifyquestionsintothesefiner-grainedclasses.Intheexperiments,thedatasetswerethoseusedin~withthetotalofabout6000questions(theexactnumberis5952),ofwhich500questionsfromTREC10~werethetestset,and4subsetsofsize1000,2000,3000and4000werecreatedbyrandomlyselectingfromother5500questions.Thesedatasetsareallavailableonhttp://L2R.cs.uiuc.edu/cogcomp/.Weusedthe4subsetsaslabeledsets,andcreated4correspondinglyunlabeledsetsbyselectingquestionsthatdonotbelongtothelabeledsets.ThedistributionoftrainingandtestingdataisshowninTable,wherethecoarseclassesareincapitals,followedbythecorrespondingfineclasses.Aslistedinthetable,someclassesconsistoffewquestions,suchas4questionsinthecurrencyandreligionclasses.</subsection>
  <subsection title="Feature selection">Inexperiments,weusedtwoprimitivefeaturetypeswhichwereautomaticallyextractedforeachquestion,namely,bag-of-wordandbag-of-pos&amp;word.Questionclassificationisalittledifferentfromtextclassification,becauseaquestioncontainsasmallnumberofwords,whileadocumentcanhavealargenumberofwords.Intextclassification,commonwordslike`what',`is',etc.areconsideredtobe``stop-words''andomittedasadimensionreductionstepintheprocessofcreatingfeatures.Thisisanimportantstepinimprovingtheperformanceofclassificationasprovenin.However,thesewordsareveryimportantforquestionclassification.Also,wordfrequenciesplayanimportantroleindocumentclassification,whereasthosefrequenciesareusuallyequalto1inaquestion,thus,theydonotsignificantlycontributetotheclassificationprecision.Inordertokeepthesewordswhilestillreducingthedimensionspace,weusedapreprocessingstep:allverbswererestoredintotheirinfinitiveforms.Forexample,theverbforms`is',`were',`was',`are'and`am'wereconvertedto`be';pluralnounswerechangedtotheirsingularforms,suchas`children'wasconvertedto`child';wordshavingtheCD(cardinalnumber)part-of-speechweremadethesamevalue,suchas`1998',`2000',`12'werechangedinto`100'.Giventhequestion:WhowasPresidentofAfghanistanin1994?Afterthereductionstep,itbecomes:WhobePresidentofAfghanistanin100?Afterthereductionstep,thevector(orvocabulary)Vofalldistinctwordsofquestionsinthecorpuswasconstructed.LetthesizeofVbeN,theneachquestionqwasconvertedintoavector(q_1,q_2,,q_N),whereq_iis1ifthewordw_iinVappearsinq,otherwiseq_iis0.Thesevectorsofnumbersweretheinputofclassifiers.Interestingly,thisdimensionreductionstepmakesSVMreachtheprecisionof81.4%trainingon5500questions,whilethesamefeatureswithSVMusedingivestheprecisionof80.2%trainingonthesamedatasetandwiththesamelinearkernel.Forbag-of-pos&amp;wordfeatures,eachwordinaquestionwasconvertedintotheformofPOS-word,wherePOSisthepart-of-speechtagofword.Wealsousedthepreprocessingstepsimilarlytowhatappliedtotheprocesstogeneratebag-of-wordfeatures,forexample`how'wastransformedinto`WRB-how',`who'wasconvertedto`WP-who',`are',`is',`am',`were'and`was'wereconvertedto`AUX-be',etc.Giventhequestion:WhowasPresidentofAfghanistanin1994?Afterthereductionstep,itbecomes:WP-WhoAUX-beNN-presidentIN-ofNN-AfghanistanIN-inCD-100?Theprocessofconvertingquestionsintovectorsofnumberswassimilartothatofbag-of-wordfeatures.Thereisadifferencebetweenbag-of-wordandbag-of-pos&amp;wordfeatures.Aword,suchas`plan'mayplaydifferentrolesindifferentquestions.Itcanbeaverbinthisquestionwhilebeinganouninanotherone.Theroleofthewordcanbedistinguishedinbag-of-pos&amp;wordfeatures,becauseitisconvertedinto`VB-plan'(ifitisaverb)or`NN-plan'(ifitisanoun)asdepictedinFig.~.Thebag-of-wordfeaturesdonothavethisability,sothebag-of-pos&amp;wordfeaturesprovidearichersetoffeatures.Concretely,forthedatasetusedinourexperiments,thesizeofthevocabularyVforbag-of-wordandbag-of-pos&amp;wordfeaturesis7953and9876,respectively.Thus,bag-of-pos&amp;wordfeaturesmaymakeclassificationalgorithmsperformbetterthanbag-of-wordfeatures.WetestedthesupervisedlearningwithSVMalgorithmonthelabeledsetofsize4000withbag-of-wordandbag-of-pos&amp;wordfeatures.ThestatisticsisrecordedinTable~,where#Tshowsthenumberoftestquestionsbelongingtoeachquestionclass;#Wand#P,respectively,showthecorrectlypredictedquestionsofeachquestionclasswithbag-of-wordandbag-of-pos&amp;word;%Wand%P,respectively,areprecisionsofclassificationwithbag-of-wordandbag-of-pos&amp;word.ThetableshowsthatSVMfailstoclassifysomequestionclasses,suchascurrency,eventorproductwithbag-of-wordandbag-of-pos&amp;wordfeatures.SVMfailstoclassifythecurrencyclassbecauseinthelabeledsetofsize4000,thereisonlyonequestionbelongingtotheclasscurrency.AnotherpossiblereasonthatmakeSVMfailstocorrectlyclassifyotherclassesisthelackofsemanticsofbag-of-wordandbag-of-pos&amp;wordasseeninthethreequestionsfromthelabeledset:+Whatisafearofshadows?intheclassENTITY:disease.medicine.Whatistheoriginofheadlice?intheclassDESCRIPTION:description.WhatisthenicknameforthestateofMississippi?intheclassLOCATION:state.listThoughthesethreequestionsbelongtodifferentclasses,theyhaverelativelysimilarforms.Thiscausesambiguityforclassificationalgorithms.Forimprovingclassificationprecision,semanticfeaturesshouldbeadded,suchasclass-specificrelatedwordsusedin.Foreachquestionclass,class-specificrelatedwordsarealistofwordsthatfrequentlyappearinthisclass.Withthismethod,awordinaquestionmayhavebothsyntacticandsemanticroles,thusthefeatureisbetter,andtheclassificationprecisionisimproved.</subsection>
  <section title="Experiments">Thissectiongivesdetailsaboutourimplementationandevaluation.BecausethefunctionSubsample(.)(inline20ofFig.~a)usesrandomnesstoremoveredundantquestions,sothesetL_igeneratedforeachh_imaybedifferentineachrun;thefinalresultofeachrunmaybedifferent,andtheresultofthefirstrunisnotalwaysthebestone.Thus,inallexperiments,eachalgorithmwasrun4timesandthebestaswellastheaverageresultswererecorded.</section>
  <subsection title="Experiments with multiple classifiers">Inthefirstexperiment,wedevelopedourprogramsbasedontheSparseNetworkofWinnows(SNoW)learningarchitecture~,whichimplementsthreelearningalgorithms:Perceptron,BayesandWinnow.WeusedthesethreelearningalgorithmstoapplyforthethreeclassifiersoftheTri-trainingalgorithm.Besides,weimplementedtheoriginalTri-trainingalgorithmwithasingleclassificationalgorithm,suchasBayes,PerceptronorWinnow.Alltheparametersofthesealgorithms,suchasthelearningrate,thresholdandtheinitialweightofPerceptronandWinnowweredefaultvalues.Thebag-of-wordfeatureswereusedintheexperiment.Thebestandaverageprecision(of4runs)oftheexperimentislistedinTable,whereTB,TPandTWrespectivelystandfortheoriginalTri-trainingwithasingleclassificationalgorithmBayes,PerceptronandWinnow;TBPWstandsforthemodifiedTri-trainingwithBayes,PerceptronandWinnowfollowingthealgorithmdepictedinFig.~b.FortheoriginalTri-trainingwithasingleclassificationalgorithmBayes,PerceptronorWinnow,wecomparetheirprecisionwiththebaselineproducedbythecorrespondinglysupervisedlearningalgorithmbeingtrainedonthesamelabeledset.Forexample,thebaselineoftheoriginalTri-trainingwithBayesianalgorithmistheprecisionofthesupervisedlearningofBayesonthesamelabeledset.ForourmodifiedalgorithmTBPW,wecompareditsprecisionwiththebestprecisionofindividuallysupervisedlearningofthethreeclassifiers(valuesinitalic)asthebaseline.WealsocarriedoutthesigntestforourmodifiedTri-trainingalgorithm,withatotalnumberof25subsetsatthe95%significancelevel(p=0.05),inwhichthecorrespondingcriticalvalueis7.Thecolumn`N'showsthenumberoftestsonsubsetsinwhichtheprecisionofsemi-supervisedlearningislessthanthebaseline.Accordingtothesigntesttheory,atestissignificantifthevalueinthecolumn`N'islessthanorequalthecriticalvalue.Thesigntestshowsthatouralgorithmissignificantatthelevelof95%foralltests.TheresultsshowthattheprecisionofsupervisedlearningofBayes,PerceptronandWinnowisnotsensitivetothesizeoflabeledsets.Concretely,whenthesizeofthelabeledsetincreases,thecorrespondingprecisiondoesnotincrease.Maybe,questiondatatypeandbag-of-wordfeaturesarenotsuitablefortheselearningalgorithms.Inthesecondexperiment,weusedtwoalgorithms:MaximumEntropyModel(MEM),andSVMwhichhasbeenproventoperformwellfortextclassification~.TheselectionofMEMisbasedonourinvestigation.IthasbetterperformancethanBayes,WinnowandPerceptron.Inthisdomain,SVMclassifierhasbetterperformancethanthatofMEMclassifier,thus,weusedtwoSVMclassifiersandoneMEMclassifierintheimplementationwiththeexpectationofmakingtwoSVMclassifierstohavehighdegreeofdecisiononfinalhypothesis.WithSVMclassifiers,weusedlinearkernel,andotherparameters(e.g.,parameterC)weredefault.Inthisdomain,otherkernelsofSVM,suchaspolynomial,radialbasicfunctionorsigmoid,givepoorperformance.ForMEMclassifier,weusedGaussiansmoothing,andalldefaultvaluesofparameters(e.g.,L-BFGSparameterestimation).Bag-of-wordfeatureswereusedforallclassifiers.Inthisconfiguration,thetwoSVMclassifiersareidenticalatthebeginning.Inthelearningloop,becauseoftherandomness,theSubsample(.)procedure(intheline20ofthealgorithminFig.~)createsdifferentL_isetsforthetwoSVMclassifiers.Astheresults,thetwoSVMclassifiershavedifferenthypotheseswhentheyarere-trained(inline25ofthealgorithminFig.~a).Table~showsthebestandtheaverageprecision(of4runs)ofdifferentalgorithms,whereTSWandTMW,respectively,standfortheoriginalTri-trainingalgorithmwithSVMandMEMalgorithms;TSSMstandsforthemodifiedTri-trainingwithtwoSVMclassifiersandaMEMclassifierfollowingthealgorithmdescribedinFig.b.WeusedtheprecisionofsupervisedlearningwithMEMandSVMonthesamelabeledsetsasthebaselinetocomparewiththeprecisionofTMWandTSW.ForTSSM,weselectedthebestprecisionofsupervisedlearningwithMEMandSVMonthesamelabeledsets(valuesinitalic)asthebaseline.Similartoourfirstexperiment,wecarriedoutthesignteston25subsetsandatthe95%significancelevel.Thecolumn`N'recordsthenumberoftestsonsubsets,inwhichtheprecisionofsemi-supervisedislessthanthebaseline.Exceptforthetestonthelabeledsetsizeof1000whichisnotimproved,ourothertestsaresignificantatthelevelof95%.Asshowninthetable,MEMandSVMaresensitivetothesizeofthelabeledsets.Theprecisionisincreasedwhenthesizeoflabeledsetincreases.ThisindicatesthatMEMandSVMaresuitableforquestiondatawithbag-of-wordfeatures.</subsection>
  <subsection title="Experiments with two different algorithms and two views">Inthethirdexperiment,weimplementedthesecondproposalofusingmorethanoneviewsfollowingthealgorithmdescribedinFig.~c.Intheory,wecanusethreedifferentalgorithmswithdistinctviews,however,ourprimarypurposeistomakethethreeclassifiersdiverseattheinitialstep,sotwodifferentalgorithms,twoviewsandasuitableassignmentofviewstoclassifiersaresufficient.Concretely,amongthethreeclassifiers,twoofthemwereSVMclassifiersandthethirdonewasaMEMclassifier.Thefirstview(featurespace)wasbag-of-word,andthesecondviewwasbag-of-pos&amp;word.WesettwoSVMclassifierstwodifferentviews,whiletheMEMclassifierusedeitherofthem.Concretely,thefirstSVMclassifierusedbag-of-wordfeatures,thesecondSVMclassifierusedbag-of-pos&amp;wordfeaturesandtheMEMclassifierusedbag-of-wordfeatures.LetTMWandTMPbetheoriginalTri-trainingalgorithmwithMEMusingbag-of-wordandbag-of-pos&amp;wordfeatures,respectively;LetTSWandTSPrespectivelybetheoriginalTri-trainingwithSVMusingbag-of-wordandbag-of-pos&amp;wordfeatures;LetTSSM2bethemodifiedTri-trainingwithtwoSVMandaMEMclassifiersfollowingthealgorithmdescribedinFig.~cusingtwoviews:bag-of-wordandbag-of-pos&amp;word.ForTMW,TMP,TSWandTSP,thebaselineistheprecisionofsupervisedlearningwithcorrespondingalgorithmandfeaturespace.ThebestprecisionoftheexperimentisgiveninTable.Thesigntestsimilartopreviousexperimentsisalsocarriedout.Exceptforthetestwiththesizeof1000,theothertestsaresignificantatthelevelof95%.Werecordedtheaverageprecision(of4runs)ofeachalgorithmoftheexperimentinTable~.Table~recordedthenumberofnewquestions(Li)addedforeachclassifierineachiterationoftheexperimentinTable~,where`Iter.'standsforiteration.Theaveragevalues(in4tests)oftheseLiarerecordedinTable~.Intheseexperiments,TMW,TMP,TSWandTSPtooktwoiterationswhileTSSM2tookatmosttwoiterations.TheinitialclassifierswereverydifferentbecauseoftheuseoffunctionBootstrapSample(.)inLine3ofFig.~a,howeverafterhavingbeenre-trainedinLine25ofFig.~,thethreeclassifiersbecameverysimilar,andtookmanyunlabelledquestionsintheseconditeration,andstopped.</subsection>
  <subsection title="Experiments with self-training algorithm">Thissectionimplementedaself-trainingalgorithmtocomparetheresultswithourmodifiedTri-trainingalgorithm.Inself-training,asingleclassifierisusedtolabelquestionsintheunlabeledsettoaugmentthelabeledsetforfurthertraining.Thepseudo-codeoftheself-trainingalgorithmisdepictedinFig.~~,whereL,Uarethelabeledandunlabeledsets,correspondingly;isathresholdintherangeof[0,1];misthenumberofiterations(mis20inourexperiments);Learnisaclassificationalgorithm;U'isasubsetofunlabeledquestions(U');L'isasetofquestionsthatarelabeledateachiteration.Inthetrainingloop,weselectapoolU'ofunlabeledquestionssmallerthanU,assuggestedby~.Ineachiteration,asubsetU'ofunlabeledquestionsisselected,andthesetL'iscreatedbyselectingquestionsfromU'whicharepredictedbythehypothesishwithconfidence(predictionprobability)greaterthanathreshold(is0.9inourexperiments).TheunionofL'andLisusedtotraintheclassifier.NotethatL'isnotmergedwithLineachiteration.Instead,itisregardedasunlabeledquestions,andputbackintotheunlabeledsetUagain.Thetrainingprocessterminatesaftermiterations.Wecarriedoutself-trainingonlabeledsetsofdifferentsize(1000,2000,3000and4000),andtheclassificationalgorithmisSVMwithbag-of-wordfeatures.TheresultsofourexperimentsaregiveninTable,where`Base.'istheprecisionofsupervisedlearningwhichisusedasthebaseline;`Self.'istheprecisionoftheself-training.Theresultsshowthatmostfinalprecisionofself-trainingisnotimproved.ThoughonlyquestionsintheunlabeledsetU'withhighpredictionprobabilityareselectedtoformthelabeledsetL',itcannotguaranteethatthosequestionsarecorrectlypredictedasourobservation.Thus,ineachiteration,thenewlycreatedlabeledsetmaycontainmislabeledquestions,andtheerrorratemayconsequentlyincrease.Ingeneral,theself-trainingisnotwellsuitableforquestiondatatypewithbag-of-wordfeatures.</subsection>
  <subsection title="Discussion">Throughexperimentswecanseethatself-trainingisnotsuitableforsolvingthistask,becauseitsmethodtoaddunlabeledquestionsforfurthertrainingtheclassifierisnotgood.TheoriginalTri-trainingalgorithmhasabettermethodofaddingunlabeledquestionsbasedontheagreementoftwoclassifiers.However,thebootstrap-samplingstepmaydecreasetheinitialprecisionofeachclassifierandthefinalprecisionishardtobeimproved.Ourtwoproposalsremovethebootstrap-samplingwhilestillensurethethreeclassifierstohavedifferenthypotheses,andtheexperimentshaveprovedtheproposalstobesuitable.</subsection>
  <section title="Conclusion">Thispaperappliedsemi-supervisedlearningtoexploitunlabeledquestionstoimprovetheperformanceofquestionclassificationtaskandproposedtwowaysofmodifyingtheTri-trainingalgorithmpresentedby~tomakeitmoresuitableforquestiondatatype.TheproposalsdealtwithaproblemattheinitialstepofTri-training,wheretheoriginallabeledsetisbootstrap-sampledtogeneratethreedifferentlabeledsets,inordertomakethethreeclassifiershavedifferenthypotheses,whichmaymaketheinitialerrorrateofeachclassifierincrease.Withthepurposeofusingtheoriginallabeledsetforallclassifiers,whileensuringthattheyarestilldiverse,inthefirstproposal,weusedmorethanonelearningalgorithmforthethreeclassifiersandthesecondproposalistousemultiplelearningalgorithmsincombinationwithmorethanoneviews.Ourexperimentsindicatethattheperformanceisimproved.Inthecurrentimplementation,wehavenotconsideredtoselectotherbetterfeaturetypes,suchasthoseusedin.Thisisoneinterestingissuetoexploreinfuturetoachievehigherprecision.OurmodifiedversionsofTri-trainingalgorithmdonothaveanyconstraintsondatatypes,therefore,onemoreissuewhichisworthstudyinginthefutureistoapplythesealgorithmsinotherdomains,suchastextclassification.studywassupportedbyJapanAdvancedInstituteofScienceandTechnology,the21^stCenturyCOEProgram:``VerifiableandEvolvablee-Society&quot;.Wewouldliketoexpressspecialthankstotheblindreviewerofthispaperforhis/hercarefulreviewaswellascorrectionsandsuggestions.document</section>
</root>
