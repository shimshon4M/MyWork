<?xml version="1.0" ?>
<root>
  <subsubsection title="^2 statistic (CHI2)">^2statistic(CHI2)estimatesthelackofindependencebetweenclassesandfeatures,whichisequaltothesummeddifferenceofobservedandexpectedfrequencyoverthecontingencytablecells.Morespecifically,lettingF_nm(n,m0,1)bethenumberofwordpairswithf_j=nands=m,andthenumberofallpairsbeN,^2statisticisdefinedas:</subsubsection>
  <section title="Introduction">Lexicalknowledgeisoneofthemostfundamentalbutimportantresourcesfornaturallanguageprocessing.Amongvariouskindsoflexicalrelations,synonymrelationisfrequentlyusedasthebasisinabroadrangeofNLPtasksincludingqueryexpansionandindexingtechniquesforinformationretrieval,andautomaticthesaurusconstruction.However,theextraction,construction,andmaintenanceoflexicalknowledgebyhandaredifficultandcostlytasks,thusvariousmethodshavebeenproposedforsimilaritycalculationandsynonymacquisition.Mostoftheacquisitionmethodscanberegardedasthecombinationofthesetwosteps:(1)contextextraction,and(2)similaritycalculation.Inthefirststep,themethodsextractusefulcontextualinformationofwordstocharacterizethetargetwordsfromlargecorpora.Inthesecondstep,wordsimilarityiscalculatedusingtheextractedcontextsinthepreviousstep.Thekeyassumptiononacquiringwordsimilarityistheso-calleddistributionalhypothesis,whichstatesthatsemanticallysimilarwordssharesimilarcontexts.Inversely,themoresimilarthecontextsofwordsare,themorelikelythewordsaresemanticallyrelated,andsimilaritycanbeobtainedbycomparingthecontextsthetargetwordshave.Thesimilarityobtainedinthiswayiscalleddistributionalsimilarity.Awiderangeofcontextualinformation,suchasword-basedcontext,wheresurroundingwordsareusedascontext,dependency-basedcontext,wherethewordshavingsyntacticalrelationswiththetargetwordsareusedascontext,anddependencypath,wherepathsinadependencytreeareusedascontext,hasbeenutilizedfordistributionalsimilaritycalculation,achievingconsiderablesuccess.However,amajorproblemwhichariseswhenadoptingdistributionalsimilarityisthatiteasilyyieldsahugeamountofcontexttypesandco-occurrences.Thiscanleadtohighdimensionalityofsemanticspaces,oftenuptotheorderoftensorhundredsofthousands,whichmakesthecalculationcomputationallyimpractical.Thisproblemposesarealchallengeespeciallyformachinelearningtechniques.Afewresearchershavebeguntoapplysupervisedlearningmethodssuchasnaivebayesandlogisticregressiontolexicalrelationacquisition,butanexcessivelyhighdimensionalitymakesthecomputationalmostimpossibleinmanycases.However,notallofthecontextsarenecessarilyuseful,andsomeofthemareevenharmfulandactasnoise,worseningthecalculationperformance.Therefore,itisdesirabletoidentifythecontributingcontextsandtoremovethepoorly-performingones,beforethesimilaritycalculationistakenon.Thisallowstoeasetheexpensivecostandtoeliminatenoisetoimproveperformance.However,whereasafewstudieswhichtackledcontextorco-occurrenceselectionprobleminalimitedwaycanbefound,therearenoqualitativemeasuresproposedorcomprehensivestudyconductedforthisproblem.Therefore,generalmethodsbasedonquantitativemeasureswhichcanbeusedforreductionandselectionofanykindofcontextcategories,types,andco-occurrencesarestronglyrequired.Inresponsetothisproblem,inthispaperweproposethreedifferentschemesofcontextselectionfordistributionalsimilarity,namely,category-based,type-based,andco-occurrencebasedselection,andshowtheireffectivenessinselectingwell-performingcontexts,intermsoftrade-offbetweenperformanceandcomputationalcostreduction.TheoverviewofthreeselectionschemesisdescribedinFig.~.Theoverallcontextselectionprocessisasfollows:firstly,thecorpusisanalyzedandalltheco-occurrencesofwordsandcontexttypesareextracted.Secondly,theextractedcontexttypesand/orco-occurrencesareselectedbasedononeofthefollowingthreeselectionschemes:category-based,type-based,andco-occurrencebasedselection.Finally,distributionalsimilarityiscalculatedusingonlytheremainingco-occurrencesandsynonymsareacquired.Category-basedselection,thesimplestandconventionalselectionscheme,limitsthevarietyofcontextbasedonsyntacticcategory.Whiletherearevariouskindsofcontextualinformationproposedsofar,weespeciallypaidattentiontotwoclasses:word-basedcontext(wbc)anddependency-basedcontext().Forwbc,i.e.,thecontextbasedonsurroundingwords,wesetcategoriesaccordingtorelativepositionsofcontextwords.Fordbc,theadoptedcategoriesarebasedongrammaticalrelations(GRs)ofdependencyrelations.Themajordrawbacksofcategory-basedselectionarethefactthatcategoriesmustbefixedbeforehand,andcontributionsofindividualcontexttypesremainunknown.Itisdesirablethatwecandeterminetheimportanceofcontextsonatype-by-typebasis.Here,shiftingourattentionfromsynonymacquisitiontootherareas,agreatdealofstudiesonfeatureselectionhasbeenconductedintheliterature,especiallyfortextcategorizationandgeneexpressionclassification.Whereasthesemethodshavebeensuccessfulinreducingfeaturesizewhilekeepingclassificationperformance,themodelofdistributionalsimilarityisradicallydifferentfromthatofclassification,andwhetherthesamemethodsareapplicabletoandeffectiveforautomaticcontexttypeselectionforthewordsimilarityproblemsisyettobeinvestigated.Inthispaperwesolvethisproblembyintroducinganewformalizationofthedistributionalsimilarity,whereitisregardedasaclassificationproblembasedonwordpairs,notindividualwords.Thisenablestheimportancecalculationofcontexttypes,leadingtothefiner-grained,type-basedselection.Thethirdandlastselectionschemeisco-occurrencebasedselection,thegeneralizationofCurran'smethodbasedoncanonicalattributes,whichassignsanimportancescoretoeachco-occurrenceofwordsandcontexts.Althoughitisdifficulttoknowthecontributionofeachcontexttypecomparedtotype-basedselection,thisenablesthefinest-grainedselectionofcontexts.Weintroduceimportancemeasureswhichcanbeusedfortype-andco-occurrencebasedselections,andinvestigatehowwelltheyworkforremovingunwantedcontexts.Automaticsynonymacquisitionexperimentsareconductedtoevaluatetheperformanceoftheselectionschemesmentionedabove,reducingtheunimportantcontextsorco-occurrences.Weusetwoevaluationmeasures:averageprecision(AP)ofacquiredsynonymsandthecorrelationcoefficient(CC)betweenthecalculatedsimilarityandthesimilarityobtainedfromWordNet.Thethreeschemesareevaluatedandcomparedintermsofperformancegain/lossandcomputationalcostreduction.Thispaperisorganizedasfollows:inSection2wereviewpreviousstudiesconcerningthecontextualinformationselectionfordistributionalsimilarity,andclarifythepositionofthispaper.InSection3,twokindsofcontextualinformation(word-basedcontextwbcanddependency-basedcontextdbc)towhichwepaidattentioninthispaperareintroduced.Sections4and5aretheapproachandevaluationmethodforsynonymacquisition,thetaskweusetocomparetheeffectivenessofcontextselectionschemes.Theacquisitionisbasedonthevectorspacemodel,andtheevaluationusesaverageprecision(AP)andcorrelationcoefficient(CC),bothofwhicharecalculatedusingreferencesetscreatedfromafewexistingthesaurisuchasWordNet.InSections6through8,whicharethekeypartofthispaper,threeselectionschemesareintroduced.Eachsectionaccompaniesexperimentdescriptionsandresults.Finally,Section9comparesallthesethreeschemesintermsofperformance/costtrade-off,andSection10concludesthispaper.</section>
  <section title="Previous studies">Asfortheselectionofcontexts,asurprisinglysmallnumberofstudieshavepaidattentiontotheextractionandselectionofcontexts,whereasacertainnumberofvariationsofcontexthasbeenproposedandmanystudieswereconductedasmentionedpreviouslyinSection1.Earlierstudiesdealtwiththeselectionproblem,ifany,onlyinalimitedway.Forexample,Rugeproposedtheuseofdependencystructureforautomaticthesaurusconstructionandshowedtheresulttobeencouraging,butneitherthefurtherinvestigationofdependencyselectionnorthecomparisonwithotherkindofcontextualinformationisprovided.Linalsousedawiderrangeofgrammaticalrelationshipincludingmodificationsandshowedthepossibilitythatotherkindofdependencyrelationsinadditiontosubjectandobjectwascontributing,althoughitisstillnotclearwhatkindofrelationsaffecttheperformance,ortowhatextent.Therearesomestudieswhicharefirsttopayattentiontothecomparisonofvariouscontextcategoriesforsynonymacquisition.Theformercomparedseveralcontextextractorssuchaswindowextractorandshallow-anddeep-parsingextractor,whilethelattercomparedvariouskindsofsyntacticalrelationsasfiner-grainedcontextcategories.However,theyhaveconductedonlyaposterioricomparisonsbasedonperformanceevaluation,andweareafraidthatthesefindingsaresomewhatlimitedtotheirownexperimentalsettingswhichmaynotbeapplicabletocompletelynewsettings,e.g.,onewithanewsetofcontextsextractedfromdifferentsources.Asfiner-grainedcontextselectionmethod,theconceptof``frequencycutoff''usedinacertainnumbersofstudiessofar,isworthmentioning.Thissimplyremovesanywordtypes(optionally,contexttypesaswell)whosefrequenciesarelessthanafixedthreshold.Thisisoneofthesimplestwaytogreatlyreducethecomputationalcostwhilekeepingtheperformancelossatminimum,althoughithasbeenusedwithitseffectivenessunquestioned,especiallycomparedwithotherselectionmethods.Type-basedselectionschemeproposedinthispaperisageneralizationofthisselectionbyfrequencycutoff,andtheireffectivenessintermsofperformanceandcomputationalcostiscompared.CurranandMoensfocusedoneachco-occurrenceofwordsandcontexts,andsuggestedassigninganindexvectorofcanonicalattributes,i.e.,asmallnumberofrepresentativeelementsextractedfromtheoriginalvector,toeachword.Whenthecomparisonisperformed,canonicalattributesoftwotargetwordsarefirstlyconsulted,andtheoriginalvectorsarereferredtoonlyiftheattributeshaveamatchbetweenthem.Thiscanberegardedasacontextselectionmethodbecauseimportantword-contextco-occurrencesare``selected''asanapproximationoftheoriginalvector.However,itisnotclearwhethertheconditionforcanonicalattributestheyadopted,i.e.,thattheattributesmustbethemostweightedsubject,directobject,orindirectobject,isoptimalintermsofperformance.Co-occurrencebasedselectionpresentedinthisstudyisageneralizationofthismethod,andtheperformanceisevaluatedinamorecomprehensiveway.</section>
  <section title="Contextual information">Inthissection,howcontexttypesandco-occurrencesareextractedfromcorporabeforetheselectionisdescribedindetail.Weusedtwokindsofcontextualinformation,namely,word-basedcontext()anddependency-basedcontext(dbc).Weadoptedthesetwokindsofcontextualinformationbecausetheyarethemostwidelyadoptedonesfordistributionalsimilaritycalculation.</section>
  <subsection title="Word-based context">Thefirstandsimplestcontext,word-basedcontext(wbc)usesthewordssurroundingatargetwordascontexts.Tocapturethis,weconsiderawindowcenteredatthetargetword,andthetokenslocatedwithinthiswindowareextracted.Thecontextsforwbcinthispaperarerepresentedasconcatenationsofrelativepositionsandtokens.Takethefollowingsentenceforexample:Theinvestigatorswerestilllookingforwitnessesandthemotiveoftheattack.quotationTheword``investigators''haswordssuchas``the,''``were(be)'',and``still''initsneighbor,soword-basedcontextof``investigator''willbeL1:the,R1:be,R2:still,andsoon.Theconsideredwindowissymmetric,andweadoptedthemaximumwindowradiusof3inthisstudy.Wechosethissettingbecauseitiscomputationallyexpensivetoconsiderwindowswiththeradiuslargerthan3,andfewresearcheswhichsupporttheireffectivenesscanbefound.</subsection>
  <subsection title="Dependency-based context">Moresophisticatedcontext,dependency-basedcontext(dep)usesthewordswhichhavesyntacticalrelationshipwithatargetword.Sincetheextractionofaccurateandcomprehensivedependencystructureisinitselfadifficulttask,thesophisticatedparserRASPToolkitver.2(RASP2)isutilizedtoextractthiskindofwordrelations.RASP2analyzesthesentenceasshowninFig.~andoutputstheextracteddependencystructureasn-aryrelationsasfollows,whicharecalledgrammaticalrelations(GRs):(ncsubjlookinvestigator_)(ncmod_lookstill)(auxlookbe)(iobjlookfor)(dobjforand)(conjandwitness)(conjandmotive)(detmotivethe)(iobjmotiveof)(dobjofattack)(detattackthe)(detinvestigatorthe)verbatimThefirstslotsrepresenttheGRlabelsthatthewordsoftheremainingslotshave.Annotationsregardingsuffix,partofspeechtags,offsetsforindividualwordsareomittedforsimplicity.WhileoutputsofRASP2aren-aryrelationsingeneral,whatweneedhereisco-occurrencesofwordsandcontexts,soweextractthesetofco-occurrencesofstemmedwordsandcontextsbytakingoutthetargetwordfromtherelationandreplacingtheslotbyanasterisk``*'':(words)-(contexts)investigator-(ncsubjlook*_)investigator-(det*the)witness-(conjand*)motive-(conjand*)motive-(det*the)motive-(iobj*of)attack-(dobjof*)attack-(det*the)...verbatimSummingalltheseupproducestherawco-occurrencecountN(w,c)ofwordwandcontextc.</subsection>
  <section title="Approach to synonym acquisition">Thissectiondescribesthesynonymacquisitionmethod,thetaskweusedfortheevaluationofdistributionalsimilarity.Inthefollowing,thenumbersofuniquewordsandcontextsaredenotedasnandm,respectively,andletN(w,c)bethenumberofco-occurrencesofthewordwandthecontextc.Firstly,weconstructavectorw_iforeachwordw_ias:[w_i=^t[(w_i,c_1)(w_i,c_2)(w_i,c_m)],]where(w,c)istheweightfunctionassignedtoeveryco-occurrence(w,c).Weadoptedpointwisemutualinformation(PMI)astheweightfunction:[(w,c)=p(w,c)p(w)p(c),]wheretheprobabilitiesarecalculatedempirically,e.g.p(w,c)=N(w,c)/_w',c'N(w',c').Usingthisvectorw_iwecanconstructtheco-occurrencematrixofwordsandcontexts:[X=[w_1w_2w_n]]Thiscorrespondstotheterm-documentmatrixwhichisfrequentlyusedinmanyofinformationretrievaltasks.Thecolumnsofthismatrixarewordvectorsw_iandtherowscorrespondtoindividualcontexttypes.Thesimilaritybetweenwordsw_1andw_2,i.e.,sim(w_1,w_2),isthenobtainedbyJaccardcoefficientasfollows:[sim(w_1,w_2)=_cC(w_1)C(w_2)((w_1,c),(w_2,c))_cC(w_1)C(w_2)((w_1,c),(w_2,c)),]whereC(w)isthesetofcontexttypesco-occurringwiththewordw,i.e.,C(w)=c|N(w,c)&gt;0.Weconductedthisweightingandsimilaritycalculationafteranycontextselectionandpre-processingoperations.TheweightingfunctionPMIandthesimilaritymeasureJaccardcoefficientwerechosenconsideringtheresultofthepreliminaryexperiment,wherewecomparedtheperformance(averageprecisionandcorrelationcoefficient)ofseveralweightfunctionssuchastf,tf.idf,andPMI,andsimilaritymeasuressuchascosineandJaccardcoefficient,whilefixingotherexperimentalparameters.PMIandJaccardcoefficientwerealsoamongthebestperformingmeasuresdescribedin.</section>
  <section title="Evaluation of synonym acquisition">Thissectiondescribesthetwoevaluationmethodsweemployedtoevaluatetheautomaticallyacquiredsynonyms---averageprecision(AP)andcorrelationcoefficient(CC).Weintroducedthosetwomeasurestoevaluatetheeffectivenessofcontextselectionfordistributionalsimilarityfromdifferentviewpoints.APevaluatestheprecisionofacquiredsynonyms,i.e.,howaccuratelysynonymsareextracted,whileCCevaluateshowaccuratelythesimilarityisassignedtoeachwordpair.Forexample,becausetheAPvaluewillbehighaslongasthesynonymsareaccuratelyextracted,itcannotdirectlyevaluatehowsimilarityisassignedtonon-synonympairs.Inshort,APevaluatesthequalityof,andismoreimportantinsometaskssuchassynonymacquisition,whileinothertasks,suchassimilarity-basedword-countsmoothing,CCisamoredirectevaluationbecauseitevaluatesthequalityofsimilarity.Byusingthemboth,theeffectofcontextselectionmethodsdescribedinthispaperinvarioustaskscanbecomeclearer.</section>
  <subsection title="Average precision">Thefirstevaluationmeasure,averageprecision(AP),isacommonevaluationschememainlyadoptedforinformationretrieval,whichevaluateshowaccuratelythemethodunderevaluationisabletoextractsynonyms.TocalculateAP,wefirstlyprepareasetofquerywords,forwhichsynonymsareobtainedtoevaluatetheprecision.WeadoptedtheLongmanDefiningVocabulary(LDV),originallyconsistedof2,194entries,asthecandidatesetofquerywords.Secondly,foreachentryinLDV,threeexistingthesauriareconsulted:Roget'sThesaurus,CollinsCOBUILDThesaurus,andWordNet.TheunionofsynonymsobtainedwhenaLDVwordislookedupasanounisusedasthereferenceset,exceptforwordsmarkedas``idiom'',``informal'',``slang'',andphrasescomprisedoftwoormorewords.NoticethatmostoftheLDVentrieshavemorethantwosensesinWordNet,inwhichcasetheunionofsynonymsfoundinallsynsetsoftheentrywasusedasthereferenceset,sincethesynonymacquisitionmodeldescribedinthispaperisnotcapableoftreatingmultiplesensesofasingleword.TheLDVwordsforwhichnonounsynonymsarefoundinanyofthereferencethesauriareomitted.Ingeneral,theproblemofsuchpolysemiesshouldnotbetreatedlightlyinsynonymacquisitiontasks.However,incontextselectionmethodsweaddressinthispaper,webelievethattheeffectivenessofcontexttypescanstillbeuniversallydetermined,whetherornotthesynonymmodelispolysemy-aware,andtheeffectofpolysemiesislimitedintheexperimentsdescribedinthispaper.Finally,fromtheremaining771LDVwords,100querywordsarerandomlychosen,whicharelistedinAppendix.Foreachofthem,theelevenprecisionvaluesat0%,10%,...,and100%recalllevelsareaveragedtocalculatethefinalAPvalue.</subsection>
  <subsection title="Correlation coefficient">Thesecondevaluationmeasureiscorrelationcoefficient(CC)betweenthetargetsimilarityandthereferencesimilarity,i.e.,thegoldstandardofsimilarityforwordpairs.TheCCvalueiscalculatedsothatitbecomeslargerwhentheobtainedsimilaritiesaremorepreciseapproximationofthereferencesimilarities.Asthereferencesimilarity,weusedLin'ssimilaritymeasure:[sim_c(x_1,x_2)=2P(C_0)P(C_1)+P(C_2),]whereC_1andC_2areclassescorrespondingtowordsw_1andw_2,respectively.Theseclasseshavex_1C_1andx_2_2asindividualsenses,andthemostspecificclassthatsubsumesbothC_1andC_2isdenotedasC_0.Thewordsimilaritybetweenthewordw_1andw_2iscalculatedbytakingthemaximumclasssimilarityvalueofallthecombinationsofsensesas:[sim_w(w_1,w_2)=_x_1C_1,x_2C_2sim_c(x_1,x_2).]Wehavetonotethatavarietyofsimilarity/distancemeasuresareproposedtoobtainwordrelatednessbasedonWordNetstructure.However,weconductedapreliminaryexperimentandverifiedthatthedifferentvaluesofCCcalculatedusingvariouscalculationtechniquesincludingtheonementionedaboveand,allofwhichcanbecalculatedutilizingWordnet::Similaritypackage,arehighlycorrelatedandthechoicedoesnotessentiallyaffecttheoverallevaluation.ThevalueofCCiscalculatedasthecorrelationcoefficientofreferencesimilaritiesr=(r_1,r_2,,r_n)andtargetsimilaritiess=(s_1,s_2,,s_n)overthewordpairsinsamplesetP_s,whichthesetof10,000randomlycreatedpairsfromLDV.</subsection>
  <section title="Category-based selection">Inthissection,thesimplest,category-basedselection,wherecontextsareselectedbasedontheircategory,isconductedandevaluated.Wefirstsetupthecategoriesforwbcanddbc,andevaluatethesynonymacquisitionperformancewhenindividualcategoriesareusedascontext.</section>
  <subsection title="Method">Thecontextcategoryisasubsetofallthecontexttypesextractedfromthecorpus.Thedistributionalsimilaritycalculationisconductedbasedoneachextractedcategoryanditsperformanceiscompared.Thisoperationcorrespondstoselectingasetofrowsintheword-contextco-occurrencematrixXinSection4atatime,asillustratedinFig.~.Thecategorieswesetdependonthecontextualinformationinuse,whichwewilldescribeinthefollowing.Noticethatwecanapplymoresophisticatedcategoryselectionmethods,e.g.,afiner-grainedoneincorporatingricherkindsofsyntacticalinformation,butweleaveittotheothertwoselectionschemesandinthissectionweconductasomewhatnaiveselectionasageneralizationofpreviousonessuchas.</subsection>
  <subsection title="Experiment">Inthisexperiment,thecontexttypesaresortedbytheimportancescoresmentionedaboveandoneswiththelowestscoreswereremovedinorder.Westartedtheselectionwithallthe172,251typesforand83,029typesfordbc,i.e.,100%oftheoriginallycontexttypes,reducingthepercentageofremainingtypesuntilonly0.2%(344forwbcand166fordbc)wereleft.Theevaluationmeasures,APandCC,werecalculatedateachstep.Theconditionssuchasthecorpus,parser(RASP2),contextualinformation(dbc,wbc),andfrequencycut-off,areexactlythesameasthepreviousexperiment(Section6.2).</subsection>
  <subsubsection title="Corpus and analysis">Weextractedwbcanddbcfromthecorpus:the``story''-typedocumentsofNewYorkTimesarticles(1997)inEnglishGigaword,consistingofapprox.202thousanddocumentsand131millionwords.ForeachofthesecategoriesmentionedinSection6.1,synonymacquisitionwasconductedusingonlyonecategoryatatime,andtheperformancewasevaluatedandcompared.Sinceourpurposehereistheautomaticextractionofsynonymousnouns,onlythecontextsofnounsareextracted.Todistinguishnouns,usingPOStagsannotatedbyRASP2,anywordswithPOStagsAPP,ND,NN,NP,PN,PPwerelabelledasnouns.</subsubsection>
  <subsubsection title="Frequency cut-off">Wealsosetcut-offfrequencies_wand_c,i.e.,thresholdsonoccurrencefrequencytofilteroutwordsorcontextswithlowfrequency,respectively,andtoreducecomputationalcost.Morespecifically,anywordswsuchthat_cN(w,c)&lt;_wandanycontexttypescsuchthat_wN(w,c)&lt;_cwereremovedfromtheco-occurrencedata.Thefirstone,thewordcut-offfrequency_w,isrelativelyeasiertofix,becauseitonlyaffectsthenumberofuniquewordscontainedinthesemanticspace.AssumingtheZipf'slaw,thenumberoftheremainingwordtypesisinverselyproportionalto_w,whichweconfirmedthroughapreliminaryexperiment.Whenfixing_w,weconsideredthetrade-offbetweentheremainingwordtypesandthecomputationalcost,andadopted_w=20becauseitcankeepmorethanhalfoftheoriginallyextractedwordtypesforallofthecontextcategories,whilegreatlyreducingthecost.Also,thecut-offfrequency_wdoeseffectAP,butnotCC,becauseitworksasafiltertoeliminatelow-frequencywordsfromtheranking,mostofwhichareirrelevanttothequeryword.Theexperimentshowedtheperformanceofsynonymacquisitionincreasedwith_walmostmonotonically.However,sincetheincreaserangeisnotsignificant(20%atmost)andthisperformanceincreaseequallyhappenstoallthecontextcategories,theinfluencecanbeignored.Thesecondone,thecontextcut-offfrequency_c,islesseasytohandle,becauseitaffectsboththesemanticspacesizeandtheperformance(APandCC).AlthoughthenumberoftheremainingcontexttypescanbeeasilyestimatedusingZipf'slaw,similarlyto_w,wehavenootherwaystoknowtheperformancechangebuttoempiricallyinvestigateit.Althoughwedonotgointothedetailedresultshere,apreliminaryexperimentindicatedthattheperformancedecreasedalmostlinearlyv.s._c,makingitdifficulttofixaclearthreshold.Weset_c=20aswell,wheretheperformancelosscanbekeptwithin10%rangewhilethecontexttypesarecut-downbymorethan70%innumber.</subsubsection>
  <subsubsection title="Computational cost estimation">Whencalculatingthedistributionalsimilaritybetweentwowordsw_iandw_j,twocorrespondingvectorsw_iandw_jareconstructedasdescribedinSection4andalltheelementsofthosevectorsandtheoverlapbetweenthemneedstobeassessed.Thisprocesstakesatimeproportionaltothenumberofelementsinthosevectors,assumingthatthevectorsaresparselyrepresented,i.e.,onlynon-zeroelementsarestoredinthedatastructure.Thus,thecomputationalcostofacquiringsynonymsforagivenquerywordisproportionaltothenumberofthenon-zeroelementsintheword-contextco-occurrencematrix,becausethesimilarityneedstobecalculatedforallthewordsw_i,,w_N.ThisroughlycorrespondstothecostestimationofcalculatingtheevaluationmeasuresAPandCC,andmostofothersimilarity-basedtasks.Intheexperiment,wethereforecountedthenumberoftheremainingco-occurrencetypes(nottokens),i.e.,theremainingnumberofthematrix'snon-zeroelementsaftertheselectionisdone,toevaluatethecomputationalcostreducedbytheselection.Itistonotethatthenumbersofwordsand/orcontextsarealsotheimportantfactorsthataffectthecomputationalcost.However,theremayexistfrequentwordsandcontexttypes(thuslongvectors)andinfrequentones(thusshortvectors).Thismakesitdifficulttoestimatetheexactamountofcalculationinvolved,usingonlythenumbersofremainingwordsorcontexttypes.Ontheotherhand,thenumberoftheco-occurrencesusedinthedistributionalsimilaritycalculationisalmostproportionaltothecomputationalcostasexplainedabove,whichwewilluseastheroughcostestimatesinthefollowingexperiments.</subsubsection>
  <subsection title="Results"/>
  <section title="Type-based selection">Wehavejustshownthateventhesimplestcategory-basedselectionworkedwellforboostingtheperformanceaswellasreducingthecost.However,ithassomedrawbackssuchasthefactthatthecontextcategoriesmustbegivenandfixedanditdoesnotallowmuchfinerselection,e.g.,selectingeffectivecontexttypesonebyone.Inthissection,weproposethesecondselectionscheme:type-basedselection,whereindividualcontexttypesarescored,andselectedorremoved.Thisselectionschemecorrespondstoremovingindividualrowsoftheword-contextco-occurrencematrixXonebyone,asillustratedinFig.~,thusenablingfiner-grainedselectioncomparedtothecategory-basedonementionedabove.Morespecifically,type-basedselectionconsistsofthesetwosteps:firstly,toeachofcontexttypes,theimportancescoreofthecontextisassignedusingoneofthemeasuresdescribedinthefollowing.Secondly,thescoredtypesaresortedbytheimportancescoreandoneswithlowvaluesofimportancescoresareeliminated.Thecalculationofdistributionalsimilarityisperformedusingonlytheremainingrows.Tocalculatetheimportancescoresforcontexttypes,weintegratetheknowledgeandresultoffeatureselectionmethodsproposedfortextcategorizationorinformationretrieval,andadoptthefiveimportancemeasures,namely,DF,TS,MI,IG,andCHI2,describedinSection7.2.NotethatthemeasuresexceptDFwerefirstdesignedinordertoselect/removefeaturesforproblems,notfordistributionalsimilarity.Therefore,howtoapplytherestofthemeasurestothisdistributionalsimilarityproblemisabigissue,whichwedealwithinthefollowingsection.</section>
  <subsection title="Formalization of distributional similarity">Toapplythepair-basedimportancemeasures,weformalizedistributionalsimilarityasa``pairclassification''problemasdescribedbelow.Firstofall,wedealwithwordpairs,insteadofindividualwords,astheinstancesofclassification,anddefinefeaturesf_1,,f_mcorrespondingtocontexttypesc_1,,c_m.Thefeaturevalueisdefinedf_j=1ifthetwowordsofthepairhavethecontextc_jincommon,andf_j=0otherwise.Then,wedefinethetargetclasss,sothats=1whenthepairissemanticallyrelated,ands=0ifnot.Thesedefined,distributionalsimilarityisformalizedasabinary,pairclassificationproblemwhichassignsthewordpairstotheclassess0,1basedonthefeaturesf_1,,f_m.Finally,tocalculatethespecificvaluesofthefollowingcontextimportancescores,wepreparetwotestsetsofrelatedwordpairsforclasss=1andunrelatedonesforclasss=0.Thisenablesustoapplyexistingfeatureselectionmethodsdesignedforclassificationproblemstotheautomaticcontextselection.Thetwotestsets,relatedandunrelatedone,arepreparedusingthereferencesetsdescribedinSection5.Morespecifically,wecreated5,000relatedwordpairsbyextractingfromsynonympairsinthereferenceset,and5,000unrelatedonesbyfirstlycreatingrandompairsoftheLDVwords,whosedetailisdescribedlater,andthenmanuallymakingsurethatnorelatedpairsareincludedintheserandompairs.</subsection>
  <subsubsection title="Document frequency (DF)">Documentfrequency(DF),commonlyusedforweightingininformationretrieval,isthenumberofdocumentsatermco-occurswith.However,inthedistributionalsimilaritysettings,DFcorrespondstowordfrequency,i.e.,thenumberofuniquewordsthecontexttypeco-occurswith:[df(c)=|w|N(w,c)&gt;0|.]ThemotivationofadoptingDFasacontextselectioncriterionistheassumptionthatthecontextssharedbymanywordsshouldbeinformative.Itistonote,however,thisconceptthathigherDFvaluescorrespondstohigherimportanceistotallycontrarytotheunderlyingassumptionofidf(inversedocumentfrequency)thatthecontexttypeswithtoohighDF,i.e.,so-calledstopwords,arenotuseful.Ontheotherhand,thecontexttypeswithtoolowDFvaluesareundoubtedlyunimportant,becauseitisveryrarethatsuchtypesaresharedbypairsofwordsandactasusefulfeaturesforpairs.WesupposethatthisDFmeasurecanroughlycapturethistendency.</subsubsection>
  <subsubsection title="Term strength (TS)">Termstrength(TS),proposedbyWilburandSirotkinandappliedtotextcategorizationbyYangandWilbur,measureshowlikelyatermistoappearin``similardocuments,''anditisshowntoachieveasuccessfuloutcomeinreducingtheamountofvocabularyfortextretrieval.Forthedistributionalsimilaritysetting,TSisdefinedas:[ts(c)=P(cC(w_2)|cC(w_1)),]where(w_1,w_2)isarelatedwordpairandC(w)isthesetofcontexttypesco-occurringwiththewordw,i.e.,C(w)=c|N(w,c)&gt;0.Notethatthevalueofts(c)issymmetricbecauseweconsiderthesimilarityrelationtobesymmetric,i.e.,when(w_1,w_2)isasynonymouswordpair,(w_2,w_1)isalsosynonymous.Thevalueofts(c)iscalculated,lettingP_Hbeasetofrelatedwordpairs,as[ts(c)=|(w_1,w_2)P_H|cC(w_1)C(w_2)||(w_1,w_2)P_H|cC(w_1)|.]WhatmakesTSdifferentfromDFisthatitrequiresareferencesetP_Hconsistingofrelatedwordpairs.Weusedthereferencesetforclasss=1asP_HdescribedinSection7.1.</subsubsection>
  <subsubsection title="Mutual information (MI)">Mutualinformation(MI),commonlyusedforwordassociationandco-occurrenceweightinginstatisticalNLP,isthemeasureofthedegreeofdependencebetweentwoevents.ThepointwiseMIvalueoffeaturefandclasssiscalculatedas:[I(f,s)=P(f,s)P(f)P(s).]Toobtainthefinalcontextimportance,wecombinetheMIvalueoverbothoftheclassesasI_max(c_j)=_s0,1I(f_j,s).Notethat,hereweemployedthemaximumvalueofpointwiseMIvaluessinceitisclaimedtobethebestin,althoughtherecanbeothercombinationssuchasweightedaverage.</subsubsection>
  <subsubsection title="Information gain (IG)">Informationgain(IG),oftenemployedinthemachinelearningfieldasacriterionforfeatureimportance,istheamountofgainedinformationofaneventbyknowingtheoutcomeoftheotherevent,andiscalculatedastheweightedsumofthepointwiseMIvaluesoveralltheeventcombinations:[G(c_j)=_f_j0,1_s0,1P(f_j,s)P(f_j,s)P(f_j)P(s).]</subsubsection>
  <subsection title="Result">Theperformancechangewhenthecontexttypesofwbcwereselected/removedisplottedinFig.~.Theoverallobservationisthattheperformancenotonlykepttheoriginallevelbutalsoslightlyimprovedevenduringthe``aggressive''reductionwhenmorethan80%oftheoriginaltypeswereeliminatedandaround30,000contexttypeswereleft.Itwasnotuntil90%(approx.17,000remaining)eliminationthattheAPvaluesbegantofall.TheusefulnessofcontextselectionwasevenmoresignificantforCC,wherethevalueincreasedmorethan20%duringthe90%reduction.Thereasonwhythisperformanceincreasewasobservedisthat,thecontexttypestowhichlowerimportancescoreswereassignedwereactually``noise''whichmayhavehinderedtheaccuratedistributionalsimilaritycalculationandtheymighthavebeenremovedbythecontextselectiontechniqueproposedhere.Weobservedaslightdifferenceregardingwhichofthefivemeasureswereeffective.Morespecifically,TS,IGandCHI2workedwellforAP,whileDFandTSdidforCC.Onthewhole,TSwasperformingthebest,withIGandCHI2comingnext,whereastheperformanceofMIquicklyworsened.Althoughthetaskwasdifferent,thisexperimentshowedaveryconsistentresultcomparedwiththeoneofYangandPedersen's.Thismeansthatfeatureselectionmethodsarealsoeffectiveforcontextselectionindistributionalsimilarity,andourformalizationoftheproblemdescribedinSection7.1turnedouttobeappropriateforthepurpose.Inordertoclarifythetrade-offbetweenthesynonymacquisitionperformanceandcomputationalcost,were-plottedtheresultofthecurrentexperimentbytakingthenumberofco-occurrences,i.e.,thenumberofnon-zeroelementsintheword-contextco-occurrencematrix,whichremainedaftertheselectionoperation,asx-axis.TheresultisdisplayedinFig.~,anditrevealsaninterestingsurprise.ThefirstthingtonoteisthatthedeclinecurveofAPv.s.co-occurrencesismorepronounced,comparedtoFig.~,makingitalmostlinear.Thisdifferenceinshapearisesfromthefactthatmostco-occurrencesremainintactduringtheplateaushape(from172,251typestoapprox.80,000types)ofFig.~,wherecontexttypesareaggressivelyeliminatedbuttheperformancekeepsalmostunchanged.Toexemplifythis,fourofthecorrespondingpointsinFigs.andaremarkedbyletters``a'',``b'',``c'',and``d.''Thisisbecausethemeasuresintroducedabove,exceptforMI,explicitlyorimplicitlyfavorfrequentcontexttypes,thustype-basedcontextselectionusingthesefourmeasuresremovesinfrequenttypesfirst,keepingasmanyco-occurrenceaspossible.TheCCcurvealsoshowsaquitedifferentshape,wherethepeaklocatedatthefarrightofFig.~isnowshiftedtowardthecenter,meaningthatthebestCCisachievedwhenaboutahalfoftheco-occurrencesareeliminated.Ontheotherhand,MI,whichwehadbeenconsideredtobetheworst-performingimportancemeasureintermsofperformance/typetrade-off,showsquitedifferentcharacteristicsandnowittopstheotherfourmeasuresanddemonstratesthebestperformance/costtrade-offoffive,whenonly10%oftheoriginalco-occurrencesareselected.ThismakesMIthebestcandidatefortheapplicationswhichposesanextremelyhighdemandforcomputationalcost,suchasmachinelearningtechniquesandclustering.OtherfourmeasuresshowedlittledifferenceamongthembetweenFigs.and,withtheirorderslightlydifferent.Theaboveobservationindicatesthat,thebestmeasuresintermsofperformance/typestrade-off,suchasTSandCHI2,donotalwaysshowthebestperformance/costtrade-off,whichcanbeachievedbyMI.Thechoiceisuptothetasksandrequirementsinpractice---theformermeasureswouldbeappropriateforthetaskswhichpresshighdemandsforlowdimensionalityoffeaturespaces,whereasthelattermeasurecanbeagoodcandidateforthetasksdealingwithsparsematricesandvectors.HerewenoticethattheresultsareslightlydifferentforAPandCC,andwouldliketodiscussthecausebelow.Incategory-basedselection,APvaluesdeclinedgreatlyfor[0,1]and[0,2]of,andobjofdbc,whilethechangeofCCvalueswasrelativelysmall,asshowninTablesand.Furthermore,thepeaksofCCvaluesshowninFigs.andwereshiftedtowardsfewercontexttypeswhencomparedtoAPvalues.Whatthesesuggestis,inordertoaccuratelycalculateAP,i.e.,toaccuratelyacquiresynonyms,weneedmorecontexttypesbecausetheoverlapofvectorshastobepreciselyobtained,whereasinordertoaccuratelycalculateCC,i.e.,toaccuratelyassignsimilarity,wedonotneedasmanycontexttypesbecausethenon-overlappingelementsofvectorsarealsoimportant.Thisisintuitivelycorrect---mostofelementsinvectorsdonotoverlapbecausetherearesomanytypesandalotofcontexttypesshouldremaininordertoobtainsufficientoverlaps.Almostthesametrendwasobservedforbothwbcanddbc,withtheresultshowninFigs.and.Fromtheresultsdescribedinthissection,wecanconcludethistype-basedcontextselectionisgeneralenoughtobeappliedforanydistributionalsimilaritysettings.</subsection>
  <section title="Co-occurrence based selection">Wehavepreviouslyshownthattype-basedselectioncouldprovideageneral-purposeframeworkforfeaturespacereduction.However,evenwithinasinglecontexttype,theimportanceofindividual-occurrencemayvaryaccordingtothewordwithwhichthecontextco-occurs.Thusfiner-grainedselectionthantype-basedoneisappropriateinsomecases.Nowletusconsiderasituationwhereasinglecontexttypecco-occurswithaveryfrequentwordw_1andaninfrequentwordw_2.Inthiscase,theco-occurrence(w_1,c)canbeignoredwithoutworseningtheperformancebecausew_1mayhaveco-occurrenceswithothercontexttypeswhicharediscriminativeenoughtopreciselycharacterizethewordw_1.Thesamewillnotbetrueforw_2---theco-occurrence(w_2,c)isoneofthescarcefeaturesw_2has,andassuch,itshouldnotbelightlyweighted,letaloneignored.Thisleadsustothethirdandfinest-grainedcontextselectionscheme,co-occurrencebasedselection,whereindividualco-occurrences(w,c)areweighted,sorted,andremoved.Thisselectionoperationcorrespondstoremoving(i.e.,replacingwithzeros)individualelementsoftheword-contextco-occurrencematrixXonebyone,asillustratedinFig.~,thusenablingthefinest-grainedselectionofthethreeschemesproposedinthispaper.</section>
  <subsubsection title="tf.idf (term frequency, inverse document frequency) weighting (tfidf)">Asoneofthetraditionalyetcommonlyadoptedweightingmeasures,tfidfiswidelyusedforinformationretrievalsystems.Therawco-occurrencefrequency,tf,isweightedbyidf:[(w,c)=N(w,c)(c),(c)=n(c),]where(c)isthenumberofwordtypeswithwhichthecontexttypecco-occurs,i.e.,(c)=|w|N(w,c)&gt;0|.Intheexperiments,theidfvaluesarenormalizedsothatthemaximumequalsto1.</subsubsection>
  <subsubsection title="Pointwise mutual information (pmi)">Theamountofinformationthattheappearancesofwandchaveincommonisdefinedastheirpointwisemutualinformation(pmi):[(w,c)=p(w,c)p(w)p(c),]wheretheprobabilitiesarecalculatedempirically,e.g.p(w,c)=N(w,c)/_w',c'N(w',c').Notethatthispmiisalsousedtoweightco-occurrencesthroughoutthispaper.</subsubsection>
  <subsubsection title="t-statistic (ttest)">t-statistic,usuallyusedtotestwhethertheappearancesofawordwandacontexttypecarestatisticallyindependent,isusedasaweightfunction:[(w,c)=p(w,c)-p(w)p(c)p(w)p(c).]t-statisticisusuallyemployedtodiscovercollocationsfromcorpora,CurranandMoensproposedusingitasaweightfunctionandshowedthatitachievedhigherperformancecomparedtotheothers.</subsubsection>
  <section title="Comparison of three selection schemes">Finally,wecompareallthethreeselectionschemesproposedinthispaperintermsofperformanceandcomputationalcost.Theresultsofcategory-basedselection(CATEGORY),tworepresentativemeasuresfromtype-basedselection(TYPE(IG)andTYPE(MI)),andfromco-occurrencebasedselection(COOCR(tfidf)andCOOCR(ttest))areplottedinasingleplane(Fig.~).TheresultofCATEGORYisrepresentedbythelabelleddiscretepointscorrespondingtoindividualcategories.Thegraphtakesthenumberofco-occurrencesleftaftertheselectionasx-axisandtheperformanceasy-axis,thuswecancomparetheperformance/costtrade-offofthethreeschemes.Thegraphrevealsthat,onthewhole,COOCR(ttest)workswellforAPandTYPE(IG)doesforCC.Thisresultlookssomewhatcontradictory,butthedifferenceinthecharacteristicsoftheevaluationmeasuresexplainsthis---recallthattype-basedselectionhelpskeepahighCClevel,whileco-occurrencebasedselectionkeepsahighAPlevel,whichwediscussedinSections7.4and8.3.Thusselectionmethodsshouldbecarefullychosendependingonthenatureofactualtaskstowhichoneisapplyingthesemethods,consideringthecharacteristicsofAPandCCasdescribedinSection5.Consideringbothevaluationmeasures,TYPE(IG)willbethechoicefor``moderate''selectionwhichremovestheco-occurrencesuptoapprox.50%,andTYPE(MI)for``aggressive''selection,wherelessthan20%areleft.Alittlesurprisingfactisthatwecannotmisstheeffectivenessofthesimplest,category-basedcontextselection.Actually,thewindows[1,0],[1,1],[2,0],and[2,1],correspondingtofourpointsfoundatupperrightofthegraph,arealmostthebestofallthreeschemes,intermsofperformance/costtrade-off.However,otherwbccategories,e.g.,[3,1],[3,0],and[1,2],stillcannotbeatCOOCR(tfidf)orTYPE(IG),andthissuggeststhattheeffectivenessofCATEGORYisonlyobservedforratherlimitedconditionssuchas[1,0].Afterall,itonlyworkedwellfortypesofcontext(wbcanddbc)wherecategoriescanbeclearlydefined,andundernoconditionsforcomputationalcostordimensionality.Itisalsodifficultfortype-basedselectiontopredictthecomputationalcostandtheperformanceswewouldobtainaftertheselection.Thisimpliesthattype-basedandco-occurrencebasedselectionmethodsstillhavetheseadvantages.First,theyaregeneralenoughtobeappliedtocontextwherecategoriescannotbeclearlydefined.Category-basedselectionmayhavedifficultieswhenappliedtonewlyproposedtypesofcontextsuchasdependencypath,buttheothertwomethodscanbeeasilyadoptedforthem.Secondly,theyareflexibleenoughtomeettheimposedconditionsintermsofcomputationalcostanddimensionality.Insometaskssuchasmachinelearning,computationalresourcesaredemandingandsomeconditionsmightbealreadygivenbeforehandintermsofcomputationalcostanddimensionality.Type-basedandco-occurrencebasedselectioncanmeetthembyadjustingthe``aggressiveness''ofselectionflexibly.Thirdly,itiseasytopredictthecomputationalcostandtheperformanceachievedaftertype-basedorco-occurrencebasedselectionisapplied,butnotforcategory-basedselection.Tosumup,insomelimitedcasescategory-basedselectionworkswell,whileinmostcasesonecanbenefitfromthegeneralityandflexibilityoftheothertwoselectionmethods.Asimilarcomparisonisconductedfordbc,andtheresultisshowninFig.~.Thecharacteristicsofthethreeschemesarealmostthesame,andagain,therelativeeffectivenessofCATEGORYisclear,thoughnotasoutstandingaswbc.Thebestcategoryisdobj,andthisevidenceagainsupportstheuseofthecombinationofsubjectandobjectrelationsascontext,asdoneinmanypreviousworks.Lastlybutnotleastimportantly,thethreeschemesarenotmutuallyexclusivebutfreelycombinedandused.Forexample,wecanfirstlyapplycategory-basedselectionusingobj,thentype-basedselectionusingDF,andfinallytheremainingco-occurrencescanbefurtherthinnedoutusingco-occurrencebasedselectionwith.Althoughwedonotinvestigatetheimpactofthiscombination,onecanchooseanyschemesaswellasmeasures,dependingonthedemandforthesemanticspacedimensionalityandcomputationalcost.</section>
  <section title="Conclusion">Inthispaper,weproposedthreeschemesofcontextselectionfordistributionalsimilarity:category-,type-andco-occurrencebasedselection.Category-basedselectionisthesimplestselectionwherecontexttypesareselected/removedbasedonthesyntacticalcategoriesthetypesbelongto.Finer-grained,type-basedselectionenablestheselectionofcontexttypesonebyone,byindividuallyassigningthemimportancescores.Forthispurpose,wereformalizedthedistributionalsimilarityproblemasaclassificationproblem,enablingtheapplicationofexistingmeasuresforfeatureselectiontothecurrenttask.Thefinest,co-occurrencebasedselectionselectsorremovesindividualco-occurrencesofwordandcontext,assigningthemimportancescores.Intheexperiments,weinvestigatedhowwellthethreeselectionschemes,aswellasindividualimportancemeasures,workforremovingunwantedcontextsforword-basedcontextwbcanddependency-basedcontextdbc.Automaticsynonymacquisitionwasusedasatasktoevaluatetheperformance,andaverageprecision(AP)andcorrelationcoefficient(CC)wereemployedasevaluationmeasures.Inthefirstexperiment,wecomparedtheperformanceandcontextreductionamount(thenumbersofremainingcontexttypesandco-occurrence)forcategory-basedselection.Itshowedthatlimitingthecategoriesactuallyincreasedtheperformance,andappropriatechoiceofcategories,suchas[1,0]-windowforwbcandfordbc,wereindeedeffectiveforbothperformanceandcomputationalcost.Thesecondexperiment,wheretheperformancechangeontype-basedcontextselectionwasevaluated,showedthattheperformancenotonlykepttheoriginallevelbutalsoslightlyimprovedevenduringthe``aggressive''reductionwhenmorethan80%oftheoriginaltypeswereeliminated.Thisimpliestheeffectivenessofourformalizationofdistributionalsimilarityproblemforthistask.Asfortheimportancemeasures,TS,IGandCHI2wereshowingoneofthebestperformance/typetrade-off,whileMIwasthebestintermsofperformance/costtrade-off.Thethirdexperimentrevealedthatco-occurrencebasedselectiondemonstratedevenbetterperformance/costtrade-offinsomecases,especiallywhenchi2orttestwereused.Finally,wecomparedabovethreeselectionschemesandclarifiedthecharacteristicsoftheschemesandthemeasures.Italsoshowedtheeffectivenessofthesimplestcategory-basedselection,andthisresultsupportstheuseofsubjectandobjectrelationsascontext,asdoneinmanypreviousworks.Atthesametime,type-basedandco-occurrencebasedselectionmethodsalsoworkforbothwbcanddbc,showingthatthesemethodcanbegenerallyandflexiblyusedforanykindsofcontextanddimensionality/computationalcostconstraints.Inthispaper,wepresentedonlyaconsiderablysimpleclassificationmodelofdistributionalsimilaritytoassignimportancescoresfordistributionalsimilarity.However,muchmoresophisticatedandadvancedmodelscanbeconsidered,whoseeffectivenessfortype-basedcontextselectionshouldbeinvestigatedinthefuture.Finally,theselectionschemeswepresentedallassumethemutualindependenceofcontexttypesandco-occurrences,althoughthisassumptionisoftenunrealistic.Somefeatureselectionmethodsincorporatingmutualcorrelationamongfeaturesareproposedformachinelearning,andthesametechniquemaybeapplicableforcontextselectionaswell.Also,dimensionalityreductiontechniquessuchasLSA(LatentSemanticAnalysis)andPLSA(ProbabilisticLSA)havebeenproposed,andtheycanbeusedtoreducethedimensionalityoffeaturespaceaswell.Althoughtheydonotalwaysreducethecomputationalcost,weshouldcomparethesedimensionalityreductiontechniquesandcontextselectionmethodsproposedhere,ortheircombination.</section>
</root>
