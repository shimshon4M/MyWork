<?xml version="1.0" ?>
<root>
  <section title="">*Appendix:Listofthe100ChosenLDVWordsaccident,activity,amount,art,attention,bill,burst,business,carecivilization,claim,club,contract,cost,crack,creature,criminal,dealdeath,decay,difference,discovery,double,dream,drop,end,establishmentevil,explanation,eye,factory,faint,fellow,figure,flash,flood,flowerform,fortune,freedom,hair,heaven,hold,horizon,host,hurry,ill,imageimprovement,job,joke,key,limit,line,lot,mass,matter,minister,mistakemix,moral,national,need,nothing,offer,opponent,plant,pot,present,probability,proof,property,relative,report,reward,ride,rule,rush,secondspite,sport,spread,stick,sum,sweep,swell,swing,team,test,thing,threattourist,treat,tribe,universe,victory,voice,vote,wedding,winverbatimdocument</section>
  <section title="Introduction">Lexicalknowledgeisoneofthemostfundamentalbutimportantresourcesfornaturallanguageprocessing.Amongvariouskindsoflexicalrelations,synonymrelationisfrequentlyusedasthebasisinabroadrangeofapplicationssuchasqueryexpansionandindexingtechniquesforinformationretrieval,andautomaticthesaurusconstruction.Italsoplaysanimportantroleinconstructinglinguisticontologiesbecausethecoretechniquesofontologyconstruction---relationassignmentandtaxonomyconstruction---wouldbealmostimpossiblewithoutsynonymdetectiontechniques.However,theextraction,construction,andmaintenanceoflexicalknowledgebyhandaredifficultandcostlytasks,thusvariousmethodshavebeenproposedforsimilaritycalculationandsynonymacquisition.Mostoftheacquisitionmethodscanberegardedasthecombinationofallorsomeofthesethreesteps:(1)contextextraction,(2)similaritycalculation,and(3)wordclustering.Firstly,themethodsextractusefulcontextualinformationofwordsto``feature''thetargetwordsfromlargecorpora.Thecontextstobeextractedvaryfromverynaiveonessuchassurroundingwordscapturedbyawindowtosophisticatedonessuchasdependencystructure.Regardlessofcontextsinuse,thekeyassumptiononacquiringwordsimilarityistheso-calleddistributionalhypothesis,whichstatesthatsemanticallysimilarwordssharesimilarcontexts.Itmeans,inotherwords,themoresimilarthecontextsofwordsare,themorelikelythewordsaresemanticallyrelated,andsimilaritycanbeobtainedbycomparingthecontextsthetargetwordshave.Thesimilarityobtainedinthiswayiscalleddistributionalsimilarity,whichisalsoreferredassecond-orderco-occurrenceinsomecases.Inthesecondstep,similarity,whichisequivalentlyreferredassemanticrelatednessinthispaper,iscalculatedbasedonthecommonalityofwords'contexts.Varioussimilaritymeasuresandweightfunctionshavebeenproposedforthispurpose,buttheseroughlyfallintotwomainstreams:vector-andprobability-basedsimilarities.Theformerincludesthetraditionaltf.idfweightingandcosinesimilarity,andthelatterisrepresentedbymutualinformation(MI)andKullbuck-Leibler(KL)divergence.Thesesimilaritymeasuresandweightfunctionshavebeenwellcomparedanddiscussedsofar.Asthethirdstep,wordclusteringisoptionallyconductedbasedonthesimilarityordistancecalculatedintheprevioussteptocreateclustersofsemanticallyrelatedwords.Again,awidevarietyofclusteringmethodsareproposed,althoughthisissueisnotwithinthescopeofthispaper.Asshownabove,whereastherehavebeenmanymethodswhichemploythecontext-basedsimilaritycalculation,theimportanceofthefirststep,i.e.,thechoiceofcontextualinformationtoadopthasbeenconsiderablyunderestimated,andalmostnoattentionhasbeenpaidtowhatkindofcontextualinformation,ortheircombinations,isusefulfordistributionalsimilarity.However,ashavepointedout,thechoiceofusefulcontextualinformationisconsideredtohaveacriticalimpactontheperformanceofanymethodsutilizingdistributionalsimilarity.Therefore,furtherinvestigationsonwhichtypesofcontextsareessentiallycontributingarestronglyrequired.Sofar,word-basedcontextanddependencyarethemostwidelyadoptedcontextualcategories.Manystudieshaveshownthatdependencyrelationismoreeffectiveforlexicalknowledgeacquisitioncomparedtoword-basedcontext.However,dependencyrelationsareusuallylimitedtotwowordssharingdirectdependencybetweenthem,anditsometimesmissesimportantwordrelations.Inresponsetothisproblem,weproposetheuseofindirectdependency---theextensionofnormaldirectdependency---asoneofthegeneralwaystoenhancecontextualinformationfordistributionalsimilaritytoimproveperformance.Thisstudyinvestigatesitseffecttotheperformanceandvariousparametersettingswhenapplyingittoactualtasks.Morespecifically,wefirstlyformalizethecontextwhichincorporatesindirectdependency,composedfromtwoormorecontiguousdependencyrelations,andthenshowitseffectivenesstotheperformancewhencomparedtotheconventionaldirectdependencyforthesynonymacquisitiontask.Theperformanceevaluationofsynonymacquisitionisbasedonthesetwomeasures:averageprecision(AP)andcorrelationcoefficient(CC),bothofwhicharecalculatedusingreferencesetscreatedfromafewexistingthesaurisuchasWordNet.Wethenpayattentiontothecontextrepresentationofindirectdependency,whichhasbeenunderestimatedinthepreviousstudiesbutisoneoftheimportantparameterswhenconstructingsemanticspaces,andcomparetheindividualperformance.Thispaperisorganizedasfollows:inSection2wereviewpreviousstudiesconcerningtheuseofcontextualinformationforlexicalknowledgeacquisitionanddistributionalsimilarity,andclarifythepositionofthispaper.InthesucceedingSection3,wementionthebackgroundofhowwecomeupwiththeideaofindirectdependency,andformalizeit.Section4describesthesynonymacquisitionmodelweused,especiallyweightfunctionsandsimilaritymeasures,andinthefollowingSection5,theevaluationmethodweemployedisdetailed,whichconsistsoftwoevaluationmeasures:averageprecision(AP)andcorrelationcoefficient(CC).Section6providestheexperimentalconditionsandresults,withthefirstpartregardingtheeffectivenessofindirectdependency,andthesecondpartbeingaboutthefourkindsofcontextrepresentationforindirectdependency:terminalword(|TW|),wordpath(|WP|),syntacticpath(|SP|),andfullpath(|FP|).Section7concludesthispaper.</section>
  <section title="Previous Studies">Sofar,variouskindsofcontextualinformationhavebeenusedtoacquirelexicalrelationsbasedonthedistributionalhypothesis.Oneofthemostfundamentalyetwell-performingonesisword-basedcontext,whichconsiderswordssurroundingatargetwordascontexts.Ithasbeenextensivelyusedinvariousapplicationsincludingwordsensedisambiguation,wordprimingincognitivesciencefield,andsynonymacquisition/thesaurusconstruction.Thewindowsizetocapturecontextsrangesfromafewwordsonbothsidestotheentireparagraphordocument,uponwhichtheacquisitionperformancegreatlydepends.Althoughword-basedcontextissimpleandeffective,ithassomedrawbacks,especiallythedatasizeproblem.Themethodcaneasilyincludealargeamountofnon-relevantcontextsandinflatethecontextsize,makingthesimilaritycalculationcomputationallyexpensive.Thisisoneofthebiggestreasonswhyitisgraduallytakenoverbymuchricherkindofcontexts.Ontheotherhand,moresophisticatedandefficientcontext,syntax-basedcontext,hasbeenemployedinmanylexicalknowledgeacquisitiontasks.Ituseswordswhichhavesyntacticrelation(mostlydependencystructure)withthetargetwordascontexts,sometimesalongwiththeirrelationlabels.Forexample,Hindleusedsubjectandobjectrelationstoacquiresynonymsautomaticallyinhislandmarkpaperinthefield.Thisstudyaccompaniedagreatdealoffollow-upsinhiswake---including,wherewordclusteringisconductedbasedonverbsandtheirobjectivenouns,and,wheretheyinvestigatedthedifferenceofsyntax-basedcontexts,andshowedthatthefinegrainedattributesimprovedtheperformance.Comparedtoword-basedcontext,theeffectiveness(orcost-effectiveness)ofsyntax-basedcontextwasshownbymanystudies.Forexample,CurranandMoensshowedthatsyntax-basedcontextcouldachievetheaccuracyofmorethan5pointshigherwithonlytwothirdsoftheoriginalsemanticspace,comparedtoword-basedcontext.Theseresultsshowthatsyntax-basedcontextiscapableofchoosinghighlyeffectivecontexts.Assuch,someformsofextensionsofsyntax-basedcontextarenaturallyworthconsideration.Syntax-basedcontextisextendedsothatitincludesnotonlywordswithdirectrelationsbutalsooneswithindirectrelations,whoseunderlyingideasareessentiallythesameastheindirectdependencydescribedinthispaper.LinandPantel,forexample,describedthesystemwhichuseddependencypathtorepresentwordrelationshipsandextendedthedistributionalhypothesistoextractequivalentpathsforinferenceruleextractionofquestionansweringtask.However,theirapplicationisratherlimitedanditseffectondistributionalsimilarityandotherkindsoftasksshouldbeinvestigated.Inthispaper,theeffectofindirectdependencyisevaluatedusingoneofthemostpopularapplicationsfordistributionalsimilarity:automaticsynonymacquisition.PadoandLapataproposedageneralframeworktorepresentthevariationsofsemanticspacewhichincorporatesindirectsyntacticrelationsofwords.Althoughtheyseparatedandformalizedvariousparametersettingswhenconstructingsemanticspaces,theydealtwithonlyoneformofcontextrepresentations(terminalword),i.e.,howdependencypathsaremappedtobasisinthesemanticspace,whichwebelievehascriticalimpactontheperformanceandwillbefullydiscussedinthispaper.Hagiwara,Ogawa,andToyamaalsopointedouttheeffectivenessofindirectdependencyforsynonymacquisitiontask.However,theirevaluationexperimentsaretoomuchdependentonspecificparametersettingsandsmallcorpora.Were-investigatetheeffectivenessofindirectdependencyusingmuchbroadersettingsandalargercorpusinthispaper.</section>
  <section title="Indirect Dependency">Inthissection,wefirstlydescribethelimitationofnormaldirectdependencyandhowwecameupwiththeideathattheextensionofdirectdependencycanbebeneficial.Next,theformalizationofindirectdependency,whichisusedthroughoutthispaper,isdescribed.</section>
  <subsection title="Limitation of Direct Dependency">Althoughpreviousstudieshaveshownthatsyntax-basedcontextiscapableofchoosinghighlyeffectivecontexts,wenoticethatthecoverageofsyntax-basedcontextisratherlimited,whichcanbeproblematicwhenfullyutilizingthecontexts.Toconfirmthis,weconductedasimplepreliminaryexperiment,wherewecomparedtheperformancesofword-basedcontextwbcandsyntactic-basedcontext(wecallthisasdepinthefollowing,sinceweuseddependencystructureassyntax-basedcontext).Theresultshowed,whiledepcertainlyoutperformedwbc,thereisonlyaslightperformancedifferencebetweenthem.Wesuspectthatthisisbecausedeponlycoverstwowordswithdirectdependency,anditcanmissalargeportionofimportantwordrelations,causingtheperformanceloss.WeshowthisusingverysimpleexamplesillustratedinFig.~.Itisobviousthatthereisastrongsemanticrelationbetweenthewordskoalaandeucalyptus,althoughthisrelationcannotbeobtainedusingdepinneitherthesentence(a)nor(b),whilewbceasilyrepresentsitastherelationkoala--eucalyptus.Theproblemhereisthatthesetwowords,koalaand,arerelatedonlyindirectlyviatwo(inthesentence(a))orthree(inthesentence(b))dependencyrelations.Suchkindsofindirectrelationsareincludedinwbcwiththewindowradiusof3butnotindep,andwesuspectthatthiskindofdifferencecausedtheperformancelossfordep.Toovercomethisproblem,wecameupwiththeideatoincludethesedependencyrelations,i.e.,wordsthatcanbereachedbyfollowingtwoormoredependencyrelations,andenhancethecontextualinformationfordistributionalsimilaritytoimprovetheperformance.Thedetailedmethodforthisextensionisdescribedinthefollowingsection.</subsection>
  <subsection title="Formalization">Asdependencyisoftenrepresentedasrelations,itisusefulandstraightforwardtoformalizeindirectdependencyasthecompositionofrelations,whichisusedthroughoutthispaper.HereweconsiderthedependencyrelationsinacertainsentencesasabinaryrelationDoverW=w_1,...,w_ni.e.DWW,wherew_1,...,w_narethewordoccurrencesinthesentences.Dependencyrelationsgenerallyhavelabelssuchas|subj|and|dobj|whichspecifywhatkindofsyntacticrelationsheadandmodifierpossess.WeencodethisinformationasequivalenceclassesofD,whichmeansthatDispartitionedintotheequivalenceclassesD_r_isuchthatD=_i=1^kD_r_i,D_r_iD_r_j=ifij,basedonthelabelsr_1,...,r_k.Notethatbothofthehead-modifier(e.g.|ncsubj|)andthecorrespondingmodifier-head(e.g.|ncsubj-of|)relationshipsareincludedinDandthedirectioninformationofdependencyisincorporatedintothelabels.Asthenextstep,wedefinethecompositionofdependencyD^2=DDasindirectdependencywheretwowordsarerelatedviatwodependencyrelationedges.Whenanindirectdependencyrelationd=(w_i,w_j)D^2iscomposedfromanedge(w_i,w_m)_r_iandanotherone(w_m,w_j)D_r_j,thelabelofd,i.e.,whichequivalenceclassdbelongsto,isalsocomposedasr_i:w_m:r_j.Inotherwords,thelabelsaretheconcatenationsofallthelabelsandwordsonthewayfromw_itow_j.Wealsodefinemultiplecompositionofdependencyrecursively:D^1=D,l&gt;1.D^l=D^l-1D.Thesearealsoindirectdependencyrelationsinabroadsense.NoticeherethatD^l(l&gt;1)cangenerallyincludereflexiverelations,butitisclearthatsuchrelationsdon'tserveasusefulwordfeatures,sowere-definethecompositionoperationsothatthecomposedrelationdoesn'tincludeanyreflexiveedges,i.e.,DD-(w,w)|wW.ThecontextsforindirectdependencyD^1,D^2,...arereferredas|dep1|,|dep2|,andsoon.SomeexamplesofindirectdependencyareshownbelowthesentenceinFig.~.Bycomposing(attack,of)_dobjand(of,motive)D_iobj,weobtainanindirectdependency(attack,motive)D_dobj:of:iobjD^2,whichweconsiderasasemanticallyimportantlexicalrelationinthesentence.Similarly,thecompositionoffourcontiguousdependencyedgesbetweenwordwitnessesandinvestigatorsgives(witnesses,investigators)_conj:and:dobj:for:iobj:looking:ncsubj-ofD^4,asshowninFig..</subsection>
  <section title="Synonym Acquisition Method">Althoughthepurposeofthecurrentstudyistoinvestigatetheeffectivenessofindirectdependency,nottheacquisitionmethoditself,wehavetoconfirmtheeffectofindirectdependencyusingvariousparametersettingsincludingweightfunctionsandsimilaritymeasuresdescribedbelow.Inthefollowing,thenumbersofuniquewordsandcontextsaredenotedasnandm,respectively,andletN(w,c)bethenumberofco-occurrencesofthewordwandthecontextc.Theoverallframeworkforsynonymacquisitionisquitesimple.Firstly,weconstructavectorw_iforeachwordw_ias:[w_i=^t[(w_i,c_1)(w_i,c_2)...(w_i,c_m)],]where(w,c)isoneoftheweightfunctionsdescribedinSection4.1.Thesimilaritybetweenwordw_iandw_j,i.e.,sim(w_i,w_j),isthenobtainedasthesimilaritybetweenthecorrespondingvectorsw_iandw_j,calculatedusingsomesimilaritymeasuresdescribedinSection4.2.</section>
  <subsection title="Weight Functions">Herewepayattentiontothesefourweightfunctionslistedbelow:|tf|,|tfidf|,|pmi|,and|ttest|.RawFrequency(tf)rawco-occurrencefrequencyofwordandcontextisusedasitis:[(w,c)=N(w,c).]Notethat,althoughthetermstfandidfarebasedoninformationretrievaltasksetting,themodelingisessentiallythesameasdistributionalsimilarityandwestillusethetermstfandidfinthispaper.tf.idfWeighting(tfidf)rawfrequency|tf|isweightedbyidf:[(w,c)=N(w,c)(c),(c)=n(c),]where(c)isthenumberofuniquewordswithwhichthecontextcco-occurs,i.e.,(c)=|w|N(w,c)&gt;0|.Intheexperiments,theidfvaluesarenormalizedsothatthemaximumequalsto1.PointwiseMutualInformation(pmi)amountofinformationthatappearancesofwandchaveisdefinedastheirpointwisemutualinformation(|pmi|):[(w,c)=p(w,c)p(w)p(c),]wheretheprobabilitiesarecalculatedempirically,e.g.p(w,c)=N(w,c)/_w',c'N(w',c').t-statistic(ttest)-statistic,usedtotestwhethertheappearancesofawordwandacontextcareindependent,isusedasaweightfunction:[(w,c)=p(w,c)-p(w)p(c)p(w)p(c).]Whereast-statisticisusuallyemployedtodiscovercollocationsfromcorpora,CurranandMoensproposedusingitasaweightfunctionandshowedthatitachievedhigherperformancecomparedtotheothers.</subsection>
  <subsection title="Similarity Measures">Thesimilaritymeasuresweemployedtocalculatetheinter-vectorsimilarityarelistedbelow:|cosine|,|jaccard-s|,|jaccard-v|.Inthefollowing,letC(w)bethesetofallthecontextsthewordwco-occurs,i.e.,C(w)=c|N(w,c)&gt;0.Cosineofthemostcommonlyusedsimilarityfunctions,especiallyforinformationretrieval,alongwithvectorspacemodeliscosinesimilarity:[sim(w_1,w_2)=_cC(w_1)C(w_2)(w_1,c)(w_2,c)_cC(w_1)(w_1,c)^2_cC(w_2)(w_2,c)^2,]whichistheinnerproductoftwovectorsw_1andw_2,normalizedbythevectorlengths.Set-basedJaccardcoefficient(jaccard-s)coefficient,originallyusedtomeasurethecommonalityoftwosets,isextendedandusedfordistributionalsimilarity.Inthemostnaiveform,itonlyusesthenumberofelementsinsets,butheretheweightsarealsotakenintoconsideration:[sim(w_1,w_2)=_cC(w_1)C(w_2)((w_1,c),(w_2,c))_cC(w_1)C(w_2)((w_1,c),(w_2,c)).]Vector-basedJaccardcoefficient(jaccard-v)isanotherextensionofJaccardcoefficientwhichisbasedonthecommonalityoftwovectors:sim(w_1,w_2)&amp;=&amp;_cC(w_1)C(w_2)(w_1,c)(w_2,c)_cC(w_1)(w_1,c)^2+_cC(w_2)(w_2,c)^2-_cC(w_1)C(w_2)(w_1,c)(w_2,c).eqnarray*NotethatwestillhaveDicecoefficient,whichisalsowidely-adoptedinmanysimilarity-basedtasks.However,weomitteditfromchoicethistime,becauseitisshownthatDiceandJaccardcoefficientaremonotonicineachother,yieldingexactlythesameresults.Aslistedabove,therearetoomanyweightfunctionsandsimilaritymeasurestohandle,letalonetheircombinations.Becausethepurposeofthispaperistheinvestigationoftheindirectdependencyeffectiveness,welimitthetarget,insteadofadoptingallthesecombinations,andselectedonlyafewpracticalcombinationsofweightsandmeasures,thatis:|tfidf+cosine|,|pmi+jaccard-s|,and|ttest+jaccard-v|.Wechose+cosinebecauseitisthemostwidelyadoptedcombinationofweightandmeasurefunctionforinformationretrieval.Thesecondone,pmi+jaccard-sisalsocommonforsimilarity-basedtask,andindividualeffectivenessofPMIandJaccardcoefficienthasbeenshown.Theweightttestandthemetricjaccard-varealsoshowntobeoneofthebestperformingin,thusthelastcombinationttest+jaccard-vwaschosenforthediversity.Wecallthesethreecombinationscalculationparametersinthefollowing.</subsection>
  <section title="Evaluation">Thissectiondescribesthetwoevaluationmethodsweemployedtoevaluatetheautomaticallyacquiredsynonyms---averageprecision(AP)andcorrelationcoefficient(CC).</section>
  <subsection title="Average Precision">Thefirstevaluationmeasure,averageprecision(AP),isacommonevaluationschememainlyadoptedforinformationretrieval,whichevaluateshowaccuratelythemethodunderevaluationisabletoextractsynonyms.TocalculateAP,wefirstlyprepareasetofquerywords,forwhichsynonymsareobtainedtoevaluatetheprecision.WeadoptedtheLongmanDefiningVocabulary(LDV),originallyconsistedof2,194entries,asthecandidatesetofquerywords.Secondly,foreachentryinLDV,threeexistingthesauriareconsulted:Roget'sThesaurus,CollinsCOBUILDThesaurus,andWordNet.TheunionofsynonymsobtainedwhentheLDVwordislookedupasanounisusedasthereferenceset,exceptforwordsmarkedas``idiom,''``informal,''``slang''andphrasescomprisedoftwoormorewords.NoticethatmostofLDVentrieshavemorethantwosensesinWordNet,inwhichcasetheunionofsynonymsfoundinallsynsetsoftheentrywasusedasthereferenceset,sincethesynonymacquisitionmodeldescribedinthispaperisnotcapableoftreatingmultiplesensesofasingleword.TheLDVwordsforwhichnonounsynonymsarefoundinanyofthereferencethesauriareomitted.Finally,fromtheremaining771LDVwords,100querywordsarerandomlychosen,whicharelistedinAppendix.Foreachofthem,theelevenprecisionvaluesat0%,10%,...,and100%recalllevelsareaveragedtocalculatethefinalAPvalue.</subsection>
  <subsection title="Correlation Coefficient">Thesecondevaluationmeasureiscorrelationcoefficient(CC)betweenthetargetsimilarityandthereferencesimilarity,i.e.,thegoldstandardofsimilarityforwordpairs.TheCCvalueiscalculatedsothatitbecomeslargerwhentheobtainedsimilarityismorepreciseapproximationofthereferencesimilarity.ThereferencesimilarityiscalculatedbasedontheclosenessoftwowordsinthetreestructureofWordNet,andthismethodisessentiallythesameasWuandPalmer's.Morespecifically,thesimilaritybetweenawordwwithsensesw_1,...,w_m_1andawordvwithsensesv_1,...,v_m_2isobtainedasfollows.Letthedepthofnodew_iandv_jbed_iandd_j,andthedepthofthedeepestcommonancestorsofbothnodesbed_dca.Thesimilarityisthencalculatedasthemaximumoverallthesensecombinations,asdonein:[sim(w,v)=_i,jsim(w_i,v_j)=_i,j2d_dcad_i+d_j.]Herewehavetonotethat,thereferencesimilaritycalculationbasedonWordNetpathlengthissuchanaivemethodthatitsometimescausesproblems,especiallythediscretenessoftheobtainedsimilarity.However,weconductedapreliminaryexperimentandverifiedthatthedifferentvaluesofCCcalculatedusingvariouscalculationtechniquesincludingtheonementionedaboveand,arehighlycorrelatedandthechoicedoesnotessentiallyaffectstheoverallevaluation.Then,thevalueofCCiscalculatedasthecorrelationcoefficientofreferencesimilaritiesr=(r_1,r_2,...,r_n)andtargetsimilaritiess=(s_1,s_2,...,s_n)overthewordpairsinthesamplesetP_s,whichiscreatedbychoosingthemostsimilar2,000wordpairsfrom4,000randomlycreatedpairsfromLDV.Toavoidtestsetdependency,alltheCCvaluesinthispaperarecalculatedastheaverageof10executionsusingindependentlycreatedtestsets.</subsection>
  <section title="Experiments">Nowwedescribetheevaluationresultsforindirectdependency,andthedetailedcomparisonoftheacquisitionandcontextparametersinthissection.</section>
  <subsection title="Condition">Weextractedcontextualinformationfromthecorpus:the``story''-typedocumentsofNewYorkTimesarticles(1994)inEnglishGigaword,consistingof45,830documentsandapprox.30millionwords.Astheextractionofaccurateandcomprehensivesyntacticrelationsisinitselfadifficulttask,thesophisticatedparserRASPToolkitver.2(RASP2)wasutilizedtoextractdependencyrelations.RASP2analyzesinputsentencesandprovidesawidevarietyofgrammaticalinformationsuchasPOStags,dependencystructure,andparsedtreesasoutput,amongwhichweemployedthedependencystructurecalledgrammaticalrelations(GRs),whichweconvertedtothedependencyrelationD.Accordingto,theRASP2systemachievesamicro-averagedF_1scoreof76.3%forrelationassignment.Evenwhentheparsermakessomeerrors,wesupposetheinfluencewouldnotbeserious---thewronglyassignedrelationsactaslessprecisecontexts,stillworkingbetterthanthenaiveword-basedcontext.Asthebasisofthesemanticspace,thesecondcomponentofeveryindirectrelation,i.e.,w_jin(w_i,w_j)D^n,wasused.Forexample,|motive|wasusedasthecontextforthewordattackin(attack,motive)D_dobj:of:iobjD^2.Sinceourpurposehereistheautomaticextractionofsynonymousnouns,onlythecontextsfornounsareextracted.Todistinguishnouns,usingPOStagsannotatedbyRASP2,anywordswithPOStagsAPP,ND,NN,NP,PN,andPP,werelabeledasnouns.Wealsosetcut-offfrequencies_wand_c,i.e.,thresholdsonoccurrencefrequencytofilteroutwordsorcontextswithlowfrequency,respectively,andtoreducecomputationalcost.Morespecifically,anywordswsuchthat_cN(w,c)&lt;_wandanycontextscsuchthat_wN(w,c)&lt;_cwereremovedfromtheco-occurrencedata.Specialattentionisrequiredtofixthevaluesof_fand_cbecausetheycandirectlyaffectthesizeofconstructedsemanticspacesand/ortheperformance.Ourstrategytofixthemistoinvestigatetheireffectseparately.Thefirstone,thewordcut-offfrequency_w,isrelativelyeasiertohandle,becauseitaffectsthenumberofuniquewordscontainedinthesemanticspacealmostexclusively.AssumingtheZipf'slaw,thenumberoftheremainingwordtypesisinverselyproportionalto_w,whichweconfirmedthroughapreliminaryexperiment.Whenfixing_w,whatweshouldconsideristhetrade-offbetweentheremainingwordtypesandthecomputationalcost---thethresholdmustbehighenoughtomakeitcomputationallypractical,andlowenoughtokeepasmuchproportionoftheoriginalwordtypesaspossible.Wefinallyadopted_w=100becauseitcankeepmorethanhalfoftheoriginallyextractedwordtypesforallofthecontextcategories,whilegreatlyreducingthecost.Also,thecut-offfrequency_fdoeseffectAP,butnotCC,becauseitworksasafiltertoeliminatelow-frequencywordsfromtheranking,mostofwhichareirrelevanttothequeryword.Theexperimentshowedtheperformanceofsynonymacquisitionincreaseswith_walmostmonotonically.However,sincetheincreaserangeisnotgreat(atmost20%)andthisperformanceincreaseequallyhappenstoallthecontextcategories,theinfluencecanbeignored.Thesecondone,thecontextcut-offfrequency_c,islesseasytohandle,becauseitaffectsboththesemanticspacesizeandtheperformance(APandCC).AlthoughthenumberoftheremainingcontexttypescanbeeasilyestimatedusingZipf'slaw,similarlyto_w,wehavenootherwaystoknowtheperformancechangebuttoempiricallyinvestigateit.Figureshowstheperformancechangeonincreasing_c,fordep3-FP.Thedep3-FPcontext,detailofwhichislaterdescribedinSection6.4,isoneofthecontextcategorieswhichisthemostsensitiveto_c.Theresultindicatestheperformancedecreasesalmostlinearlyv.s._c,makingitdifficulttofixaclearthreshold.Weset_c=100aswell,wheretheperformancelosscanbekeptwithin15%rangewhilethecontexttypesarecut-downbymorethan95%innumber.</subsection>
  <subsection title="Effect of Indirect Dependency">Nowwecomparetheperformancesofdirectandindirectdependencycontexts.Figureshowstheperformancesofsynonymacquisitionwhenthecalculationparameteris|tfidf+cosine|,|pmi+jaccard-s|,and|ttest+jaccard-v|.Theycomparethefive|dep|categoriesandcombinations:C^1(|dep1|),C^2(|dep2|),C^3(|dep3|),C^1C^2(|dep12|),andC^1C^2C^3(|dep123|),whereC^nisthesetofcontextsextractedfromD^n.Theoverallobservationisthataddingtheindirectdependency|dep2|totheconventionaldirectdependency|dep1|,i.e.,|dep12|,increasedtheperformanceinsomecases,andinmostcases|dep2|didn'toutperform|dep1|byitself.Morespecifically,thistendencyisdependentonthecalculationparameters---for|ttest+jaccard-v|theperformanceincreasedwhenever|dep2|wasaddedto|dep1|.For|tfidf+cosine|,|dep2|hadnegativeeffectsonCCvalues,buttheabsolutevalueofperformancewaslow(around0.04)inthefirstplace,andthiseffectcanbeignored.Tolookfurtherintotheeffectofindirectdependency,welistsomespecificexamplesoftheacquiredsynonymsinTable,inthedescendingorderofsimilarity.Thetablealsocomparesthecaseswhen|dep1|and|dep12|wereusedascontext.Theshownresultisthecasewhentheacquisitionwasconductedusing|ttest+jaccard-v|ascalculationparameter.Theoverallresultsofthetop-rankedrelevantwordslookedquiteencouraging,showingasmallnumberoftotallyirrelevantwordsinthelist,whicharemarkedbyasteriskmarksbasedonsubjectivejudgement.Furthermore,someoftheirrelevantwordsappearinginthe|dep1|listsdisappearedandwerereplacedbymorerelevantwordstothetargetwordsinthe|dep12|list,asmusselandnudityforthetargetwordlanguage,aswellascentimeterandconvenienceforthetargetwordjewelryshow.Wecangraspthegeneraltendencythat|dep12|improvesthequalityofacquiredsynonymsfromthelist.Someofthecomparisons,includingtheonementionedabove,showspartialeffectivenessofindirectdependency,buttheoverallresultdoesnotlookencouraging.Morespecifically,3outof6(twoevaluationmeasuresandthreecalculationparameters)comparisonsshowedthenegativeeffectof|dep12|over|dep1|,andtheresultismuchdependentonthechoiceofcalculationparameters.Nowrememberthatweusedtheterminalwordpositionedattheendofeachdependencypathasthecontexts,i.e.,basesoftheconstructedsemanticspace.However,wesuspectthatthiskindofcontextrepresentationistoocoarsetofullyrepresentthedetaileddifferenceofsyntacticalrelationofwords.GetbacktotheexampleillustratedinFig.~,andtakethesentence(a)forexample.Whentherelation:(koala,eucalyptus)_ncsubj:eat:dobj-ofD^2inthissentenceisused,thecontextforkoalawillbe|eucalyptus|.However,itisconsideredtobeimportantthattherelationisnot``Koalasgroweucalyptuses'',nor``Koalashateeucalyptuses'',but``Koalaseateucalyptuses'',andthiskindofwordrelationcannotbeencodedinthisterminalwordrepresentation.Themediatingwordeatshouldalsobeincludedinthiscase.Furthermore,thereisabigdiscrepancyinthemeaningbetweenthesentences``Koalaseateucalyptuses''and``Eucalyptuseseatkoalas''.Theserelationsaredegeneratedintoanidenticalrelation|koalas-eat-eucalyptus|evenifthemediatingwordeatisincluded,unlesssyntacticrelationsofwordsaretakenintoconsideration.Thuswesupposethatmuchrichercontextrepresentationswhichincorporatemediatingwordsandsyntacticrelationshavepositiveeffectsontheacquisitionperformance,andwilldiscussthisissueinthefollowingsections.</subsection>
  <subsection title="Context Representations">Inthissection,weconsiderfourkindsofcontextrepresentationswhenconvertingindirectdependencyformalizedinSection3.2tocontexts.Thisrepresentationcorrespondstothebasismappingfunctionwhichmapsdependencypathsintodimensionsinsemanticspacedescribedin,althoughthefunctionanditseffecthavenotbeendiscussedsofar.ThefourcontextrepresentationsconsideredaresummarizedinTable.Inthefollowing,supposeanindirectdependency(w_0,w_l)D_rD^l,wherethelabelr=r_1:w_1:r_2:...:w_l-1:r_l.Herewedefinethesequenceofwordsw_0,w_1,...,w_lasdependencypathandlasthelengthofthepath.Wecallw_0astargetword,whosecontextsaretobeobtained,andw_l,i.e.,thewordpositionedattheendofthepath,asterminalword.TerminalWord(TW)isthesimplestcontextrepresentationwhichhasbeenusedsofar,whereanindirectdependencyisrepresentedbytheterminalwordw_l.Forexample,fromtheindirectdependencyexample:(attack,motive)D_dobj:of:iobjD^2,thecontextforthetargetwordattackis|motive|andvicevarsa,regardlessofthesyntacticrelationsthesetwowordshave.Becausethecontextisrepresentedonlybytheterminalwordnomatterhowlongthedependencypathis,thisisthecoarsestcontextrepresentationofthefour,andisthesameastheoneadoptedin,whereit'sreferredasword-basedbasismapping.WordPath(WP)additiontotheterminalword(|TW|),thisrepresentation,wordpath(|WP|),takesintoaccountallthemediatingwordsalongthedependencypath.Thecontextforthetargetwordw_0isrepresentedasw_1:w_2:...:w_l-1:w_l(|of:motive|fortheexampleshownabove).Noticethatthis|WP|representationisanequivalencepartitioningof|TW|basedontheconcatenationofmediatingwords.SyntacticPath(SP)representationtakesintoaccounttheterminalwordandthesyntacticrelationsthetwowordshave.Herethecontextforthetargetwordw_0isrepresentedasr_1:r_2:...:r_l:w_l.Forexample,thecontextforthetargetwordattackisrepresentedas:|dobj:iobj:motive|.Inturn,thecontextformotiveis|iobj-of:dobj-of:attack|andthedirectionofthepathiscompletelyopposite.Noticethatthis|SP|representationisanotherequivalencepartitioningof|TW|basedontheconcatenationofsyntacticrelations.FullPath(FP)finestrepresentationofcontextisfullpath(|FP|),wherethemediatingwordsofindirectdependencyarealsoconsideredinadditiontothesyntacticrelationsandtheterminalword.Morespecifically,thecontextforthetargetwordw_0isrepresentedasr_1:w_1:r_2:w_2:...:w_l-1:r_l:w_l(|dobj:of:iobj:motive|fortheexampleshownabove).Noticethatthis|FP|representationistheequivalencepartitioningofboth|WP|and|SP|.</subsection>
  <subsection title="Comparison of Context Representations"/>
  <subsubsection title="Extracted Semantic Spaces">Beforemovingontotheresultforcontextrepresentations,Tableweshowthestatistics,i.e.,thenumbersofuniquewords,contexts,andco-occurrences,oftheconstructedsemanticspacesusingthreekindsofcontextualcategories,threestepsofindirectdependency,andfourformsofcontextrepresentations.Itisshownthatincorporatingindirectdependencygreatlyincreasesthevariationofbothcontextsandco-occurrences(compare|dep1-WP|and|dep2-WP|,forinstance).Ontheotherhand,whentheindirectdependencywasextendedto|dep3|,forsomeofthecontextrepresentations(|WP|and|FP|),thenumbersofuniquecontextsandco-occurrencesactuallydecreased.Weattributethistothecontextrepresentationgranularity---thesetworepresentationsaresofine-grainedthatmostofthecontextswereremovedbecausetheyoccurredlessthan_ctimes,causingthedecreasesinthesemanticspacesizes.Asforthe|wbc|,weobservethedatasizeproblemheretoo---thesizeof|wbc|contextsisgreaterthanmostofthecontextsettings,althoughtheperformanceof|wbc|doesnotexceed|dep|'s,asthepreviousexperimentshowed.</subsubsection>
  <subsection title="Comparisons of Other Parameters"/>
  <subsubsection title="Effect of Multiple Steps of Indirect Dependency">Weconfirmedtheperformanceimprovementfor18outof24comparisons(2performancemeasures,3parametersettings,and4contextrepresentations)ofbeforeandafterthe|dep2|addition,thuswecanconcludethatincorporatingindirectdependencyiseffectiveonthewholeforsynonymacquisitiontasks.However,extendingthedependencyto|dep3|worsenedtheperformanceformostcases,andadding|dep3|to|dep12|,i.e.|dep123|,didn'tfurtherimprovetheresult,either.Thisresultsuggeststhatextendingandaugmentingdirectdependencyjustonestepissufficientinpractice.</subsubsection>
  <subsubsection title="Calculation Parameters">Lastly,webrieflymentiontheeffectofcalculationparameter,whichturnedouttobeanotherimportantfactorwhenacquiringsynonymsautomatically.While|pmi+jaccard-s|performedthebestforAP,CCvaluesfor|pmi+jaccard-s|wereexceptionally``sensitive''totheperformancedifferences.Itisknownthattheweightfunction|pmi|hasthetendencytodetectinterestingandrareresults,andwesuspectthatthispropertyaffectednegativelyinthiscase.Theparameter|ttest+jaccard-s|,onthecontrary,showedstableandrelativelygoodperformance.Onthewhole,thecomparisonofFigs.,,andsuggeststhat|pmi+jaccard-s|and|ttest+jaccard-v|werethebestchoices,andthisresultgoesroughlyalongwithCurranandMoens'comparativestudy.</subsubsection>
  <section title="Conclusion">Inthispaper,weextendedthenormaldirectdependencytocoverindirectlyrelatedwordsandenhancethecontextualinformationfordistributionalsimilarity.Worddependencywasdefinedasabinaryrelationoverthesentencewords,andthisindirectdependencywasformalizedasthecompositionofthedependencyrelation.Wealsoproposedfourkindsofcontextrepresentations,whichhavebeenunderestimatedinthepreviousstudiesutilizingdependency-basedcontexts.Weinvestigatedtheeffectivenessofindirectdependencyusingautomaticsynonymacquisitiontaskundervariousparametersettingsincludingweightfunctionsandsimilaritymeasures.Theacquisitionperformancewasevaluatedbasedontwoevaluationmeasures,APandCC,bothofwhicharebasedontheexistingthesauriincludingWordNet.Theexperimentshowedthatincorporatingindirectdependencyinadditiontodirectdependencywaseffectivefortheacquisitionperformance.Theimprovementwasespeciallyclearwhenfine-grainedcontextrepresentations,namely|WP|and|FP|,wereused.Whencomparedtotheconventionalword-basedcontext,muchhigherperformancewasachievedbydependency-basedcontextwithmuchsmallerdatasize,whichmeansthattheindirectcontextiseffectiveintermsofqualityaswellascomputationalcost.Theuseofindirectdependency,alongwiththefine-grainedcontextrepresentations,isaveryefficientwaytoincreasethecontextualinformationvariety,takingintoconsiderationthefactthatthediversityofthecontextsislikelytobeessentialtotheacquisitionperformance.Tonote,severalmethodswhichutilizelatentanalysistechniquessuchasSVDtocompresstheoriginaldimensionalityofsemanticspacewereproposedandshowneffective.Theextensionofcontextproposedinthispaperdoesnotconflictwiththesetechniques.Rather,theycanbeincorporatedandmayresultinevenbetteroutcome,whichshouldbeinvestigatedinthenearfuture.Inthisstudy,wetreatedallthecontextsequally,butwesupposethatthereareeffectivecontextsaswellasuselessones,anddifferentweightsshouldbeassignedtodifferentcontexts.PadoandLapatatacklethisissuebyintroducingpathvaluefunction,whichassignsdifferentweightstothepathsusingsuchparametersaspathlengthandwordrelations.Optimalsettingsforthispathweightingshouldbefurtherinvestigatedandaddressedinanotherarticleinthefuture.Asthefuturework,theeffectivenessofourmethodsandcontextrepresentationsshouldbeconfirmedusingotherkindsofapplicationsandwordrelationsotherthansynonyms.Also,becausewherewestartedfromthistimeisthe``difference''ofword-basedcontextsanddependency-basedcontexts,theinvestigationofotherkindsofusefulcontextualinformationandtheircomparisonshouldbeconductedasthefuturework.</section>
</root>
