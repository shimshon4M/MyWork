    \documentclass[japanese]{jnlp_1.4}
\usepackage{jnlpbbl_1.1}
\usepackage[dvips]{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hangcaption_jnlp}
\usepackage{array}


\Volume{15}
\Number{3}
\Month{July}
\Year{2008}

\received{2007}{12}{12}
\revised{2008}{3}{27}
\accepted{2008}{3}{30}

\setcounter{page}{91}


\jtitle{Webを用いた未知語検索キーワードのシソーラスノードへの\\
	割付け手法}
\jauthor{後藤　和人\affiref{Author_1} \and 土屋　誠司\affiref{Author_2} \and 渡部　広一\affiref{Author_1} \and 河岡　司\affiref{Author_1}}
\jabstract{
日常的な会話の中では，新語や固有名詞などシソーラスに定義されていない単語（未知語）が使用される．未知語についての知識がなければ，適切に会話を行うことができない．Webを利用することで，未知語について調べることができる．
しかし，Webには膨大な情報が存在するため，必要な情報を効率的に得ることは困難である．未知語に対する適切なシソーラスのノードを提示することによって，未知語の意味を獲得することができる．未知語理解はコーパスなど言語データに依存する研究が多く，対応できない未知語が存在するという問題点がある．本論文では，連想メカニズムを構成する概念ベースと関連度計算，さらにWebを用いて，未知語を概念化することで各ノードとの関連性を評価し，固有名詞を含んだ未知語をシソーラス上の最適なノードへ分類する手法を提案する．
}
\jkeywords{シソーラス，概念ベース，関連度計算，未知語}

\etitle{Allocation Method of an Unknown Search Keyword to \\
	a Thesaurus Node by Using Web}
\eauthor{Kazuto Goto\affiref{Author_1} \and Seiji Tsuchiya\affiref{Author_2} \and Hirokazu Watabe\affiref{Author_1} \and Tsukasa Kawaoka\affiref{Author_1}} 
\eabstract{
The words which are not defined in Thesaurus appear in the daily conversation, including new words and proper nouns. It is unable to carry conversation with no knowledge about these words. We can search about an unknown word by using Web. 
However, it is difficult to obtain necessary information efficiently because large amounts of information exists in Web.
The meaning of the unknown word is acquired by presenting an appropriate node of Thesaurus. As for the research on understanding of the unknown word, there are a lot of researches which relies on language data including corpus. 
These researches have a problem that there are unknown words which can not be responded. 
This paper proposes the technique for finding the best node for the unknown word included proper nouns by conceptualizing the unknown word and evaluating relationship to each node.
}
\ekeywords{Thesaurus, Concept-base, Degree of Association, Unknown Words}

\headauthor{後藤，土屋，渡部，河岡}
\headtitle{Webを用いた未知語検索キーワードのシソーラスノードへの割付け手法}

\affilabel{Author_1}{同志社大学大学院工学研究科知識工学専攻}{Dept. of Knowledge Engineering \& Computer Sciences, Doshisha University}
\affilabel{Author_2}{徳島大学大学院ソシオテクノサイエンス研究部}{Institute of Technology and Science, The University of Tokushima}



\begin{document}
\maketitle


\section{はじめに}
我々は，人間と自然な会話を行うことができる知的ロボットの実現を目標に研究を行っている．ここで述べている「知的」とは，人間と同じように常識的に物事を理解・判断し，応答・行動できることである．人間は会話をする際に意識的または無意識のうちに，様々な常識的な概念（場所，感覚，知覚，感情など）を会話文章から判断し，適切な応答を実現しコミュニケーションをとっている．本論文では，それらの常識的な判断のうち，未知語の理解に着目し，研究を行っている．

知的ロボットとの円滑なコミュニケーションを実現するにあたり，重要となる技術が自然言語処理である．近年，自然言語処理において，単語を意味的に分類したシソーラス\cite{NTT_Thesaurus:97}，\cite{G.A.Miller:95}が数多く構築されている．これらのシソーラスは，情報検索や機械翻訳など多くの分野で利用されている．会話処理にシソーラスを用いた場合，会話文中にシソーラスに定義されていない単語（以下，未知語と呼ぶ）が含まれると，その会話文を理解することができない．そのため，未知語が大局的にどのような意味を持つのかを知る必要がある．未知語が所属するべきシソーラスのノードを提示することで，未知語の内容を簡明に表示することができると考える．

これを実現するためには，ある単語から概念を想起し，さらに，その概念に関係のある様々な概念を連想できる能力が重要な役割を果たす．これまで，ある概念から様々な概念を連想できるメカニズムを，概念ベース\cite{okumura:07}と関連度計算\cite{watabe:06}により構成し，実現する方法が提案されている．そこで本論文では，連想メカニズムおよびシソーラスの体系的特徴を基に未知語を所属するべき最適なノードへ分類する手法を提案する．

これまでにも同種の研究がなされている．\cite{uramoto:96}では，言語データとしてISAMAP\cite{tanaka:87}を利用し，未知語をシソーラスに分類する手法としてコーパス中の出現回数などの統計情報を用いている．また\cite{maeda:00}では，言語データとしてNTTシソーラス\cite{NTT_Thesaurus:97}を利用し，未知語をシソーラスに分類する手法として統計的決定理論の1つであるベイズ基準を用いている．一方で\cite{sakaki:07}では，検索エンジンのヒット件数に対して$\chi^2$値を用いた関連度の指標を用いることで，シソーラスの自動構築を行う手法が提案されている．
また\cite{bessho:06}では，コーパスにおける単語同士の共起頻度を用いて単語をベ
クトル表現で表すことで，概念ベースを作成している．そして，概念ベースに登録していない
単語のベクトル表現を，意味空間への射影による手法および分散最小性に基づく手法を用いて
推定し，概念ベースを拡張する方法が提案されている．
このようにこれまでの研究は，コーパスやシソーラスなどの言語データに存在する単語と未知語の共起頻度を利用することで，両者の関連性を比較し，未知語を既存のシソーラスに分類するものである．そのため，これまでの研究は，用いる言語データに存在しない未知語の場合，共起頻度を獲得することができないため，対応できないという問題を抱えている．

本論文では，共起頻度に加えて，ある概念から様々な概念を連想できる連想メカニズムを用いている．その結果，固有名詞を含んだ未知語に対応した柔軟なメカニズムの構築を実現している．



\section{未知語分類システム}\label{system}

未知語分類システムの構成を図\ref{fig:system}に示す．未知語分類システムは，単語を意味的に分類した分類体系の1つであるNTTシソーラス\cite{NTT_Thesaurus:97}と，未知語をNTTシソーラスのノードに分類するための未知語分類処理により構成されている．
また，未知語分類処理においては，複数の国語辞書や新聞などから機械的に構築した大規模な知識ベースである概念ベース\cite{okumura:07}と，概念と概念の関連の強さを定量的に評価する関連度計算\cite{watabe:06}（以下，これらを合わせて連想メカニズムと呼ぶ）を用いることにより，未知語とNTTシソーラスのノードとの関連付けを行っている．なお本論文では，未知語とNTTシソーラスのノードに対して関連度計算を行うために，概念化という処理を行っている．概念化とは，ある単語に属性と重みの集合を与えることである．

\begin{figure}[b]
\begin{center}
\includegraphics{15-3ia5f1.eps}
\end{center}
\caption{未知語分類システムの構成}
\label{fig:system}
\end{figure}

本論文では，常識的な会話処理において用いられる一般名詞については，ノードとリーフを
あわせて13万語の以上の単語が収録されているシソーラスと，約9万語の概念を収録する概
念ベースを用いることで対応することができると考え，未知語に関する表現として，固有名詞
を扱う．さらに，固有名詞の中でも，1 つの単語のみから人間がその単語の意味を判断できる
固有名詞を扱う．例えば，「G ショック」は「時計」，「クイニーアマン」は「パン」であると
判断できる．逆に，「イオン」であれば，「企業」と判断する人間だけでなく，「電離現象」と
判断する人間もいると考えられる．このように，人間が一意に判断できないことは，判断する
手法が存在しないと考え，多義的な要素を持つ固有名詞については扱わないものとしている．
また，\ref{acquiring_attribute_of_unknown_word}節で述べる未知語の概念化では，未知語の属性とその重みの獲得をWebから行う．
そのため検索にヒットしない，つまり，Webに存在しない未知語は扱わないものとしている．


\section{構成技術}\label{constructing_technique}

本章では，本研究を構成する技術であるシソーラス，連想メカニズム，および，属性の重み付け手法について述べる．

\subsection{シソーラス}\label{thesaurus}

シソーラスとは，単語を意味的に分類した分類体系である．シソーラスの多くは木構造を持ち，名詞の集合を分類した名詞シソーラスや用言の集合を分類した用言シソーラスなどがある．また，木構造の葉（以下，リーフと呼ぶ）のみに単語が所属する分類シソーラスと根及び中間ノードにも単語が所属する上位下位シソーラスがある．

本論文では，木構造を持つ名詞シソーラスであり，上位下位シソーラスの1つであるNTTシソーラス\cite{NTT_Thesaurus:97}を用いる．NTTシソーラスは一般名詞の意味的用法を表す2710個のノードの上位—下位関係，全体—部分関係が木構造で示されたものである．ノードに所属する名詞として約13万語のリーフが分類されている．図\ref{fig:thesaurus}にNTTシソーラスの木構造の一部を示す．

\begin{figure}[b]
\begin{center}
\includegraphics{15-3ia5f2.eps}
\end{center}
\caption{NTTシソーラスの木構造（一部）}
\label{fig:thesaurus}
\end{figure}

本論文では，未知語を最も詳しく説明するノードに分類するという考えから，未知語を分類
するノードを最下位ノード（1926個）に限定している．
さらにその中で，固有名詞である未知語が分類されることはないと判断できるノードを人手で削除している．なお判断基準としては，3名の被験者に各最下位ノードに未知語が分類されるノードか否かを判断してもらい，そのうち3名全員が未知語は分類されないと判断したノードを削除している．結果，使用するノード数は385個となっている．表\ref{table:filtering_node}に選別したノードの一例を示す．

\begin{table}[t]
\caption{使用するノードの選別}
\input{05table01.txt}
\label{table:filtering_node}
\end{table}


\subsection{連想メカニズム}\label{association_mechanism}

連想メカニズムは概念ベースと関連度計算により構成されており，概念ベース\cite{okumura:07}は，ある単語から語意の展開を行い，関連度計算\cite{watabe:06}は，語意の展開結果を利用し，単語の間にある関連性の強さを数値として表す手法である．

\subsubsection{概念ベース}\label{concept_base}

概念ベースとは複数の国語辞書や新聞などから機械的に構築した単語（概念）とその意味特徴を表す単語（属性）の集合からなる知識ベースである．概念には属性とその重要性を表す重みが付与されている．概念ベースには約9万語の概念が収録されており，1つの概念に平均約30個の属性が存在する．
ある概念$A$は属性$a_i$とその重み$w_i$の対の集合として，式\ref{eq:concept_base}で表される．
\begin{equation}
A=\{(a_1,w_1),(a_2,w_2),\cdots,(a_i,w_i),\cdots,(a_n,w_n)\}
 \label{eq:concept_base}
\end{equation}
任意の一次属性$a_i$は，その概念ベース中の概念表記の集合に含まれている単語で構成されている．したがって，一次属性は必ずある概念表記に一致するため，さらにその一次属性を抽出することができる．これを二次属性と呼ぶ．概念ベースにおいて，「概念」は$n$次までの属性の連鎖集合により定義されている（図\ref{fig:concept_base}）．

\begin{figure}[t]
\begin{center}
\includegraphics{15-3ia5f3.eps}
\end{center}
\caption{概念ベース}
\label{fig:concept_base}
\end{figure}

本論文では，\ref{acquiring_attribute_of_unknown_word}節で述べる未知語の概念化，および，\ref{acquiring_attribute_of_node}節で述べるシソーラスのノードの概念化に概念ベースを用いている．


\subsubsection{関連度計算}\label{degree_of_association}

関連度とは，概念と概念の関連の強さを定量的に評価するものである．概念と概念の間にある関連性を定量的に評価する手法として，ベクトル空間モデルが広く用いられている．しかし，本論文では，概念を定義する属性集合とその重みを含めた一致度に基づいた関連度計算方式を利用している．これは，関連度計算方式が有限ベクトル空間によるベクトル空間モデルよりも良好な結果が得られるという報告がなされているためである\cite{watabe:01}．本論文では重み比率付き関連度計算方式を使用し，実験を行う\cite{watabe:06}．

任意の概念$A$，$B$について，それぞれ一次属性を$a_i$，$b_j$とし，対応する重みを$u_i$，$v_j$とする．また，概念$A$，$B$の属性数を$L$個，$M$個$(L<M)$とする．
\begin{gather*}
A=\{(a_i,u_i) \mid i=1〜L\} \\
B=\{(b_j,v_j) \mid j=1〜M\}
\end{gather*}
このとき，概念$A$，$B$の重み比率付き一致度$\mathit{MatchWR}(A,B)$を以下の式\ref{eq:MatchWR1}，\ref{eq:MatchWR2}で定義する．
\begin{gather}
 MatchWR(A,B)=\sum_{a_i=b_j}\min(u_i,v_j)\label{eq:MatchWR1} \\
 \min(\alpha,\beta)=
	\begin{cases}
	  \alpha & (\beta > \alpha) \\
	  \beta & (\alpha \geq \beta)
	\end{cases}
	\label{eq:MatchWR2}
\end{gather}

概念$A$，$B$ の属性$a_i$，$b_j$ に対し，$a_i = b_j$となる属性（概念$A$，$B$ に共通する属性）があった場合，共通する属性の重みの共通部分，つまり，小さい重み分のみ一致するとの考えに基づいている．定義から明らかなように，両概念の属性と重みが完全に一致する場合に，一致度は1.0となる．

次に，属性の少ない方の概念を$A$とし$(L \leqq M)$，概念$A$の属性を基準とする．
\[
 A=\{(a_1,u_1),(a_2,u_2),\cdots,(a_i,u_i),\cdots,(a_L,u_L)\}
\]

そして概念$B$の属性を，概念$A$の各属性との重み比率付一致度$\mathit{MatchWR}(a_i,b_{xi})$の和が最大になるように並び替える．
\[
 B_x = \{(b_{x1},v_{x1}),(b_{x2},v_{x2}),\cdots,(b_{xi},v_{xi}),\cdots,(b_{xL},v_{xL})\}
\]

これによって，概念$A$の一次属性と概念$B$の一次属性の対応する組を決める．対応にあふれた概念$B$の属性は無視する（この時点では組み合わせは$L$個）．
ただし，一次属性同士が一致する（概念表記が同じ）ものがある場合 ($a_i=b_j$) は，別扱いにする．これは概念ベースには約9万の概念が存在し，属性が一致することは稀であるという考えに基づく．
従って，属性の一致の扱いを別にすることにより，属性が一致した場合を大きく評価する．具体的には，対応する属性の重み$u_i$，$v_j$の大きさを重みの小さい方にそろえる．このとき，重みの大きい方はその値から小さい方の重みを引き，もう一度，他の属性と対応をとることにする．
例えば，$a_i=b_j$で$u_i=v_j+\alpha$とすれば，対応が決定するのは$(a_i,v_j)$と$(b_j,v_j)$であり，$(a_i,\alpha)$はもう一度他の属性と対応させる．このように対応を決めて，対応の取れた属性の組み合わせ数を$T$個とする．

重み比率付き関連度とは，重み比率付き一致度を比較する概念の各属性間で算出し，その和の最大値を求めることで計算する．これを以下の式\ref{eq:DoA}により定義する．
\begin{equation}
  DoA(A,B)=\sum_{i=1}^T\{MatchWR(a_i,b_{xi}) \times (u_i+v_{xi}) 
	\times (min(u_i,v_{xi})/max(u_i,v_{xi}))/2\}
	\label{eq:DoA}
\end{equation}

以下，重み比率付き関連度を関連度と略し，この関連度\cite{watabe:06}を用いる．関連度の値は概念間の関連の強さを0〜1の間の連続値で表す．1に近づくほど関連が強い．概念$A$と$B$に対して関連度計算を行った例を表\ref{table:degree_of_association}に挙げる．

最後に，概念「机」と「椅子」を例に用いて，関連度の計算例を説明する．概念「机」と「椅子」の一次属性および二次属性を表\ref{table:example_primary_attribute}，表\ref{table:example_secondary_attribute}に示す．

\begin{table}[b]
\begin{minipage}[t]{200pt}
\caption{関連度計算の例}
\input{05table02.txt}
\label{table:degree_of_association}
\end{minipage}
\hfill
\begin{minipage}[t]{200pt}
\caption{概念「机」と「椅子」の一次属性}
\input{05table03.txt}
\label{table:example_primary_attribute}
\end{minipage}
\end{table}

まず，概念「机」と「椅子」の一致度の計算を行う．例えば，概念「机」の一次属性「学校」
と概念「椅子」の一次属性「木」は，「木造」という共通する属性を持っているため，一致度は
以下のように計算される．
\[
 MatchWR(学校,木)=\min(0.2,0.4)=0.2
\]
同様に全ての一次属性の組み合わせについて一致度を計算した結果を表\ref{table:example_dom_matrix}に示す．
\begin{table}[t]
\begin{minipage}[t]{282pt}
\caption{概念「机」と「椅子」の二次属性}
\input{05table04.txt}
\label{table:example_secondary_attribute}
\end{minipage}
\hfill
\begin{minipage}[t]{118pt}
\setlength{\captionwidth}{118pt}
\hangcaption{概念「机」と「椅子」の一致度行列}
\input{05table05.txt}
\label{table:example_dom_matrix}
\end{minipage}
\end{table}

次に，関連度の計算を行う．関連度の計算は，まず属性が完全に一致している部分から行わ
れる．続いて，一致度の大きい部分から順に対応を決める．この場合，表\ref{table:example_dom_matrix}から一次属性「勉
強」と「勉強」，「学校」と「教室」，「本棚」と「勉強」の順に対応が決まることになる．結果，
関連度は次式のように計算される．
\begin{align*}
 DoA(机,椅子) & = 1.0\times(0.3+0.3)\times(0.3/0.3)/0.2+0.4\times(0.6+0.3)\times(0.3/0.6)/2	\\
	& \quad {} +0.1\times(0.1+0.2)\times(0.1/0.2)/2	\\
	& =0.3975
\end{align*}


本論文では，\ref{narrowing_node}節で述べる未知語とシソーラスのノードとの関連の強さの判断に関連度計算を用いる．


\subsection{属性の重み付け手法}\label{weighted_attribute}

本節では，本論文が提案する手法で用いる，対象としている文書に出現する単語の重み付け手法であるTF・IDF\cite{tokunaga:99}とSWeb-idf\cite{tuji:04}について述べる．

\subsubsection{TF・IDF}\label{tf_idf}

TF・IDFによる重み付けとは，対象としている単語の頻度と網羅性に基づいた重み付け手法である．文書$d$における索引語$t$の重み$w(t,d)$は以下の式\ref{eq:tf}によって得られる．
\begin{equation}
 w(t,d)=tf(t,d){\times}idf(t)
	\label{eq:tf}
\end{equation}
$\mathit{tf}(t,d)$は文書$d$における索引語$t$の出現頻度である．また，$\mathit{idf}(t)$は検索対象文書数$N$と索引語$t$が出現する文書の数$\mathit{df}(t)$によって決まり，式\ref{eq:idf}によって定義される．
\begin{equation}
 idf(t)=\log\frac{N}{df(t)}+1
	\label{eq:idf}
\end{equation}

本論文では，\ref{acquiring_attribute_of_node}節で述べるシソーラスのノード属性の概念化，および，\ref{determining_node}節で述べるノード動詞の構築にTF・IDFを用いている．

\subsubsection{SWeb-idf}\label{SWeb-idf}

SWeb-idf (Statics Web-Inverse Document Frequency) とは，Web上の単語のIDFを統計的に調べたIDF値である．まず，無作為に選んだ固有名詞1000語を作成する．
表\ref{table:proper_noun}に無作為に選択した
固有名詞の一部を示す．


\begin{table}[t]
\begin{minipage}[t]{200pt}
\caption{SWeb-idfの作成に用いた固有名詞（一部）}
\input{05table06.txt}
\label{table:proper_noun}
\end{minipage}
\hfill
\begin{minipage}[t]{200pt}
\caption{SWeb-idfの例}
\input{05table07.txt}
\label{table:SWeb-idf}
\end{minipage}
\end{table}

この作成した1000語に対して個々に検索エンジン\footnote{検索エンジンはgoogleを用いた．http://www.google.co.jp}で検索を行い，1語につき検索上位10件の検索結果ページの内容を取得する．
よって，得られた検索結果ページ数は10000ページとなる．
この10000ページから，複数の国語辞書や新聞などから概念（単語）を抽出した知識ベースで
ある概念ベースの収録語数である約9万語とほぼ同等の単語数が得られたことから，獲得した
10000ページをWebの全情報情報空間とみなしている．そして，その中での単語のIDF値を表すSWeb-idfは，式\ref{eq:SWeb-idf}で求められる．
\begin{equation}
 SWeb \verb|-| idf(t)=\log\frac{N}{df(t)}\hspace{2em}(N=10000)
	\label{eq:SWeb-idf}
\end{equation}

これらにより得られた単語とそのIDF値をデータベースに登録した．
なお$\mathit{df}(t)$項は，全文書空間（10000ページ）に出現する概念$t$の頻度である．
獲得したSWeb-idfの値の例を表\ref{table:SWeb-idf}に挙げる．
なお，固有名詞の選び方を変えてもSWeb-idfの値に大きな変化は見られないという報告がな
されている\cite{tuji:04}．

本論文では，\ref{acquiring_attribute_of_unknown_word}節で述べる未知語の概念化にSWeb-idfを用いている．


\section{未知語分類手法}\label{node_mapping}

本論文が提案する手法では，未知語を入力した後に，未知語とシソーラスのノードに対して関連度計算を行うために，未知語の概念化およびシソーラスのノードの概念化を行う．そして，概念化された未知語およびノードを用いて未知語が所属するシソーラスのノード決定を行う．

処理の流れとしては，まず，未知語を入力した後に，未知語とノードに対して関連度計算を行うために，未知語とノードの概念化を行う．次に，概念化された未知語およびノードに対して関連度計算を行い，所属候補ノードを絞り込む．さらに，ノード動詞および共起ヒットを用いて未知語が所属するべきノードを決定する．図\ref{fig:flow_node_mapping}に未知語をシソーラスのノードへ分類する流れを示す．

\begin{figure}[b]
\begin{center}
\includegraphics{15-3ia5f4.eps}
\end{center}
\caption{ノード決定の流れ}
\label{fig:flow_node_mapping}
\end{figure}



\subsection{未知語の概念化}\label{acquiring_attribute_of_unknown_word}

以下の手順により，未知語の概念化を行うために，未知語の属性とその重みをWebから獲得する．

\begin{enumerate}
\item 
入力された未知語をキーワードとして検索エンジンを用いて検索を行い，検索上位100件の検索結果ページの内容を取得する．
\item 
HTMLタグなど不要な情報を取り除いた文書群に対して，形態素解析ソフト「茶筌」\footnote{奈良先端科学技術大学院大学．http://chasen-legacy.sourceforge.jp/}を用いて形態素解析を行い，自立語を抽出する．

\item 
得られた自立語の中から概念ベースに存在する単語のみを未知語の属性として抜き出す．

\item 
得られた属性の頻度にSWeb-idf（\ref{SWeb-idf}節参照）の値を掛け合わせたものを属性の重みとし，得られた重み順に並び替える．なお，SWeb-idfのDBに存在しない属性については，Web上にあまり存在しない単語と考え，SWeb-idf値の最大値を掛け合わせている．
\end{enumerate}

表\ref{table:attribute_unknown_word}に未知語を概念化した例を示す．


\subsection{シソーラスのノードの概念化}\label{acquiring_attribute_of_node}

入力された未知語の属性とその重みはWebの検索を用いて獲得したが（\ref{acquiring_attribute_of_unknown_word}節参照），比較対象であるシソーラスのノードは概念ではないため，関連度計算による比較を行うことができない．そのため，シソーラスのノードの概念化を以下の手順で行う\cite{ito:04}．

\begin{table}[t]
\caption{未知語「Gショック」，「クイニーアマン」の属性とその重み（一部）}
\input{05table08.txt}
\label{table:attribute_unknown_word}
\end{table}

\begin{enumerate}
\item ノードに所属する全てのリーフに対して概念ベースを参照し，リーフを概念とみなすことでその一次属性を取得する（図\ref{fig:acquiring_attribute_of_node}）．

\begin{figure}[t]
\begin{center}
\includegraphics{15-3ia5f5.eps}
\end{center}
\caption{ノード「時計」の属性取得}
\label{fig:acquiring_attribute_of_node}
\end{figure}

\item (1) の作業を全てのノードに対して行う．

\item リーフを概念とみなすことで取得した一次属性に対して，TF・IDFを利用（\ref{tf_idf}節参照）して各属性の重みを求める．具体的には，取得した一次属性の重みをTFとみなし，また，全てのノードの数を式\ref{eq:idf}の$N$，取得した一次属性が出現するノードの数を式\ref{eq:idf}の$\mathit{df}(t)$とみなしてIDFを求める．そして，得られた重み順に属性を並び替える．
\end{enumerate}

表\ref{table:attribute_node}にシソーラスのノードの1つである「時計」を概念化した例を示す．

\begin{table}[t]
\caption{ノード「時計」の属性とその重み（一部）}
\input{05table09.txt}
\label{table:attribute_node}
\end{table}

\subsection{シソーラスのノードの絞込み}\label{narrowing_node}

以下の手順により，処理回数を少なくするためにシソーラスのノードの絞込みを行う．

\begin{enumerate}
\item
 \ref{acquiring_attribute_of_unknown_word}節で説明した手法を用いて，概念化を行った未知語$\mathit{query}$を式\ref{eq:query}で定義する．なお，$q_i$が属性，$w_i$がその重みである．
 \begin{equation}
	\mathit{query}=\{(q_1,w_1),(q_2,w_2), \cdots, (q_i,w_i), \cdots, (q_M,w_M) \}
	\label{eq:query}
 \end{equation}

\item
 シソーラスのノード集合$\mathit{NODE}$を式\ref{eq:node}で定義する．
 \begin{equation}
	\mathit{NODE}=\{(\mathit{node}_1),(\mathit{node}_2), \cdots, (\mathit{node}_{385}) \}
	\label{eq:node}
 \end{equation}

また，\ref{acquiring_attribute_of_node}節で説明した手法を用いて，概念化を行ったシソーラスのノード$\mathit{node}_i$を式\ref{eq:node_attribute}で定義する．なお，$n_{ij}$が属性，$w_{ij}$がその重みである．
 \begin{equation}
	\mathit{node}_i=\{(n_{i1}, w_{i1}),(n_{i2}, w_{i2}), \cdots, (n_{ij}, w_{ij}), 
		\cdots, (n_{iN}, w_{iN}) \}
	\label{eq:node_attribute}
 \end{equation}

\item
 概念化を行った未知語$\mathit{query}$とシソーラスの各ノード$\mathit{node}_i$に対して関連度計算を行い，関連度$\mathit{DoA}(\mathit{query}, \mathit{node}_i)$を求める．
そして，0.02以上の関連度を持つノードを所属候補ノードとする．よって，所属候補ノード集合$\mathit{NODE}'$は以下の式\ref{eq:candidate_node}で定義される．なお，関連度の閾値0.02は，0.0から0.05まで0.001毎に変化させて実験を行った結果，最も高い精度を得られた値を閾値として採用したものである．この実験については，\ref{threshold_evaluation}節で述べる．また，閾値によりノード数を385個から10個程度に絞り込むことができ，\ref{determining_node}節で述べるノード動詞や共起ヒットを用いる処理において，処理回数を20分に1以下にすることに成功している．
 \begin{equation}
	\mathit{NODE}'=\{\mathit{node}_i \mid \mathit{DoA}(\mathit{query}, \mathit{node}_i) \ge 0.02, \mathit{node}_i \in \mathit{NODE}\}
	\label{eq:candidate_node}
 \end{equation}
\end{enumerate}


\subsection{所属ノードの決定}\label{determining_node}

\ref{narrowing_node}節の処理により求めた所属候補ノード集合$\mathit{NODE}'$に対してノード動詞や共起ヒットを用いたノード決定を行う．

\subsubsection{ノード動詞}\label{node_verb}

NTTシソーラスは作成者がある分類基準に従って単語を体系的に分類したものである．そのため，NTTシソーラスには
「あるノードに所属するリーフは，そのリーフの直後に現れる助詞を伴う動詞が同じである」という関係が存在する．例えば，ノード「茶」に属するリーフ「番茶」や「麦茶」などには，「番茶を飲む」や「麦茶を飲む」など直後に現れる助詞を伴う動詞が共に「を飲む」であることが分かる．
ノード動詞とはこの関係を利用して，ノードに設定したキーワードのことであり，ノード決定に利用する．

具体的には，入力された未知語にノードごとに対応する助詞を伴う動詞（ノード動詞）を連結したキーワードを検索エンジンに入力し，HIT数を獲得する．そして，獲得したHIT数を\ref{determining_node_method}節で述べるノード得点の算出に利用する．
例えば，未知語が「マイルドセブン」，所属候補ノードが「たばこ」である場合，ノード「たばこ」のノード動詞である「を吸う」を連結した「マイルドセブンを吸う」というキーワードの検索を検索エンジンで行ったときのHIT数を求める．

以下にノード動詞の構築方法を示す．
\begin{enumerate}

\item ノードに属しているリーフをすべて抜き出す．

\item それぞれのリーフをキーワードとして検索エンジンで検索し，各リーフについて検索上位1000件の検索結果ページを取得する．そして，その文書内でキーワードの直後に出現する「格助詞+動詞（サ変名詞を含む）」部分を全て抜き出す．

\item (2) の操作を全てのノードに対して行う．

\item (3) で得られた「格助詞＋動詞（サ変名詞を含む）」に対して，TF・IDF（\ref{tf_idf}節参照）を利用して，重みを求める．具体的には，取得した「格助詞+動詞（サ変名詞を含む）」の数をTFとみなし，また，全てのノードの数を式\ref{eq:idf}の$N$，「格助詞+動詞（サ変名詞を含む）」が出現するノードの数を式\ref{eq:idf}の$\mathit{df}(t)$とみなしてIDFを求める．そして，最も大きな重みを持つ「格助詞＋動詞（サ変名詞を含む）」をノード動詞に決定する．

\end{enumerate}

表\ref{table:node_verb}に構築したノード動詞の例を示す．

\begin{table}[b]
\caption{ノード動詞（一部）}
\input{05table10.txt}
\label{table:node_verb}
\end{table}


\subsubsection{共起ヒット}\label{coincidence_hit}

「単語の意味は，どのような単語と共起するかという観点から特徴付けられる」というHarrisの分布仮説から\cite{harris:68}，関係のある2語は，ある文書に共に出現すると考えられる．そこで，未知語とノード名のAnd検索を検索エンジンを行い，HIT数を獲得する．そして，獲得したHIT数を\ref{determining_node_method}節で述べるノード得点の算出に利用する．
例えば，未知語が「マイルドセブン」，所属候補ノードが「たばこ」である場合，「マイルドセブン」と「たばこ」のAnd検索を検索エンジンで行ったときのHIT数を求める．


\subsection{所属ノード決定手法}\label{determining_node_method}

未知語の所属ノードを決定する計算式を式\ref{eq:determining_node_method}に示す\footnote{なお，logの計算はヒット数が3件以上のときに行い，2件以下の場合は1としている．}．所属候補ノード$\mathit{node}_i$の中でノード得点$\mathit{NodeValue}(\mathit{node}_i)$が最も高いノードを所属ノードとする．

$\mathit{DoA}(\mathit{query}, \mathit{node}_i)$は未知語$\mathit{query}$と$\mathit{node}_i$の関連度，$\mathit{VerbHit}(\mathit{node}_i)$は未知語にノード動詞を連結したキーワードの検索を検索エンジンで行ったときのHIT数，$\mathit{CoincidenceHit}(\mathit{node}_i)$は未知語とノード名のAnd検索を検索エンジンで行ったときのHIT数を表す．
\begin{equation}
\begin{aligned}[b]
 & NodeValue(node_i) \\
 & \quad = DoA(\mathit{query}, \mathit{node}_i) \times \log(\mathit{VerbHit}(\mathit{node}_i))
	\times\log(\mathit{CoincidenceHit}(\mathit{node}_i))
	\label{eq:determining_node_method}
\end{aligned}
\end{equation}

以下に未知語「Gショック」および「クイニーアマン」を例に，所属ノード決定手法におけ
る式\ref{eq:determining_node_method}の結果をノード得点上位5個まで例示したものを表\ref{table:calculation_example_doa}，
\ref{table:calculation_example_node_verb}，\ref{table:calculation_example_coincidence_hit}，
\ref{table:calculation_example_node_value}に示す．表\ref{table:calculation_example_doa}が
未知語とノード得点上位5個のノードとの関連度，表\ref{table:calculation_example_node_verb}が未知語のノード得点上位5個のノ
ードが持つノード動詞とノード動詞を用いたときのHIT数，表\ref{table:calculation_example_coincidence_hit}が共起ヒットを用いたとき
のHIT数，表\ref{table:calculation_example_node_value}が未知語のノード得点上位5個のノードに与えられたノード得点を表している．

\begin{table}[b]
\caption{所属ノード決定手法の計算例（関連度）}
\input{05table11.txt}
\label{table:calculation_example_doa}
\end{table}

\begin{table}[t]
\caption{所属ノード決定手法の計算例（ノード動詞）}
\input{05table12.txt}
\label{table:calculation_example_node_verb}
\end{table}
\begin{table}[t]
\caption{所属ノード決定手法の計算例（共起ヒット）}
\input{05table13.txt}
\label{table:calculation_example_coincidence_hit}
\end{table}
\begin{table}[t]
\caption{所属ノード決定手法の計算例（ノード得点）}
\input{05table14.txt}
\label{table:calculation_example_node_value}
\end{table}


\section{評価}\label{evaluation}

本論文で提案している手法の評価を行うために，20 人から各10 個ずつシソーラスに存在し
ない固有名詞とその固有名詞に対する正解ノードを重複することのないように記入してもらう
ことで，合計200 語の未知語を持つテストセットを作成した．なお，テストセットに用いる固
有名詞は，\ref{system}章で述べたように一意にその固有名詞の意味を判断できる，つまり，多義的な要
素を持たない固有名詞に限定している．
評価に使用したテストセットの一部を表\ref{table:testset}に示す．

\begin{table}[t]
\caption{テストセット（一部）}
\input{05table15.txt}
\label{table:testset}
\end{table}

テストセットの各未知語の入力に対して，本論文が提案する手法で出力した結果として，正解ノードを得た未知語を正解，得られなかった未知語を不正解として精度を算出する．


\subsection{閾値調査の評価}\label{threshold_evaluation}

\ref{narrowing_node}節で述べたシソーラスのノードの絞込みにおいて関連度の閾値を決定するために，閾値を0.0から0.05まで0.001毎に変化させて未知語の所属ノードの決定を行ったときの実験結果を図\ref{fig:changing_threshold}に示す．図\ref{fig:changing_threshold}より，関連度の閾値が0.014から0.02の間で，最も高い66.0\%の精度が得られている．そこで，その間で最もノードを絞り込むことができる関連度の0.02が閾値として適当であると考えられる．これをシソーラスのノードの絞込みを行う際に用いる閾値とした．

\begin{figure}[t]
\begin{center}
\includegraphics{15-3ia5f6.eps}
\end{center}
\caption{関連度の閾値の変化に伴う精度}
\label{fig:changing_threshold}
\end{figure}


\subsection{提案手法の評価}\label{unknown_word_evaluation}

提案手法を用いて，各未知語に対して分類するノードを出力する．なお，式\ref{eq:determining_node_method}では，ノード得点$\mathit{NodeValue}(\mathit{node}_i)$が最大となる第1位の候補のみ出力しているが，この実験では第1位の候補から第10位の候補まで出力している．評価結果を図\ref{fig:plural_precision}に示す．
なお，横軸は考慮した累積のノードの数を表している．また，縦軸は考慮しているノードの中に1つでも正解ノードを得た未知語を正解，1つも正解ノードを得られなかった未知語を不正解として算出した精度を表している．
図\ref{fig:plural_precision}より，第10位の候補まで出力することで9割を超える高い精度が得られていることから
判断できるように，結果として全体的に未知語と関連があると考えられるノードを獲得するこ
とができた．特に，第1位に関連が強いと考えられるノードが得られた場合，第2位から第5
位までに正解ノードが得られる傾向にあった．例えば，正解ノードが「教師」である未知語
「新島襄」を入力した場合，第1位に正解ノードである「教師」と関連が強い「教育」が得ら
れ，第2位に正解ノードである「教師」が得られた．

\begin{figure}[t]
\begin{center}
\includegraphics{15-3ia5f7.eps}
\end{center}
\caption{複数のノードを提示したときの精度}
\label{fig:plural_precision}
\end{figure}



\section{既存手法との比較}\label{related_research}

ここでは既存手法として，ベクトル空間法に基づく手法について説明する．この手法では，シソーラスにはNTTシソーラス\cite{NTT_Thesaurus:97}，学習データ及び未知語データにはEDRコーパス\cite{EDR:94}の共起辞書を用いている．EDRコーパスは22万文からなる文章のデータベースであり，係り受け関係にある単語対を抽出した共起辞書を用いている．

\subsection{ベクトル空間法に基づく手法}\label{vector_space_method}

ベクトル空間法に基づく手法は，シソーラスの各ノードの特徴ベクトルと未知語の特徴ベクトルの類似度をベクトル間の余弦を用いて算出し，類似度の高いノードに未知語を分類する．最も単純なベクトル空間法では，特徴ベクトルは名詞と動詞の共起頻度によるベクトルである．
ノードの特徴ベクトルの各要素は，そのノードに属する名詞と動詞との共起頻度を足し合わせたものであり，未知語の特徴ベクトルの各要素は，未知語と動詞の共起頻度そのものとなっている．以下に，ベクトル空間法を詳しく説明する．

ベクトル空間法では，式\ref{eq:vector_space_method1}，\ref{eq:vector_space_method2}，\ref{eq:vector_space_method3}によって未知語$\mathit{unknown}$を分類するノードが決定される．式\ref{eq:vector_space_method1}よりベクトル空間法では，未知語の特徴ベクトル$\mathit{vec}(\mathit{unknown})$と余弦の値が最高になる特徴ベクトル$\mathit{vec}(\mathit{node}_i)$に対応するノード$\mathit{node}_i$に未知語$\mathit{unknown}$を分類する．

式\ref{eq:vector_space_method1}，\ref{eq:vector_space_method2}，\ref{eq:vector_space_method3}についての説明を行う．まず，シソーラスに既に分類されている名詞（リーフ）$\mathit{noun}_i$の集合$\mathit{NOUN}$，シソーラスのノード$\mathit{node}_i$の集合$\mathit{NODE}$，共起を考慮する動詞$\mathit{verb}_i$の集合$\mathit{VERB}$を以下に定義する．また，$\mathit{unknown}$は未知語を表している．
\begin{align*}
 \mathit{NOUN} & =\{\mathit{noun}_1, \mathit{noun}_2, \cdots, \mathit{noun}_i, \cdots, \mathit{noun}_{\mathit{noun}\_\mathit{num}} \} \\
 \mathit{NODE} & =\{\mathit{node}_1, \mathit{node}_2, \cdots, \mathit{node}_i, \cdots, \mathit{node}_{\mathit{node}\_\mathit{num}} \} \\
 \mathit{VERB} & =\{\mathit{verb}_1, \mathit{verb}_2, \cdots, \mathit{verb}_i, \cdots, \mathit{node}_{\mathit{verb}\_\mathit{num}} \} 
\end{align*}
次に，ノード$w$と動詞$z$が共起したことを表す1つの学習データを以下に定義する．
\[
 \{ (w,z) \mid w{\in}\mathit{NODE}, z{\in}\mathit{VERB} \} 
\]
$(w,z)^N$は$N$個の学習データからなる系列である．学習データを生成するために用いる元々の文章の中では，名詞$\mathit{noun}_i$と動詞$\mathit{verb}_i$が共起しているが，学習データを生成する時点で名詞と動詞の二項組$(\mathit{noun}_i, \mathit{verb}_j)$をノードと動詞の二項組$(\mathit{node}_k, \mathit{verb}_j)$に変換する．なお，ノード$\mathit{node}_k$は名詞$\mathit{noun}_i$が属するノードであり，複数のノードに属する場合は複数の二項組に変換する．
したがって，未知語$\mathit{unknown}$が属するノード$\mathit{node}^*$と未知語$\mathit{unknown}$と共起した動詞$y$の系列$y^M$は，以下のように表すことができる．
\[
 \{ (\mathit{node}^*, y^M) \mid \mathit{node}^* {\in} \mathit{NODE}, y {\in} \mathit{VERB} \} 
\]
しかし，$\mathit{node}^*$は未知であり，実際に観測される未知語データは未知語$\mathit{unknown}$と共起した動詞$y$の系列$y^M$の二項組$(\mathit{unknown}, y^M)$である．よって，未知語分類問題は学習データ$(w,z)^N$と未知語データ$(\mathit{unknown}, y^M)$を観測したもとで未知語$\mathit{unknown}$が属するノード$\mathit{node}^*$を推定する問題となる．

$d_{\cos} \{ (w,z)^N , (\mathit{unknown}, y^M) \}$は，学習データ$(w,z)^N$と未知語データ$(\mathit{unknown}, y^M)$を引数に取り，未知語$\mathit{unknown}$を分類するべきノードを決定する関数を表す．$\mathit{vec}(\mathit{node}_i)$はノード$\mathit{node}_i$の特徴ベクトル，$\mathit{vec}(\mathit{unknown})$は未知語$\mathit{unknown}$の特徴ベクトルである．
また，$\mathit{co}\bigl( (\mathit{node}_i,\mathit{verb}_i) \mid (w,z)^Z \bigr)$は学習データ$(w,z)^N$中の$(\mathit{node}_i, \mathit{verb}_j)$の数でノード$\mathit{node}_i$と動詞$\mathit{verb}_j$が共起した回数，$\mathit{co}(\mathit{verb}_i \mid y^M)$は未知語データ$(\mathit{unknown}, y^M)$の$y^M$中の$\mathit{verb}_i$の数で未知語$\mathit{unknown}$と動詞$\mathit{verb}_i$が共起した回数を表す．
$\cos$はベクトル間の余弦の値を求める関数，$\mathit{vec}_A \cdot \mathit{vec}_B$はベクトル$\mathit{vec}_A, \mathit{vec}_B$間の内積，$\parallel \mathit{vec} \parallel$はベクトル$\mathit{vec}$のノルムである．
{\allowdisplaybreaks
\begin{gather}
\begin{aligned}[b]
 d_{\cos} \{ (w,z)^N , (\mathit{unknown}, y^M) \}
	& = \arg \max_{\mathit{node}_i} \{ \cos (\mathit{vec}(\mathit{node}_i), 
	\mathit{vec}(\mathit{unknown})) \} \\
	& = \arg \max_{\mathit{node}_i} \left\{ \frac{\mathit{vec}(\mathit{node}_i) 
	\cdot \mathit{vec}(\mathit{unknown})}
		{ \parallel \mathit{vec}(\mathit{node}_i) \parallel \parallel 
	\mathit{vec}(\mathit{unknown}) \parallel} \right\}
	\label{eq:vector_space_method1}
\end{aligned}\\
\begin{aligned}[b]
 \mathit{vec}(\mathit{node}_i) = 
	& \bigl\{ \mathit{co}\bigl( (\mathit{node}_i,\mathit{verb}_1) \mid (w,z)^Z \bigr), 
	\mathit{co}\bigl( (\mathit{node}_i,\mathit{verb}_2) 
	\mid (w,z)^Z \bigr), 
	\cdots, \\
	& \mathit{co}\bigl( (\mathit{node}_i,\mathit{verb}_i) \mid (w,z)^Z \bigr), 
	\cdots, \mathit{co}\bigl( (\mathit{node}_i,\mathit{verb}_{\mathit{verb}\_\mathit{num}}) 
	\mid (w,z)^Z \bigr)\bigr\}
	\label{eq:vector_space_method2}
\end{aligned}\\
\begin{aligned}[b]
 \mathit{vec}(\mathit{node}_i) = 
	& \bigl\{ \mathit{co}(\mathit{verb}_1 \mid y^M), \mathit{co}(\mathit{verb}_2 \mid y^M)), 
	\cdots, \\
	& \mathit{co}(\mathit{verb}_i \mid y^M) \cdots, \mathit{co}
	(\mathit{verb}_{\mathit{verb}\_\mathit{num}} \mid y^M) \bigr\}
	\label{eq:vector_space_method3}
\end{aligned}
\end{gather}
}

なお，上記のような単純に共起頻度を用いるベクトル空間法以外に，各共起頻度に重み付けを行うTF・IDF法を導入したベクトル空間法も提案されており，情報検索などの分野において実用化されている手法は，TF・IDF法を導入したベクトル空間法である\cite{witten:99}．
TF・IDF法を導入したベクトル空間法では，式\ref{eq:vector_space_method2}および式\ref{eq:vector_space_method3}において，特徴ベクトルの第$i$要素に$\log\frac{\mathit{node}\_\mathit{num}}{a(\mathit{verb}_i)}$を掛け合わせたものを特徴ベクトルとして採用し，その上で式\ref{eq:vector_space_method1}を用いて未知語の分類を行う．ただし，$a(\mathit{verb}_i)$は動詞$\mathit{verb}_i$との共起頻度が1以上のノードの数である．


\subsection{比較評価}\label{comparison_with_related research}

比較実験の方法を以下に示す\cite{maeda:00}．
\begin{enumerate}

\item NTTシソーラスに既に分類されている名詞（リーフ）の中で概念ベース（\ref{concept_base}節参照）に存在する単語から1000語を未知語と仮定して抽出する．

\item NTTシソーラスに属している残りのリーフとEDRコーパス頻出動詞上位500語との共起回数を算出し，学習データを作成する．

\item さらに，NTTシソーラスから取り出しておいた1000語の未知語について，学習データと同様にEDRコーパス頻出動詞上位500語との共起回数を共起辞書から算出し，1000個の未知語データを作成する．

\item 学習データと未知語データをもとにベクトル空間法（式\ref{eq:vector_space_method1}）を用いて，各未知語に対する所属ノードを出力する．また，本論文で提案する手法（式\ref{eq:determining_node_method}）を用いて，各未知語に対する所属ノードを出力する．

\end{enumerate}

抽出された未知語とその未知語が所属するノードの例を表\ref{table:testset_leaf}に示す．


\begin{table}[b]
\caption{未知語と仮定してNTTシソーラスから抽出したリーフ（一部）}
\input{05table16.txt}
\label{table:testset_leaf}
\end{table}

図\ref{fig:comparison_result}に実験結果を示す．
図\ref{fig:comparison_result}のCosは共起頻度のみによるベクトル空間法，TF・IDFはTF・IDF法を導入したベクトル空間法，提案手法が本論文で提案している手法に対応する．

本実験において，未知語が元のNTTシソーラスにおいて分類されていたノードに分類できた場合を正解とする．また，未知語が複数のノードに所属していた場合には，出力したノードがその中のどれか1つと一致すれば，正解とみなしている．
なお，図\ref{fig:plural_precision}と同様に，横軸は考慮した累積のノードの数を表している．また，縦軸は考慮しているノードの中に1つでも正解ノードを得た未知語を正解，1つも正解ノードを得られなかった未知語を不正解として算出した精度を表している．

\begin{figure}[t]
\begin{center}
\includegraphics{15-3ia5f8.eps}
\end{center}
\caption{手法ごとの精度}
\label{fig:comparison_result}
\end{figure}

図\ref{fig:comparison_result}より，提案手法の精度は共起頻度によるベクトル空間法 (Cos) より13〜30\%高く，
TF・IDF法を導入したベクトル空間法 (TF・IDF) に対しても10〜20\%高くなっており，
提案手法がベクトル空間法に基づく手法よりも優れた結果を示している．

本来，本論文で提案している手法は，固有名詞を中心とする既存のシソーラスに分類されていない未知語に対して有効な手法である．
その一方で，本実験で用いた既存のシソーラス（NTTシソーラス）から抽出した仮想的な未知語の実体は一般的な単語である．
一般的な単語は多くの文書で使
用されるため，\ref{acquiring_attribute_of_unknown_word}節で説明した手法を用いると，広範囲にわたるページから属性を獲得する
ことになる．その結果，獲得できる属性にばらつきが生じ，適切な属性を獲得することが困難
である．
そのため，本論文で提案している手法は，本実験に対しては不利な部分があるといえる．

この点を踏まえると，本実験において不利な部分を持っているにも関わらず，提案手法は良好な結果が得られたといえる．したがって，本論文で提案する手法が未知語に限らず，一般的な単語に対しても柔軟に機能することを示しているといえる．


\section{おわりに}

本論文では，ある概念から様々な概念を連想できる連想メカニズムを基に，シソーラスに定義されてない単語（未知語）が大局的に見てどういうものであるかを，シソーラスのノードに分類して提示する手法を提案した．さらに，連想メカニズムに未知語とシソーラスの体系的特徴を利用した共起頻度を組み合わせることで精度向上を図る手法を考え，その有効性を実験によって検証した．

結果として，第10位の候補まで出力することで未知語を9割を超える精度で正しいシソーラスのノードに分類することに成功し，未知語を分類するべきシソーラス上の最適なノードを提示できることを示した．さらに，第1位の候補のみを出力した場合，66.0\%の精度が得られたことから，未知語をシソーラスに自動的に分類でき，シソーラスの自動構築にもつながると考えられる．

今後の研究課題としては，得られた10個の候補から正解ノードを絞り込む方法について検討する．
さらに，未知語を分類するべきノードを絞り込んだ後，前後の文脈から正解ノードを決定
する方法を検討していきたい．また，未知語により適切な属性を与えるために，検索キーワー
ドに対して適切なキーワードを追加するなど，より適切な検索結果ページを獲得する方法につ
いても検討する必要がある．これにより，文中に多義的な要素を持つ未知語が含まれる場合で
も，未知語をノード名に正しく置き換えることで，円滑な自然言語処理を行うことができると
期待される．



\acknowledgment

本研究は文部科学省からの補助を受けた同志社大学の学術フロンティア研究プロジェクトにおける研究の一環として行った．


\bibliographystyle{jnlpbbl_1.3}
\begin{thebibliography}{}

\bibitem[\protect\BCAY{別所\JBA 内山\JBA 片岡}{別所\Jetal }{2004}]{bessho:06}
別所克人\JBA 内山俊郎\JBA 片岡良治 \BBOP 2004\BBCP.
\newblock \JBOQ 単語・意味属性間共起に基づく概念ベースの拡張方式\JBCQ\
\newblock \Jem{情報処理学会研究報告}, {\Bbf 2006}  (84), \mbox{\BPGS\ 29--34}.

\bibitem[\protect\BCAY{G.A.Miller}{G.A.Miller}{1995}]{G.A.Miller:95}
G.A.Miller \BBOP 1995\BBCP.
\newblock \BBOQ WordNet: A lexical database for English\BBCQ\
\newblock {\Bem Commun. ACM}, {\Bbf 38}  (11), \mbox{\BPGS\ 39--41}.

\bibitem[\protect\BCAY{I.H.Witten, A.Moffat, \BBA\ T.C.Bell}{I.H.Witten
  et~al.}{1999}]{witten:99}
I.H.Witten, A.Moffat, \BBA\ T.C.Bell \BBOP 1999\BBCP.
\newblock {\Bem Managing Gigabytes}.
\newblock Morgan Kaufmann Pub.

\bibitem[\protect\BCAY{池原\JBA 宮崎\JBA 白井\JBA 横尾\JBA 中岩\JBA 小倉\JBA
  大山\JBA 林}{池原\Jetal }{1997}]{NTT_Thesaurus:97}
池原悟\JBA 宮崎正弘\JBA 白井諭\JBA 横尾昭男\JBA 中岩浩巳\JBA 小倉健太郎\JBA
  大山芳史\JBA 林良彦 \BBOP 1997\BBCP.
\newblock \Jem{日本語語彙大系}.
\newblock 岩波書店.

\bibitem[\protect\BCAY{伊藤\JBA 渡部\JBA 河岡}{伊藤\Jetal }{2004}]{ito:04}
伊藤俊介\JBA 渡部広一\JBA 河岡司 \BBOP 2004\BBCP.
\newblock \JBOQ
  情報検索における未知語理解支援方式〜未知語のシソーラスノードへの分類〜\JBCQ\
\newblock \Jem{情報処理学会自然言語処理研究会資料}, {\Bbf 2004-NL-159}  (1),
  \mbox{\BPGS\ 61--66}.

\bibitem[\protect\BCAY{前田}{前田}{2000}]{maeda:00}
前田康成 \BBOP 2000\BBCP.
\newblock \JBOQ
  統計的決定理論に基づく既存名詞シソーラスへの未知語登録方法に関する考察\JBCQ\
\newblock \Jem{電子情報通信学会論文誌}, {\Bbf J83-A}  (6), \mbox{\BPGS\
  702--710}.

\bibitem[\protect\BCAY{日本電子辞書研究所}{日本電子辞書研究所}{1994}]{EDR:94}
日本電子辞書研究所 \BBOP 1994\BBCP.
\newblock \Jem{EDR電子化辞書利用マニュアル第2.1版}.

\bibitem[\protect\BCAY{奥村\JBA 土屋\JBA 渡部\JBA 河岡}{奥村\Jetal
  }{2007}]{okumura:07}
奥村紀之\JBA 土屋誠司\JBA 渡部広一\JBA 河岡司 \BBOP 2007\BBCP.
\newblock \JBOQ 概念間の関連度計算のための大規模概念ベースの構築\JBCQ\
\newblock \Jem{自然言語処理}, {\Bbf 14}  (5), \mbox{\BPGS\ 41--64}.

\bibitem[\protect\BCAY{榊\JBA 松尾\JBA 内山\JBA 石塚}{榊\Jetal
  }{2007}]{sakaki:07}
榊剛史\JBA 松尾豊\JBA 内山幸樹\JBA 石塚満 \BBOP 2007\BBCP.
\newblock \JBOQ Web上の情報を用いた関連語のシソーラスの構築について\JBCQ\
\newblock \Jem{自然言語処理}, {\Bbf 14}  (2), \mbox{\BPGS\ 3--31}.

\bibitem[\protect\BCAY{田中\JBA 仁科}{田中\JBA 仁科}{1987}]{tanaka:87}
田中穂積\JBA 仁科喜久子 \BBOP 1987\BBCP.
\newblock \JBOQ 上位／下位関係シソーラスISAMAPの作成[I] [II]\JBCQ\
\newblock \Jem{情報処理学会自然言語処理研究会}, {\Bbf 64}  (4), \mbox{\BPGS\
  25--44}.

\bibitem[\protect\BCAY{徳永}{徳永}{1999}]{tokunaga:99}
徳永健伸 \BBOP 1999\BBCP.
\newblock \Jem{情報検索と言語処理}.
\newblock 東京大学出版会.

\bibitem[\protect\BCAY{辻\JBA 渡部\JBA 河岡}{辻\Jetal }{2004}]{tuji:04}
辻泰希\JBA 渡部広一\JBA 河岡司 \BBOP 2004\BBCP.
\newblock \JBOQ wwwを用いた概念ベースにない新概念およびその属性獲得手法\JBCQ\
\newblock \Jem{第18回人工知能学会全国大会論文集}, \mbox{\BPGS\ 2D1--02}.

\bibitem[\protect\BCAY{浦本}{浦本}{1996}]{uramoto:96}
浦本直彦 \BBOP 1996\BBCP.
\newblock \JBOQ
  コーパスに基づくシソーラス—統計情報を用いた既存のシソーラスへの未知語の配置
\JBCQ\
\newblock \Jem{情報処理学会論文誌}, {\Bbf 37}  (12), \mbox{\BPGS\ 2182--2189}.

\bibitem[\protect\BCAY{渡部\JBA 奥村\JBA 河岡}{渡部\Jetal }{2006}]{watabe:06}
渡部広一\JBA 奥村紀之\JBA 河岡司 \BBOP 2006\BBCP.
\newblock \JBOQ 概念の意味属性と共起情報を用いた関連度計算方式\JBCQ\
\newblock \Jem{自然言語処理}, {\Bbf 13}  (1), \mbox{\BPGS\ 53--74}.

\bibitem[\protect\BCAY{渡部\JBA 河岡}{渡部\JBA 河岡}{2001}]{watabe:01}
渡部広一\JBA 河岡司 \BBOP 2001\BBCP.
\newblock \JBOQ 常識的判断のための概念間の関連度評価モデル\JBCQ\
\newblock \Jem{自然言語処理}, {\Bbf 8}  (2), \mbox{\BPGS\ 39--54}.

\bibitem[\protect\BCAY{Z.S.Harris}{Z.S.Harris}{1968}]{harris:68}
Z.S.Harris \BBOP 1968\BBCP.
\newblock {\Bem Mathematical Structures of Language(Interscience Tracts in Pure
  and Applied Mathematics 21)}.
\newblock Interscience.

\end{thebibliography}

\begin{biography}
\bioauthor{後藤　和人}{
2006年同志社大学工学部知識工学科卒業．
同大学院工学研究科知識工学専攻博士前期課程在学．
知識情報処理の研究に従事．
}

\bioauthor{土屋　誠司}{
2000年同志社大学工学部知識工学科卒業．
2002年同大学院工学研究科知識工学専攻博士前期課程修了．
同年，三洋電気株式会社入社．
2007年同志社大学大学院工学研究科知識工学専攻博士後期課程修了．
同年，徳島大学大学院ソシオテクノサイエンス研究部助教．
主に，知識処理，概念処理，意味解釈の研究に従事．
言語処理学会，人工知能学会，情報処理学会，電子情報通信学会各会員．
}

\bioauthor{渡部　広一}{
1983年北海道大学工学部精密工学科卒業．
1985年同大学院工学研究科情報工学専攻修士課程修了．
1987年同精密工学専攻博士後期課程中途退学．
同年，京都大学工学部助手．
1994年同志社大学工学部専任講師．
1998年同助教授．工学博士．
現在同教授．
主に，進化的計算法，コンピュータビジョン，概念処理などの研究に従事．
言語処理学会，人工知能学会，情報処理学会，電子情報通信学会，システム制御情報学会，精密工学会各会員．
}

\bioauthor{河岡　　司}{
1966年大阪大学工学部通信工学科卒業．
1968年同大学院修士課程修了．
同年，日本電信電話公社入社，情報通信網研究所知識処理研究部長，
NTTコミュニケーション科学研究所所長を経て，現在同志社大学工学部教授．
工学博士．
主にコンピュータネットワーク，知識情報処理の研究に従事．
言語処理学会，人工知能学会，情報処理学会，電子情報通信学会，IEEE (CS) 各会員．
}

\end{biography}


\biodate


\end{document}
