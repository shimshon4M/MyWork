\documentstyle[theapa]{jnlp_b5e}

\setcounter{page}{21}
\setcounter{巻数}{1}
\setcounter{号数}{1}
\受付{4}{1}{31}
\採録{4}{7}{15}

\setcounter{secnumdepth}{2}

\title{}
\author{}
\jkeywords{}

\etitle{A Comparative Study of Automatic Extraction\\
of Collocations from Corpora:\\
Mutual Information vs. Cost Criteria}

\eauthor{Kenji Kita\affiref{TokushimaUniv}   \and
        Yasuhiko Kato\affiref{Kokken}        \and
        Takashi Omoto\affiref{TokushimaUniv} \and
        Yoneo Yano\affiref{TokushimaUniv}}

\headauthor{Kita,~K.~et~al.}
\headtitle{A Comparative Study of Automatic Extraction of Collocations}

\affilabel{TokushimaUniv}
          {Tokushima University, Faculty of Engineering}
          {Tokushima University, Faculty of Engineering}
\affilabel{Kokken}
          {The National Language Research Institute, Section for Dictionary Research}
          {The National Language Research Institute, Section for Dictionary Research}


\eabstract{
While corpus-based studies are now becoming a new methodology
in natural language processing,
second language learning offers one interesting potential application.
In this paper, we are primarily concerned with the acquisition
of collocational knowledge from corpora
for use in language learning.
First we discuss the importance of collocational knowledge
in second language learning,
and then take up two measures, mutual information and cost criteria,
for automatically identifying or extracting collocations from corpora.
Comparative experiments are made between the two measures
using both Japanese and English corpora.
In our experiments,
the cost criteria measure proved more effective in extracting interesting collocations
such as fundamental idiomatic expressions and phrases.
}

\ekeywords{Corpus, Collocations, Mutual Information,
Cost Criteria, Second Language Learning}


\begin{document}

\maketitle

\section{Introduction}

Recent rapid advances in computer technology,
especially the advent of large storage devices and parallel computers,
and numerous data collection efforts
have caused a shift in natural language applications
from a knowledge-based to a corpus-based or data-intensive approach.
The knowledge-based approach focused on abstraction of language,
describing linguistic phenomena through minimal core knowledge
such as parts-of-speech, syntactic and semantic rules.
Linguistic phenomena, however, vary so vastly that
they cannot be described through core knowledge.
In addition, hand-coding knowledge takes a lot of time and hard work.
The knowledge-based approach, therefore, has been found wanting
in developing large-scale practical NLP systems.

On the other hand,
the corpus-based approach makes no claim about the compactness
of the knowledge.
Rather, the corpus-based approach derives more power
from massive quantities of textual data than from hand-coded knowledge,
being able to compensate for the weakness of the knowledge-based approach
through authentic examples and various statistics of language use.
With the availability of large corpora in recent years,
many successful results have been derived from corpus-based studies.
These include part-of-speech tagging \cite{Kupiec92},
parsing \cite{Magerman90},
example-based machine translation\cite{Sumita92},
statistical machine translation \cite{Brown90,Brown93},
language modeling \cite{Jelinek90,Kita92}
and many other related areas.

One interesting potential use of corpora
is for second language learning.
Kita et al. \cite{Kita93b} discussed various way of using corpora
in language learning,
which includes translation skill acquisition through bilingual corpora,
oral/aural skill acquisition through speech corpora,
multimedia language learning through structured corpora, and so forth.
The greatest advantage of using corpora in language learning is that
the corpora provide a body of evidence for the function and usage of words
and expressions.
At the same time,
deriving lexical knowledge
from large-scale corpora via automated procedures,
as well as its use in language learning CAI systems,
is one of the most important issues.

In this paper, we are primarily concerned with the acquisition
of collocational knowledge from corpora.
In Section~2, we describe why collocational knowledge is
important in second language learning.
In Section~3, we discuss the automatic extraction of collocations,
taking up two measures, mutual information and cost criteria,
for identifying or extracting collocations from corpora.
In Section~4, we describe comparative experiments in extracting collocations
and discuss the two measures.


\section{Importance of Collocational Knowledge in Language Learning}

There has been much theoretical and applied research
on collocations,
both from a linguistic and an engineering point of view.
Consequently, the definition of collocation differs according to
the researcher's interest and standpoint.
This paper adopts the most comprehensive definition:
a collocation is a cohesive word cluster,
including idioms, frozen expressions and compound words.

The importance of collocations has been stressed
in an extensive literature.
From a language learning viewpoint, it can be summarized as follows:

\begin{itemize}
\item
In language learning, learners must pay attention to
how words are used rather than to individual words by themselves.
Collocational knowledge indicates
which words co-occur frequently with other words and
how they combine within a sentence.
Therefore, collocational knowledge is especially effective
in sentence generation
\cite{Smadja90}.

\item
Collocational knowledge is very difficult
to acquire for second language learners.
A typical example is the pair of words ``strong'' and ``powerful''
\cite{Church91,Smadja91}.
These two words have similar meanings,
but their usage is quite different.
For example, native English speakers prefer saying
``strong tea'' to ``powerful tea'',
and prefer saying ``powerful car'' to ``strong car''.
For non-natives, however,
it is difficult to catch the subtle distinctions between these two words.
These lexical preferences were sometimes ignored
in the traditional knowledge-based approach;
nevertheless they are the most important source
for word choice and word ordering.

\item
It is pointed out that human translation process
is based on analogical thinking \cite{Nagao84}.
First, a human translator properly decomposes a sentence
into certain fragmental phrases, then s/he translates
each fragmental phrase by analogy with other examples,
and finally composes fragmental translations into one sentence.
Recently, following this idea,
example-based machine translations (EBMT) have been widely explored.
In some EBMT systems, several kinds of translation knowledge
are utilized, such as the string-, pattern-, and grammar-level
knowledge types.
Collocations are particularly suitable for translation units
in either the string- or pattern-level knowledge.


\item
From a cognitive point of view,
it is said that human language acquisition is governed by
the law of maximal efficiency \cite{Wolff91}.
In other words, data compression, often called chunking, is performed to
minimize storage demands in the brain.
A chunk is considered to be a pattern which repeatedly appears
in a variety of contexts.
Collocations are good candidates for chunk units.
\end{itemize}


\section{Extracting Collocations from Corpora}

In the past, several approaches have been proposed to
extract collocations from corpora.
Church et al. \cite{Church90,Church91} introduced the association ratio,
which indicates how strongly two words are related,
based on the information-theoretic concept of mutual information.
Smadja et al. \cite{Smadja90,Smadja91} take into account word distance
as well as word strength for a measure of word association.
Also, Basili et al. \cite{Basili92} proposed a syntax-based approach.
Particularly, mutual information plays a central role
in recent lexical statistical research.
To take a few examples,
Hindle and Rooth \cite{Hindle93} applied mutual information to disambiguate
prepositional phrase attachments,
and Brown et al. \cite{Brown92} used it in determining word classes.

In this section, after surveying how mutual information
can be used to extract collocational information,
we introduce another measure, called {\em cost criteria} \cite{Kita93},
to automatically extract interesting collocations
from corpora.
Comparative experiments and discussions will be described
in the next section.

\subsection{Mutual Information}

The mutual information between two words $x$ and $y$ is defined
as follows \cite{Church90,Church91}:
\begin{equation}
  I(x,y) = \log \frac{P(x,y)}{P(x)P(y)}
\end{equation}
Here, $P(x)$ and $P(y)$ are word occurrence probabilities,
and can be estimated from the number of occurrences of the words, $f(x)$ and $f(y)$,
and the number of words in the corpus, $N$.
\begin{equation}
  P(x) = \frac{f(x)}{N} \,\,\,\,\, \mbox{and} \,\,\,\,\, P(y) = \frac{f(y)}{N}
\end{equation}
$P(x,y)$, the joint probability of $x$ and $y$, is estimated in a similar way.
\begin{equation}
  P(x,y) = \frac{f(x,y)}{N}
\end{equation}
where $f(x,y)$ is the number of occurrences of
$x$ followed by $y$.

The mutual information $I(x,y)$ compares the probability
of observing $x$ and $y$ together
with the probabilities of observing $x$ and $y$ simply by chance.
Thus, a large value indicates that the two words $x$ and $y$
have a strong relationship.
By extracting word pairs with large mutual information values,
we can obtain common collocations.

Because mutual information values are defined for two words,
this simple method can only extract collocations of length two.
However, a generalization is suggested in \cite{Jelinek90} as follows:
\begin{enumerate}
\item   Start out from the basic vocabulary $V_{0}$. Set $n = 0$.
\item   Augment the vocabulary $V_{n}$ by all word sequences ``$x$ $y$''
        for which $I(x,y) > Thr$, where $Thr$ is a predetermined threshold.
\item   From Step~2, a new vocabulary $V_{n+1}$ is established.
\item   Adjust the vocabulary size $N$ to reflect the new vocabulary $V_{n+1}$.
\item   Resume from Step~1 with $V_{n+1}$ as its basis.
\end{enumerate}
With this iterative procedure, the final vocabulary includes
collocations of arbitrary length.

\subsection{Cost Criteria}

The cost criteria measure is based on the assumptions that
(1) collocations are recurrent word sequences,
and (2) the recurrent property is captured by
the absolute frequency of a word sequence.
However, a simple absolute frequency approach does not work,
because the frequency of a sub-sequence is always higher
than that of the original word sequence.
Therefore, as a measure for identifying or extracting collocations,
absolute frequency is not appropriate.
Instead, we consider a processing cost for a word sequence,
and introduce cost criteria 
which can quantitatively estimate
the extent to which processing is reduced
by considering a word sequence as one unit.

Before the presentation of a formal definition,
we introduce the following notation:
\begin{eqnarray}
        \alpha    & \cdots & \mbox{a word sequence.}\\
        |\alpha|  & \cdots & \mbox{the length of $\alpha$.}\\
                  &        & \mbox{(the number of words in $\alpha$)} \nonumber \\
        f(\alpha) & \cdots & \mbox{number of occurrences of $\alpha$ in a corpus.}
\end{eqnarray}

We define a {\em reduced cost} $K(\alpha)$ for a sequence $\alpha$ as:
\begin{equation}
        K(\alpha) = (|\alpha| - 1) \times f(\alpha)
\end{equation}
$K(\alpha)$ is interpreted as follows.
Assume here that, in the corpus, there exists a word sequence $\alpha$,
which is composed of $|\alpha|$ words and occurs $f(\alpha)$ times.
Also assume that the cost of processing one word is 1.
Similarly, when processing $\alpha$ as a single unit,
its processing cost is 1.
Here, we assume that:
if a word sequence is processed one word at a time,
its processing cost is proportional to its length.
This assumption is made because of the following reason.
We are now concerned with the problem of identifying the sequence $\alpha$
as a collocation.
But since this problem is reduced to a string pattern matching problem,
it is natural to assume that
the cost is proportional to the length of the sequence $\alpha$.
That is, the processing cost for $\alpha$ is $|\alpha|$.
By considering $\alpha$ as one unit, the processing cost is reduced
to $|\alpha|-1$.
Since $\alpha$ appears $f(\alpha)$ times,
we can conclude that the total reduced cost becomes
$(|\alpha| - 1) \times f(\alpha)$, which is the definition of $K(\alpha)$.

In reality, however, the problem is not so simple,
because word sequences are not mutually disjoint.
Consider the case where a word sequence $\alpha$ is a sub-sequence of $\beta$
(for example, $\alpha = \mbox{``in spite''}$, $\beta = \mbox{``in spite of''}$).
Then, we have:
\begin{equation}
        f(\alpha) \geq f(\beta)
\end{equation}
Further, the word sequence $\alpha$,
$f(\beta)$ times out of $f(\alpha)$ times, will be identified as $\beta$.
Thus, the actual reduced cost for $\alpha$ is defined as:
\begin{equation}
        K(\alpha) = (|\alpha| - 1) \times (f(\alpha) - f(\beta))
\label{Eq:Kmod}
\end{equation}

Finally, we can extract collocations from a corpus by the following steps:
\begin{enumerate}
\item   Calculate $K(\alpha)$ for each word sequence $\alpha$ in a corpus.
\item   Rank a word sequence $\alpha$ by using the value $K(\alpha)$.
\item   Extract higher rank word sequences as collocation candidates.
\item   Re-calculate $K(\alpha)$ for each $\alpha$ in the collocation candidates.
To be more precise,
by checking the sequence/sub-sequence relation
between every two word sequences in the collocation cancidates,
modify the $K(\alpha)$ values according to Equation~\ref{Eq:Kmod}.
\end{enumerate}


\section{Experiments and Discussions}

\subsection{The ADD Corpus}

In our experiments, the ADD (ATR Dialogue Database) Corpus \cite{Ehara90}
created by ATR Interpreting Telephony Research Laboratories in Japan
was used.
The ADD Corpus is a large structured database of dialogues collected
from simulated telephone or keyboard conversations
which are spontaneously spoken or typed in Japanese or English.
This corpus consists of parallel texts of Japanese and English,
aligned by utterance.
Also, sentences in ADD are morphologically analyzed
and annotated with various kinds of syntactic, semantic, and phonological information.

Currently, the ADD Corpus contains textual data from two tasks (text categories);
one consists of simulated dialogues between a secretary and participants
at international conferences (Conference Task),
and the other of simulated dialogues between travel agents and customers (Travel Task).

In our experiments,
we used the keyboard dialogues from the Travel Task,
which include approximately 120,000 Japanese words
and 100,000 English words.
The telephone dialogue include linguistic phenomena,
such as filled pauses (``ah'', ``uh'', etc.),
restarts (repeating a word or phrase) and interjections,
so we did not use them for our experiments.
The aim in our research is to extract linguistically
neat expressions,
thus we excluded telephone dialogues from the experiments.


\subsection{Results and Discussions}

Table~\ref{Tab:CollocationJapanese}
shows some interesting Japanese collocations extracted
using respectively mutual information and cost criteria.
Table~\ref{Tab:CollocationEnglish} shows some English ones.
In these tables, collocations are listed in descending order
with respect to their values
(i.e. mutual information or cost reduction values).


\begin{table}[p]
\caption{Some examples of extracted collocations. (Japanese) }
\label{Tab:CollocationJapanese}
\begin{center}
\begin{tabular}{p{6.5cm}|p{6.5cm}}
\hline
\multicolumn{1}{c}{Mutual Information} & \multicolumn{1}{|c}{Cost Criteria} \\
\hline
ichi ryuu no orchestra ni yoru ensou            &  desho u ka  \\
Jouzankei-onsen to set ni nat ta golf-pack      &  desu ka  \\
Kunitachi-shi Ishida                            &  desho u  \\
chijou e                                        &  mashi ta  \\
night-tour ya dinner-show                       &  sou desu  \\
kaihatsu ga sakan                               &  sou desu ka  \\
buchou ya kachou                                &  to iu koto  \\
6 mai tsuzuri                                   &  sou desu ne  \\
hizuke henkou sen wo koe                        &  masu ka  \\
moushikomi kin toshite o azukari                &  desu ne  \\
Shinjuku-ku Naitou-chou 1 banchi                &  o negai shi masu  \\
kokunai sen no daiya                            &  itashi masu  \\
hakkou kaisha ni teishutsu                      &  o negai itashi masu  \\
Kenya Tanzania Safari                           &  to omoi masu  \\
yuuran sen no senchou                           &  te ori masu  \\
kaisui yoku                                     &  tai no desu ga  \\
yuukyuu kyuuka                                  &  wakari mashi ta  \\
Matsushima-wan meguri                           &  kashikomari mashi ta  \\
resort kaihatsu                                 &  ni nari masu  \\
umi to yama                                     &  to iu no ha  \\
yuujin no hahaoya                               &  shi tai no desu ga  \\
Hachiman-daira Towada Hiraizumi                 &  to iu koto de  \\
danjo betsu no uchiwake                         &  na n desu ga  \\
hakubutsu kan                                   &  shi tai no desu  \\
dou nenpai                                      &  shouchi itashi mashi ta  \\
senmon yougo                                    &  to iu koto desu  \\
yuukou kigen                                    &  sou na n desu  \\
genkin kakitome                                 &  arigatou gozaimashi ta  \\
Shanghai Sian                                   &  sa se te itadaki masu  \\
Setagaya-ku Kyoudou                             &  o mata se itashi mashi ta  \\
seinen gappi                                    &  sou na n desu ka  \\
moyori no eki                                   &  shitsurei itashi masu  \\
choushoku to chuushoku                          &  yoroshii desho u ka  \\
Fuji-ginkou honten                              &  ka mo shire mase n  \\
gouka kyakusen                                  &  irasshai masu ka  \\
\hline
\end{tabular}
\end{center}
\end{table}


\begin{table}[p]
\caption{Some examples of extracted collocations. (English) }
\label{Tab:CollocationEnglish}
\begin{center}
\begin{tabular}{p{6.5cm}|p{6.5cm}}
\hline
\multicolumn{1}{c}{Mutual Information} & \multicolumn{1}{|c}{Cost Criteria} \\
\hline
yacht harbor                    &  is that so  \\
Echigo Yuzawa                   &  thank you very much  \\
Fifth Avenue                    &  I would like to  \\
General Affairs                 &  I see  \\
Mitsuboshi trading              &  my name is  \\
slide projector                 &  sorry to have kept you waiting  \\
strong background               &  is that right  \\
cross the International Date    &  in that case  \\
the F1 Grand Prix               &  I understand  \\
Shiretoko Sightseeing Boat Inc. &  thank you for  \\
it's my pleasure                &  do you have any  \\
at the Hotel New Tanda          &  good bye  \\
give a speech                   &  would you like to  \\
head Mr. Kuwata                 &  I am very sorry  \\
wine production                 &  a little  \\
Wall Street                     &  be able to  \\
jazz dance                      &  I got it  \\
my mother in law                &  I'll be waiting for your call  \\
to the historic sites           &  may I have your name and address  \\
I am not that familiar          &  how much  \\
Keirin and Peking               &  all right  \\
cause the inconvenience         &  as soon as possible  \\
holding a paper                 &  then would you give me your  \\
baths and toilets               &  the other day  \\
Las Vegas                       &  make the reservations  \\
Queen Elizabeth                 &  a lot of  \\
Main Branch                     &  I will call you  \\
Sales Department                &  that's right  \\
self introduction               &  how about  \\
zip code                        &  at that time  \\
international cards             &  the application fee  \\
to the Grand Canyon             &  is that okay  \\
The Hyatt Regency               &  I appreciate your  \\
flight number JS                &  of course  \\
Canadian Rockies and Vancouver  &  so please hold the line  \\
\hline
\end{tabular}
\end{center}
\end{table}

In the experiments,
we examined word sequences up to a 10 word length.
When using mutual information,
the threshold was set to 0.0.
This means that we calculated mutual information values
for all the word sequences which appeared in the corpus.
When using cost criteria,
we generated approximately 10,000 collocation candidates.
The collocations listed in the tables were selected manually
from the higher ranked ones.

Before discussing the results,
we first overview the characteristics of Japanese phrases.
In general, the order of major constituents in a Japanese sentence is rather free.
However, predicate phrase positioning is dominated by
the so-called predicate-phrase ending constraint:
a predicate phrase appears at the end of its clause.
Furthermore, a predicate phrase often has a complex form,
consisting of a main predicate such as a verbal noun, verb or adverb,
combinations of auxiliary predicates, and a sentence-final particle.
These auxiliary predicates and sentence-final particles
add various complementary meanings to a sentence,
such as honorific, causative,
and prohibitive meanings, etc.

As can be seen from the experimental results
(Table~\ref{Tab:CollocationJapanese}),
the method based on mutual information tends to
extract task-dependent compound noun phrases,
while cost criteria tends to extract complex predicate phrase patterns.
Almost all the collocations extracted are in this category.
For example,
the collocations ``desho u ka'' and ``desu ka'',
which had a high cost reduction,
are used very often to make interrogative sentences in Japanese.
The collocation ``tai no desu ga'' is usually used to
express a speaker's request,
whose meaning is ``(I) would like to''.

The comments above are also true of the English data.
Mutual information tends to extract compound noun phrases,
while cost criteria tends to extract frozen phrase patterns
such as ``thank you very much'' and ``I would like to''.

Why does mutual information fail to extract these patterns?
Here, let us take ``I will'' as an illustrative example,
which has been picked out by cost criteria
(``I will'' is omitted from Table~\ref{Tab:CollocationEnglish})
but not by mutual information.
In our corpus,
``I'' occurs 2,907 times, ``will'' occurs 920 times,
and ``I  will'' occurs 264 times.
Therefore, we have
\begin{eqnarray}
  I(\mbox{I},\mbox{will}) & = & \log \frac{\frac{264}{100,000}}{\frac{2907}{100,000} \frac{920}{100,000}} \nonumber \\
        & = & 3.3
\end{eqnarray}
This value is not so large,
so the two words ``I'' and ``will'' cannot be considered to
have a significant relationship.

According to the same reasoning,
patterns such as ``I would like to'' and ``thank you very much''
are excluded as collocation candidates.
However, in the ADD Corpus,
more than fifty per cent of the sentences that involve the word ``would''
are subsumed under the pattern ``(I) would like to $\sim$''.
Therefore, this pattern should be included in the collocation list.

Another drawback using mutual information is the sparseness of data.
A corpus cannot provide sufficient data about every word-word relationship.
Some word pairs may have high mutual information values
in spite of their low frequency in the corpus.
For example,
the first ranked collocation was ``yacht harbor'',
which occurs only twice in the ADD Corpus.
On the contrary, since the cost criteria measure is based on absolute frequency,
such phenomena never happens.

Now, let us consider why mutual information
tends to extract task-dependent compound nouns.
As noted above, mutual information values become unstable
for low-frequency words.
Consequently, even if the incidence of a word pair occuring is low,
it is quite possible that the mutual information value becomes greater.
These low-frequency words, however, often depend on the topic of the text.
This is one reason
why mutual information tends to extract task-dependent compound nouns.


\section{Conclusion}

With the growing availability of large textual resources,
corpus-based studies are gaining more and more attention among computational linguists
and computer scientists.
In particular, automatic acquisition of lexical knowledge from corpora
is one of the most important and interesting issues.
In this paper, we have taken up the problem of how to acquire collocational knowledge
and discussed its importance for language learning.
We have also described comparative experiments
using mutual information and cost criteria.
Our experiments demonstrated that
mutual information tends to
extract task-dependent compound noun phrases,
while cost criteria tends to extract predicate phrase patterns.

Unfortunately,
the current implementation can only extract collocations
of uninterrupted word sequences.
Our next plan is to refine the method to extract collocations
of interrupted sequences,
and to utilize lexical information such as parts-of-speech
in order to prevent an improper word sequence
from being recognized as a collocation.
For example, when applying cost criteria to English corpora,
unwelcome word pairs, such as ``on the'', ``for the'' or ``of the'',
are extracted
because such pairs occur frequently in the language.
However, these word pairs can be excluded by using
parts-of-speech information.
Also, we hope to incorporate extracted collocations
into a language learning CAI system.

\vspace*{2mm}

\acknowledgment
{
The idea of cost criteria was developed
while the first author was staying at
ATR Interpreting Telephony Research Laboratories.
The authors are deeply grateful to co-researchers in ATR,
Kentaro Ogura
(currently with NTT Network Information Systems Laboratories)
and Tsuyoshi Morimoto,
for their fruitful discussions and comments.
The authors are also grateful to
the members of our laboratory in Tokushima University
for their help and encouragement.
Special thanks to Gerardo Ayala, Ingrid Kirschning
and John Phillips
for reading the manuscript.
}

\bibliographystyle{theapa}
\bibliography{kita}


\clearpage

\begin{biography}

\biotitle{}

\bioauthor{Kenji Kita}
{
Kenji Kita received the B.S. degree in mathematics 
and the Ph.D degree in electrical engineering,
both from Waseda University,
Tokyo, Japan, in 1981 and 1992, respectively.
From 1983 to 1987, he worked for the Oki Electric Industry Co. Ltd., Tokyo, Japan.
From 1987 to 1992,
he was a researcher at ATR Interpreting Telephony
Research Laboratories, Kyoto, Japan.
Since 1992, he has been with the Faculty of Engineering,
Tokushima University, Tokushima, Japan,
where he is currently an Associate Professor.
He is a member of the Acoustical Society of Japan,
the Information Processing Society of Japan,
the Institute of Electronics, Information and Communication Engineers,
and the Association for Computational Linguistics.
His current research interests include speech recognition,
natural language processing, and corpus linguistics.
}

\bioauthor{Yasuhiko Kato}
{
Yasuhiko Kato received the B.A. degree from Seijo University and the M.A. degree
from Saitama Univeristy, both in linguistics.
From 1984 to 1991, he worked for the Oki Electric Industry Co. Ltd., Tokyo, Japan.
Among these years, also worked as a member of CICC and EDR.
Since 1991, he has been a researcher at the National Language Research Institute,
Tokyo, Japan.
He is a member of the following societies:
the Linguistic Society of Japan, the Society for the Study of Japanese Language,
the Folklore Society of Japan, the Information Processing Society of Japan and
the Association for Computational Linguistics.
His current research interests include semantics and lexicology for dictionary
description, corpus linguistics and natural language processing.
}

\bioauthor{Takashi Omoto}
{
Takashi Omoto received the B.E. degree from Tokushima University, Tokushima, Japan, in 1994.
He is currently a master course student in Tokushima University.
}

\bioauthor{Yoneo Yano}
{
Yoneo Yano received the B.E., M.E., and Ph.D degrees in communication engineering
from Osaka University, Osaka, Japan, in 1969, 1971, and 1974, respectively.
Since 1974 he has been with Faculty of Engineering, Tokushima University, Tokushima, Japan.
He is currently a Professor in Information Science and Intelligent Systems.
}


\end{biography}

\end{document}

