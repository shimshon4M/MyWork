<?xml version="1.0" ?>
<root>
  <title/>
  <author/>
  <jkeywords/>
  <section title="Introduction">Recentrapidadvancesincomputertechnology,especiallytheadventoflargestoragedevicesandparallelcomputers,andnumerousdatacollectioneffortshavecausedashiftinnaturallanguageapplicationsfromaknowledge-basedtoacorpus-basedordata-intensiveapproach.Theknowledge-basedapproachfocusedonabstractionoflanguage,describinglinguisticphenomenathroughminimalcoreknowledgesuchasparts-of-speech,syntacticandsemanticrules.Linguisticphenomena,however,varysovastlythattheycannotbedescribedthroughcoreknowledge.Inaddition,hand-codingknowledgetakesalotoftimeandhardwork.Theknowledge-basedapproach,therefore,hasbeenfoundwantingindevelopinglarge-scalepracticalNLPsystems.Ontheotherhand,thecorpus-basedapproachmakesnoclaimaboutthecompactnessoftheknowledge.Rather,thecorpus-basedapproachderivesmorepowerfrommassivequantitiesoftextualdatathanfromhand-codedknowledge,beingabletocompensatefortheweaknessoftheknowledge-basedapproachthroughauthenticexamplesandvariousstatisticsoflanguageuse.Withtheavailabilityoflargecorporainrecentyears,manysuccessfulresultshavebeenderivedfromcorpus-basedstudies.Theseincludepart-of-speechtagging,parsing,example-basedmachinetranslation,statisticalmachinetranslation,languagemodelingandmanyotherrelatedareas.Oneinterestingpotentialuseofcorporaisforsecondlanguagelearning.Kitaetal.discussedvariouswayofusingcorporainlanguagelearning,whichincludestranslationskillacquisitionthroughbilingualcorpora,oral/auralskillacquisitionthroughspeechcorpora,multimedialanguagelearningthroughstructuredcorpora,andsoforth.Thegreatestadvantageofusingcorporainlanguagelearningisthatthecorporaprovideabodyofevidenceforthefunctionandusageofwordsandexpressions.Atthesametime,derivinglexicalknowledgefromlarge-scalecorporaviaautomatedprocedures,aswellasitsuseinlanguagelearningCAIsystems,isoneofthemostimportantissues.Inthispaper,weareprimarilyconcernedwiththeacquisitionofcollocationalknowledgefromcorpora.InSection~2,wedescribewhycollocationalknowledgeisimportantinsecondlanguagelearning.InSection~3,wediscusstheautomaticextractionofcollocations,takinguptwomeasures,mutualinformationandcostcriteria,foridentifyingorextractingcollocationsfromcorpora.InSection~4,wedescribecomparativeexperimentsinextractingcollocationsanddiscussthetwomeasures.</section>
  <section title="Importance of Collocational Knowledge in Language Learning">Therehasbeenmuchtheoreticalandappliedresearchoncollocations,bothfromalinguisticandanengineeringpointofview.Consequently,thedefinitionofcollocationdiffersaccordingtotheresearcher'sinterestandstandpoint.Thispaperadoptsthemostcomprehensivedefinition:acollocationisacohesivewordcluster,includingidioms,frozenexpressionsandcompoundwords.Theimportanceofcollocationshasbeenstressedinanextensiveliterature.Fromalanguagelearningviewpoint,itcanbesummarizedasfollows:languagelearning,learnersmustpayattentiontohowwordsareusedratherthantoindividualwordsbythemselves.Collocationalknowledgeindicateswhichwordsco-occurfrequentlywithotherwordsandhowtheycombinewithinasentence.Therefore,collocationalknowledgeisespeciallyeffectiveinsentencegeneration.knowledgeisverydifficulttoacquireforsecondlanguagelearners.Atypicalexampleisthepairofwords``strong''and``powerful''.Thesetwowordshavesimilarmeanings,buttheirusageisquitedifferent.Forexample,nativeEnglishspeakersprefersaying``strongtea''to``powerfultea'',andprefersaying``powerfulcar''to``strongcar''.Fornon-natives,however,itisdifficulttocatchthesubtledistinctionsbetweenthesetwowords.Theselexicalpreferencesweresometimesignoredinthetraditionalknowledge-basedapproach;neverthelesstheyarethemostimportantsourceforwordchoiceandwordordering.ispointedoutthathumantranslationprocessisbasedonanalogicalthinking.First,ahumantranslatorproperlydecomposesasentenceintocertainfragmentalphrases,thens/hetranslateseachfragmentalphrasebyanalogywithotherexamples,andfinallycomposesfragmentaltranslationsintoonesentence.Recently,followingthisidea,example-basedmachinetranslations(EBMT)havebeenwidelyexplored.InsomeEBMTsystems,severalkindsoftranslationknowledgeareutilized,suchasthestring-,pattern-,andgrammar-levelknowledgetypes.Collocationsareparticularlysuitablefortranslationunitsineitherthestring-orpattern-levelknowledge.acognitivepointofview,itissaidthathumanlanguageacquisitionisgovernedbythelawofmaximalefficiency.Inotherwords,datacompression,oftencalledchunking,isperformedtominimizestoragedemandsinthebrain.Achunkisconsideredtobeapatternwhichrepeatedlyappearsinavarietyofcontexts.Collocationsaregoodcandidatesforchunkunits.</section>
  <section title="Extracting Collocations from Corpora">Inthepast,severalapproacheshavebeenproposedtoextractcollocationsfromcorpora.Churchetal.introducedtheassociationratio,whichindicateshowstronglytwowordsarerelated,basedontheinformation-theoreticconceptofmutualinformation.Smadjaetal.takeintoaccountworddistanceaswellaswordstrengthforameasureofwordassociation.Also,Basilietal.proposedasyntax-basedapproach.Particularly,mutualinformationplaysacentralroleinrecentlexicalstatisticalresearch.Totakeafewexamples,HindleandRoothappliedmutualinformationtodisambiguateprepositionalphraseattachments,andBrownetal.useditindeterminingwordclasses.Inthissection,aftersurveyinghowmutualinformationcanbeusedtoextractcollocationalinformation,weintroduceanothermeasure,calledcostcriteria,toautomaticallyextractinterestingcollocationsfromcorpora.Comparativeexperimentsanddiscussionswillbedescribedinthenextsection.</section>
  <subsection title="Mutual Information">Themutualinformationbetweentwowordsxandyisdefinedasfollows:Here,P(x)andP(y)arewordoccurrenceprobabilities,andcanbeestimatedfromthenumberofoccurrencesofthewords,f(x)andf(y),andthenumberofwordsinthecorpus,N.P(x,y),thejointprobabilityofxandy,isestimatedinasimilarway.wheref(x,y)isthenumberofoccurrencesofxfollowedbyy.ThemutualinformationI(x,y)comparestheprobabilityofobservingxandytogetherwiththeprobabilitiesofobservingxandysimplybychance.Thus,alargevalueindicatesthatthetwowordsxandyhaveastrongrelationship.Byextractingwordpairswithlargemutualinformationvalues,wecanobtaincommoncollocations.Becausemutualinformationvaluesaredefinedfortwowords,thissimplemethodcanonlyextractcollocationsoflengthtwo.However,ageneralizationissuggestedinasfollows:StartoutfromthebasicvocabularyV_0.Setn=0.AugmentthevocabularyV_nbyallwordsequences``xy''forwhichI(x,y)&gt;Thr,whereThrisapredeterminedthreshold.FromStep~2,anewvocabularyV_n+1isestablished.AdjustthevocabularysizeNtoreflectthenewvocabularyV_n+1.ResumefromStep~1withV_n+1asitsbasis.Withthisiterativeprocedure,thefinalvocabularyincludescollocationsofarbitrarylength.</subsection>
  <subsection title="Cost Criteria">Thecostcriteriameasureisbasedontheassumptionsthat(1)collocationsarerecurrentwordsequences,and(2)therecurrentpropertyiscapturedbytheabsolutefrequencyofawordsequence.However,asimpleabsolutefrequencyapproachdoesnotwork,becausethefrequencyofasub-sequenceisalwayshigherthanthatoftheoriginalwordsequence.Therefore,asameasureforidentifyingorextractingcollocations,absolutefrequencyisnotappropriate.Instead,weconsideraprocessingcostforawordsequence,andintroducecostcriteriawhichcanquantitativelyestimatetheextenttowhichprocessingisreducedbyconsideringawordsequenceasoneunit.Beforethepresentationofaformaldefinition,weintroducethefollowingnotation:&amp;&amp;||&amp;&amp;&amp;&amp;f()&amp;&amp;eqnarrayWedefineareducedcostK()forasequenceas:K()isinterpretedasfollows.Assumeherethat,inthecorpus,thereexistsawordsequence,whichiscomposedof||wordsandoccursf()times.Alsoassumethatthecostofprocessingonewordis1.Similarly,whenprocessingasasingleunit,itsprocessingcostis1.Here,weassumethat:ifawordsequenceisprocessedonewordatatime,itsprocessingcostisproportionaltoitslength.Thisassumptionismadebecauseofthefollowingreason.Wearenowconcernedwiththeproblemofidentifyingthesequenceasacollocation.Butsincethisproblemisreducedtoastringpatternmatchingproblem,itisnaturaltoassumethatthecostisproportionaltothelengthofthesequence.Thatis,theprocessingcostforis||.Byconsideringasoneunit,theprocessingcostisreducedto||-1.Sinceappearsf()times,wecanconcludethatthetotalreducedcostbecomes(||-1)f(),whichisthedefinitionofK().Inreality,however,theproblemisnotsosimple,becausewordsequencesarenotmutuallydisjoint.Considerthecasewhereawordsequenceisasub-sequenceof(forexample,=,=).Then,wehave:Further,thewordsequence,f()timesoutoff()times,willbeidentifiedas.Thus,theactualreducedcostforisdefinedas:Finally,wecanextractcollocationsfromacorpusbythefollowingsteps:CalculateK()foreachwordsequenceinacorpus.RankawordsequencebyusingthevalueK().Extracthigherrankwordsequencesascollocationcandidates.Re-calculateK()foreachinthecollocationcandidates.Tobemoreprecise,bycheckingthesequence/sub-sequencerelationbetweeneverytwowordsequencesinthecollocationcancidates,modifytheK()valuesaccordingtoEquation~.</subsection>
  <section title="Experiments and Discussions"/>
  <subsection title="The ADD Corpus">Inourexperiments,theADD(ATRDialogueDatabase)CorpuscreatedbyATRInterpretingTelephonyResearchLaboratoriesinJapanwasused.TheADDCorpusisalargestructureddatabaseofdialoguescollectedfromsimulatedtelephoneorkeyboardconversationswhicharespontaneouslyspokenortypedinJapaneseorEnglish.ThiscorpusconsistsofparalleltextsofJapaneseandEnglish,alignedbyutterance.Also,sentencesinADDaremorphologicallyanalyzedandannotatedwithvariouskindsofsyntactic,semantic,andphonologicalinformation.Currently,theADDCorpuscontainstextualdatafromtwotasks(textcategories);oneconsistsofsimulateddialoguesbetweenasecretaryandparticipantsatinternationalconferences(ConferenceTask),andtheotherofsimulateddialoguesbetweentravelagentsandcustomers(TravelTask).Inourexperiments,weusedthekeyboarddialoguesfromtheTravelTask,whichincludeapproximately120,000Japanesewordsand100,000Englishwords.Thetelephonedialogueincludelinguisticphenomena,suchasfilledpauses(``ah'',``uh'',etc.),restarts(repeatingawordorphrase)andinterjections,sowedidnotusethemforourexperiments.Theaiminourresearchistoextractlinguisticallyneatexpressions,thusweexcludedtelephonedialoguesfromtheexperiments.</subsection>
  <subsection title="Results and Discussions">Table~showssomeinterestingJapanesecollocationsextractedusingrespectivelymutualinformationandcostcriteria.Table~showssomeEnglishones.Inthesetables,collocationsarelistedindescendingorderwithrespecttotheirvalues(i.e.mutualinformationorcostreductionvalues).Intheexperiments,weexaminedwordsequencesuptoa10wordlength.Whenusingmutualinformation,thethresholdwassetto0.0.Thismeansthatwecalculatedmutualinformationvaluesforallthewordsequenceswhichappearedinthecorpus.Whenusingcostcriteria,wegeneratedapproximately10,000collocationcandidates.Thecollocationslistedinthetableswereselectedmanuallyfromthehigherrankedones.Beforediscussingtheresults,wefirstoverviewthecharacteristicsofJapanesephrases.Ingeneral,theorderofmajorconstituentsinaJapanesesentenceisratherfree.However,predicatephrasepositioningisdominatedbytheso-calledpredicate-phraseendingconstraint:apredicatephraseappearsattheendofitsclause.Furthermore,apredicatephraseoftenhasacomplexform,consistingofamainpredicatesuchasaverbalnoun,verboradverb,combinationsofauxiliarypredicates,andasentence-finalparticle.Theseauxiliarypredicatesandsentence-finalparticlesaddvariouscomplementarymeaningstoasentence,suchashonorific,causative,andprohibitivemeanings,etc.Ascanbeseenfromtheexperimentalresults(Table~),themethodbasedonmutualinformationtendstoextracttask-dependentcompoundnounphrases,whilecostcriteriatendstoextractcomplexpredicatephrasepatterns.Almostallthecollocationsextractedareinthiscategory.Forexample,thecollocations``deshouka''and``desuka'',whichhadahighcostreduction,areusedveryoftentomakeinterrogativesentencesinJapanese.Thecollocation``tainodesuga''isusuallyusedtoexpressaspeaker'srequest,whosemeaningis``(I)wouldliketo''.ThecommentsabovearealsotrueoftheEnglishdata.Mutualinformationtendstoextractcompoundnounphrases,whilecostcriteriatendstoextractfrozenphrasepatternssuchas``thankyouverymuch''and``Iwouldliketo''.Whydoesmutualinformationfailtoextractthesepatterns?Here,letustake``Iwill''asanillustrativeexample,whichhasbeenpickedoutbycostcriteria(``Iwill''isomittedfromTable~)butnotbymutualinformation.Inourcorpus,``I''occurs2,907times,``will''occurs920times,and``Iwill''occurs264times.Therefore,wehaveI(,)&amp;=&amp;264100,0002907100,000920100,000&amp;=&amp;3.3eqnarrayThisvalueisnotsolarge,sothetwowords``I''and``will''cannotbeconsideredtohaveasignificantrelationship.Accordingtothesamereasoning,patternssuchas``Iwouldliketo''and``thankyouverymuch''areexcludedascollocationcandidates.However,intheADDCorpus,morethanfiftypercentofthesentencesthatinvolvetheword``would''aresubsumedunderthepattern``(I)wouldliketo''.Therefore,thispatternshouldbeincludedinthecollocationlist.Anotherdrawbackusingmutualinformationisthesparsenessofdata.Acorpuscannotprovidesufficientdataabouteveryword-wordrelationship.Somewordpairsmayhavehighmutualinformationvaluesinspiteoftheirlowfrequencyinthecorpus.Forexample,thefirstrankedcollocationwas``yachtharbor'',whichoccursonlytwiceintheADDCorpus.Onthecontrary,sincethecostcriteriameasureisbasedonabsolutefrequency,suchphenomenaneverhappens.Now,letusconsiderwhymutualinformationtendstoextracttask-dependentcompoundnouns.Asnotedabove,mutualinformationvaluesbecomeunstableforlow-frequencywords.Consequently,eveniftheincidenceofawordpairoccuringislow,itisquitepossiblethatthemutualinformationvaluebecomesgreater.Theselow-frequencywords,however,oftendependonthetopicofthetext.Thisisonereasonwhymutualinformationtendstoextracttask-dependentcompoundnouns.</subsection>
  <section title="Conclusion">Withthegrowingavailabilityoflargetextualresources,corpus-basedstudiesaregainingmoreandmoreattentionamongcomputationallinguistsandcomputerscientists.Inparticular,automaticacquisitionoflexicalknowledgefromcorporaisoneofthemostimportantandinterestingissues.Inthispaper,wehavetakenuptheproblemofhowtoacquirecollocationalknowledgeanddiscusseditsimportanceforlanguagelearning.Wehavealsodescribedcomparativeexperimentsusingmutualinformationandcostcriteria.Ourexperimentsdemonstratedthatmutualinformationtendstoextracttask-dependentcompoundnounphrases,whilecostcriteriatendstoextractpredicatephrasepatterns.Unfortunately,thecurrentimplementationcanonlyextractcollocationsofuninterruptedwordsequences.Ournextplanistorefinethemethodtoextractcollocationsofinterruptedsequences,andtoutilizelexicalinformationsuchasparts-of-speechinordertopreventanimproperwordsequencefrombeingrecognizedasacollocation.Forexample,whenapplyingcostcriteriatoEnglishcorpora,unwelcomewordpairs,suchas``onthe'',``forthe''or``ofthe'',areextractedbecausesuchpairsoccurfrequentlyinthelanguage.However,thesewordpairscanbeexcludedbyusingparts-of-speechinformation.Also,wehopetoincorporateextractedcollocationsintoalanguagelearningCAIsystem.*2mm</section>
</root>
