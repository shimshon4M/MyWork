


\documentstyle[epsf,nlpbbl]{jnlp_e_b5}

\setcounter{page}{45}
\setcounter{巻数}{3}
\setcounter{号数}{1}
\setcounter{年}{1996}
\setcounter{月}{1}
\受付{April}{7}{1995}
\再受付{June}{12}{1995}
\採録{July}{6}{1995}

\setcounter{secnumdepth}{2}

\title{}
\author{}
\jkeywords{}

\etitle{Towards a Linguistic Treatment of Compounds\\
in a Machine Translation Environment}

\eauthor{Sophia Ananiadou \affiref{ManchesterUniv}}

\headauthor{Sophia Ananiadou}
\headtitle{Compounds in MT}

\affilabel{ManchesterUniv}
          {Department of Computing the Manchester Metropolitan University}
          {Department of Computing the Manchester Metropolitan University}

\eabstract{
This paper concentrates on research into the translation of
compounds in a transfer based MT system. We determine how
contemporary linguistic theory can contribute to the
characterisation, representation and translation of compounds. We
determine productive, compositional and translatable compounds.
We discuss evolving strategies for effecting translation of these
compound types. The results obtained are readily adaptable to
models which have a stratificational linguistic framework are
able to emulate feature value percolation. The research
undertaken was reductionist in nature, leading to a set of
compositionally translatable productive compound types being
isolated.
}

\ekeywords{Machine Translation, Compounds, Morphology}


\begin{document}

\maketitle

\section{Introduction}

This  paper  deals  with the linguistic treatment  of  general 
language compounds within a Eurotra-like MT framework based on  a 
strongly  theoretically  motivated  approach.  The  computational 
treatment of compounding is notoriously one of the most difficult 
areas in Natural Language Processing (NLP). Here lies the trap
 of adopting a heuristic approach 
having  in view only parsing efficiency.  It is our strong belief 
that  in  the area of Machine Translation  no  effort  should  be 
spared  in  exploring elegant linguistic solutions which  can  be 
easily   and   adequately   transferred  into   a   computational 
environment.   We   should   also   bear  in  mind  that  in any 
multilingual   system, such as Eurotra, accommodating   languages
from   three  different families i.e. Romance, Germanic, Greek, 
there is an even  stronger 
motivation  to  incorporate  as much as  possible  universal  and 
generalised theory. 

We  adopt  here  the  general  trends  of  lexicalist  generative 
morphology.  Our  work follows the principles of \cite{Selkirk82}, 
\cite{Williams81a,Williams81b}, \cite{DiSciullo87} and \cite{Lieber83} 
among others.

As our research was carried out within the Eurotra  project,  our 
grammars   and  lexical  entries  were  written  in  the  Eurotra 
formalism.  We  thus  assume a  unification-based  interpretation 
mechanism  involving rules that map between levels of  linguistic 
representation   and  other  rules  that  generate  the  set   of 
structures at some level. Representations and rules are expressed 
via bundles of attribute-value pairs (feature bundles). The major 
linguistic  object  that is manipulated is the tree.   We do  not 
enter  into details of the formalism here,  as  descriptions  are 
widely available in the literature (e.g. \cite{Copeland91}). 
We also assume a transfer-based approach to translation, with the 
transfer  level  being a deep syntax dependency based  level.
Both the analysis and synthesis phases are stratified. Each level 
has a defined function. The levels are the following (Cencioni,R. 
pp. 114--115 in \cite{Copeland91}): 

\begin{itemize}
\item {\bf AT Actual Text}: Written text, in a word processor format.
\item {\bf ETS Eurotra Text Structure} \hspace{1mm}Separate the text from the structure of a
document.The structure is kept so that the original document  can be
reconstituted in the \mbox{target language}.
\item {\bf ENT  Eurotra  Normalized Text}:  Words are  decomposed  into  word 
morphemes
such as suffixes,  prefixes and stems.  ENT attempts to find  all possible
combinations when decomposing words. Invalid decompositions are
discarded by EMS.
\item {\bf EMS  Eurotra Morphological Structure}:  Combinations of  morphemes are
analysed  to  produce a feature bundle containing a reference  to 
the root
lexical unit, together with features indicating how this unit is
modified by the associated morphemes.
\item {\bf ECS Eurotra Configurational (constituent) Structure}: This level
identifies  phrases and components within the sentences.  ECS  is 
surface oriented.
\item {\bf ERS  Eurotra  Relational Structure}: This level  handles  relations 
between
items, e.g. subject-verb agreement. A number of surface oriented
structures, such as articles, auxiliaries are abstracted away and
replaced by features.
\item {\bf IS  Interface  Structure}:  This  is the most  abstract  level  of 
Eurotra. This
can be characterised  as a deep syntactic representation which
incorporates interlingual descriptions of subsystems.
\end{itemize}



It should however be noted that, although our solutions are
expressed in a particular formalism,  they are not strongly bound 
to that formalism.  Nevertheless,  any solution is conditioned by 
the  formalism used to express that solution.  We shall therefore 
consider,  where appropriate,  in what way our solution is indeed 
conditioned  by adoption of the Eurotra formalism and  point  out 
how  much  of this conditioning is shared by the large  class  of 
formalisms of which the Eurotra one is a member.  Also, it should 
be  noticed that we are not overmuch concerned with computational 
efficiency:   we  are  more  concerned  with  finding  linguistic 
solutions and with criteria for choosing between solutions.

Furthermore, we emphasize that the Eurotra formalism was used for 
two  major  purposes:  to  build a prototype MT system  and  as  a 
vehicle   that   allowed  research   and   experimentation   into 
linguistics  and  computational linguistics to be carried out  (a 
purpose  which  is  very much still alive).  Our work  is  to  be 
interpreted  as  use  of the  Eurotra  framework  for  linguistic 
research  oriented towards machine translation.  In  passing,  we
note that results are generally portable to other rule based environments.
At the time we carried out this research, Eurotra
placed  little  emphasis on semantics.  Because of this  lack  of 
semantics   our  research  was  confined  to  morphological   and 
syntactic aspects.



\section{Definition of compounds}

Compounds    are   morphological   objects  whose  structure   is 
generated  by  a  restricted set  of  morphological  rules.  They 
contain at least two lexemes; lexeme is taken to be either a word 
or a stem/root constituent.

The area of compounding is an extremely difficult area,  taken as 
a   whole.   It  lies in the grey area between   morphology   and 
syntax,  thus apparently subject to two sets of operations.  This 
is   due  to the existence in many languages of one (text)   word  
- -- clearly   belonging to the morphological domain  -- and  multi  
(text)  word compounds,  interpretable as being in the domain  of 
syntax.

However,   both  types  fulfill  the same  function:   a  oneword 
compound (consisting of one text word) and  a  multiword compound 
(consisting of several text words) can both be  interpreted  as 
expressing   one  major  lexical  category,   as  each  being   a 
morphological object. 

In  this  paper  we  will  concentrate  on  compounds  which  are 
compositional,   translatable and tractable such as {\it safety belt} 
$\rightarrow$
{\it zwnj asfaleias} (belt safety) $\rightarrow$ {\it ceinture de securit\'{e}}.

 By compositional, we 
mean in fact compositional and productive to a high degree.  That 
is, compositionality is not taken to be absolute. Productivity is 
in  turn defined by:

\begin{itemize}

\item morphological and phonological regularity in word formation
\item semantic transparency

\end{itemize}

By  translatable,   we  mean  that  consistent  mappings  between 
compound   types   in  various  languages  could  be   reasonably 
established  for  our purposes.   By tractable,   we   mean  that  
mechanisms  available  in  our  Eurotra  environment  could    be 
employed  with  a reasonable amount of  success.  Thus, the scope 
of  our  work  was  limited by  the  above  constraints,  and  in 
particular  by the set of consistent translation mappings between 
compound  types which could be  established.  Thus,  although  we 
could  have monolingually tackled a wide range of compound types, 
our  work was defined by the need to establish computable  cross-
linguistic mappings between productive compound types in a  deep-
syntax dependency framework in the absence of semantics.


\section{Characteristics of compounds}

The  characteristics  of  compounds that  critically  enter  into 
consideration  for  any  motivated account  are  as  follows: 

{\bf [A] [orthography is no guide]}:
Oneword  and  multi-word compounds  exist.  English  for  example 
demonstrates   e.g.   oneword  compounds,   multi-word  compounds  
and hyphenated  compounds.   
Furthermore,   there  is  little  identity  of  mapping   between 
languages  in orthographic terms:   a oneword compound in English 
does  not  necessarily imply a mapping to a oneword  compound  in 
say, Greek or vice versa. \\

\begin{center}
{\em  kapnokalliergeia $\Rightarrow$ tobacco cultivation \\
      frogman $\Rightarrow$ vatrahanthrwpos\\}
\end{center}

\noindent
What  is important to retain is that a number of  elements  taken 
together may function as a compound,  no matter whether they form 
one text word or many.

{\bf [B] [lexical versus phrasal entities]}:
Often  it  has been argued that compounds are  closely  connected 
with phrases.  Historically,  linguists have stressed the phrasal 
status  of many compounds.  Some scholars have concluded that one 
cannot  distinguish  between syntactic  phrases  and  compounding 
(\cite[p.~233]{Bloomfield35}; \cite{Lees60}). In these studies compounds 
are   generated   as   phrasal  structures  with  the   help   of 
transformations.  In this approach,  a N+N compound such as  {\it boy 
friend} is derived from the sentence {\it friend who is boy}.

However,    there   is   today   strong  evidence   that   multi-
element  compounds    forming   one   text   word    belong    to 
morphology,   and  much  evidence  that  compounding  in  general  
belongs  to  the area of  Morphology (\cite{Lieber83}, \cite{Selkirk82}, 
\cite{DiSciullo87}, \cite{Sproat92}).   As  such,   
it   must  be 
treated separately from Syntax.  This  follows from the theory of 
the  autonomy  of Morphology,  according to which it   has   been  
recognized   that   Morphology  is  an   independent  grammatical  
domain,  different from syntax.  
There  is  considerable  debate  over  the  status  of  multiword 
compounds.  However,  we  view multiword compounds as atomic with 
respect to syntax.  That is, phrasal syntactic rules typically do 
not operate within  the  structure  of  compounds.  This  follows  
from   the  Principle of Syntactic Atomicity (cf. \cite{DiSciullo87})
which blocks rules of syntax from applying within compounds.
 
{\bf [C] [the notion of atomicity]}:
As   we mentioned in {\bf b)} multiword compounds should be  considered 
as  lexical entities.   This is the case especially for multiword 
compounds  which  appear  on  the surface  to  be  isomorphic  to 
phrasal categories  (i.e.   to NPs for example). However a closer 
look  at  the  internal structure of  such objects  reveals  some 
basic differences between this kind of element and phrasal units: 
the  internal structure of multiword compounds  is  distinguished 
from  syntactic  phrases by being opaque to  the  application  of 
some basic phrasal syntactic rules  -- that is,   these   objects  
must be considered atomic  with  respect  to syntax. 
\footnote{We  note nevertheless that some syntactic rules such 
as coordination `blood or tumour cell' can also apply on compound 
nouns. This issue demands more investigation.}

Examples  of  such rules that cannot apply within  compounds  are 
given  by \cite[pp.~49--52]{DiSciullo87}.  We summarize their 
arguments here:

{\bf i) pronominal  reference  is  not allowed  in  words  (including     
compounds)}

\noindent
This predicts correctly that we do not find

\begin{center}
{\em * it robber}
\end{center}
\noindent
where  `it' refers to some bank.  

One  possible explanation why pronominal reference is not allowed 
is  that reference is assigned only to NPs,  however  within  the 
compound we have only N.

{\bf ii) degree modification}:
There  are restrictions as far as the modification of  adjectival 
constituents by degree adverbs is concerned.  This is due to  the 
fact  that the adjective cannot be taken as a separate  syntactic 
unit from the head noun.

\noindent
In the case of the compound
{\it industrial  revolution}  restrictions operate  which  prevent  a 
degree adverb from being prepended:
\begin{center}
{\em * very industrial revolution} 
\end{center}
\noindent
but  in  the case of the phrasal unit {\it industrial city}  this  is 
possible: {\it very industrial city}. 

{\bf iii) insertion of adjectives}:
True  compounds cannot be separated by 
adjectives,  except in cases where the adjective forms a compound 
with the element to its right,  e.g. {\it surface energy} $\rightarrow$ 
{\it surface 
free energy}.
\begin{center}
   {\em  apple pie \\
   * apple big pie\\}
\end{center}

{\bf iv) absence of morphological markers}:
\cite[p.~42]{Sproat92}  notes  that morphological  markers  which  are 
obligatory   on  phrasal  modifiers  are  absent  when  the  same 
modifiers occur within compounds.
\begin{enumerate}
{\em
\item       rot + er Wein \\
       (red + nom/sg/masc/strong wine)\\
       wine which is red in colour\\

\item    Rot + wein \\
       (red + wine)\\
        red wine qua category of wine\\}
\end{enumerate}

In  {\bf a.}  the adjectival inflectional ending  -er  is  obligatorily 
present,  indicating  that this is a phrase,  whereas in {\bf b.} it is 
obligatorily absent, indicating that this is a compound.

To summarize this point, the {\bf Principle  of  Syntactic Atomicity} 
blocks rules of syntax from applying within compounds.

 
{\bf [D] [headness]} :
We  deal  only  with  compounds which  are  headed  objects.  The 
location  of  the  syntactic head of a compound  depends  on  the 
language. \cite{Williams81a} argues that compounds have their heads 
to  the  right  ({\bf Righthand  Head  Rule}   -- RHR).  He  based  his 
arguments on English which in general follows this principle.  An 
exception  to  the  RHR are nouns of the form V P  e.g.  [push-V 
up-P]-N, [run-V down-P]-N. In this case we are dealing with exocentric 
compounds where clearly there is no head at all. 

Lieber  claimed  that  there are  languages  which  exhibit  left 
headness  (Romance)  or right headness.  We find languages  which 
demonstrate the double possibility of right and left headness. As 
an example we have Italian and Greek.

\citeA[p.~243]{Scalise88} provides examples with Italian compounds with 
a right head:
\begin{center}
{\em terreMOTO  $\Rightarrow$ earthquake\\
 sanguiSUGA $\Rightarrow$  leech\\}
\end{center} 

These compounds are called `Latin' whereas the ones which exhibit 
a left headness are called `Italian' compounds.

\begin{center}

{\em  NAVE traghetto $\Rightarrow$  ferry-boat\\
 DIVANO letto   $\Rightarrow$  divan-bed\\}
\end{center}


In Greek we find both cases as well. In most cases the head is on 
the  right  hand side.  In the case of left headness  the  second 
member is always in the genitive case.

\begin{center}
{\em  zwnj asfaleias $\Rightarrow$ safety belt\\
 belt safety (Genitive case)}

\end{center}

In compound constructions,  the head plays a key  role,  enabling 
proper  percolation of feature values to  dominating  nodes,  and 
furthermore  proper assignment of relational information at other 
stages.

The  notion  of  head  has a  relatively  recent  application  in 
morphology,  whereas  in  syntax  it  is  well  established.  For 
discussions  on the notion of `head' in morphology and in  syntax 
we refer the reader to \cite{Zwicky85}. We also refer the reader to 
\cite{Corbin87} for an alternative view.
 
\section{State of the Art - Linguistics}

Compounding,  especially in English, has been studied extensively 
in linguistics,  syntactic and semantic criteria being  exploited 
in attempts to interpret compounds.  Those basing their work on a 
combination  of  syntactic  and semantic criteria  include \cite{Adams73}
and \cite{Marchand69},  those  basing  their  work  on  
uniquely  semantic  criteria include \cite{Downing77}.   
The  link  
between  compounds   and sentential paraphrases is to be found in 
\cite{Quirk86}.  As an  example, the compounds `glow-worm' 
and `punch-card', which  are superficially  similar consisting of 
verb+noun,  show  underlying differences in the relations between 
their constituents \cite[p.~1570]{Quirk86}:

\begin{center}
{\em glow-worm $\mapsto$ the worm glows, i.e. verb + subject}
\end{center}
\noindent
but
\begin{center}
{\em punch-card $\mapsto$  X punches the card, ie. verb + object}
\end{center}

Regarding   descriptions   based on semantic  grounds,  there  is  
much  discussion   about  the number and the  type  of   semantic  
relations  that  can or should be identified between elements  of 
compounds.

\cite{Levi78} proposes a set of semantic relations for the analysis 
of  compounds  which he views as all being derived by one of  two 
syntactic  processes:   by the deletion of the predicate  of  the 
underlying  sentence or by the nominalisation of the predicate in 
the underlying sentence. 
\cite{Downing77} examines the semantic and pragmatic features  of 
english compounds, suggesting at least 12 relations including:

\begin{itemize}
\item whole-part  : duck foot
\item part-whole  : pendulum clock
\item composition : stone furniture
\end{itemize}

\noindent
She claims that:

\begin{quotation}
The  semantic  relationships that hold between  the  members  of 
\mbox{compounds}  cannot  be characterized in terms of a finite list  of 
appropriate compounding relationships.
\end{quotation}

\noindent
\cite{Warren78} provides another example of work in  the  semantic 
area,  which  also makes reference to the role of different  text 
types  in the search for a set of semantic relations to  describe 
compounds.   She   determines  a  number  of  semantic   classes 
underlying   the   types  of  semantic  relations   between   the 
constituents of noun+noun compounds (e.g.{\bf source-result,  copula, 
resemblance}). 


According   to  \cite{Selkirk82},  verbal compounds in English  can  
be  interpreted  via  the designation of  grammatical   functions  
e.g.  {\bf Subject,   Object,   To-Object},  etc.  Each  compound  has 
associated  with  it   a  lexical  form  which  consists   of   a   
predicate-argument  structure  and  a designation of  the  above-
mentioned    grammatical  functions.    The   argument  structure 
represents the  structure  of thematic relations.  The difference 
between  verbal and non-verbal compounds  lies in the  fact  that 
argument  structure  is  important in the interpretation  of  the 
former,  but not of the latter.  The grammar for verbal compounds 
must  assign grammatical functions to the non-head constituent of 
the compound.

Another   account  of  the  structural  principles  of   compound 
formation   in  terms  of  argument structure is  due  to  \cite{Lieber83}.
She proposes a  principle (or constraint) - the {\bf Argument  
Linking  Principle (ALP)} - which  accounts  for  both primary and  
synthetic  compounds.   In general,   verbs  and prepositions are 
considered to be  argument-taking  lexical items,  adjectives and 
nouns  non-argument taking.  In compounds such  as  `hand-weave', 
`weave' satisfies its argument structure  in  the  sentence  into 
which   the  compound  verb  is inserted.   The  noun  `hand'  in 
this   case   is    interpreted    as    a   semantic    argument 
(instrumental).

Lieber argued that on the basis of whether or not each of the two 
elements in a compound is argument taking or non-argument  taking 
we can determine the relative productivity of compound types. The 
most  productive  case in English is the one where both  elements 
are   non-argument   taking   e.g.   noun+noun,   adjective+noun, 
noun+adjective, adjective+adjective. 

Given  the  under-researched  nature  of  semantic  relations  in 
Eurotra  at  the  time (and the  lack  of  semantic  solutions 
generally)  and the emphasis on deep syntax representations as  a 
transfer level,  our work concentrated on applying,  adapting and 
developing notions drawn from,  among others, Selkirk, Lieber and 
Williams.

\section{State of the Art - NLP}

MT  systems  have  generally  failed to  tackle  the  problem  of 
compounds  in anything \mbox{other} than ad  hoc  fashion.  
Major  MT  systems based on a dependency treatment  have
found  compounds  to be problematic  (ARIANE-78, \cite{Guilbaud87}) 
which  is significant given the adoption of dependency treatments 
at   the  higher  levels  of  linguistic  analysis   in   Eurotra 
approaches.  By {\it dependency treatment} we refer to a 
grammar where the linguistic structure is analysed by identifying  
a 
nuclear  element around which additional elements are arranged as 
dependents- classically,  either  arguments  (args) or  modifiers 
(mods).

Sublanguage based MT systems,(e.g. TAUM, \cite{Lehrberger82}) tried
a
different approach to the analysis of long nominal compounds
(TAUM termed these  {\it empilages}). The
proper bracketing of compounds assumes an understanding of the
semantic and syntactic relations between the components of
compounds
which in turn often relies on a
study of the cooccurrences of components within noun sequences
in the
sublanguage under study.
 This is feasible in a sublanguage where
the number of relations is rather restricted. TAUM identified
about 50 relations for the analysis of nominal compounds. For a
more detailed analysis of the system, we
refer the reader to \cite[pp.~81--106]{Lehrberger82}.

In monolingual NLP, \cite{Finin86}, \cite{McDonald82}, \cite{Sparck-Jones83}
and \cite{Hoeppner82}  have  concentrated  on 
semantic, knowledge-based treatments. Each author comments on the 
difficulty  of  processing compounds, \cite[p.~165]{Finin86} giving  a 
summary   of  the  problems  encountered  with  English   nominal 
compounds even within a model taking account of discourse context 
in a sublanguage domain. Sparck Jones notes that the delimitation 
of  a compound involves several different types of  knowledge.  A 
compound  must  be analysed and interpreted in  relation  to  its 
internal structure (compound internal operations) and also to its 
role in the textual context external to it.

In  NLP,  therefore,  we  found little to guide our work as  most 
research  appeared  to be concerned with semantic  or  conceptual 
processing and thus suffered from the well-known problem of being 
either  portable but hopelessly general or too specific and  non-
portable. As stated earlier, we were furthermore concentrating on 
syntactic approaches.

\section{Proposed abstract representation for transfer}

We now present our proposed abstract representation for compounds 
which  we   believe  is adequate for representing the  types   of  
compounds  that  we  may  hope  to   translate   compositionally,   
given   the constraints  we described already.  In a Eurotra-like 
approach,  transfer  takes place at what is called the IS  level, 
thus  we  are concerned with  specifying  linguistic  information 
within  the  constraints  of  that  level.  We  start  by  giving 
representation  requirements  for translation purposes as  it  is 
advisable  to know what representational target we have to  reach 
rather than starting out by attempting to tackle surface-oriented 
aspects in the hope that a reasonable target representation  will 
somehow fall out of thin air.

Some  modifications to our proposal would be  required  to  treat 
a wider variety of types of compounds,   e.g.  the  addition   of 
semantic   relations.  This is not an area which we investigated. 
However,  this  is  a  question  of  degree  of  specificity  and 
exhaustiveness  in  the  set of relations,   rather than  one  of 
fundamental lack of  a mechanism. 

We  propose  that  all compounds,   that  can  be   analysed  and 
translated compositionally,  no matter whether they are   oneword  
or  multiword  compounds,   receive  a  structured representation 
at the transfer level.

In  particular,  these structured representations should preserve 
lexical  unit  (LU)  information    on    daughter    nodes   and   
should   not   involve featurisation of daughter node LUs in a
single overall feature bundle.  Once information is featurised in 
this  manner,  we  lose the possibility of future access  to  the 
original internal structure.

Transfer   mappings  in Eurotra are of  two  main  types:  simple 
transfer and complex transfer.  The former takes place by default 
unless there are specific rules which call for different mappings 
due typically to structural differences between languages.  Thus, 
simple  transfer  may take place on a  structured  representation 
where   the   same  structure  is  present  in  both   languages. 
Comparative  study  of compound patterns reveals  which  mappings 
demand  complex  transfer  rules to  be  written. 
There are in turn two types of complex transfer rules: ones which 
deal  with  highly specific cases  where,  e.g.  fairly  complete 
information on particular lexical units appears,  and ones  which 
express  specific  mappings for entire classes of object.  It  is 
with  this  latter type that we are concerned as we  are  dealing 
with productive compound mappings across languages.

For illustrative purposes we give transfer mappings to
English that were identified as being translationally
compositional and productive:

{\bf [Greek to English]}
\begin{itemize}
{\em
\item  Nnom + N gen $\rightarrow$ N +N \\
    zwnj asfaleias  $\rightarrow$ safety zone \\
     {[head] [modifier in genitive case]} \\
\item  Nnom + Nnom $\rightarrow$  N + N \\
        paidi [head] thavma $\rightarrow$  wonder kid [head] \\
          
\item  {\bf oneword compounds}   N + N $\rightarrow$ N + N \\
        kapnokalliergeia $\rightarrow$ tobacco cultivation\\}
\end{itemize}


Our     structured      representations   of    compounds     are  
hierarchical, binary  
branching structures. They are frame based, i.e. nodes carry, where 
appropriate,  information  about arguments to  be  satisfied,  in 
terms  of  roles,  etc.,  depending on the  level. 
The structures  are built  by  manipulation 
of  structure  building  rules (in Eurotra,  b-rules)  and   non-
structure  building  rules  (such  as  feature  rules  which  add 
information on frames to ensure  the  correct choice of b-rules).  
Feature   information   is  either   inserted at  this  level  or 
inherited  from previous  levels (cf.  barlevel =  zero,  below), 
particularly  from the morphological level (in Eurotra,  the  EMS 
level).

The   minimum  feature information needed to represent  compounds  
of  both types  (oneword and multi-word) at the transfer level is 
the following:

\begin{itemize}
\item dependency role 
\item morphosyntactic category
\item morphological class (mc) 
\item dependency frame (isframe)
\item bar level 
\item status of elements
\end{itemize}

Dependency  role is the key organising feature of the Eurotra  IS 
level,   whose  values  refer  to  dependency  roles: 
governor, arguments and modifiers. 

As  for  morphosyntactic category,  the Eurotra IS  carries  such 
information even though based on deep dependency syntax, as it is 
important for translation. It is especially important in our case 
as transfer rules may need to refer to morphosyntactic values  in 
determining  how  a compound should be translated  (as  in  `zwnj 
asfaleias'  where  the genitive case marker is critical  for  the 
determination of left head position,  as described in the example 
above).

The  values  of the feature morphological class  belong to    the  
range    of    morphological   categories   relevant    to    the 
morphological structure of compounds, that is, affix, stem (where 
needed),      word  and,  for    binding   elements (e.g. linking 
vowels as in {\it o} in {\it kapn-o-kalliergeia} in Greek),
transitional vowel indication.                            

As  far as dependency frame information is concerned whenever  it 
is  inherited it should be done so by reference to syntactic  and 
semantic heads of  compounds.
\footnote{arg1 and arg2 stand for the external and first  internal 
arguments or deep subject and deep object of a predicate}

   Frames  may  be satisfied  within  
or   outside  a compound.  Furthermore,   we  believe  that frame 
information  should   not   be deleted  from  daughter  nodes  of 
compounds  upon being the  subject of  inheritance,   but  should 
remain  there  in  case of  need.   Given  that   information  on 
daughter  nodes  at  the  transfer level must  be  consulted   to  
ensure proper selection of transfer rules,  frame information  on 
daughter nodes is required for similar purposes. 

For   example,   in  the compound `tobacco cultivation'  the  top  
node  acquires the information `is\_frame = arg12'
\footnote{that is, takes two arguments, i.e. subject and object.}
which is 
percolated from the head daughter  node  `cultivate' (assuming  a 
derivational  analysis).   Of   course,   when  an  argument   is 
satisfied  within a compound,  e.g.  in morphology it  cannot  be 
satisfied in syntax.  An illustration is the following example:

\begin{center} 
    {\em  kapnokalliergeia} (tobacco cultivation) \\
but\\

{\em     * y kapnokalliergeia toy kapnoy}\\
     * the tobacco cultivation of tobacco\\
\end{center}


The   bar  level  feature  characterizes  the   topmost   feature 
bundle  of the structured representation of a compound and it  is 
inherited  from previous levels of analysis where  it serves   to  
block  the main syntactic rules from applying  to   the  internal  
structure   of compounds.   At the transfer level,   this feature 
must   be  available  for reinterpretation  in  order  to  ensure 
transparency  to grammars which assign  relational  and  semantic  
information   to   the  daughter feature bundles.  Its  value  is 
always `zero' for a compound and it will have been calculated  at 
an earlier level.

As \citeA[p.~52]{DiSciullo87} point   out:
\begin{quotation}
   The 
referential  opacity  of  $X^0$ is a consequence  of  the  syntactic 
(filter) opacity of $X^0$ - in other words,  words are opaque to all 
sentence-level operations or descriptions.
\end{quotation}
               
It  may  not   be sufficient to mark only  the  topmost   feature  
bundle,  as daughter feature bundles may in that case be taken as 
independent  input   to  rules not intended to  treat  compounds,  
hence   causing unwanted  overgeneration.   Thus we can mark  the 
different  elements of the compound:  the top node  is given  the  
attribute-value  pair  {\em compound=yes} and the  daughters  receive  
the marking {\em compound\_part=yes}.  In the case of compounds having 
more than 2 elements, such markings will appear as appropriate at 
each  level of the compound's structure.  Note that we are not at 
this stage concerned with the possibility of multiple analyses of 
a compound,  but simply with being able to represent information. 

Normally,   all  information  concerning  non-dependency  related 
features  will  have  been added earlier in  the  analysis  phase 
before arriving at the transfer level, when rules consolidate and 
integrate   this  information  as  part  of  the  transfer  level 
information.

There   follows   an  example  of  IS  structure  building  rules 
necessary  to  build  a transfer level representation   for   two  
English  compound   types,   as  exemplified  by  the   compounds 
{\it dressmaking}  (1.i.)  and {\it table leg} (1.ii.).  These rules  are 
written  as  Eurotra b-rules,  in which square brackets  after  a 
feature  bundle  enclose its daughter feature  bundles.  Thus, in 
terms  of  context-free  rules  the  left  hand  side  (LHS)   is 
represented  by  the top most feature bundle and right hand  side 
(RHS) categories are given between square brackets.  Here we have 
a simple sequence of two categories on the RHS. We  have 
omitted irrelevant detail.

\begin{enumerate}
\item $b_{comp1}$ = \{mc=word,cat=n,barlevel=zero\}\\
              {[\{role=gov,mc=word,cat=n,isframe=arg2\},\\
               \{role=arg2,mc=word,cat=n,isframe=arg0\}]}.

\item  $b_{comp2}$ = \{mc=word,cat=n,barlevel=zero\}\\
               {[\{role=gov,mc=word,cat=n,isframe=arg0\},\\
                \{role=mod,mc=word,cat=n,isframe=arg0\}]}.
\end{enumerate}

As  these  are  general rules dealing  with  types  of  compound, 
specific  lexical unit information does not appear.  Also,  in an 
actual  Eurotra  grammar,  these rules would be  much  barer  and 
contain variables,  as unification would play a strong role.  The 
spirit of the rules however would not change.  In the above rules 
we  see  that these manipulate only objects that  are  nouns  and 
words  to  build a structure which will be marked not only  as  a 
noun  and a word but also as having bar-level 0:  in other  words 
the resultant structure is marked as an atomic object. 
The  rules differ only with respect to the way frame  information 
is handled: 1.i. indicates that the head {\bf (role=gov)}, in this case 
`making' which is a deverbal, argument taking noun {\bf (isframe=arg2) }
demands that its internal argument play the role of deep object.

\section{Calculation of IS representation}

We  now  address  possible  means  of  calculating  the  type  of 
representation    presented   above.    In  order  for   transfer 
representations   to    be   properly    built,    the    correct  
dependency  roles  must  be  assigned.  These assignments  depend   
on     proper    calculation    of  morphological  level      and 
syntactic  level  representations.  This is a basic mechanism not 
only of a Eurotra approach but of many systems and frameworks. In 
general,  information at one level is derived from information at 
an earlier level and is thus dependent upon it:  if it is  wrong, 
the derived information will also be wrong.

This   process   however  becomes  complicated  in  cases   where 
information  normally  thought  of  as  being  confined  to   one 
component is instead distributed over several levels and thus has 
to be dealt with by several components. 

We have already seen how,  in a Eurotra model,  certain  surface-
oriented information is present at the transfer level. This is an 
interesting  aspect of a theory of MT which views components in a 
way different to that of `normal' NLP.  That is,  not only do  we 
need  to  handle  different kinds of  linguistic  information  at 
different  levels  of linguistic description (the `normal'  case) 
but  also we must allow for certain of these different  kinds  of 
information to be available at levels above their `natural' level 
and particularly at the transfer level,  partially or wholly,  in 
original or derived form.  This implies that we are dealing  with 
multifunctional     representations,     the     most     complex 
multifunctionality  occurring at the transfer level.  This allows 
the   linguist   to   conceive  of   different   views   over   a 
representation.  A  pure syntactic view,  for example,  will  not 
`see'  morphological  information  although  it may  in  fact  be 
physically present in a representation.

Thus,   in   our  approach,   the  processing  of   morphological 
information is not confined to a morphological component applying 
early in analysis and late in synthesis.    We see  morphological 
information as appearing over  all levels of representation above 
the  classic morphological one.  The case of compounds makes this 
clear:  morphological information about the make-up of a compound 
must  be  available at the transfer level in order to  guide  the 
mapping  to  a  target language compound type  (it  may  also  be 
required  in  synthesis,  although we do not discuss this  here). 
This  information must be carried through the various  levels  in 
analysis and also conform to the requirements of each level.  For 
example,  in Eurotra there is a move from a constituent structure 
representation  to  a  dependency structure representation  at  a 
certain  point,  thus morphological information must fit in  with 
this and/or be modified as necessary. 

If  morphological information is to  remain  available,  although 
perhaps  in modified form due to local level requirements,  it is 
important  to  ensure that it is not lost or  adversely  affected 
during  its passage,  or,  because of its presence,  cause a non-
morphological  component to reject it.  Here,  we return  to  the  
notion  of atomicity.  This first becomes a crucial notion at the 
level which deals primarily with syntactic constituent  structure 
and  is intimately bound with the decision to regard compounds as 
belonging to phrasal or lexical categories. 

If one chooses to primarily analyse  a multiword compound at  the 
syntactic  constituent  structure level and assign it  a  phrasal 
category,   it  will  not   unsurprisingly  receive   a   phrasal  
interpretation   thereafter.  Even  if we set aside  the  various 
arguments  discussed  above  against  interpreting  compounds  as 
phrases,  it  is  clear that we would lose most if not all  of  a 
compound's   morphological  interpretation  if  it  were  to   be 
characterised  as  a  phrasal  category.  This  would  result  in 
paraphrase translation,  as we would have lost the ability to map 
between  morphological  compound  types at  the  transfer  level. 
Furthermore, many phrasal rules would apply in a spurious manner, 
unless  somewhat complex,  linguistically ad hoc mechanisms  were 
used to block them.

However, we can appeal to the notion of atomicity to help us, and 
combine  this  with  a view of compounds as  belonging  to  major 
lexical categories. 

In what follows, we shall be concerned with two points:

\begin{itemize}
\item  theoretical linguistic issues 
\item  computational linguistic issues within a given framework

\end{itemize}
A  major  theoretical linguistic issue  concerns  locality:  this 
states roughly that,  if morphology is handled by a morphological 
component,  syntax by a syntactic component, and so on, then e.g. 
things local to syntax should be taken care of by syntax.

When  we  come  to  implement  such a  view  in  a  computational 
linguistic framework, we then try to respect linguistic locality. 
This becomes difficult, as we have seen, when we are dealing with 
multifunctional representations.  However,  there is a particular 
problem  that is raised in computational linguistic  terms  which 
apparently causes locality to be radically violated. This problem 
arises  due  to the underlying nature of the  Eurotra  formalism. 
Interestingly for us, the existence of this problem enabled us to 
think of a solution that:

\begin{itemize}
\item  was linguistically satisfying
\item  respected locality
\item  was computationally linguistically elegant
\end{itemize}

We now explain the background to this problem.

In  Eurotra,  the  morphological level (EMS) is intended to  deal 
only  with  individual  wordforms:   it  is  thus  restricted  to 
operations   such   as   inflection,   derivation   and   oneword 
compounding.   It  outputs  sequences  of  trees  each  of  which 
describes a wordform.
 
In   our  linguistic approach,   all compounds are  analysed  and 
assigned   a  structure before  the syntactic component  applies: 
if  compounds are atomic at the syntactic level this must be  the 
case.  If  we  instead  analyse multiword  compounds  within  the 
syntactic component, we violate the locality principle. If we are 
to   be  linguistically  consistent,  we  must  ensure  therefore  
that 
compounds  arrive as input to the syntactic component  as  atomic 
objects.

If  the morphological component outputs only individual  wordfrom 
descriptions  and  the  syntactic  component  is  constrained  to 
carry  out its analysis on syntactic atoms,  we then must find  a 
way  of  ensuring  that  sequences of  individual  wordforms  are 
mapped  to  structures representing  multiword  compounds,  where 
appropriate.  There is no problem for oneword compounds as  these 
can  be  analysed  as such and marked as syntactic atoms  in  the 
morphological  component (they could alternatively be  marked  as 
syntactic atoms in  the mapping to the syntactic component).

An   obvious   solution  would  be  to  arrange   for   sequences 
representing elements of multiword compounds to be recognised  in 
the  mapping  between the morphological and syntactic  components 
and to be inserted into appropriately marked structures.

The  Eurotra  framework offers a  transduction  facility  between 
levels   of  representation,    using  what  are  called  {\bf
t-rules} 
(translation rules). 
Unfortunately,  the t-rule formalism does not allow the  linguist 
to  specify sequences of feature bundles on the left hand side of 
her t-rules.

There  is  an  interesting discussion to be  held  regarding  the 
division  of  labour between grammars for linguistic  levels  and 
grammars  that  map between levels of linguistic  representation. 
However,  here we have no choice whatsoever:  we cannot  increase 
the  power  of the t-rule component,  even if we wanted  to,  for 
formal reasons.

This formal constraint however then forced us to abandon what  we 
suspected would in any case have been a computational  linguistic 
solution  in  the  bad sense,  i.e.  a  solution  unmotivated  by 
linguistic   theory  which  was  however  pragmatically   useful.

We then seemed to be at an impasse.  However, upon reflection, we 
noted that the existing computational morphological component did 
not in any case do what we considered a linguistic  morphological 
component should do:  handle multiword compounds. A solution then 
presented   itself:   to   create  an  additional   computational 
morphological component which dealt with multiword compounds.  We 
would  then view the linguistic morphological component as  being 
split over two computational morphological components.

In terms of processing, instead of the standard Eurotra model:

\begin{center}

{\bf [...]$\rightarrow$  EMS  $\rightarrow$   EMS-ECS t-rules $\rightarrow$ ECS $\rightarrow$ [...]}
\end{center}

we would then have:

\begin{center}
{\bf [...]  $\rightarrow$ EMS $\rightarrow$  EMS-ECS t-rules $\rightarrow$ EMCS $\rightarrow$ EMCS-ECS t-rules  $\rightarrow$ 
ECS $\rightarrow$  [...]}
\end{center}


(where the MC in EMCS stands for Multiword Compound)

EMCS deals specifically with multiword compounds:  it effectively 
passes  all  other  material through  without  change.  It  takes 
sequences  of wordform representations as its legal input,  which 
describe monomorphemic,  inflected,  derived or oneword  compound 
forms  and  outputs  representations  that  in  addition  contain 
structured  representations  of  multiword  compounds  marked  as 
syntactic  atoms.  Depending  on whether we think  all  compounds 
should  be  marked  as syntactic atoms within the  one  component 
rather than when first analysed, we could arrange to mark oneword 
compounds  as  syntactic atoms within EMCS rather  than  in  EMS. 
Indeed,  a  purist  might  prefer to insert  syntactic  atomicity 
markers only in the mapping from EMCS to ECS,  however we do  not 
consider  this  to be a major issue:  it is only  important  that 
objects should arrive as input to ECS already so marked.

At  the point of entry into the syntactic component,  oneword and 
multi-word  compounds  will  appear with at  least  the  following 
information  attached,  in general terms (we omit other  details, 
including lexical unit details):

\begin{verbatim}
{cat=n,bar=zero,mc=word,compound=yes} 
[ {cat=n,bar=zero,mc=word,compound_part=yes}, 
 {cat=n,bar=zero,mc=word,compound_part=yes}]
\end{verbatim}
 
We therefore preserve locality of analysis in both our  two-stage 
linguistic morphology component and in our syntactic component.
ECS  is presented with unconsolidated objects that  are  to 
all  intents and purposes opaque below the topmost feature bundle.
\footnote{
An unconsolidated object is one where the  relations 
between the feature bundles  are {\bf weak}, i.e. subject to possible 
modification,  for  example  in  terms  of  addition  of  feature 
bundles.}

We recall that  the  topmost feature bundle of such objects would 
have to  contain  all 
information  relevant to ECS regarding inflection and  head-hood. 
This can be assured by percolation/elevation either at EM(C)S or 
in the relevant t\_rules.   On exit from ECS,   later  t\_rules  and  
generators  will    apply  normally  to  assign  relevant   role  
information.   
\footnote{ A generator is associated with a linguistic  level. 
It  takes  an  unconsolidated object and  applies  various  rules 
including  structure  building  rules  to complete  and  fill  in 
information  and  to establish final  relationships  between  the 
nodes (i.e. consolidate the structure).}

In  addition,  through adoption of the above augmented model,  we 
reflect  the status of multiword compounds which are seen to  lie 
on the boundary between morphology and syntax.  This organisation 
also reflects the widespread view that derivation and  inflection 
are  ordered  (in  Level  Ordering  terms)  before 
native   multiword   compounding.   Level  Ordering   assumes   a
stratum/level  ordering of morphological operations which imposes 
useful constraints on the combination of morphemes.  For  further 
details  see \cite{Mohanan86}, \cite{Siegel74}. \cite{Sproat92} in 
particular   notes  that  level  ordering  should  be  {\em exploited 
further} in a computational environment. \cite{Ananiadou94} in fact 
describes a computational grammar which instantiates a four level 
ordered morphology of English.

The  relationship  between linguistic  theory  and  computational 
linguistic processing can be appealed to further when we consider 
what happens within the ECS component.  It might be objected that 
ECS  must  know  about  morphology,  in order  to  recognize  and 
consolidate  the  structures passed to it:  it  must  not  reject 
compound  word structures,  it must treat them as atomic,  but it 
must also be able to incorporate them as transparent entities  in 
order that adequate ECS-internal initial tree structures be built 
for them.  However, our response to such an objection is that the 
building   of  initial  internal  structures  is  effectively   a 
computational   linguistic  task.   Once  the  initial   internal 
representations are built,  they then become exposed, as it were, 
to  the linguistic theory of that level,  by which time compounds 
have been marked as atomic for syntax although their substructure 
is still there:  this information arrived from EMCS.  We may thus 
conceptually view the ECS component as being partitioned into two 
sets of rules: one to deal with representations of compounds when 
building initial representations, the other to deal, according to 
the  linguistic  syntactic theory,  with representations of  non-
compounding  phenomena which will include dealing with  compounds 
as  atomic  objects.   The  substructure  of  compounds  will  be 
automatically preserved,  so long as there are no explicit  rules 
which delete it (typically when mapping between levels).


It  is important  to  grasp the difference between lexicalisation  
of   a  compound  and  rendering a compound atomic  (or  opaque),  
and so we  emphasize this point here.   Compounds are lexicalised 
only if they do  not fulfill  the  criteria  for  analysis,  i.e.  
if   they   are   non-compositional,    compositional   but   not 
productive,    they  show   no  morphological   or   phonological 
regularity   in  word  formation   and  they  have  no   semantic 
transparency.   These compounds are  entered in  the  dictionary.  
Compound  terms  are  also  candidates  for  lexicalisation.  The 
question  remains  as  to when   we lexicalise   those  compounds 
identified for  lexicalisation.  This is an important decision in 
a  Eurotra-type framework,  as each level has its own  dictionary 
associated with it,  conceptually. Thus, an object might become a 
candidate  for lexicalisation at a late level  of  analysis.   We 
might  opt  for  an early lexicalisation or  for  lexicalisations 
which  take place at different levels according to circumstances. 
We  believe  that  as  soon a  compound   is  identified   as   a  
candidate  for  lexicalisation  it  should  be lexicalised.  That 
is,  its substructure is deleted (e.g.  in the transduction phase 
from EMS/EMCS to ECS) and all relevant information concerning the 
compound  is to be found in the single feature bundle  which  now 
represents it.   The  LU associated  with  the  lexicalised  form 
will   be   found   in  the dictionary associated with  the  next 
level of analysis (this implies that the lexicographer must  have 
entered   the  appropriate  lexicalised  LU  in  the  appropriate 
dictionary).

As   an  example  we  offer  the  case  of  the  Greek   compound 
{\it oksugonokollysy} (welding)  which will be analysed in  EMS  and 
then  lexicalised  during the passage from EMCS to  ECS  via  the 
following t\_rule (or translation (transduction) rule): 

\begin{verbatim}
  emcsgr-ecsgr.t
 t_oksugonokollysy = WORD:{cat=n}
                 [ ~:{cat=n,lu=oksugon},
                   ~:{lu=o},
                   ~:{cat=n,lu=kollysy}]
      =>  WORD:{lu=oksugon_o_kollysy}
\end{verbatim}

The  effect  of  this  rule is to build  a  new  feature  bundle, 
with  the  information specified on the RHS,  and  to  unify  the 
feature  bundle  of the LHS with it that carries the  same  index 
(here,  WORD).  No  other information is carried across from  the 
LHS:  in  effect,  all LHS substructure dominated by the  feature 
bundle indexed by WORD is deleted (via the marker \verb@~@). Note that other 
information  may  be inserted by unification in the  RHS  feature 
bundle:  the  LHS  feature bundle indexed by WORD  could  contain 
other attribute-value pairs -- they are simply not mentioned here 
as they are of no interest for this rule.


Lexicalisation  is  a destructive operation in the sense that  no
substructure  is  preserved in  subsequent  levels.  Rendering  a
compound   atomic  or  opaque  on  the  other   hand   does   not  
involve      any    destructive    operation.     Feature-marking 
(barlevel=zero)  is used to block  the application of whole  sets 
of  rules to substructures of compounds,   thus  to  all  intents 
and  purposes  such  rules  never {\em see}  the  internal makeup  of  
compounds,   although  the  entire representation,   substructure  
included,   is  carried through  the particular generator  (here, 
ECS).


It  will  
be  impossible  for ECS and IS rules  to distinguish structurally 
between  what  were \mbox{originally} oneword and multiword compounds as 
each will now have the same representation.  If desired, however, 
clearly  appropraite  features could be attached to  reflect  the 
original textual nature of the compound.

\section{Problems}

It should be emphasized that work was concentrated on classifying 
compound   types,    monolingually   and    cross-linguistically, 
specifying  representations  adequate for transfer  and  evolving 
strategies  to compute these representations.  We have not as yet 
addressed other areas such as:

\begin{description}
\item[a)]  how  to establish the scope of compounds (Adjective  +  Noun 
sequences in
English  for  example  are notoriously  difficult  to  interpret, 
especially when there are several adjectives).
\item[b)] how to determine the correct segmentations (for oneword forms)
and parses for compounds;
\item[c)]  how  to  know  when to synthesize a  oneword  compound  in  a 
language which admits oneword and multiword forms.
\item[d)] how to deal with the problem of double analysis:
\end{description}
\begin{center}
{\em                 N + N = true compound \\
                 N + N = nonce sequence of nouns, not a compound}
\end{center}

Regarding this last point, if  rules apply in parallel then double 
analyses will result.  If 
rules apply sequentially, then the wrong choice may be made, if something
is marked as a compound when it is not (or vice versa depending on the
order of rules).

The solution  to  {\bf d)}  is presumably  dependent  on  having access to
semantics or world knowledge. It is possible also that statistical or
probabilistic information could be derived from corpora which would allow
preference rules to be written to help in the disambiguation of
double analysis cases.
 
In  general,  a  system  would probably want to  preserve  double 
analyses, just in case, until quite late in the translation stage, if it
were a batch system. An interactive system could clearly rely on human
intervention to help resolve ambiguous analyses of nominal sequences.

Resolution  of the first and second points above  demand  greater 
knowledge  than  at present available.  We can achieve a  set  of 
segmentations  and  a  set  of parses for  a  compound,  and  can 
constrain our analysis further by use of e.g.  argument  linking, 
however  it  is extremely difficult to know,  in the  absence  of 
greater  knowledge,  which of the remaining parses is the correct 
one.  Compounds  in English consisting of strings of nouns are  a 
case  in point,  demanding semantic and real world knowledge  for 
their interpretation. 
\footnote{The problem of processing compounds is also crucial
for other fields of NLP besides MT, such as text understanding,
intelligent tutoring, term recognition etc.} 

These and other areas demand investigation. However, the research
reported on here was more concerned with establishing theoretical
principles for analysis, representing and synthesising compounds,
no previous such work having been undertaken in Eurotra. We 
therefore operated largely with the assumption that we were 
working with a certain type of object, rather than addressing
e.g. the problem of how to identify such an object in the first
place in running text. However, with a theoretically motivated
foundation, others can now proceed to look at the other areas
mentioned above.



\def\refname{}
\bibliographystyle{nlpbbl}
\bibliography{epaper}

\begin{biography}

\biotitle{}

\bioauthor{Sophia Ananiadou}
{
Sophia Ananiadou is a senior lecturer in the department of
Computing at the Manchester Metropolitan University and a
research fellow at UMIST. She studied at the Universities of
Athens and Paris Jussieu VII and obtained her PhD from the
University of Manchester.
She has worked for the
Eurotra MT system on developing transfer modules
and on doing research into morphology. Her current work includes
knowledge acquisition from technical corpora, developing
methodologies for sublanguage MT. From October 1995 she will stay
at the Communication Science Laboratories of  NTT as an invited
researcher.
}

\bioreceived{Received}
\biorevised{Revised}
\bioaccepted{Accepted}

\end{biography}

\end{document}
