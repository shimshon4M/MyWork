<?xml version="1.0" ?>
<root>
  <title/>
  <author/>
  <jkeywords/>
  <subsubsection title="">*DefinitionofSymbols</subsubsection>
  <section title="Introduction">Recently,probabilisticlanguagemodelshavebeenshowneffectiveinmanynaturallanguageapplications.Onesuchapplicationisautomaticspeechrecognition.Speechinherentlycontainsambiguitiesanduncertaintiesthatcannotberesolvedbypureacousticinformation.Duringrecognition,manyacousticallysimilarhypothesesarebuilt.Toeffectivelyrankthesehypotheses,thespeechrecognizerisrequiredtorelyonlinguisticlikelihoodaswellasacousticlikelihood.Aprobabilisticlanguagemodelprovidesthebasisforcalculatinglinguisticlikelihood.Onewell-knownprobabilisticlanguagemodelisaprobabilisticcontext-freegrammar(CFG),*-1mmthatisagrammarwhoseproductionruleshaveattachedtothemaprobabilityofbeingused.Theseproductionprobabilitiesareusuallyestimatedfromatrainingcorpusunderaprobabilisticindependentassumption,thatthechoiceofaproductionruleisindependentofthecontext.But,thissimpleassumptionoftenresultsinapoorestimateofprobability.Recently,morepowerfullanguagemodelsbeyondsimpleprobabilisticCFGshaveattractedconsiderableattention;someofthemtakecontext-sensitiveprobabilitiesintoaccount.ThispaperwilldescribeanimprovedprobabilisticCFG,calledthemixtureprobabilisticCFG,basedonanideaofcluster-basedlanguagemodeling.Thebasicideaofthismodelinvolvesclusteringatrainingcorpusintoanumberofsubcorpora,andthentrainingprobabilisticCFGsfromthesesubcorpora.Attheclustering,thesimilarlinguisticobjects(e.g.,belongingtothesamecontext,topicordomain)areformedintoonecluster.TheresultingprobabilisticCFGsbecomecontext-ortopic-dependent,andthusaccuratelanguagemodelingwouldbepossible.Thispaperisorganizedasfollows.Section~2givesanoverviewofaprobabilisticCFG.Section~3describesamixtureprobabilisticCFG.Section~4containsevaluationexperiments,includinglanguagemodelevaluationexperimentsfromtheviewpointofperplexityreductionandspeechrecognitionexperiments.Section~5describesanotherevaluationexperimentsusingtheIFT-annotateddialoguecorpus.Finally,Section~6presentsourconclusions.</section>
  <section title="Probabilistic CFG: An Overview">AprobabilisticCFG(Fujisaki,~T.,Jelinek,~F.,Cocke,~J.,Black,~E.andNishino,~T.~~1991)extendsaCFGsothateachproductionruleisoftheform&lt;!!A,p!!&gt;,wherepistheconditionalprobabilityofAbeingrewritteninto.TheprobabilitiesofallA-productions(ruleshavingAontheLHS)shouldsumto~1.IntheprobabilisticCFG,theprobabilityofaderivationcanbecomputedastheproductoftheprobabilitiesoftherulesused.SupposethatisaderivationofwfromthestartsymbolS,thentheprobabilityofthisderivationDisgivenbyTheprobabilityofasentencewisthesumoftheprobabilitiesofallpossiblederivationsforw.Theproductionprobabilitiesareestimatedfromatrainingcorpusasfollows:</section>
  <subsection title="Mixture Probabilistic CFG">AmixtureprobabilisticCFGisbasedonthesecondapproach.Inaconventionalmanner,productionprobabilitiesareestimatedusingthewholetrainingdata.InamixtureprobabilisticCFG,however,wedividethetrainingcorpusintoNclusters,andestimateseparateprobabilitydistributionforeachcluster.Thus,asaresult,wehaveNprobabilitydistributionsfortheCFG.NowsupposethatthetrainingcorpusTisdividedintoNclustsersT_1,T_2,,T_N.Thatis,LetP_i(S)denotetheprobabilityofsentenceSusingtheprobabilitydistributionobtainedfromclusterT_i.Then,themixtureprobabilisticCFGcalculatestheprobabilityofSasfollows:InEquation~,q_iistheprobabilityofsentenceSarisingfromclusterT_iandcalculatedasfollows:Here,|T_i|indicatesthenumberofsentencesinclusterT_i.</subsection>
  <subsection title="Cluster-Based Language Modeling">Therearetwodifferentapproachesforcluster-basedlanguagemodeling.Thefirstapproachaddressesthedatasparsenessproblem.Inprobabilisticlanguagemodeling,modelparametersareusuallyestimatedaccordingtotheirfrequenciesinatrainingcorpus.However,sincetheamountofavailabledataislimited,manyeventsareinfrequentanddonotoccurinthecorpus.Tocircumventthisproblem,thetrainingdataisclumpedintoanumberofclusters,whicharethenusedtosmoothprobabilitiesofoccurrenceforinfrequentevents.Aclass-basedn-grammodelisatypicalexampleofthisapproach.Thesecondapproachaimstoincreasethemodelprecision.Thebasicassumptioninthisapproachisthatthelanguagemodelparametershavedifferentprobabilitydistributionsindifferenttopicsordomains.Thetrainingcorpuscontainstextsfromvariouskindsoftopicsordomains.Thisapproachfirstdividesthetrainingcorpusintoanumberofsubcorporaaccordingtotheirtopicsordomains,andthenperformstopic-ordomain-dependentlanguagemodeling.Worksbelongstothiscategory.</subsection>
  <section title="Evaluation Experiments"/>
  <subsection title="Corpus and Grammar">In*1mmour*1mmevaluation*1mmexperiments,*1mmwe*1mmused*1mmthe*1mmADD*1mm(ATRDialogueDatabase)*1mmcorpus,whichwascreatedbyATRInterpretingTelephonyResearchLaboratoriesinJapan.TheADDcorpusisalargestructureddatabaseofdialoguescollectedfromsimulatedtelephoneorkeyboardconversationswhicharespontaneouslyspokenortypedinJapaneseorEnglish.Currently,theADDcorpuscontainstextualdatafromtwotasks(textcategories);oneconsistsofsimulateddialoguesbetweenasecretaryandparticipantsatinternationalconferences(ConferenceTask),andtheotherofsimulateddialoguesbetweentravelagentsandcustomers(TravelTask).Inourexperiments,weusedthekeyboarddialoguesfromtheConferenceTask.Intheexperiments,wealsousedaJapaneseintra-phrasegrammarfortheConferenceTask.Thisgrammardoesnotdescribeasentencestructure,butitdescribesconstraintsinsideJapanesephrases.Figure~showssomeproductionsinourgrammar.InFigure~,thegrammarsymbolsquotedby*1mm&lt;&gt;*1mmindicate*1mmnonterminal*1mmsymbols.*1mmThestartsymbol,indicatedby&lt;start&gt;,isrewrittenintophrasecategorynames.Forexample,&lt;interj&gt;,&lt;conj&gt;and&lt;np&gt;arenonterminalsymbolsforinterjectionwords,conjunctionalphrasesandnounphrases,respectively.Ourgrammarwaswrittenforphone-basedspeechrecognition,thusterminalsymbolsarephonenames.Table~showsthesizeofthegrammarandthetraining/evaluationdata.</subsection>
  <subsection title="Corpus Clustering">CorpusclusteringisrequiredtoderiveprobabilitydistributionsinamixtureprobabilisticCFG.Inourevaluationexperiments,theclusteringwasconductedusingphrasecategorynamessuchas*1mm&lt;interj&gt;*1mm,*1mm&lt;conj&gt;*1mmor*1mm&lt;np&gt;*1mm.Wefirstsegmentedthetrainingcorpusintophrases,andthenassignaphrasecategorynametoeachphrase.Categoryassignmentwascarriedoutbyanalyzingeachphraseusingthetheintra-phrasegrammar.Inthisway,thetrainingcorpuswasdevidedintoanumberofclustersaccordingtotheirphrasecategories.Thereisonethingthatshouldbenotedhere.SincetheparametersforthemixtureprobabilisticCFGarederivedbystatisticalestimationfromeachcluster,thesizeofeachcluster(thenumberofphrasesbelongingtoeachcluster)islargelyreponsibleforthequalityofthemodel.Inotherwords,inordertoestimatethereliableprobabilities,eachclustermusthaveenoughdata.Inourexperiments,theintra-phrasegrammarhad109phrasecategories.However,afterclusteringbasedonthese109categories,someclustershadveryfewdata.Forthereliablestatisticalestimation,clustershavingfewerthan10phrases(32clustersintotal)weremergedintoonecluster.Asaresult,wehad78clustersobtained.</subsection>
  <subsection title="Evaluation Results">ToevaluatethequalityofamixtureprobabilisticCFG,wecalculatedthetest-setperplexity.Asacomparison,wealsocalculatedthetest-setperplexityofasimpleprobabilisticCFG.Thetest-setperplexityistheinformation-theoreticaveragebranchingofwordsalongthetestsentences(test-set),andisusedasameasureofthedifficultyofarecognitiontaskrelativetoagivenlanguagemodel.Ingeneral,speechrecognitionperformanceisexpectedtoincreaseasthetest-setperplexitydecreases.Thus,alanguagemodelwithlowperplexityisbetter.Asstatedearlier,terminalsymbolsoftheCFGarephonenames.Therefore,weactuallycalculatedthetest-setperplexityperphone.Aformulaforthetest-setperplexityperphone,PP,isgivenby:	PP&amp;=&amp;2^LP	LP&amp;=&amp;-1N_W_i=1^N_S_2P(S_i)eqnarraywhereN_Sisthetotalnumberofphrasesinthetest-set,N_Wisthetotalnumberofphonesinallphrases,andP(S_i)isthelanguagemodelprobabilityforthei-thphraseS_i.TheresultsofperplexitymeasurementsaresummarizedinTable~.ThemixtureprobabilisticCFGhadatest-setperplexityof2.47/phone,whilesimpleprobabilisticCFGhadatest-setperplexityof2.77/phone.ThisresultsupportstheeffectivenessofthemixtureprobabilisticCFG.</subsection>
  <subsection title="Speech Recognition Experiments">Wealsoconductedspeechrecognitionexperimentsusingthreelanguagemodels:	PureCFG(withoutproductionprobabilities),	SimpleprobabilisticCFG,	MixtureprobabilisticCFG.Asthespeechrecognitionsystem,weusedtheHMM-LRsystem,whichisanintegrationofhiddenMarkovmodels(HMM)andgeneralizedLRparsing.TheHMM-LRsystemisasyntax-directedcontinuousspeechrecognitionsystem.Thesystemoutputssentencesthatthegrammarcanaccept.Thespeechrecognitionexperimentswereconductedunderthespeaker-dependentcondition,usingdiscrete-type,context-independentHMMswithoutdurationcontrol.TheresultsreportedinTable~comparethreelanguagemodelsintermsofphraserecognitionperformance.Themixturemodelattainsthebestperformance.</subsection>
  <section title="Sentence-Level Evaluation Experiments">Intheprevioussection,wehavedescribedevaluationexperimentsbasedonphrase-levelclustering.Thissectiondescribesanotherevaluationexperiments,inwhichsentence-levelclusteringareperformedbasedondiscourse-levelinformation.Inthisevaluation,weusedadialoguecorpusinwhicheachutteranceisannotatedwithanutterancetype.Thecorpususedintheevaluationiswhatwecall``ModelDialogues'',whichconsistsof10dialogueswith225sentences.EachutteranceisannotatedwithIFT(IllocutionaryForceType),whichisanabstractionofthespeaker'sintentionintermsofthetypeofactionthespeakerintendsbytheutterance.Thefollowing9IFTsareusedinthiscorpus:phaticphaticexpressionssuchasgreetings(e.g.Hello,Good-bye)expressiveidiomsthatexpressthespeaker'sfeeling(e.g.Thankyou,You'rewelcome)responseidiomaticresponsesandshortanswers(e.g.Yes,Isee,That'sright)promisethespeakercommitshimselftoperformanaction(e.g.Iwillsendyouaregistrationform)requestthespeakerasksthehearertoperformanaction(e.g.PleasegotoKitaoojistationbysubway)informinformativeexpressions(e.g.Wearenotgivinganydiscountthistime)questionifYes-Noquestions(e.g.Doyouhavetheannouncementoftheconference?)questionrefWHquestions(e.g.WhatshouldIdo?)questionconfconfirmations(e.g.Youhavealreadytransferedtheregistrationfee,right?)Ourexperimentusedasententialgrammarfortheconferenceregistrationtask,whichhad2,789productionrulesincluding1,591words.Asstatedabove,utterancesinthedialoguecorpusareannotatedwiththeirIFTs.UsingtheseIFTs,wedividedthedialoguecorpusinto9clusters,andthenestimatedproductionprobabilitiesfromtheseclusters.TheresultingprobabilisticCFGwasevaluatedbycalculatingperplexity.Inthisexperiment,sincetheamountofavailabledataissmall(namely,10dialogueswith225sentences),alldatawereusedbothinthetrainingstageandevaluationstage.Thatis,perplexitymeasurementwasperformedagainst225sentences,whichwerealsousedintheprobabilityestimation.WithoutIFTclustering,theperplexitywas2.18perphone,butusingIFTclustering,theperplexitywasreducedto1.82perphone.Althoughthisevaluationwasconductedusingsmall-scaledata,ourresultsuggeststheeffectivenessofthemixtureprobabilisticCFG.</section>
  <section title="Conclusion">Inthispaper,wehaveproposedanimprovedprobabilisticCFG,calledthemixtureprobabilisticCFG,basedonanideaofcluster-basedlanguagemodeling.Thebasicideaofthismodelinvolvesclusteringatrainingcorpusintoanumberofsubcorpora,andthentrainingprobabilisticCFGsfromthesesubcorpora.Attheclustering,thesimilarlinguisticobjects(e.g.,belongingtothesamecontext,topicordomain)areformedintoonecluster.TheresultingprobabilisticCFGsbecomecontext-ortopic-dependent,andthusaccuratelanguagemodelingwouldbepossible.TheproposedmodelwasevaluatedusingtheADD(ATRDialogueDatabase)corpusandaJapaneseintra-phrasegrammar.Theeffectivenessofthemodelwasconfirmedbyperplexityreductionandspeechrecognitionexperiments.authorwouldliketothankMasaakiNagata(NTTInformationandCommunicationSystemsLaboratories)andTsuyoshiMorimoto(ATRInterpretingTelecommunicationsResearchLaboratories)forprovidinguswiththeIFT-annotateddialoguecorpus.document</section>
</root>
