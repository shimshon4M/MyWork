\documentstyle[epsf,jnlpbbl]{jnlp_j_b5}

\setcounter{page}{129}
\setcounter{巻数}{3}
\setcounter{号数}{4}
\setcounter{年}{1996}
\setcounter{月}{10}
\受付{1995}{12}{6}
\再受付{1996}{4}{1}
\採録{1996}{6}{14}

\setcounter{secnumdepth}{2}

\title{認知単位の{\bf bigram}を用いた日本語文解析の一方法}
\author{横田 和章\affiref{NODA} \and 藤崎 博也\affiref{NODA}}
\headauthor{横田 和章・藤崎 博也}
\headtitle{認知単位のbigramを用いた日本語文解析の一方法}
\affilabel{NODA}{東京理科大学基礎工学部電子応用工学科}
{Dept. of Applied Electronics, Faculty of Industrial Science and Technology, Science University
of Tokyo}

\jabstract{
現在，自然言語処理システムの多くは，処理単位として形態素を
用いているが，人間はもっと大きな単位で文を処理していることが
既に分かっている．この単位を認知単位と呼ぶ．
この知見から，人間の文解析処理は，認知単位の検出処理と，
検出した認知単位の取捨選択の2段階に分離できるものと考えられている．
本論文では，この考えに基づき，第一段階として状態遷移図を用いて認知単位を検出し，
第二段階として bigram を用いて認知単位を選択する，計二段階からなる
文解析法を提案するものである．この方法を用いて誤りを含んだテキストに対し
誤り訂正を行う実験を行った結果，形態素を単位とした bigram を
用いるよりも良い結果を得ることができた．
}

\jkeywords{
文解析，bigram，状態遷移図，有限オートマトン，認知単位
}

\etitle{A method for sentence analysis of Japanese\\
based on bigrams of cognitive units}

\ekeywords{
Sentence analysis, Bigram, State transition diagram, Finite automaton，Cognitive unit
}

\eabstract{
While most natural language processing systems adopt morphemes as units of
processing, humans are known to use larger processing units, which we call
cognitive units. The human process of sentence analysis can be considered
as consisting of two stages: detection and selection of cognitive units.
Based on this idea, this paper proposes a method for sentence analysis which
first detects possible cognitive units using a state transition diagram, and then
selects correct cognitive units on the basis of their bigrams. The proposed method was applied to text error correction, and the experimental results
confirmed that it can achieve a higher performance than that can be
attained using morpheme bigrams.
 
}

\begin{document}
\maketitle

\section{まえがき}

現在，機械による文解析の処理単位としては，形態素が利用される
ことが多いが，
これは，形態素を用いることにより辞書の語数を制限でき，計算機の
記憶を経済的に利用できるという利点があるからである．

bigram による解析方式は，文解析や音声認識など様々な分野において高い
評価を得ているものの\cite{jeli,naka}，
文字や形態素を単位とした bigram による解析は，
単位が小さすぎて，文の局所的な性質を解析しているのに過ぎないと考えられる．
しかし，trigram や 4-gram 以上になると，しばしば計算機の記憶容量の
限界を超えてしまい，実用的ではない．

筆者らは，知覚実験により，人間による文解析には，形態素より長い単位が
用いられていることを既に明らかにしている\cite{yoko,yoko0}．
従って，人間の場合と同様の長い単位を解析に用いれば，
機械においても高い処理効率が得られると期待される．
本論文は，このような観点から，bigram の単位として認知単位を用いる
方法を提案するものである．

形態素より長い単位を解析に用いる方法は，他にもいくつか報告されている．
例えば，音声認識の分野において，伊藤らは休止を単位とした解析を行う方法
を提案している\cite{ito}．また，テキスト処理において，文法的な解析が
難しい発話を処理するために，発話を部分的に構文解析する方法なども
提案されている．
しかし，これらの解析に用いられている長い単位は，解析の効率化のために
便宜上導入されたもので，比較的専用の用途にのみ使用できるものである．

人間における文解析処理が複数段階の処理からなると仮定すれば，
認知単位はその複数段階の処理において主に単位として利用されていると
考えられる．従って，機械における処理を同様に多段に分けて考えるとすれば，
認知単位はこの多くの段階において単位として汎用的に利用できる
ことが期待される．

機械の処理が，形態素解析，構文解析，意味解析，談話解析からなる
とすれば，認知単位を利用することにより構文解析の処理を効率化
できることが既に筆者らにより実証されている\cite{yoko0}．
本論文では，認知単位を利用することにより形態素解析に相当する
処理の効率化を行なう方法を提案し，認知単位の有効性を実証する．


\section{認知単位の知覚実験}

筆者らは知覚実験の結果から，人間による文解析には形態素より長い単位が
用いられていることを既に明らかにしている\cite{yoko,yoko0}．
図\ref{zu1} は文献\cite{yoko0}において，このことを確かめる
ために行った実験の結果である．

\begin{figure}[tbh]
\begin{flushleft}
\small
\makebox[30mm][r]{   50ms:} この\\
\makebox[30mm][r]{  100ms:} この問題は\\
\makebox[30mm][r]{  150ms:} この問題は\\
\makebox[30mm][r]{  200ms:} この問題は解決\\
\makebox[30mm][r]{  250ms:} この問題は解決\\
\makebox[30mm][r]{  300ms:} この問題は解決\\
\makebox[30mm][r]{  350ms:} この問題は解決ずみ\\
\makebox[30mm][r]{  400ms:} この問題は解決ずみ\\
\makebox[30mm][r]{  450ms:} この問題は解決ずみと\\
\makebox[30mm][r]{  500ms:} この問題は解決ずみと\\
\makebox[30mm][r]{  550ms:} この問題は解決ずみ\\
\makebox[30mm][r]{  600ms:} この問題は解決ずみというつもりなのか\\
\makebox[30mm][r]{  650ms:} この問題は解決ずみというつもりなのか\\
\makebox[30mm][r]{  700ms:} この問題は解決ずみというつもりなのか\\
\makebox[30mm][r]{  750ms:} この問題は解決ずみというつもりなのかもしれないが私は\\
\makebox[30mm][r]{  800ms:} この問題は解決ずみというつもりなのか\\
\makebox[30mm][r]{  850ms:} この問題は解決ずみというつもりなのかもしれないが\\
\makebox[30mm][r]{  900ms:} この問題は解決ずみというつもりなのかもしれないが私は\\
\makebox[30mm][r]{  950ms:} この問題は解決ずみというつもりなのかもしれないが私はそう　思わない．\\
\makebox[30mm][r]{ 1000ms:} この問題は解決ずみというつもりなのかもしれないが私はそう　思わない．\\
\makebox[30mm][r]{ stimulus} この問題は解決ずみというつもりなのかもしれないが私はそうは思わない．\\
\end{flushleft}
\caption{認知単位知覚実験の結果例} \label{zu1}
\end{figure}


この実験では，コンピュータのディスプレイ上に30字程度の漢字かな混じり文を
短時間表示し，被験者に口頭で読んでもらう．
文は24文用意してあり，被験者が文を覚えないようランダムな順番で表示される．
提示時間は 50ms から 1s まで 50ms 刻みで長くしてゆく．
こうして，提示時間と，被験者が読むことのできた文字数との関係を調べる．

図\ref{zu1} の実験では，「この問題は解決ずみというつもりなのかもしれないが
私はそうは思わない．」という文を提示している．
結果は図の様な階段状になり，人間が文字単位で文を
処理しているのではないことは明らかである．また，読めた部分の
最後に着目すると，それはすべて文節境界となりうる形態素で終っている．
更に，「$\cdots$というつもり」や「$\cdots$かもしれないが」などのように，
複数の文節にまたがる句が一度に検出されていると思われるケースもある．
従って，人間は文節よりも長い句を検出していると推察される．
特に，「$\cdots$は」，「$\cdots$している」，「$\cdots$とい\\うつもり」，
「$\cdots$かもしれないが」のように，それだけでは意味をなさず，
先行する句の後について補助的な意味を表すような句は，
先行する句とともに一度に検出されている．
この結果から，人間の場合，まず長い句を一度に検出する処理を行い，
この処理の後，検出された長い句を単位として，更に高次の解析を行っているもの
と考えることができる．

この実験の結果から，人間においては次のような句が一度に検出される
ことが見出された．

\begin{enumerate}
\item 文節
\item 「$\cdots$かもしれない」などの慣用句
\item 「$\cdots$している」などの補助用言を含んだ句
\end{enumerate}

この単位を本論文では認知単位と呼ぶ．

この実験では口頭により被験者に文を読んでもらっているため，発話
された文は，脳内の処理を経て出力されたものである．従って，認知
単位は，意味処理を含む脳内の多段の処理において主に単位として利用さ
れているものと考えられる．

\section{認知単位の検出方法}

前節における実験の結果から，
人間における文解析過程は，認知単位を検出する処理と，認知単位を組み合わせて
文を認識する処理の2段階に分離できるものとみなせる．
このモデルに従い，機械においても文解析の処理を，認知単位を検出する処理と，
検出した認知単位の取捨選択の処理の2段階に分け
れば，解析の効率を高めることができると期待される．


前者の処理において，通常の形態素の辞書を用いて
文から認知単位を検出するには，どのような
形態素の並びが認知単位になるかという局所的な文法が必要である．
認知単位の内部における形態素の並びには，多重埋め込み的なものは少ない．
従って，この局所的な文法は状態遷移図で記述するのがふさわしい．よって
本研究では，認知単位を有限オートマトンで検出することにした．

\vspace*{-4mm}
\subsection{状態遷移図による認知単位の表現}
本研究では，処理の対象として，NHKラジオの気象通報の始めに放送される
天気概況文を用いた．
この例を図\ref{zu2} に示す．
これらの文に現れる認知単位を表層的な形式から128に分類し，
人手で128の受理状態を持つ状態遷移図を作成した．得られた
状態遷移図の一部を図\ref{zu3} に示す．図中$Z_i$は受理状態，
$S_i$は中間状態である．



\begin{figure}[b]
\small
オホーツク海には，発達中の低気圧があって，北北東へ進んでいます．

一方，中国東北部には高気圧があって，ほとんど停滞しています．

西日本は晴れ，東日本はくもりで，北日本では所々で雨が降っています．

尚，北海道周辺海域と三陸沖では所々濃い霧の為見通しが悪くなっています．

日本近海は，北海道東方海上から関東海域北部にかけて，シケています．

気温は，北海道，北陸，東海で，平年より１度高い他は，平年並か，１度から２度低くなっています．
\caption{天気概況文の例} \label{zu2}
\end{figure}

\begin{figure}[b]
\begin{center}
\epsfile{file=8-3.eps,width=90mm}
\end{center}
\caption{認知単位を受理する状態遷移図} \label{zu3}
\end{figure}
名詞句はあらかじめ地名，海，方角，数字，高気圧・低気圧，台風，波，霧，
天気などに分類してあり，
この状態遷移図においては，品詞の並びが同じでも名詞句の分類が異なる
名詞節は，異なる受理状態に遷移する．
従って，「日本海では」，「日本海は」，「気温は」は，すべて別の
受理状態に遷移する．
これは述語句や，その他の修飾句に関しても同様で，
品詞の並びだけではなく，意味的に気圧配置，気温，気圧，波，
霧のどの状態を示すのに使われるかによっても分類される．
従って，「悪くなっており」，「悪くなっています」，「高くなっています」
はすべて別の受理状態に遷移する．

天気概況文は気圧配置，天気，海上，霧，気温に関する文に大別できるが，
それぞれ表現の形式が限定されているため，比較的厳格な文法によりその文法を
記述できると考えられる．また，出現する形態素の数が限られており，同じ形態素が
何度も反復して現れる．従って，小規模なコーパスから得られたデータでも，
精度の高い解析が行える．


\subsection{誤りを含んだ文からの認知単位の検出}

前節で構成した有限オートマトンにより，文から認知単位を検出する
手順は以下の通りになる．

\newtheorem{tejun}{}
\begin{enumerate}
\item 文全体を走査し，形態素を検出して形態素ラティスを得る．
\item 得られた形態素ラティスに対し，有限オートマトンによる走査を
行い，認知単位を検出する．
\end{enumerate}


誤りを含んだ文に対し，文解析によって誤り訂正を行うタスクは，
通信やOCRなど様々な分野にしばしば求められるタスクである．

認知単位は，意味処理を含む脳内の多段の処理において主に単位と
して利用されているものと考えられるため，このような誤りを含む
テキストから誤りを取り除く場合にも人間はいずれかの段階で認知
単位を用いているものと思われる．従って，機械により誤り訂正を
行う場合にも認知単位は有効であると期待される．


誤りを含む文においては，形態素が近い綴りをもつ別の文字列に置き換わる
ことがある．従って，誤りを含む文に対して誤り訂正を行なうためには，
手順(1)において形態素を検出する際に，
厳密に文の一部に一致する形態素だけでなく，ごく近い形態素についても，
誤りによって文の一部に変化する可能性を推定しながら検出する必要がある．

本研究では，このタスクに対応するため，手順(1)において
図\ref{zu4}(a)のように距離1の形態素も検出することにした．
尚，この検出には DP 照合法を用いた\cite{rabi}．

\begin{figure}
\vspace{3mm}
\begin{center}
\epsfile{file=8-4.eps,width=97mm}
\end{center}
\caption{認知単位の検出} \label{zu4}
\end{figure}
誤りには，挿入，欠落，置換の3種類がある．ここでは，誤りが
図\ref{zu6} に示すように，次のモデルに従って発生するものと考える．
\subsection*{[誤りのモデル]}
\begin{quotation}
情報源の1文字あたり，確率$P_n$で誤りが発生する．誤りが発生した場合，
次のいずれかがそれぞれ条件つき確率$1/3$で起こる．

\begin{description}
\item{\gt 挿入:} 情報源の1文字の前または後ろに1文字が挿入され，2文字となる．
\item{\gt 欠落:} 情報源の1文字が失われる．
\item{\gt 置換:} 情報源の1文字が別の1文字に置き換わる．
\end{description}

\end{quotation}

以降の実験では，このモデルに従い誤りを含んだ文字列を発生させており，
挿入または置換の際必要となる文字としては，簡単のため平仮名46文字
のいずれかをランダムに選んで使用している．


今，$a$ と $b$ をそれぞれ文字列とし，$a$の長さは$l$文字， $a$ と $b$ の距離
は $d$ であるとする．上記の誤りによって $a$ が $b$ に変化する確率$P_e(a,b)$
は，$l - d$文字に誤りが発生せず，$d$文字に誤りが発生したと考えることにより，
次の式で近似できる．

\begin{figure}
\begin{center}
\epsfile{file=8-5.eps,width=63mm}
\end{center}
\vspace*{-2mm}
\caption{誤りのモデル} \label{zu6}
\end{figure}
\begin{equation}
P_e(a,b) \approx (1 - P_n)^{l - d}(\frac{P_n}{3})^d 
\end{equation}
　 実際には$d+1$文字以上の誤りによって，$a$が$b$に変化する場合も考えられるが，
本研究では$P_e$が十分小さいと考え，上式においてはこれらの場合は無視している．

手順(2)においては図\ref{zu4}(b)のように，手順(1)により得た形態素ラティス
に基づいて形態素を組み合わせることにより，認知単位を検出して認知単位
ラティスを作り，上式を用いて変化確率を計算するものとした．
\vspace*{-1mm}
\section{bigram による認知単位の取捨選択}
\vspace*{-1mm}
前節に示した手順により，文から認知単位を検出できる．
検出した認知単位は，状態遷移図における受理状態に
より128に分類される，この認知単位に対して
取捨選択を行い，文を組み立てるには，様々な方法が考えられる．
筆者らは既に，自動的に獲得した文法に基づき，認知単位を利用して
構文解析を行う方法を提案している\cite{yoko0}．

本論文では，誤りを含んだテキストから誤りを取り除く実験を行うが，
この実験において上記の構文解析を行うと，探索経路が莫大となって
計算に時間がかかる．
従ってこの実験には，より簡単な処理で効果的に
誤り訂正を行える方法が適している．

このような観点から，本論文では
bigram を用いて解析を行うことにした．bigram による方法は，自然
言語のようなマルコフ性を有する系列に対し効果的に取捨選択を行うことが
でき，特に音声認識などの分野では高い評価を得ている．

本研究では簡単のため意味解析は行わないが，
このように誤りを含むテキストを処理する場合，
構文解析や意味解析など，より高度な解析が必要な場合にも，
あらかじめ認知単位の bigram により不的確な候補を効率的に削除しておく
ことにより処理が効率化するものと思われる．

bigram の出現頻度表により記述できる性質は，系列の単純マルコフ的な性質に
限られるが，状態遷移図は多重マルコフ的な性質をも表すことができる利点を持つ．
しかし，認知単位の境界は分岐数が多いため，認知単位の多重マルコフ的な性質を
調べるのは極めて難しい作業となる．
従って，本研究では状態遷移図による解析は認知単位内にとどめる．

今，認知単位の系列$A={a_1, a_2, a_3,\ldots a_n}$の出現確率を$P(A)$とすれば，
\hspace*{-1mm}bigram モデルでは，\\系列$A$の出現確率は次のように表すことができる．
\vspace*{-1mm}
\begin{eqnarray}
P(A)& = & P(a_1|\mbox{Start}) P(a_2|a_1) P(a_3|a_2) \ldots \nonumber \\
& & P(a_n|a_{n-1})
\end{eqnarray}
　 ここで，\hspace*{1mm}$P(a_i|a_{i-1})$ \hspace*{1mm}は認知単位\hspace*{1mm}$a_{i-1}$\hspace*{1mm}の次に\hspace*{1mm}$a_i$\hspace*{1mm}が生起する条件つき
確率である．\hspace*{1mm}また，$P(a_1|\mbox{Start})$は文頭に$a_1$が生起する条件つき確率
である．従って，$A={a_1, a_2, \ldots a_n}$ が誤りによ\\り変化して
$B={b_1, b_2, \ldots b_n}$ として生起される確率は次のようになる．

\begin{eqnarray}
P_p(A, B) & = & P(a_1|\mbox{Start}) P_e(a_1, b_1) \nonumber \\
& & P(a_2|a_1) P_e(a_2,b_2) \nonumber \\
& & P(a_3|a_2) P_e(a_3,b_3) \ldots  \nonumber \\
& & P(a_n|a_{n-1}) P_e(a_n,b_n)  \label{eqa1}
\end{eqnarray}
\begin{figure}
\begin{center}
\epsfile{file=8-6.eps,width=46mm}
\end{center}
\caption{認知単位の bigram} \label{zu7}
\vspace*{3mm}
\end{figure}

ここでは，$a_i$として認知単位の状態遷移図における受理状態の記号
$z_j(0 \le j \le 127)$を用いる．従って，$P(a_i|a_j)$，$P(a_i|\mbox{Start})$を
あらかじめコーパスにより図\ref{zu7} の形の bigram にして求めておき，
(\ref{eqa1})式の$P_p(A, B)$を最大とする系列$A$を幅優先探索法に
よって探索する．

形態素を単位とした bigram を用いる方法では，
文を局所的に解析することしかできず，
また，より大域的な解析を行うために trigram や 4-gram を用いることにすれば，
必要とされる記憶容量が指数関数的に増大する他，巨大なコーパスを必要と
するなど様々な問題が生じる．この代わりに，このように認知単位を単位とした
bigram を用いることにより，記憶容量を抑えながら，大域的な解析を行うことが
できる．

\section{評価}

以上の方式を用いて，誤りを含むテキストに対して誤り訂正を施す
実験を行った．まず，解析の元となる bigram を作成するため，
認知単位に分割されたコーパスが必要である．このコーパスは
手入力した天気概況文1569文から作成した．
今回用いた天気概況文の場合，出現する形態素が限られているため，図\ref{zu3}
の状態遷移図で文を走査し，単純な最長一致法で区切ることにより
各文を認知単位に分割することができた．その際，
各文を処理する途上で通った状態遷移図の経路から，
形態素の区切りも検出し，コーパスに形態素情報として付加した．
得られたコーパスの異なり形態素数は196，のべ形態素数は26834，
1形態素あたりの平均文字数は3.90，perplexity は 3.2 となった．

こうして作成したコーパスから，更に認知単位の bigram 
を作成し，図\ref{zu6} のモデルにより誤りを混ぜた文300文に対し，
誤り訂正を施して認知単位に区切る実験を行った．
比較のため同じコーパスから，形態素情報を用いて
形態素の bigram を作成し，同じ文に誤り訂正を施し
形態素に区切る実験も行った．


この計算には SUN の SPARC Station 20 モデル 612 を使用した．
実験の結果，正しく誤り訂正ができた割合(正解率)と
300文の処理にかかった時間とを表\ref{hyo1} に示す．

\begin{table}[h]
\caption{誤り訂正実験結果}\label{hyo1}
\begin{center}
{\small (探索の各段階における経路数$=$300)}\\[3mm]
\begin{tabular}{c|c|cc} \hline \hline
bigram の & 誤り率 & 正解率 & 計算時間 \\
種類 & $P_n[\%]$ & [\%] & [s]\\ \hline \hline
形態素 & 0 & 100.0 & 1324 \\
       & 2 & 85.0 & 178 \\
 & 4 & 73.0 & 163 \\
 & 6 & 63.7 & 188 \\
 & 8 & 58.7 & 180 \\
 & 10 & 51.6 & 194 \\ \hline
 認知単位 & 0 & 100.0 & 7158 \\
 & 2 & 88.7 & 739 \\
 & 4 & 76.3 & 636 \\
 & 6 & 73.3 & 609 \\
 & 8 & 68.0 & 548 \\
 & 10 & 55.0 & 554 \\ \hline \hline
\end{tabular}
\end{center}
\vspace*{-3mm}
\end{table}


このような探索方法では，長い文においてあらゆる誤りの組合せを
すべて調べるには，巨大なメモリと極めて長い計算時間とを要する．
これらを制限するため，探索の各時点で，(\ref{eqa1})式により
探索経路を評価し，最も評価値の高い300経路だけを残す方式を採った．

このような経路限定を行うと，誤りが無い場合に比べ，誤りが
ある場合では経路の数が増えて，解が300経路に残らない確率が高くなり，
結果として処理は高速になる．

誤りが全くない場合は形態素，認知単位のいずれの bigram を
利用した方法でも，100\%正しく文を区切ることができた．
また，その他の場合は正解率は後者が前者に比べて3\%〜10\%程高くなった．
このことから前者では，
解の経路の評価値が300位以下に落ちて，途中で失われる確率が，後者に比べて高
いと考えられる．

処理中に消費するメモリの量については，探索経路数が等しいため
両方法ともほぼ同等である．しかし，認知単位を用いた解析の場合，
解の経路を残す確率が高い分，計算の時間が長くなっている．
この計算時間を評価するため，形態素のbigramを用い，探索経路数を
1200として誤り訂正を行った．その結果を表\ref{hyo2} に示す．

\begin{table}[bth]
\caption{誤り訂正実験の結果}\label{hyo2}
\begin{center}
{\small (探索の各段階における経路数$=$1200)}\\[3mm]
\begin{tabular}{c|c|cc} \hline
bigram の & 誤り率 & 正解率 & 計算時間 \\
種類 & $P_n[\%]$ & [\%] & [s]\\ \hline
形態素 & 0 & 100.0 & 5875 \\
 & 2 & 89.0 & 3827 \\
 & 4 & 75.0 & 3755 \\
 & 6 & 70.7 & 3576 \\
 & 8 & 67.0 & 3540 \\
 & 10 & 60.6 & 3583 \\ \hline
\end{tabular}
\end{center}
\end{table}

この表の場合，認知単位のbigramを用いて探索経路を300とした場合とほぼ同等の
正解率が得られているが，計算時間は誤り率0の場合を除いてほぼ7倍となっている．
また，メモリ消費量は探索経路数に比例するため4倍である．



これらの結果は，認知単位をあらかじめ検出しておいて解析に用いる
ことにより，処理を効率化できることを示している．

\section{むすび}

人間の文解析を認知単位の検出と取捨選択の2段階からなるとみなし，
このモデルに基づいて，局所的な解析に有限オートマトンを用い，
大域的な解析を bigram に基づいて行う方法を提案した．
有限オートマトンの処理は，他の解析方法と比べて直線的で
あり高速である．認知単位内の形態素の並びのように，局所的な
解析は直線的なモデルが適合する．

bigram より更に高度な文解析法では，一層再帰的な処理を行うため，
より長い計算時間を要することが多い．
このような解析方法においても，認知単位のような局所的な範囲に
ついては直線的な解析法を用いることにより効率化できると考える．

\vfill

\acknowledgment

本研究を進めるにあたり，貴重な助言をいただいた東京工科大学の
亀田弘之氏に深く感謝する．また，
データ入力やプログラミングを補助してくれた
東京理科大学藤崎研究室の阿部賢司氏に感謝する．

\vfill

\bibliographystyle{jnlpbbl}
\bibliography{nlp003}

\begin{biography}
\biotitle{略歴}
\bioauthor{横田 和章}{
1989年東京理科大学基礎工学部電子応用工学科卒業．
1993年同大学大学院修士課程了．
1996年同大学大学院博士後期課程了．
現在，(株)東芝 青梅工場所属．
}
\bioauthor{藤崎 博也}{
1954年東京大学工学部電気工学科卒業．MIT・KTH(1958--1961)．
1962年東京大学大学院博士課程了．
工学博士．
同年東京大学工学部専任講師．
1963年同助教授．1973年同教授．1991年東京大学名誉教授，
東京理科大学基礎工学部教授．
音声生成・知覚・情報処理，自然言語処理等の研究に従事．
昭和38年度電気通信学会稲田賞，昭和42年度同学会論文賞，
昭和42年度電気学会論文賞，
昭和47年度電子通信学会業績賞，
1987年IEEE音響・音声・信号処理学会功績賞，1988年米国音響学会特別功績賞，
1989年東京都科学技術功労表彰受賞．
}
\bioreceived{受付}
\biorevised{再受付}
\bioaccepted{採録}
\end{biography}
\end{document}

