<?xml version="1.0" ?>
<root>
  <title>開発者の視点からの機械翻訳システムの技術的評価---テストセットを用いた品質評価法---</title>
  <author>井佐原均内野一荻野紫穂奥西稔幸木下聡柴田昇吾杉尾俊之高山泰博土井伸一永野正成田真澄野村浩郷</author>
  <jabstract>機械翻訳システムの開発者がシステムの技術的評価を翻訳品質に注目して客観的に行う手法を開発した。評価過程の客観性と評価結果の解釈の客観性を維持するために、本手法では単なる評価用例文集ではなく、システムの出力を評価するための設問と、その設問がどのような言語現象を対象としているかについての解説とを各例文に付与したテストセットを用いている。各例文は基本的な言語現象と現在の機械翻訳システムにおいて処理が困難である言語現象のそれぞれを出来る限り網羅するように収集された。今回、英日機械翻訳システム、日英機械翻訳システムのそれぞれについての評価用テストセットを作成した。これらを用いて商用の機械翻訳システムでの評価実験を繰り返すことにより、機械翻訳システムの能力の差異を提示できることが示された。</jabstract>
  <jkeywords>機械翻訳，品質評価，コーパス</jkeywords>
  <section title="はじめに">機械翻訳システムの長い歴史の中で、システム評価は常に大きな課題の一つであった。システムの研究開発が健全に進むためには、客観的かつ正確な評価法が必要となる。このため、ユーザの立場から評価を行うもの、開発者の立場から評価を行うもの、また、技術的側面から評価を行うもの、経済的側面から評価を行うものと、多くの研究者によって様々な視点からの評価法が検討されてきた。これらの検討に基づいて、(社)日本電子工業振興協会によって一連の機械翻訳システム評価基準が開発されてきた(野村・井佐原1992,NomuraandIsahara1992a,NomuraandIsahara1992b,日本電子工業振興協会1993)。本稿で提案する機械翻訳システムの評価法は、システムの改良を続ける開発者の立場から、機械翻訳システムの技術面を翻訳品質に注目して評価するものである。機械翻訳システムの訳文の品質面での評価に関しては、従来からのいわゆるＡＬＰＡＣレポート型の評価法に加えて、近年、いくつかの提案がなされている。ある程度まとまった文章を翻訳し、そこから得られる理解の度合を評価しようとするものとして、ＡＲＰＡによる機械翻訳システム評価(Whiteetal1994)や、ＴＯＥＦＬのテストを用いる方法(Tomita1992)が提案されているが、これらはシステム間の現時点での性能の比較評価には用いることが出来ても、評価結果を直接システム改良に結び付けることは困難である。これに対し、個別の例文を収集することにより評価用の例文集を作成し、その各例文の翻訳結果を評価し、対応する言語現象の処理能力を判定しようとする提案がいくつかなされている。これらのうちには、単に文を集めるのみで、その後の例文の利用法(評価過程)は個別の評価者に任せようというものから、本稿で提案するように、客観的評価のためにさまざまな情報を付加しようというものまで、いくつかの段階がある。わが国においては、(社)日本電子工業振興協会が既に昭和６０年に機械翻訳例文資料として、翻訳における曖昧性に関する問題点に着目して、英文および和文を収集分類し公開している(日本電子工業振興協会1985)。また同協会は昭和６２年度に、機械翻訳システムの技術レベルを評価するために、文の複雑さの定量化、文の複雑さや文体の定性的特徴の抽出、標準的例文の収集を行なった(日本電子工業振興協会1988,石崎・井佐原1988)。この他、英語を話す人間と日本語を話す人間との間にある言語理解法の違い(言い替えると、日本語と英語の発想法の違い)に注目して日本語の言語表現を分類し、それらの表現の翻訳能力を評価する試験文集を作成するもの(池原・小倉1990,池原他1994)や、言語学的観点から日本語および英語の言語表現の構造に注目し、その表現上の構造的特性を的確に表すような試験文集を作成すること(成田1988)が提案されてきた。後者は、個々の言語現象に対する翻訳の可否を示すことの必要性から、一定の内容の文の言い換えなどによって日本語および英語の言語表現と翻訳能力の関係を言語学者の立場から評価することを提案している。本稿で論じる機械翻訳システム評価用テストセットは、以上のような、ＡＬＰＡＣレポート以来の品質評価に関する研究を踏まえて、誰でも客観的かつ実用的な評価を行なえる評価法の確立を目指し作成したものである。次節以下では、テストセットを用いた評価法の全体を流れる基本的な考え方、英日機械翻訳用テストセット、日英機械翻訳用テストセットについて、順次説明していく。</section>
  <section title="テストセットを用いた機械翻訳システムの品質評価法"/>
  <subsection title="本評価法の利点">これまで、機械翻訳システムの品質評価法として種々の方法が提案されているが、それらの方法に関しては一貫して客観的評価が困難であるという指摘が行なわれてきた。本稿ではまず、従来までの評価法と比べての本評価法の利点を、以下の二つの客観性に基づいて検討する。*1cm(1)評価過程が客観的であること*1cm(2)評価結果の判断が客観的に行なえることたとえば、ＡＬＰＡＣレポート等に代表される評価法は、評価の軸として「忠実度」「理解容易度」といった、その解釈が評価者の主観的判断に依存する基準を採用している。その結果、評価結果が評価者によって大きく異なってしまうという問題があり、(１)の客観性を満たしていない。この評価のばらつきは不完全な翻訳結果を評価する際に特に顕著に現れるが、現実の機械翻訳システムを評価し、開発過程にフィードバックする際には、翻訳に成功した場合よりも失敗した場合についての検討が重要である。この種の評価法においては評価結果は数値で表現されているため、ある意味では、(２)の客観性を満たしているともいえよう。しかしながら開発者にとっては、自己のシステムが、どの言語現象をどの原因によって処理できなかったのかを判断することが特に重要であり、言語現象が複雑に絡みあった文の翻訳結果を単純に得点化するだけでは有効とはいえない。システム改良に用いるためにその評価結果を解釈しようとする場合には主観的な判断に頼らざるを得ないので、実用的にはこの評価法は(２)の客観性を満たしているとはいえない。一方、我々の開発した評価法においては、これら二つの客観性は共に保たれている。ここでは、単にそれに答えるだけで、システム開発者が自己のシステムの性能評価を行なえるように作られたyes/no設問を各例文に付加することにより、翻訳結果を評価する手続きを明確化した。評価過程で必要とされる手順は単純なyes/no疑問文に答えることだけであり、誰でも機械翻訳システムを同様に評価することが出来る。不完全な翻訳文に対しても、評価者によって評価が大幅に変わるということはない。さらに、各例文には翻訳処理と言語現象との関係を表す解説が付加されており、これにより、システム開発者はなぜ自己のシステムが問題の言語現象を正しく解析できないのかを知ることが出来る。すなわち、我々のテストセットに基づく評価結果を用いて、機械翻訳システムの改良法を決定することが出来る。機械翻訳システムの評価に関しては、既に述べたように、評価すべき言語現象を含む文を集めた評価用例文集の作成という試みもなされている(成田1988,池原・小倉1990,池原他1994)。このような例文集を用いれば、もしシステムが、ある例文を正しく翻訳できないと評価された場合には、システム開発者はただちにその例文が問題としている言語現象をそのシステムが処理できないということが分かる。この点において、この手法もまた(２)の客観性を保持している。しかしながら、この手法には以下の二つの問題がある。例文の翻訳結果を評価する手順が明示されていない。評価結果から機械翻訳システムの不備な点を見つけ出す過程が評価者の言語直観に頼っている。例文を集めただけのもの(テストスゥィート(TestSuite))では、個々のシステムのadhocな評価は可能であっても、評価法としては確立しない。明確に記述された手続きにしたがって、誰でも同じように機械翻訳システムを評価できることが必要である。この目的のために各例文に設問や訳出例を付与しているということを明確にする意味で、我々の評価法においては「テストセット(TestSet)」という名称を用いている。また、評価結果を機械翻訳システムの改良に用いるためには、さまざまな言語現象を単に羅列しておくだけでは不十分である。文法体系の中での各言語現象の位置づけを明確にしておくことも必要である。このような考察のもと、我々は上で述べた評価過程の客観性と結果の判断の客観性という二つの客観性を追求した品質評価を可能とする品質評価用テストセットを提案してきた。ここで用いるテストセットは、考慮すべき文法項目を系統立てて収集し、その各項目に例文を付加して作られた。各例文に解説や設問を付加することによって評価の手順を明確に記述することが可能となった。各テストセットには、評価用例文、その人間による模範訳、システムの出力(翻訳結果)を評価するための設問などが記述されており、評価者はテストセット中の例文を翻訳し、その翻訳結果を参照しながら各例文に付与された設問に回答していく。ここで各設問は判断のポイント(すなわち、例文のどの部分が、どのような役割で、どのような訳文となっていれば良いか)が明示されたyes/no質問文であり、評価者によって判断が異なることがないように作られている。この判断をさらに容易にするために、既存の機械翻訳システムでの翻訳結果を用いた回答例が付与されている。以上により評価過程の客観性を実現している。また、各例文には、その文がどのような言語現象を評価するためのものであるかを説明する解説が付与されており、開発者はその例文に対する翻訳結果から自己のシステムが十分には対応していない言語現象を容易に理解することが出来る。これにより、評価結果の判断の客観性を実現できる。</subsection>
  <subsection title="本評価法の基本的立場(どのような情報を開発者に与えるか)">この評価用テストセットは、個々の機械翻訳システムに依存しない汎用の品質評価法として作成している。したがって、対象とするシステムがルールベース・知識ベース・用例ベース・直接型といった機械翻訳のどの手法を採用しているかには依存しない。このテストセットの目的は機械翻訳システムの開発者が自己のシステムの性能を向上するために、システムの処理できない言語現象を正確に把握することである。その言語現象を処理可能にするための手法は、個々のシステムあるいは個々の手法によって異なっており、その判断は開発者に任すこととし、評価基準としては、そこには立ち入らない。用例ベースの手法とルールベースの手法に共通する解決策を評価法が示すということは現実的ではない。また、個々のシステムによって、対象とする文書が異なっており、各言語現象の出現頻度も異なっている。したがって、システムの欠点のうちで、どの欠点が最も重大であるかを決定することは、当事者にのみ可能なことである。本テストセットの目標は、その当事者の判断を可能な限り援助することにある。ここではテストセット中の各例文には、頻度に関する情報を記述するのではなく、その例文が判断する言語現象を記述してある。翻訳対象となる文書が特定の言語現象に偏っている場合には、評価者はこのテストセットのうちで、必要な言語現象に対応する部分についてのみ翻訳し、その結果を評価すれば良い。自分にとって重要な言語現象を取り扱えるかどうかが、個々の開発者あるいはユーザがシステムを評価する場合には重要であり、評価法としての独自の頻度による一般的な得点化を行なうのは、むしろ誤った評価の原因になると考える。また、評価に例文を用いることについては、その例文に対して高い評価が出るようにシステムを修正することが可能であること、また、全ての言語現象を網羅できるわけではないことなどの問題点が指摘される。しかしながら、ここで提案する評価法はシステム間の相対的な性能評価のために用いるものではない。開発者が自己のシステムの改良のために、その欠点を把握することが目的であり、この本来の目的のためには本テストセットに対してチューニングをすることに意味はない。また、本テストセットは単なる例文集ではなく、各例文にはその対象とする言語現象が解説されており、さらには必要に応じて関連文と、その模範訳が付加されている。これらの文を翻訳し検討することにより、単に一つの例文を処理できるかどうかを判断するだけではなく、その例文に関連する言語現象についての処理能力も知ることが出来る。さらに、個々の開発者が処置するべき問題として、テストセット中の例文に存在する未定義語の問題がある。例文中に(そのシステムにとっての)未定義語があった場合には、評価者は例文中に現れた未定義語を辞書登録するか、あるいは例文中の未定義語を既にシステムに登録されている類似の単語に変更することが要求される。繰り返すが、この評価法はシステム間の優劣を決めることが目的ではなく開発者が自分のためにシステムの欠陥を見つけて、それを修正することを主たる目的としている。したがって、評価者(すなわち開発者)は単純に評価結果を受け入れるのではなく、「翻訳に成功しているが偶然良い訳語が記述されていただけだ。」「翻訳に失敗したが、それはその単語が未定義であったためで、類似の現象自体は取り扱う能力がある。」等については、各自の(自己のシステムについての)知識に基づいて判断する必要がある。また、評価の結果、さまざまな欠陥が見つかった場合に、限られた人的資源の中で、どのような順序でそれを解決していくかという問題もある。しかしながら、各開発者毎に資源の制約や、そのシステムが主として対象とする文書(あるいは、対象とする言語現象)が異なるため、一般的な優先順位を予め定めておくことは現実的ではない。本テストセットは、比較的近い将来に正しい処理の実現が可能な言語現象に重点をおいて作っているが、取り扱えなかった言語現象の内で、まずどの現象を処理可能にするかという優先順位付けは、個々のシステムの開発者に任せられている。なお、このように近い将来に対応できるものに重点を置いて言語現象を収集しテストセットとした場合、機械翻訳システムの技術水準の向上に伴って、対象とする言語現象を継続的に追加あるいは削除していくことが望まれる。常にその時点で機械翻訳において問題となっている言語現象を１０００文程度のテストセットで示すのが理想であろう。ただし、最低限の解析能力を試すための基本的な構造の文は現在も(そのような基本的な構造の解析は既にほとんどのシステムにおいて解決されている問題であるとしても)テストセット中に含まれている。このような基本文はシステムの最低水準を保証するものとして、将来に亙ってもテストセットに含まれると想定している。</subsection>
  <subsection title="評価用例文の収集">テストセットの例文は機械翻訳システムや自然言語処理システムを実際に開発してきた経験に基づいて、著者らによって収集された。例文の収集に当たっては、我々は以下の２点を重視した。(1)基本的な言語現象を網羅すること。(2)機械翻訳システムにとって処理することが困難な言語現象を含む例文を選択すること。ここでは特に曖昧性の問題を重視した。言い替えると、(１)は評価すべき文法現象を系統立てて収集分類(トップダウンの手法)し、それらの現象に対応する例を集めることである。一方(２)は機械翻訳システムによって翻訳することが困難であるような例文を収集する(ボトムアップの手法)ことである。特に我々は処理の困難さが近い将来に解決できるであろうような言語現象に注目した。そして機械翻訳システムの評価のための例文を系統立てて分類した。さらに、我々はこれらの例について、いくつかの商用システムを用いて翻訳評価実験を繰り返し、テストセットを以下の点に焦点を当てながら改良した。これらは全て、評価過程において客観性を維持するために重要な要素である。設問に曖昧性がないこと例文に不必要な複雑さがないこと翻訳結果に曖昧性がないことなお、テストセット中の英文は、その英語としての品質を保証するため、英語を母国語とし、日本語を理解する自然言語処理研究者によって、チェックされ修正された。なお、このテストセットを用いた品質評価法の提案の主旨と、作成の詳しい経緯については、参考文献(井佐原他1992,日本電子工業振興協会1993,日本電子工業振興協会1994,Isaharaetal1994,日本電子工業振興協会1995a,Isahara1995)を参照されたい。また、テストセットの全容は、参考文献(日本電子工業振興協会1995b)に示されている。</subsection>
  <section title="英日機械翻訳システム品質評価用テストセット">本節では、英日機械翻訳システムの品質評価用テストセットについて説明する。このテストセットは、機械翻訳システムが処理すべき様々な言語現象を含んだ英語例文770文とその模範訳、及びシステムの出力(翻訳結果)を評価するための設問などからなる。</section>
  <subsection title="概要">我々は、英日機械翻訳システムの評価基準として、システム開発者が自己のシステムの不備をチェックすることを主要目的とした品質評価用テストセットを作成した。本テストセットにおける例文の収集に際しては、「基本的な言語現象を網羅すること」「機械翻訳システムが取り扱うことが困難な言語現象を、主に曖昧性の解消に注目して収集・分類すること」を試みた。また、システムの出力(翻訳結果)を見ながら回答していくことで品質に関する客観的な判断が可能となるように、各文に判断のポイントを明示したyes/no疑問文の形式の設問を付与している。このように本テストセットは客観的な品質評価の実現を目指して作成したものなので、ユーザが各機械翻訳システムの出力品質を比較する際に利用することも可能である。本テストセットの作成作業は、平成４年度からの３年間で行った。平成５年度末までに第１段階として、英語の単文を中心に評価すべき項目を抽出して評価基準を設定し、309の基本例文を収集・評価して「電子協平成５年度版テストセット」としてまとめた。これに加えて、今回さらに以下の作業を行って項目の充実を図った。平成５年度版テストセットが単文中心だったのに対して、接続詞、関係詞、比較、話法、挿入、並列など、より複雑な構造を持つ複文・重文に関連する項目を重点的に英文法の解説書などから抽出して収集複数の文法書などを参考にすることにより、単文内の項目に関しても、平成５年度版テストセットでカバー出来ていない項目を収集。特に、代名詞、前置詞、記号、数量表現などに関して新規の設問を多数作成文法項目の洩れを防ぐため、英字新聞から英文テキスト300文を選出して市販の英日機械翻訳システムで試訳し、翻訳が困難となる問題点を抽出上記の作業により、これまでの309項目と併せて延べで約1000の項目を抽出した。最終的にこれを整理して、770項目からなるテストセットとしてまとめた。例文と関連文を合わせると、合計で1450文ほどの規模のテストセットとすることが出来た。また、本テストセットの実用性の検証と設問の修正のために、ハードウェアタイプの異なる８種の市販の英日機械翻訳システムを対象とした評価を行った。このテストセット中の各項目は、文番号、例文、その模範訳、○×で答えることが出来る質問文、主として機械翻訳システムによる訳出例、例文と関連する言語現象を含む文、関連する項目の番号、解説から成り立っている。テストセットの例を図１に示す。以下では、このテストセットを用いた品質評価の手順、対象とする言語現象、テストセットの書式について述べる。*1em２．１．１多品詞（品詞認定）２．１．１．２名詞/助動詞【番号】2.1.1.2-1【例文】Thetrashcanwasthrownaway.【訳文】ごみカンは捨てられた。【質問】&quot;can&quot;が「カン/缶」のように名詞として訳されていますか？【訳出例】○(くず缶/ごみ容器/くず入れ)は(廃棄された/[投げ]捨てられた)。×ごみは捨てられ得る。【関連文】Thelastwillwasopened.「最後の遺言書は開けられた。」【参照項目】2.1.1.2-2,2.1.1.2-3【解説】&quot;canwas&quot;の並びから、&quot;can&quot;が助動詞でないことがわかる。【番号】2.1.1.2-2【例文】Thetrashcanbethrownaway.【訳文】ごみは捨てられ得る。【質問】&quot;can&quot;が「〜できる/得る」のように助動詞として訳されていますか？【訳出例】○(くず/ごみ/くだらない人間)は(廃棄できる/[投げ]捨てられることができる)。×ごみカンは捨てられた。【関連文】【参照項目】2.1.1.2-1,2.1.1.2-3【解説】2.1.1.2-1とは逆に、ここでは&quot;can&quot;は名詞ではなく助動詞。verbatimsmall図１英日機械翻訳システム用テストセットの例*1em</subsection>
  <subsection title="テストセットの利用法">本テストセットは、以下の利用法を想定している。(1)評価対象となる英日機械翻訳システムを用意する。(2)そのシステムでテストセット中の【例文】を翻訳する。(3)【質問】【訳出例】を見て、その翻訳結果が○か×かを判断する。(4)システム開発者は、○×の分布からシステムの能力、開発段階を評価する。特に、×と判断した項目に関連する文法・辞書を追加することで、システムの改良を図る。((5)ユーザは、各システムの○×の分布から、出力品質面での優劣を比較する。)(6)各項目についてさらに詳細に評価を行う場合は、【関連文】を利用する。原則として翻訳結果と質問文を見るだけで○×を回答出来るようになっているが、【訳出例】(各訳出例には、質問に対する○×が予め付与されている)を参照することによって、さらに容易に判断が出来るようになっている。本テストセットを用いて○×の分布を見ることで、システムの対応が不十分な(可能性がある)項目を容易に抽出できる。ただし本テストセットでは、各項目(例文)間の重要度、頻度などの差異は考慮していないので、単純に○の数をカウントして正解率をシステム間で比較することは、本評価法の意図するところではない。</subsection>
  <subsection title="テストセットの構成">本テストセットは、機械翻訳システムが処理すべき様々な言語現象を含んだ英語例文770文からなる。内訳と項目ごとの設問数を図２に示す。品質評価の対象項目の収集に当たっては、網羅性を保証するトップダウンのアプローチと、機械翻訳における問題点を実際の翻訳結果から抽出して、その問題性によって例文の粗密を決定するボトムアップなアプローチを組み合わせている。把握部においては、英文法の解説書(江川1964,Hornby1977,小川他1991,荒木他1992,村田1992)などを参考に英語の文法現象を収集し、そのレベルによって、品詞、文の部分構造、文構造の３段階に分類した。特に動詞、形容詞、名詞に関してその基本的な用法を網羅するために、ホーンビーの分類した文型(Hornby1977)を設問項目として採用した。ただしホーンビーのパターンの中でも、機械翻訳システムの品質評価において特に必要でないとみなした区分については分類を省略している。同様に助動詞等の基本的な用法の中でも、機械翻訳において対象となることが極めて稀であると思われるものについては省略した。選択部においては、翻訳で実際に問題となる言語現象を、構文構造の曖昧性に関するものと、コロケーション(他の語との共起による訳し分け)に関するものに分類した。</subsection>
  <subsection title="テストセットの書式">本テストセットの各項目の書式を図３に示す。なお、テストセット中で、[]で囲まれた部分は挿入可能な表現を、(/)で囲まれた部分はいずれかを選択する表現を示す。たとえば、&quot;Ａ[Ｂ]Ｃ(Ｄ/Ｅ)Ｆ&quot;という記号列は、``ＡＢＣＤＦ'',``ＡＢＣＥＦ'',``ＡＣＤＦ'',``ＡＣＥＦ''の４種の記号列を表す。</subsection>
  <section title="日英機械翻訳システム品質評価用テストセット">日英翻訳システム品質評価用テストセットも英日翻訳システム用と同様に、開発者が自己のシステムの不備な点を発見するための評価法であり、テストセット中の各例文に付与された設問に答えることによって、客観的に評価を下せるように作られている。しかしながら、英日翻訳と日英翻訳の技術レベルの違いに基づいて、英日用のテストセットとは少し異なった視点でテストセットの開発を行なった。実際のテストセットの例を図４に示す。我々は、客観的評価を実現するテストセットの採用に加えて、日本語処理システムの開発者の利便を考え、言語現象と処理モジュールとの対応を取ることができる形式の評価方法の開発を行なった。すなわち、評価用例文と、その翻訳結果を評価する手段(設問)を提供するだけでなく、各言語現象に対応してシステムがどのような処理を行なっているかを把握するための解説も付与している。解説によって示される言語現象の処理方法を利用して開発者は、そのシステム全体としての言語現象の処理能力を評価するとともに、処理の各段階が充分な能力を持っているかどうかを把握できる。具体的には言語現象を約４０種類に大別し、その各項目について問題となっている言語現象をどのように処理しているかを調べるための解説が付加されている。言語現象の項目リストを図５に示す。ここでは必要に応じて、用いている知識や処理結果の取り扱い等も併せて説明される。各項目内の個別の言語現象については、その言語現象を含む日本語文、その英訳、ここで確認するべき要素の解説が記述されている。設問数は約３３０、機能確認のための対訳例は、約４００文の構成となっている。また、開発者がこのテストセットを使用する際の利便性を考え、テストセットの書式を揃え、各文にインデックスをつけることにより、機械上での検索を容易に行なえるようにした。上記の各項目に付けられたインデックスは基本的に図６のような構造である。図６の??????の部分には、数字またはアルファベットが使用される。最初の２文字がタイトルまたはサブタイトルの章番号を表す。次の３文字が、各項目中の設問に付与された番号であり、設問は最大３階層になっている。最後の１文字が例文及び翻訳例の文番号を示す。解説、コメントはその対象とする項目と同じ文番号となる。これらのインデックスを検索のキーとして、各種のＯＳの検索コマンドを使用することにより、機械翻訳にかけるための原文のみの抽出や、項目リストの抽出など、簡単に必要な部分だけを抜きだして使用することが出来る。使用例を図７に示す。</section>
  <section title="おわりに">本稿では、機械翻訳システムの翻訳品質を開発者の視点から評価する手法を提案した。この手法は、評価用の各例文に質問と解説を付加したテストセットを用いることにより評価過程を明確化した客観的品質評価法である。本稿で提案したテストセットは、評価用の例文に、その人間による訳、システムの出力を評価するための設問、(もしあった場合には)関連する文、文法事項の解説等を付与したものである。例文は基本的な言語現象と、機械翻訳において現在課題となっている言語現象を網羅することを念頭において収集された。テストセット中の設問は評価するべき点を明確にするように作成されている。各例文の翻訳結果が与えられると、システム開発者はその例文に付与されている設問に答えていくだけで、自己のシステムの評価を行なうことが出来る。設問は○×式であり、評価者によって判断が分かれないように作られている。これにより、客観的な評価が可能となる。さらに、解説を参照することにより、システム開発者は自己のシステムがどの言語現象を処理できないかを正確に認識することが出来る。ここで提案した英日及び日英翻訳システム用のテストセットは無料で一般に公開されている。我々は、この評価法が機械翻訳システムの一層の発展の一助となることを期待してやまない．document</section>
</root>
