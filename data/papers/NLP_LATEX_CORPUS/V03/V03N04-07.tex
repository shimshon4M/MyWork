\documentstyle[epsf,jnlpbbl]{jnlp_j_b5}

\setcounter{page}{115}
\setcounter{巻数}{3}
\setcounter{号数}{4}
\setcounter{年}{1996}
\setcounter{月}{10}
\受付{1995}{12}{6}
\再受付{1996}{4}{1}
\採録{1996}{4}{19}

\setcounter{secnumdepth}{2}

\title{日本語の文法および未知の認知単位の自動獲得のための一方法}
\author{横田 和章\affiref{NODA} \and 亀田 弘之\affiref{KOKA} \and 藤崎 博也\affiref{NODA}}
\headauthor{横田 和章・亀田 弘之・藤崎 博也}
\headtitle{日本語の文法および未知の認知単位の自動獲得のための一方法}
\affilabel{NODA}{東京理科大学基礎工学部電子応用工学科}
{Dept. of Applied Electronics, Faculty of Industrial Science and Technology,
Science University of Tokyo}
\affilabel{KOKA}{東京工科大学工学部情報工学科}
{Dept. of Information Engineering, Faculty of Engineering, Tokyo Engineering University}
\jabstract{
筆者らは,コーパスに基づいて形態素を基本とした日本語文法を自動獲得する方法を既に提案している.
本論文は，この方法における処理単位として，形態素の代わりにより
長い単位 --- 認知単位 --- を用いた新しい方法を提案するものである．
認知単位は，人間を被験者とした知覚実験の結果から
得られた人間の文解析の単位である．こうした，形態素より長い単位を解析に
用いることにより，構文解析における経路数を抑えることができる．
しかし，単純に認知単位を辞書に登録して用いるだけでは，
未知認知単位の出現確率が高まり，結果として文解析の正解率が低下する．
この現象を抑えるため，既知認知単位から未知認知単位を推定する新しい方法を
更に取り入れた．
この方法で天気概況文コーパスを処理し，得られた文法に基づき構文解析を行った
結果，形態素を処理単位とした解析に比べ高い処理効率を得ることができた．
}

\jkeywords{
構文解析，コーパス，文脈自由文法，シミュレーテッド・アニーリング法，認知単位
}

\etitle{A method for automatic acquisition of Japanese\\ grammar
and unknown cognitive units}

\eauthor{Kazuaki Yokota\affiref{NODA} \and Hiroyuki Kameda\affiref{KOKA} \and Hiroya Fujisaki\affiref{NODA}} 

\eabstract{
A method for automatic acquisition of a grammar of Japanese based on morphemes
from a corpus has already been proposed by the authors.
This paper proposes a new method based on cognitive units, which have been
experimentally found to be the units of the
human process of sentence analysis, and have been known to be larger than
morphemes. While the use of cognitive units can 
reduce the number of search paths, it may increase the number of
unknown units and may degrade system performance. In order to cope
with this problem, a method has been further introduced for identifying
an unknown cognitive unit from known cognitive units. The proposed method was
applied to the analysis of the `weather-forecast' corpus,
and the acquired grammar was used for parsing. The results indicated a higher
processing efficiency for systems using cognitive units than for those using
morphemes.
}

\ekeywords{
Syntactic analysis, Corpus，Context-free grammar, Method of simulated annealing,
Cognitive unit
}


\begin{document}
\maketitle

\section{まえがき}

自然言語の機械による処理方法の一つに，人間が与えた規則を用いて解析する方法が
ある．
この方法では，一般に
知識が複雑になるほど精密な解析ができるが，
この複雑化に伴い知識獲得が難しくなるため，
解析の対象となる話題を限定することがほぼ必須となる．
この点において，人間により与えられた規則にのみ
基づく解析は，限界にきているとの見方もある．

これに対して，自然言語に関する統計的情報を自然言語処理に利用
する研究が盛んに行われている\cite{utsu,kudo,mich}．
人間によって与えられた規則を元に解析を行う
方法においても，規則の適用される確率を統計的に調べておくことに
より良い結果が得られることが多く，統計的な情報を自然言語処理に
用いることは処理の効率化に効果があるとみられる．

筆者らは既に，統計情報を自然言語処理に利用する方式の一つとして，
コーパスに基づいて日本語文法を自動獲得する方法を提案している\cite{yoko2}．
この獲得法は，まず構文木情報の付加されたコーパスから多数の文の構文木を作成し，
それぞれの節点にランダムに非終端記号を割り当て，その後この割当てを
エントロピーにより評価し，エントロピーが最小となるよう
シミュレーテッド・アニーリング法により割り当てを変更する
ものである\cite{shan,asai,patr}．

この方法を新聞記事の文法の獲得に適用した所，得られた文法は終端記号と
非終端記号との間の置き換え規則のエントロピーが比較的高いことがわかった．
従って，この獲得法の単位として形態素より長い単位 --- 認知単位 --- を
利用することによりエントロピーを下げれば，パーザの動作効率を高めることが
できると期待される．


本論文ではこのような知見に基づき，形態素より長い単位を
人間による知覚実験の結果から定義し，
文法の自動獲得に応用した，新しい方法を提案する．


\section{文法の自動獲得法}

今，図\ref{zu1}(a)のように例文と構文木が与えられていると仮定する．
この場合，文脈自由文法における終端記号の集合は$T=\{東, 日本, では, 
晴れて, い, ます\}$
と書ける．非終端記号の集合を$N=\{n_1, n_2, n_3, n_4\}$，
\hspace*{-1mm}初期記号を$n_1$として，\hspace*{-1mm}この構文木を同図(b)のように変形し，各節点に
非終端記号を割り当てたとする．すると，
各節点の上下関係から次のshift-reduceパーザの規則を得ることができる\cite{yoko2}．
\newpage

\begin{figure}
\begin{center}
\epsfile{file=7-1.eps,width=90mm}
\end{center}
\caption{構文木の各節点に対する非終端記号の割り当て} \label{zu1}
\end{figure}

\begin{center}
\begin{minipage}[t]{6cm}
\begin{flushleft}
\baselineskip=15pt
\smallskip
$R1$ : 東 $\rightarrow$ $n_2$ \\
$R2$ : 日本 $\rightarrow$ $n_3$ \\
$R3$ : では $\rightarrow$ $n_3$ \\
$R4$ : 晴れ $\rightarrow$ $n_4$ \\
$R5$ : て $\rightarrow$ $n_3$ \\
$R6$ : い $\rightarrow$ $n_2$ \\
$R7$ : ます $\rightarrow$ $n_4$ \\
$R8$ : $n_2$, $n_3$ $\rightarrow$ $n_4$ \\
$R9$ : $n_4$, $n_3$ $\rightarrow$ $n_2$ \\
$R10$ : $n_2$, $n_4$ $\rightarrow$ shift \\
$R11$ : $n_4$, $n_3$ $\rightarrow$ $n_2$ \\
$R12$ : $n_2$, $n_2$ $\rightarrow$ shift \\
$R13$ : $n_2$, $n_2$ $\rightarrow$ shift \\
$R14$ : $n_2$, $n_4$ $\rightarrow$ $n_3$ \\
$R15$ : $n_2$, $n_3$ $\rightarrow$ $n_4$ \\
$R16$ : $n_2$, $n_4$ $\rightarrow$ $n_1$ \\
\end{flushleft}
\end{minipage}
\end{center}

この規則の中には$R10$，$R14$，$R16$のように左辺が等しく右辺が異なる
ものが存在する．このような規則は，shift-reduceパーザの探索空間を広げ，
処理速度を低下させる．

先の規則において左辺が$n_i, n_j$の時，右辺が$n_k$となる条件つき確率を
$P_{n_i n_j}(n_k)$，右辺が shift となる条件つき確率を
$P_{\mbox{\footnotesize Shift} n_i n_j}$とする．
同様に，左辺が終端記号$t_i$の時，右辺が$n_j$となる条件つき確率を
$P_{t_i}(n_j)$で表す．
更に規則$n_i,n_j\rightarrow n_k$の出現確率を$P(n_i, n_j, n_k)$，
規則$n_i, n_j\rightarrow$ shift の出現確率を
$P_{\mbox{\footnotesize shift}}(n_i, n_j)$とする．
また規則$t_i \rightarrow n_j$の出現確率を$P(t_i,n_j)$とする．
すると，上記のshift-reduceパーザの規則はこれらの確率を用いて
次のように書き直すことができる．

\[
\begin{array}{ll}
P(東,n_2) = 0.06, & P_{東}(n_2) = 1.00 \\
P(日本,n_3) = 0.06, & P_{日本}(n_3) = 1.00 \\
P(では,n_3) = 0.06, & P_{では}(n_3) = 1.00 \\
P(晴れ,n_4) = 0.06, & P_{晴れ}(n_4) = 1.00 \\
P(て,n_3) = 0.06, & P_{て}(n_3) = 1.00 \\
P(い,n_2) = 0.06, & P_{い}(n_2) = 1.00 \\
P(ます,n_4) = 0.06,& P_{ます}(n_4) = 1.00 \\
P(n_2, n_3, n_4) = 0.13, & P_{n_2 n_3}(n_4) = 1.00 \\
P(n_4, n_3, n_2) = 0.13, & P_{n_4 n_3}(n_2) = 1.00 \\
P_{\mbox{\footnotesize shift}} (n_2, n_4) = 0.06, & 
   P_{\mbox{\footnotesize shift} n_2 n_4} = 0.33  \\
 P_{\mbox{\footnotesize shift}} (n_2, n_2) = 0.13, &
   P_{\mbox{\footnotesize shift} n_2 n_2} = 1.00  \\
P(n_2, n_4, n_3) = 0.06, & P_{n_2 n_4}(n_3) = 0.33 \\
P(n_2, n_3, n_4) = 0.13, & P_{n_2 n_3}(n_4) = 1.00 \\
P(n_2, n_4, n_1) = 0.06, & P_{n_2 n_4}(n_1) = 0.33 \\
\end{array}
\]

$R1\cdots R16$において左辺が等しく右辺が異なるような規則を減らすということは，
上記の条件つき確率のすべてを1または0に近づけるということに対応する．
これは更に次の(\ref{eq5})式で定義されるエントロピー$H$を
減少させることに等しい\cite{shan}．
\vspace*{-2mm}
\begin{eqnarray}
H & = & \sum_{i,j} P_{\mbox{\footnotesize shift}}(n_i, n_j)
\log P_{\mbox{\footnotesize shift} n_i n_j} \nonumber \\
& & - \sum_{i,j,k} P(n_i, n_j, n_k) \log P_{n_i n_j}(n_k) \nonumber \\
& & - \sum_{i,j} P(t_i, n_j) \log P_{t_i}(n_j) \label{eq5}
\end{eqnarray}

従って，構文木の各節点に対する非終端記号の割当てを，\hspace*{-1mm}$H$が最小となるように
変更すれば，shift-reduceパーザの動作に最適な規則を得ることができる．
\hspace*{-2mm}この最小化は組合わせ最適化問題に対応する．この場合，組合せ空間には
いくつもの極小値が存在する．よって，この方法ではシミュレーテッド・
アニーリング法を用いて$H$の最小化を行う\cite{patr}．

こうして獲得したshift-reduceパーザの動作規則からは，
容易に文脈自由文法を作り出すことができる．すなわちこの方法は，
文法を獲得する方法であるとみなせる．

また，$H$を用いて perplexity $Q$ を次のように計算できる．
\vspace*{-2mm}
\begin{equation}
Q = \exp (H) \label{eq6}
\end{equation}

$Q$は物理的には，一つの左辺に対して平均いくつの右辺が存在するかを示す
値となる．


新聞記事7500文を対象として，この方法により解析を行った結果，
左辺が形態素である規則の条件つき生起確率$P_{t_i}(n_j)$は，
左辺が非終端記号2個の規則の条件つき生起確率$P_{n_in_j}(n_k)$
と比べて，値が小さくなることが示された\cite{yoko2}．

これは，一つの形態素が多数の用法を持っていることを示している．
従って複数の形態素を組合わせたより長い文字列を，この解析の
単位として用いることにより，shift-reduce
パーザの解析効率をより高めることができると予想される．

\section{認知単位の知覚実験}

機械による文解析は，専ら形態素を単位として行われている\cite{taka}．
しかし，人間では果たしてどのような単位が用いられているのだろうか．
これを確かめるため，知覚実験を行った．

この実験では，コンピュータのディスプレイ上に30字程度の漢字かな混じり文を
短時間表示し，被験者に口頭で読んでもらう．
使用したディスプレイは，640$\times$400ピクセルの15インチディスプレイで，
被験者から約1mの距離に配置してあり，16$\times$16ピクセルの白いフォント
で文が提示される．
被験者は成人男子大学生4名である．

まず，表示時間を 50ms として
1文を提示し，被験者に直ちに口頭で再生してもらう．
文は24用意してあり，この実験を全文につき行なう．
24文の実験が終了すると，提示時間を 100ms として
再度24文の実験を行なう．
ただし，次に現れる文が予測できないよう文を提示する順序を変える．
表示時間を 50ms 刻みで 1s まで長くしながらこの実験を繰り返し，
表示時間と，被験者が読むことのできた文字数との関係を調べる．

画面上には，文の開始位置が常に示されており，
被験者には提示の前に視点をその位置に
移動しておくように指示してある．
従って，被験者は文を文頭から認識することになる．
被験者にはあらかじめ文を覚えないよう伝えてあるが，
実験を繰り返すうちに文頭付近を覚えることは避けられない．
しかし，提示時間は徐々に長くなるため，被験者が再生できた文字列の
末尾付近については認識経験が少なく，
記憶による影響は小さい．

実験結果を図\ref{zu2} に示す．\hspace*{-2mm}この例では，\hspace*{-2mm}「この問題は解決ずみというつもりなの
かもしれないが私はそうは思わない．」という文を表示している．結果は図のような
階段状になり，人間が文字単位で文を処理しているのではないことは
明らかである．また，読めた部分の最後に着目すると，それはすべて文節境界と
なりうる形態素で終っている．
更に，「$\cdots$というつもり」\hspace*{-1mm}や\hspace*{-1mm}「$\cdots$かもしれないが」などのように，
複数の文節にまたがる句が一度に検出されているケースもある．
従って，人間は文節よりも長い句を一度に検出していることが分かる．
「$\cdots$は」，「$\cdots$している」，「$\cdots$というつもり」，
「$\cdots$かもしれないが」のように，それだけでは意味をなさず，先行する句の
後について補助的な意味を表すような句は，先行する句の一部として
先行する句とともに一度に検出されている．
この結果から，人間の場合，まずこのような補助的な句を含めた長い句を
一度に検出する処理を行い，
この処理の後，検出された長い句を組合わせて文を処理している
と考えられる．

24文の実験結果から，人間は，次のような句を一度に検出していることが
分かった．

\begin{figure}
\begin{flushleft}
\small
\makebox[30mm][r]{   50ms:} この\\
\makebox[30mm][r]{  100ms:} この問題は\\
\makebox[30mm][r]{  150ms:} この問題は\\
\makebox[30mm][r]{  200ms:} この問題は解決\\
\makebox[30mm][r]{  250ms:} この問題は解決\\
\makebox[30mm][r]{  300ms:} この問題は解決\\
\makebox[30mm][r]{  350ms:} この問題は解決ずみ\\
\makebox[30mm][r]{  400ms:} この問題は解決ずみ\\
\makebox[30mm][r]{  450ms:} この問題は解決ずみと\\
\makebox[30mm][r]{  500ms:} この問題は解決ずみと\\
\makebox[30mm][r]{  550ms:} この問題は解決ずみ\\
\makebox[30mm][r]{  600ms:} この問題は解決ずみというつもりなのか\\
\makebox[30mm][r]{  650ms:} この問題は解決ずみというつもりなのか\\
\makebox[30mm][r]{  700ms:} この問題は解決ずみというつもりなのか\\
\makebox[30mm][r]{  750ms:} この問題は解決ずみというつもりなのかもしれないが私は\\
\makebox[30mm][r]{  800ms:} この問題は解決ずみというつもりなのか\\
\makebox[30mm][r]{  850ms:} この問題は解決ずみというつもりなのかもしれないが\\
\makebox[30mm][r]{  900ms:} この問題は解決ずみというつもりなのかもしれないが私は\\
\makebox[30mm][r]{  950ms:} この問題は解決ずみというつもりなのかもしれないが私はそう　思わない．\\
\makebox[30mm][r]{ 1000ms:} この問題は解決ずみというつもりなのかもしれないが私はそう　思わない．\\
\makebox[30mm][r]{ stimulus} この問題は解決ずみというつもりなのかもしれないが私はそうは思わない．\\
\end{flushleft}
\caption{認知単位知覚実験の結果例} \label{zu2}
\vspace*{5mm}
\end{figure}

\begin{enumerate}
\item 文節
\item 「〜かもしれない」などの慣用句
\item 「〜している」などの補助用言を含んだ句
\end{enumerate}

このような句を，本論文は認知単位と呼ぶことにする\cite{yoko}．


\section{認知単位を用いた文法の獲得法}

\subsection{認知単位を用いた文法の獲得法の概要}

2節で述べた獲得法により獲得した文法を用いるshift-reduceパーザは，
認知単位の内部まで係受けを調べるため，探索空間は膨大なものとなる．
しかし，前節の考察から，人間においては，認知単位の内部については，
通常解析は行っていないと考えられる．
よって，2節の解析方法の単位として，3節
で述べた認知単位を用いれば，探索空間は大きく狭まり検索効率
は向上すると考えられる．

本研究では，図{\ref{zu3a}} のように，認知単位を用いた
構文木を作り文法を獲得するものとした．

\begin{figure}[htb]
\begin{center}
\epsfile{file=7-3.eps,width=82mm}

\end{center}
\caption{認知単位を用いた構文木} \label{zu3a}
\end{figure}

\subsection{天気概況文における機械処理用の認知単位}

この文法獲得を行うにあたり，NHKの気象通報の始めに放送される天気概況文1000文を，
構文木情報を含めて手入力し，コーパスとして用いた．
使用した天気概況文の例を図\ref{zu3} に示す．

人間の認知単位は知覚実験により得られるもので，
人間における認知単位をすべて明らかにして辞書を作成するためには
膨大な知覚実験が必要となる．
従って，本研究では簡単のため，人間同様と思われる単位を新たに定義し，
機械処理用の認知単位として解析に利用することとした．
以下，本論文ではこの機械処理用の認知単位を，単に認知単位と呼ぶ．
本論文では，天気概況文における機械処理用の認知単位を次のように定義する．

\subsection*{[天気概況文における機械処理用の認知単位の定義]}
\begin{quotation}
\[
認知単位 = \left\{\begin{array}{l}自立語\\自立語+補助的な語\\\end{array}\right.
\]

ただし，複合語は1つの自立語として扱う．また，活用する語については
活用語尾も含めて1つの自立語とする．
補助的な語は，以下の形態素もしくはその組み合わせとする．

\begin{enumerate}
\item 「が」，「は」，「です」，「ます」のような付属語列
\item 継続，状態を示す「いる」，「おる」
\item 状態を示す「なる」
\end{enumerate}

\end{quotation}

\begin{figure}[t]
\scriptsize
オホーツク海には，発達中の低気圧があって，北北東へ進んでいます．

一方，中国東北部には高気圧があって，ほとんど停滞しています．

西日本は晴れ，東日本はくもりで，北日本では所々で雨が降っています．

尚，北海道周辺海域と三陸沖では所々濃い霧の為見通しが悪くなっています．

日本近海は，北海道東方海上から関東海域北部にかけて，シケています．

気温は，北海道，北陸，東海で，平年より１度高い他は，平年並か，１度から２度低くなっています．
\caption{天気概況文の例} \label{zu3}
\end{figure}

\newpage
\subsection{認知単位を用いた文法の獲得実験}
\vspace{4mm}
\begin{figure}[b]
\begin{center}
\epsfile{file=7-5.eps,width=110mm}

\end{center}
\caption{認知単位を用いた文法の自動獲得の様子} \label{zu4}
\end{figure}

前節に示したコーパスを用いて実際に文法の自動獲得を行なった．
非終端記号数$N_n = \hspace*{1mm}20, 40, 60, 80$における，シミュレーテッド
・アニーリング法による文法獲得の様子を図\ref{zu4} に示す．
$C_p$は温度パラメータであり，初期値を経験的に
定め，項比 0.98 の等比数列に従い減少させた．\hspace*{-2mm}各$C_p$について，
各節点とも$2N_n$回の非終端記号の更新を行った．
この結果得られた$Q$の最終値を表\ref{hyo1} に示す．
獲得の結果は，すべて$Q$が$1.3$以下になった．
これは動作規則の左辺1つに対して，右辺が$1.3$以下と
なる規則が得られたことを示している．

\vfill

\begin{table}
\caption{獲得により得られた$Q$の最終値} \label{hyo1}
\begin{center}
\begin{tabular}{c|c}
\hline \hline
非終端記号数$N_n$ & 最終値 \\ \hline \hline
20 & 1.21 \\
40 & 1.06 \\
60 & 1.09 \\
80 & 1.10 \\
\hline \hline
\end{tabular}
\end{center}
\end{table}

\section{未知の認知単位の自動獲得法}

前節に述べた方法により，認知単位を基本とした文法を獲得することが
できる．従って，コーパスに出現した認知単位のすべてを辞書に納めておけば，
前節で獲得した動作規則に基づいて構文解析を行うことができる．
しかし，認知単位は形態素を複数組合わせたものであるため，
認知単位の中には極めて出現率が低いものがいくつも存在し，
これらの認知単位は限られたコーパス中においては，一度も出現しない
可能性がある．従って収録文数が限られたコーパスでは，形態素を
単位とした解析法よりも，認知単位を単位とした解析法の方が
未知語の比率が高くなる．このようなコーパスの場合，
2節の方法の単位として単に認知単位を用いただけでは，
未知の認知単位を含む文が解析不能となり，結果的に解析効率が低下する．

この現象を抑えるためには，未知の認知単位に関する知識を，
既知の認知単位に関する知識から推定する必要がある．

本研究ではこの推定を行うため，認知単位を，形態素を基本とする
状態遷移図で表現できると仮定する．
すると，
例えば「東日本では」という認知単位が「東」，「日本」，「では」
という3つの形態素からなり，shift-reduceパーザの動作規則または文脈自由文法
の生成規則によって，非終端記号$n_1$に置き換え可能であるとすると，
この認知単位を図\ref{zu5} のように，隠れ状態$u_1\ldots u_4$を持つ
状態遷移図モデルで表現し，取り扱うことができる．

\begin{figure}[b]
\begin{center}
\epsfile{file=7-6.eps,width=71mm}

\end{center}
\caption{認知単位の状態遷移図モデル} \label{zu5}
\end{figure}



同図における各状態に対して更に，\hspace*{-1mm}$u_1$を初期状態，$u_2$，$u_3$を中間状態，
$u_4$を受理状態と呼ぶ\\ことにし，この受理状態$u_4$が
非終端記号$n_1$に対応しているものと考える．

コーパスに出現するすべての認知単位に対しこのモデルを適用し，
各認知単位についてすべて異なる隠れ状態$u_i$を生成すると，
おびただしい数の隠れ状態が必要となる．
このため，隠れ状態の総数$N_u$より小さい$N_s$を考え，
\hspace*{-2mm}$u_1$のような初期状態を初期状態記号$s_1$に写像し，\hspace*{-1mm}
$u_2$，\hspace*{-1mm}$u_3$のような中間状態を中間状態記号$s_2\ldots s_{N_s}$のいずれかに
写像する写像$s_y = T_{us}(u_x)$を考え，状\\態を統一化する．

写像$T_{us}$を決めると，コーパスより，状態$s_i$から単語$w_j$
を通って$s_k$に移る確率$P_s(s_i, w_j, s_k)$と条件つき確率
$P_{s s_i}(w_j, s_k)$を調べることができる．
同様に状態$s_i$から単語$w_j$
を通って$n_k$に移る確率$P_s(s_i, w_j, n_k)$と条件つき確率
$P_{s s_i}(w_j, n_k)$も調べることができる．

これらの確率を用いて，状態遷移図のエントロピーは次のように
表現される．
\begin{eqnarray}
H_s & = & - \sum_{i,j,k} P_s(s_i, w_j, s_k) \log P_{s s_i}(w_j, s_k) \nonumber \\
& & - \sum_{i,j,k} P_s(s_i, w_j, n_k) \log P_{s s_i}(w_j, n_k) \label{eq7}
\end{eqnarray}

このエントロピー$H_s$を用いて，状態遷移図における
平均分岐数は次のように求まる．
\begin{equation}
Q_s = \exp (H_s) \label{eq8}
\end{equation}

$Q_s$は物理的には，一つの状態から平均いくつの枝が出ているかを示している．

この方法で得られた状態遷移図を用いて，有限オートマトンで認知単位を検出すると
すれば，この分岐数$Q_s$が小さい程オートマトンの決定性が高まり，
動作が効率的になる．

$Q_s$が最小となるよう$T_{us}$を求める問題は，組合わせ最適化問題
となる．この組合わせ空間に\\はやはり多数の極小値が存在するため，
本研究ではシミュレーテッド・アニーリング法により$Q_s$を最小化する．

$T_{us}$による状態の統一化により，ある認知単位に関する状態遷移図と，
その認知単位に近い用法を持つ別の認知単位に関する状態遷移図は交差
する．
この交差により，認知単位に関する知識は統一化され，未知の認知単位も
受理できるようになる．
従って，こうして得た状態遷移図は未知の認知単位を含めた状態遷移図となる．
$T_{us}$の最適化は，エントロピー$H_s$を基準としてこの交差を最適化
することになる．
本研究では，この最適化により未知の認知単位に関する状態遷移図も最適化される
と仮定し，最適化により得た状態遷移図に基づき認知単位を受理する．

このように形態素を基本とした状態遷移図を用いる方法では，
最終的には辞書としては形態素の辞書を持つことになる．
しかし，有限オートマトンによる解析は，shift-reduceパーザ
のようなより高次の解析方法に比べて動作が直線的であるため
一般に高速である．従って，認知単位内の解析に
状態遷移図と有限オートマトンを用いる方法は，
文全体をshift-reduceパーザで処理する方法に比べ
高速となる利点がある．



\section{未知の認知単位の自動獲得実験}

\begin{figure}[b]
\begin{center}
\epsfile{file=7-7.eps,width=126mm}

\end{center}
\caption{認知単位を表現する状態遷移図の獲得} \label{zu6}
\end{figure}



4節で獲得した認知単位に関する知識に基づき，
5節で述べた方法により未知の認知単位も含めた状態遷移図を獲得する
実験を行った．状態記号数$N_u$は20とした．この獲得の様子を図\ref{zu6} に示す．
この結果，最終的に得られた$Q_s$は
ほぼ9程度となった．
$Q_s$の最終値を表\ref{hyo2} に示す．

非終端記号数$N_n=20$として獲得した場合の，状態遷移図
における遷移確率の高い枝の一部を表\ref{hyo3} に示す．

状態遷移図の獲得により，形態素は自動的に
クラスタリングされるが，
このクラスタリングは人間のクラスタリングに類似していることが
分かる．

\begin{table}[htb]
\caption{獲得により得られた$Q_s$の最終値} \label{hyo2}
\begin{center}
\begin{tabular}{c|c}
\hline \hline
非終端記号数$N_n$ & 最終値 \\ \hline \hline
20 & 9.02 \\
40 & 8.71 \\
60 & 9.23 \\
80 & 9.36 \\
\hline \hline
\end{tabular}
\end{center}
\end{table}


\begin{table}[htb]
\begin{center}
\caption{獲得により得られた状態遷移表(部分)} \label{hyo3}
\scriptsize
\begin{tabular}{c|c|ccc} \hline \hline
前状態 & 次状態 & & 主な形態素 \\ \hline \hline
1 & 2 & 一部 & 西 & 北 \\ 
 & 3 & シケて & 進んで& 停滞して \\ 
 & 4 & 父島 & 別 & 霧  \\ 
 & 5 & 移動して& 降って & 晴れて \\
 & 6 & ６& 他&  \\
 & 7 & 黄海 & 気温 & 三陸沖 \\
 & 8 & １& ２ & ３ \\ 
 & 9 & 千島 & 日本 & 発達中  \\
 & 10 & 前線& 東北 & 平年並 \\ 
 & 11 & 低気圧 \\
 & 12 & 悪く & 高く & 大シケと \\ 
 & 13 & サハリン & 沖縄 & 九州 \\ 
 & 14 & かけて & 西日本 & 南  \\ 
 & 15 & 九州 & オホーツク海 & 沖縄\\ 
 & 16 & 見通し & 高気圧 & 波 \\ 
 & 17 & 東  & 東シナ海 & 東北東 \\ 
 & 18 & 沖縄 & 関東 & 東海 \\
 & 19 & シケて & 見込み & 中 \\
 & 20 & 平年 \\ 
 & $n_2$ & 一方 & 尚 & 又 \\ 
 & $n_4$ & ため & 為 \\  
 & $n_5$ & 共に & 所々 & 伴った \\ 
 & $n_6$ & かけて& 高い \\ 
 & $n_7$ & あって \\  
 & $n_{10}$ & あって & 大体 & 低い \\ 
 & $n_{13}$ & ほとんど & ほぼ & 大体 \\ 
 & $n_{15}$ & 濃い & 発達した & ほとんど \\ 
 & $n_{16}$ & あって \\  
 & $n_{18}$ & 次第に & 伴った \\ 
 & $n_{19}$ & 今後 & 日本海 \\  \hline \hline 
\end{tabular}
\end{center}
\end{table}

\section{獲得した知識に基づく構文解析}

以上の議論に基づき，(A)形態素を基本として文法を獲得する方法，
(B)認知単位を基本として文法を獲得し，認知単位をそのまま辞書に登録する
方法，(C)認知単位を基本として文法を獲得し，認知単位を受理する状態遷移図
を獲得する方法の3つの評価を行った．



評価は，獲得した文法に基づき，別に用意した100文を構文解析することに
よって行った．
計算はSUN の SPARC Station 20 モデル
612 を用いた．この結果を表\ref{hyo4} に示す．

表中において正解率はこの結果，最も確からしいと判断された構文木が，
コーパスに与えられている構文木と等しい確率を示す．

探索は best-first search 法を用いているため，
既存の知識では最終的に構文木が
生成できない文があると，その文の処理に極端に時間がかかる．
その一方で，解に容易に到達できる場合もあり，文によって
処理時間のばらつきが大きい．
非終端記号20では，このように探索に時間がかかる文が多く，
(A)(B)(C)ともに長い時間を要している．特に認知単位
を用いた(B)および(C)では，表\ref{hyo1}
における$Q$が，他に比べて大きく，
この結果長い時間がかかっているものと思われる．
従って，認知単位を用いた方法では，非終端記号20では
十分に整理された文法が得られないものと考えられる．

しかし，非終端記号数が$40$以上になると(B)(C)，特に(B)は，
処理速度が著しく高速になる．
これは，認知単位を用いる方が，構文木を構成する葉の候補
の数が少ないため，構文解析での探索経路が少なくなることに起因する．


\begin{table}
\caption{獲得した知識に基づく構文解析結果} \label{hyo4}
\begin{center}
\begin{tabular}{c|c|cc}
\hline \hline
方法 &$非終端記号数N_n$[個] & 時間[s] & 正解率[\%] \\ \hline \hline
(A) & 20 & 1156 & 47 \\
& 40 & 347 & 43 \\
& 60 & 181 & 44 \\
& 80 & 267 & 40 \\ \hline
(B) & 20 & 1877 & 44 \\
& 40 &3 & 42 \\ 
& 60 & 4 & 35 \\
& 80 & 4 & 38 \\ \hline
(C) & 20 & 2799 & 49 \\
& 40 & 226 & 44 \\
& 60 & 95 & 44 \\
& 80 & 9 & 43 \\
\hline \hline
\end{tabular}
\vspace*{-4mm}
\end{center}
\end{table}


正解率でみると，(C)が最も良く，ついで(A)(B)の順となっている．
(B)が(A)に比べて劣るのは未知認知単位があるためであると考えられる．
(C)において未知認知単位の自動獲得が有効に機能しているのが分かる．

\section{むすび}
以上，認知単位を用いた文法の自動獲得法を提案した．
認知単位を基本とした文法を用いる解析法は，
形態素を基本とした文法を用いる解析法に比べ効率が高い．
一般に自然言語の文法は多重折込み要素が存在するため，
文全体の解析に状態遷移図を利用するのは適当ではないが，
認知単位のような狭い範囲に状態遷移図を利用するのは
有効であることが明らかとなった．

本方法以外の文解析方法においても，
認知単位を利用することにより
処理を効率化することができるものと考えられる．


\acknowledgment

本研究を進めるにあたり，データ入力やプログラミングに協力してくれた
東京理科大学藤崎研究室の阿部賢司氏並びに青島直哉氏に深く感謝する．

\bibliographystyle{jnlpbbl}
\bibliography{nlp002t}

\begin{biography}
\biotitle{略歴}
\bioauthor{横田 和章}{
1989年東京理科大学基礎工学部電子応用工学科卒業．
1993年同大学大学院修士課程了．
1996年同大学大学院博士後期課程了．
現在，(株)東芝 青梅工場所属．
}
\bioauthor{亀田 弘之}{
1982年東京大学工学部電子工学科卒業．
1984年同大学大学院修士課程了．
1987年同大学大学院博士課程了．
工学博士．
現在，東京工科大学工学部助教授．自然言語処理の研究に従事．
}
\bioauthor{藤崎 博也}{
1954年東京大学工学部電気工学科卒業．MIT・KTH(1958--1961)．
1962年東京大学大学院博士課程了．
工学博士．
同年東京大学工学部専任講師．
1963年同助教授．1973年同教授．1991年東京大学名誉教授，
東京理科大学基礎工学部教授．
音声生成・知覚・情報処理，自然言語処理等の研究に従事．
昭和38年度電気通信学会稲田賞，昭和42年度同学会論文賞，
昭和42年度電気学会論文賞，
昭和47年度電子通信学会業績賞，
1987年IEEE音響・音声・信号処理学会功績賞，1988年米国音響学会特別功績賞，
1989年東京都科学技術功労表彰受賞．
}
\bioreceived{受付}
\biorevised{再受付}
\bioaccepted{採録}
\end{biography}
\end{document}

