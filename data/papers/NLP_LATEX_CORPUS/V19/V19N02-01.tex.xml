<?xml version="1.0" ?>
<root>
  <jtitle>言い換えと逆翻字を用いた片仮名複合名詞の分割</jtitle>
  <jauthor>鍜治伸裕喜連川優</jauthor>
  <jabstract>日本語を含めた多くの言語において，複合名詞内部の単語境界は空白で分かち書きされない．こうした複合名詞を構成語列へと分割する処理は，多くの自然言語処理の応用において重要な基礎技術となる．日本語の場合，片仮名語は生産性が高く未知語が多いことから，特に片仮名複合名詞の扱いが技術的な問題となる．この問題の解決を図るため，本論文は片仮名複合名詞の言い換えと逆翻字を分割処理に利用する方法を提案する．実験では，言い換えと逆翻字をラベルなしテキストから抽出し，その情報を利用することによって，分割精度が統計的に有意に向上することを確認した．</jabstract>
  <jkeywords>言い換え，逆翻字，片仮名語，複合名詞分割，単語分割</jkeywords>
  <subsection title="パラメータ">最後に，パラメータの値を変化させたときの影響について調査を行った（図--）．図とは，様々な値のに対する，単語対応付き翻字対の抽出数および逆翻字素性の発火した割合（節において議論したもの）を示している．これらの図から，の値をある程度小さく設定すれば，十分な数の翻字対が抽出され，その結果として多くの事例において素性が発火するようになることが分かる．図はとF値の関係を示している．さきほどの2つの図との比較すると，翻字対の抽出数と素性の発火数の増加が，F値の向上に直接結びついていることが分かる．パラメータの値が極端に大きい場合(e.g.,-20)においては，F値が低下する傾向が見られたものの，パラメータによらずF値はおおよそ一定であった．この結果から，提案手法の精度はパラメータ設定に敏感ではなく，パラメータ調整は難しい作業ではないことが示唆される．また，少なくとも実験において調べた範囲では，提案手法はパラメータ値によらず，基本素性のみを用いた場合よりも高いF値を達成することができた．そのため，パラメータの微調整が提案手法の性能に与える影響は小さいと言うことができる．</subsection>
  <section title="はじめに"/>
  <subsection title="片仮名語と複合名詞分割">外国語からの借用(borrowing)は，日本語における代表的な語形成の1つとして知られている．特に英語からの借用によって，新造語や専門用語など，多くの言葉が日々日本語に取り込まれている．そうした借用語は，主に片仮名を使って表記されることから片仮名語とも呼ばれる．日本語におけるもう1つの代表的な語形成として，単語の複合(compounding)を挙げることができる．日本語は複合語が豊富な言語として知られており，とりわけ複合名詞にその数が多い．これら2つの語形成は，日本語における片仮名複合語を非常に生産性の高いものとしている．日本語を含めたアジアおよびヨーロッパ系言語においては，複合語を分かち書きせずに表記するものが多数存在する（ドイツ語，オランダ語，韓国語など）．そのような言語で記述されたテキストを処理対象とする場合，複合語を単語に分割する処理は，統計的機械翻訳，情報検索，略語認識などを実現する上で重要な基礎技術となる．例えば，統計的機械翻訳システムにおいては，複合語が構成語に分割されていれば，その複合語自体が翻訳表に登録されていなかったとしても，逐語的に翻訳を生成することが可能となる．情報検索においては，複合語を適切に分割することによって検索精度が向上することがBraschlerらの実験によって示されている．また，複合語内部の単語境界の情報は，その複合語の省略表現を生成または認識するための手がかりとして広く用いられている．高い精度での複合語分割処理を実現するためには，言語資源を有効的に活用することが重要となる．例えば，AlfonsecaらAlfonsecaCICLing08は単語辞書を学習器の素性として利用しているが，これが分割精度の向上に寄与することは直感的に明白である．これに加えて，対訳コーパスや対訳辞書といった対訳資源の有用性も，これまでの研究において指摘されている．英語表記において複合語は分かち書きされるため，複合語に対応する英訳表現を対訳資源から発見することができれば，その対応関係に基づいて複合語の分割規則を学習することが可能になる．複合語分割処理の精度低下を引き起こす大きな要因は，言語資源に登録されていない未知語の存在である．特に日本語の場合においては，片仮名語が未知語の中の大きな割合を占めていることが，これまでにも多くの研究者によって指摘されている．冒頭でも述べたように，片仮名語は生産性が非常に高いため，既存の言語資源に登録されていないものが多い．例えばBreenBreen09らによると，新聞記事から抽出した片仮名語のうち，およそ20%は既存の言語資源に登録されていなかったことが報告されている．こうした片仮名語から構成される複合名詞は，分割処理を行うことがとりわけ困難となっている．分割が難しい片仮名複合名詞として，例えば「モンスターペアレント」がある．この複合名詞を「モンスター」と「ペアレント」に分割することは一見容易なタスクに見えるが，一般的な形態素解析辞書には「ペアレント」が登録されていないことから，既存の形態素解析器にとっては困難な処理となっている．実際に，MeCabver.~0.98を用いて解析を行ったところ（解析辞書はNAIST-jdicver.~0.6.0を用いた），正しく分割することはできなかった．</subsection>
  <subsection title="言い換えと逆翻字の利用">こうした未知語の問題に対処するため，本論文では，大規模なラベルなしテキストを用いることによって，片仮名複合名詞の分割精度を向上させる方法を提案する．近年では特にウェブの発達によって，極めて大量のラベルなしテキストが容易に入手可能となっている．そうしたラベルなしテキストを有効活用することが可能になれば，辞書や対訳コーパスなどの高価で小規模な言語資源に依存した手法と比べ，未知語の問題が大幅に緩和されることが期待できる．これまでにも，ラベルなしテキストを複合名詞分割のために利用する方法はいくつか提案されているが，いずれも十分な精度は実現されていない．こうした関連研究については節において改めて議論を行う．提案手法の基本的な考え方は，片仮名複合名詞の言い換えを利用するというものである．一般的に，複合名詞は様々な形態・統語構造へと言い換えることが可能であるが，それらの中には，元の複合名詞内の単語境界の場所を強く示唆するものが存在する．そのため，そうした言い換え表現をラベルなしテキストから抽出し，その情報を機械学習の素性として利用することによって，分割精度の向上が可能となる．これと同様のことは，片仮名語から英語への言い換え，すなわち逆翻字に対しても言うことができる．基本的に片仮名語は英語を翻字したものであるため，単語境界が自明な元の英語表現を復元することができれば，その情報を分割処理に利用することが可能となる．提案手法の有効性を検証するための実験を行ったところ，言い換えと逆翻字のいずれを用いた場合においても，それらを用いなかった場合と比較して，F値において統計的に有意な改善が見られた．また，これまでに提案されている複合語分割手法との比較を行ったところ，提案手法の精度はそれらを大幅に上回っていることも確認することができた．これらの実験結果から，片仮名複合名詞の分割処理における，言い換えと逆翻字の有効性を実証的に確認することができた．本論文の構成は以下の通りである．まず節において，複合名詞分割に関する従来研究，およびその周辺分野における研究状況を概観する．次に節では，教師あり学習を用いて片仮名複合名詞の分割処理を行う枠組みを説明する．続いて節と節においては，言い換えと逆翻字を学習素性として使う手法について説明する．節では分割実験の結果を報告し，それに関する議論を行う．最後に節においてまとめを行う．</subsection>
  <section title="関連研究"/>
  <subsection title="複合語分割">これまでにも，ラベルなしテキストを用いた複合語分割手法はいくつか提案されている．それらはいずれも，複合語の構成語の頻度をラベルなしテキストから推定し，その頻度情報に基づいて分割候補を順位付けするものとなっている．とりわけ本研究と関連が深いのはであり，彼らもまた片仮名複合名詞を対象としている．しかし，こうした単語頻度に基づく手法は，対訳資源を用いた手法と比較して，十分な分割精度が得られないという問題が指摘されている．実際，我々の実験においても，これら単語頻度に基づく手法と提案手法との比較を行ったが，提案手法の方が大幅に高い分割精度を実現可能であることを確認した．一方，AlfonsecaらAlfonsecaCICLing08は，ラベルなしテキストではなくクエリログを複合語分割に利用することを提案している．しかし彼らの実験報告によると，クエリログを用いなかった場合の精度が90.45%であるのに対して，クエリログを用いた場合の精度は90.55%であり，その改善幅は極めて小さい．一方，本研究の実験（節）では，提案手法の導入によって精度は83.4%から87.6%へと大きく向上し，なおかつ，その差は統計的に有意であることが確認された．また，クエリログは一部の組織以外では入手が困難であるのに対し，提案手法に必要なラベルなしテキストは容易に入手することが可能である．HolzとBiemannHolz08は独語の複合語に対する分割手法と言い換え手法を提案しており，本研究との関連性が高い．しかし，彼らが提案しているアルゴリズムは，複合語の分割と言い換えをパイプライン的に行うものであるため，提案手法とは異なり，言い換えに関する情報は分割時に用いられない．</subsection>
  <subsection title="その他の関連研究">片仮名複合名詞の分割処理は単語分割の部分問題であると考えることができる．そのため，既存の単語分割器を用いて片仮名複合名詞の分割処理を行うことも可能であるが，実際問題として，それでは十分な分割精度を得ることは難しい（節の実験結果を参照）．この原因として，既存の単語分割器は辞書に強く依存した設計となっており，未知語が多い片仮名語の解析に失敗しやすいことが挙げられる．これに関する議論はが詳しい．単語分割の視点から見た本研究は，片仮名複合名詞という特に解析が困難な言語表現に焦点をあてた試みであると言える．節において我々は，片仮名複合名詞の分割のために逆翻字を利用する手法を提案する．提案手法は，技術的な観点から見ると，ウェブから片仮名語の逆翻字を自動抽出する既存手法と関連が深い．しかしながら，そうした関連研究は翻字辞書や翻字生成システムを構築することを目的としており，自動抽出した逆翻字を複合語の分割処理に利用する試みは本研究が初めてである．</subsection>
  <section title="教師あり学習に基づく手法">本論文では，片仮名複合名詞xが入力として与えられたとき，それを構成語列=(y_1,y_2y_||)へと分割する問題を取り扱う．ここでは，出力が1語（すなわち||=1）である場合もありうることに注意をされたい．1節においても議論したように，片仮名名詞は英語の翻字が多く，提案する素性の1つもその性質を利用したものとなっているため，以下では入力される片仮名語は英語の翻字であると仮定する．この仮定が実テキストにおいてどの程度成立しているのかを検証することは難しいが，例えばウェブ検索エンジンのクエリにおいては，片仮名のクエリの約87%は翻字であることが報告されている．このデータから上記の仮定にはある程度の妥当性があることが推測され，実テキストを処理する際にも提案手法の効果を期待することができる．我々は片仮名複合名詞の分割処理を「片仮名複合名詞xに対する構成語列を予測する構造予測問題」と捉えて，これを以下のような線形モデルを用いて解く．[^*=_Y(x)()]ここでY(x)は入力xに対する全分割候補の集合を表す．()は分割候補の素性ベクトル表現，は訓練事例から推定される重みベクトルである．表に我々が実験で用いた素性テンプレートを示す．テンプレート1からは，ある構成語1-gramが出現したか否かを示す2値素性が，訓練事例に出現した全ての構成語1-gramについて生成される．テンプレート2は同様の2-gram素性である．テンプレート3からは，構成語の文字数(1,2,3,4,5)を示す2値素性が5種類生成される．テンプレート4は構成語yが外部辞書に登録されているか否かを表す2値素性であり，構成語yが外部辞書に登録されていれば1を返す2値素性が1つ生成される．テンプレート5から7は，片仮名複合名詞の言い換えと逆翻字を用いたものであり，節と節において詳しく説明する．以下の議論では，テンプレート1から4によって生成される素性を基本素性，テンプレート5から生成される素性を言い換え素性，テンプレート6と7から生成される素性を逆翻字素性と呼んで互いに区別をする．重みベクトルは任意の学習アルゴリズムを用いて最適化することが可能であるが，ここでは計算効率を考慮して平均化パーセプトロンを用いた．平均化パーセプトロンはオンライン学習アルゴリズムの一種であり，高速に学習を行うことができると同時に，多くのタスクにおいてSVMなどのバッチ学習アルゴリズムと比較しても遜色のない精度を達成できることが知られている．パーセプトロンの訓練時およびテスト時には^*を求める操作が必要となるが，セミマルコフモデルにおいて用いられるのと同様の動的計画法によって効率的に実行可能である．</section>
  <section title="言い換え素性">本節では，片仮名複合名詞の言い換え表現を，教師あり学習の素性として使う方法について述べる（表におけるテンプレート5に対応する）．</section>
  <subsection title="複合名詞の言い換え">一般的に，複合名詞は様々な形へと言い換えることが可能であるが，そうした言い換え表現の中には，元の複合名詞の単語境界を認識する手がかりとなるものが存在する．以下に具体例を示す．アンチョビパスタex:anchovyアンチョビ・パスタ[3pt]アンチョビのパスタlingexample(b)は，複合名詞(a)の構成語間に中黒を挿入することによって生成された言い換え表現である．同様に(c)は助詞「の」を挿入することによって生成された言い換え表現である．もしラベルなしテキストにおいて(b)や(c)のような言い換え表現を観測することができれば，このことは複合名詞(a)を「アンチョビ」と「パスタ」に正しく分割するための手がかりとなることが考えられる．</subsection>
  <subsection title="言い換え規則">このような言い換えを利用して片仮名複合名詞の分割処理を行うため，複合名詞の言い換え規則を7つ作成した（表）．言い換え規則の作成にあたっては，KageuraらKageura04の研究を参考にしながら，分割処理に有用と思われるものを人手で選定した．作成した言い換え規則は全てX_1X_2X_1MX_2という形式をしており（X_1とX_2は名詞，Mは助詞などの機能語），左辺が言い換え前の複合名詞，右辺が言い換え後の表現に対応している．</subsection>
  <subsection title="言い換え頻度に基づく素性">これらの規則を用いて，次のように新しい素性を定義する．まず前処理として，以下のような正規表現を用いることにより，片仮名複合名詞の言い換え表現の出現頻度をラベルなしテキストから求める．ただし(katakana)は片仮名1文字にマッチする特殊文字である．また+は文字の繰り返しを表す量指定子であり，最長一致が適用されるものとする．このような正規表現を用いることによって，単語分割処理を行わずに言い換え表現を抽出することができるのは，表のような片仮名複合語の言い換え表現に対象を限定しているためである．上記の正規表現にマッチするテキストは，必ず前後が片仮名以外の文字（漢字や平仮名）に囲まれていることになる．そのような文字種の変わり目には，単語境界が存在する場合が多いため，このような単純な文字列処理であっても言い換え表現を抽出することが可能になっている．分割処理時に分割候補が提示された際には，構成語2-gramに対する言い換え素性Para(y_i-1,y_i)の値を次のように定義する．まずX_1=y_i-1，X_2=y_iと代入することにより，表の規則から言い換え表現を生成する．そして，生成された7つの言い換え表現の頻度の和をFとしたとき，その対数(F+1)を素性値として用いる．ここでは素性値の計算に非常に単純な方法を用いているため，X_1やX_2に名詞ではなく，名詞連続が代入された場合であっても，素性が発火してしまうということがある．また逆に，正解となる構成語よりも小さな単位の文字列が代入された場合であっても，同様に素性が発火してしまうことがあり，精度に悪影響を及ぼす可能性がある．しかし，このような手法であっても実験において分割精度の向上を十分確認することができたため，シンプルさを重視して現在のような手法とした．素性値として頻度ではなく対数頻度を用いているのはスケーリングのためである．予備的な実験においては，頻度をそのまま素性値として用いることも行ったが，対数頻度を用いた場合の方が高い精度が得られた．なお，Fではなく(F+1)としているのは，F=1であった場合に素性値が0となるのを防ぐためである．</subsection>
  <section title="逆翻字素性">片仮名語の多くは英語を翻字したものであり，元となる英語表現が存在する．以下では，そのような英語表現のことを原語と呼び，片仮名語と原語の対のことを翻字対と呼ぶこととする．我々は，片仮名語が原語の発音情報をおおよそ保持しているという特性を利用することによって，単語単位での対応関係が付与された翻字対（単語対応付き翻字対）をラベルなしテキストから自動抽出する（表）．そして，得られた単語対応付き翻字対に基づいて，分割結果に出現する単語n-gramが，英単語n-gramと対応付け可能であるかを示す2値素性を用いる（表におけるテンプレート6と7に対応する）．以下本節では，テキストから単語対応付き翻字対を自動抽出する方法について説明する．</section>
  <subsection title="括弧表現">日本語においては，括弧表現を使って片仮名語の原語がテキスト中に挿入される場合がある．アメリカでジャンクフード(junkfood)と言えば...ex:junkトラックバックスパム(spam)を撃退するため...lingexampleいずれの例文においても，下線を引いた片仮名語に対して，その原語が括弧を使って併記されている．我々はこのような括弧表現を利用することにより，単語対応付き翻字対の自動抽出を行う．こうした括弧表現から単語対応付き翻字対の抽出を行うためには，少なくとも以下の3つのことが技術的な問題となる</subsection>
  <subsection title="発音の類似性の利用">これまでにも，前述のような括弧表現から翻字対を自動抽出する研究は数多く存在するが，問題Cに対する本質的な解決策はいまだ提案されていない．これまでの研究においては，基本的に既存の単語分割器を用いることによって片仮名語の分割が行われている．しかし，節において議論を行ったように，片仮名語の分かち書きを行うことは現在のところ技術的に困難であり，このようなアプローチは望ましくない．我々は上記の3つの問題を解決するため，片仮名語と原語の発音の類似性を利用することを提案する．以下の議論では，説明のために，まず問題Cだけを議論の対象とする．具体例として，片仮名語「ジャンクフード」と原語「junkfood」に対して，それらの発音の類似性に基づき以下のような部分文字列の対応関係が得られたとする．[ジャン]_1[ク]_2[フー]_3[ド]_4ex:junk2[jun]_1[k]_2[foo]_3[d]_4lingexampleここでは，括弧で囲まれて同じ番号を添えられている部分文字列が，互いに対応関係にあるものとする．括弧表現内の英語は空白を使って分かち書きされているため，上記のような部分文字列の対応関係を利用すれば，片仮名語と英単語が1対1に対応するように片仮名列を分かち書きすることができる．また，その過程において，単語間の対応関係も明らかにすることができる．残る問題Aおよび問題Bに対しても，発音の類似性に基づいて同様に解決を図ることが可能である．以下の例において，下線が引かれた片仮名語と括弧内の英語表現が翻字対であるか否かを判定することを考える．検索エンジン(Google)を使って...ex:googleトラックバックスパム(spam)を撃退する...lingexampleこのように，括弧内に原語ではない表現が出現したり，片仮名語の開始位置が正しく認識されなかった場合には，片仮名列とアルファベット列の発音の類似度が低くなることが期待できるため，フィルタリングできると考えられる．単語対応付き翻字対の具体的な抽出手順については，節において説明を行う．</subsection>
  <subsection title="発音モデル">片仮名語と原語における部分文字列の対応関係の発見には，JiampojamarnらJiampojamarn07が提案した生成モデルを用いる．fとeをそれぞれ片仮名列とアルファベット列とし，これらの間の対応関係を見つけることを考える．ただし，原語には空白が存在する可能性があるが，空白に対応する片仮名文字列は存在しないことから，部分文字列の対応を求めるときにはアルファベット列から空白を取り除いておく．例えば「ジャンクフード」と「junkfood」の部分文字列対応を求める場合には「f=ジャンクフード」「e=junkfood」とする．ここで，Aをそれらの間の部分文字列の対応とする．具体的には，Aは対応付けられている部分文字列の組(f_i,e_i)の集合であり，f=f_1f_2f_|A|およびe=e_1e_2_|A|となる．この部分文字列対応Aの確率を以下のように定義する．[p(f,e,A)=_(f_i,e_i)Ap(f_i,e_i)]一般にAは観測することができないため隠れ変数として扱い，モデルのパラメータは翻字対(f,e)の集合からEMアルゴリズムを用いて推定する．詳細は文献を参照されたい．表に「ジャンクフード」と「junkfood」に対する部分文字列対応Aの具体例，および実験において計算された確率値を示す．この確率モデルを用いて，与えられた(f,e)に対する部分文字列の対応を次のように決定する．[A^*=_Ap(f,e,A)]このときA^*の中の部分文字列e_iが空白をまたいでしまうと（ジャンクフードの例であればe_i=kfooなどとなった場合），A^*を使って片仮名列fを分かち書きすることができなくなってしまう．そこで，アルファベット列eが空白を含んでいた場合は，前述のとおり空白を取り除いて確率値の計算を行うが，空白の存在した箇所は記憶しておき，部分文字列e_iが空白をまたがないという制約を加えての計算を行う．</subsection>
  <subsection title="単語対応付き翻字対の抽出">この発音モデルを用いて，以下のような手順で単語対応付き翻字対の抽出を行う．ただし，手順2においては，表記揺れやタイポなどの要因により，1つの片仮名列に対して複数の逆翻字が見つかる可能性がある．その場合は，各片仮名列fに対して，最もスコアの高い翻字対(f,e)のみを保持して，それ以外のものは使用しない．</subsection>
  <section title="実験と議論">本節では，提案する2つの素性（言い換え素性と翻字素性）が片仮名複合名詞の分割処理の精度に与える効果について報告を行う．</section>
  <subsection title="実験設定">発音モデルのパラメータ推定に必要な翻字対のデータは，外国人の名前を日本語で表記するときにはほぼ常に翻字が行われることに着目し，Wikipediaを用いて自動的に構築した．構築手順としては，まず「存命人物」のカテゴリに所属するWikipeida記事のタイトルを抽出することにより，片仮名表記の人名リストを作成した．そして次に，Wikipediaの言語間リンクを利用し，各人名に対する原語を抽出した．これにより17,509の翻字対を収集することができた．このように自動収集したデータの中には翻字対として不適切なものも含まれている可能性があるが，大量のデータを手軽に用意できるという利点を重視して，この方法を採用している．実際，このようにパラメータ推定のためのデータを大量に生成するアプローチは，翻字生成において有効であることが報告されている．パラメータ推定時には，EMアルゴリズムの初期値を無作為に10回変化させ，尤度が最大となったモデルを以降の実験で用いた．平均化パープトロンの学習に必要なラベル付きデータは，日英対訳辞書EDICTjwb/edict_doc.htmlを利用して手作業で作成した．具体的には，まず，EDICTの見出し語から，翻字である片仮名（複合）名詞を無作為に抽出した．そして，EDICTに記載されている英訳に基づき，単語境界のラベルを付与した．この結果，5286の片仮名語データを得た．このデータにおける構成語数の分布を調べたところ，構成語が1語のものが3041，2語のものが2081，3語以上のものが164となっていた(3041+2081+164=5286)．また，複合名詞1つあたりの平均文字数および平均構成語数は6.60および1.46であった．以下本節において報告する実験結果は，このラベル付きデータを用いて2分割交差検定を行ったものである．言い換え及び逆翻字を抽出するためのテキストには，ウェブから収集した17億文のブログ記事を用いた．このテキストを用いることによって14,966,205の言い換え表現と，116,027の単語対応付き翻字対を抽出することができた．表とに，実際に抽出された言い換え表現（の候補）と単語対応付き翻字対の具体例を示す．単語対応付き翻字対の抽出を行う際には閾値を設定する必要がある．は確率の対数に対する閾値であるため，0より小さな任意の値を設定することが可能であるが，ここでは-10,-20,-150の範囲で値を変化させ，実験において最も高いF値が得られた値(=-80)を採用した．</subsection>
  <subsection title="ベースライン手法">実験では，3つの教師なし学習手法(Unigram,GMF,GMF2)，2つの教師あり学習手法(AP,AP+GMF2)，3つの単語分割器(JUMAN,MeCab,KyTea)との比較を行った．以下ではこれらベースライン手法について簡単に説明を行う．</subsection>
  <subsection title="ベースライン手法との比較">表に提案手法(Proposed)とベースライン手法との比較結果を示す．この表の結果から以下のようなことが分かる．まず，ProposedとAPの結果の比較から，言い換え素性と逆翻字素性を導入することにより，分割精度が大きく向上したことが分かる．マクネマー検定を行ったところ，この精度変化は統計的に有意なものであることが確認された(p&lt;0.01)．この結果は，提案する2つの素性の有効性を示すものである．次に，提案手法の精度は，全ての教師なし学習ベースライン(Unigram,GMF,GMF2)及びAP+GMF2の精度を上回っていることが確認できる．これらの結果は，複合名詞の言い換えや逆翻字の情報が，構成語の頻度情報よりも効果的であることを示唆している．なお，マクネマー検定を行ったところ，これらの精度向上も全て統計的に有意であることが確認できた(p&lt;0.01)．単語分割器(JUMAN,MeCab,KyTea)の結果は，これまでに単語分割タスクにおいて報告されている精度を大きく下回っている．このことから，一般的な単語分割と比較して，片仮名複合語の分割処理が困難なタスクであることが分かる．さらに，提案手法の精度は，単語分割器の精度を大きく上回っており，提案手法が既存の単語分割器の弱点補強に有用であることが示唆されている．例えば，既存の単語分割器によって「片仮名表記の名詞の連続」と解析された部分を，提案手法を用いて再分割することにより，解析結果の改善を期待することができる．表に，MeCabでは分割に失敗したが，Proposedでは正しく分割することができた例を示す．まず最初の例では，片仮名語「ディクショナリー」がNAIST-jdicに登録されていなかったため，MeCabは分割に失敗している．一方，Proposedにおいては，以下のような単語対応付き翻字対が学習されており，これに基づいて発火した逆翻字素性(1-gram)が有効に働いた結果，正しく分割することに成功している．次の例では「メイン」と「タイトル」が両方ともNAIST-jdicに登録されているにも関わらず，MeCabは分割に失敗している．これは，MeCabの未知語処理に起因する誤りであると考えられる．その一方でProposedが分割に成功しているのは，例えば「メインのタイトル」といった言い換え表現に基づく素性など，分割を示唆する素性がより多く発火しているためだと推測できる．最後の例では，NAIST-jdicに人名「トミー」が登録されているため，MeCabは過分割を行ってしまっているが，Proposedでは「アナトミー」に対する逆翻字素性が適切に発火しており，過分割を防ぐことに成功している．本論文の趣旨からは外れるが，3つの単語分割器のなかではKyTeaの精度が他の2つを大きく引き離している点は非常に興味深い．これは，JUMANやMeCabの解析アルゴリズムが，辞書引きによる候補選択に強く依存しているのに対して，KyTeaはそのような候補選択を行っていないことが要因と考えられる．</subsection>
  <subsection title="未知語に関する考察">実験に使用した5286の片仮名複合名詞のうち，2542は少なくとも1つの未知語を構成語に含んでいた．ただし，ここで言う未知語とは，訓練データに出現せず，なおかつ外部辞書NAIST-jdicにも登録されていない単語のことを指す．未知語が分割精度に与える影響について考察するため，提案手法を含む3つの教師あり学習手法(AP,AP+GMF2,Proposed)と単語分割器MeCabの分類結果を，1つ以上の未知語を含む2542の片仮名複合名詞と残る2744の片仮名複合名詞に分けて集計した（表）．以下では，前者のサブセットをw/OOVデータ，後者をw/oOOVデータと呼ぶ．この表から，3つの教師あり学習手法については，w/oOOVデータに対しては90%を越える高い精度が達成されているのに対して，w/OOVデータの精度は大きく低下していることが確認できる．同様の傾向はMeCabの結果においても見られる．MeCabは汎用的な単語分割器であるため，複合名詞分割というタスクに特化して学習された提案手法(Proposed)やその他の教師あり学習手法（APやAP+GMF2）と比べると，精度自体はどちらのデータにおいても大きく低下している．しかし，w/OOVデータよりもw/oOOVデータのほうが精度が高くなるという傾向は，依然として確認することができる．これらの結果は，片仮名複合名詞の分割処理を困難にしている要因は未知語であるという我々の主張を支持するものである．3つの教師あり学習手法は，w/oOOVデータについてはほぼ同じ精度を達成していることが分かる．これは，既知語に対しては，基本素性だけを使ってすでに高い分類精度が達成されているため，これ以上の精度向上が困難であるからだと考えられる．一方，精度向上の余地が残されているw/OOVデータについては，3つのシステムの間に大きな精度の差を見てとることができる．そのため，表の結果よりも，言い換え素性と翻字素性を導入する効果をより直接的に確認することができる．</subsection>
  <subsection title="言い換え素性と翻字素性の効果">言い換え素性と翻字素性の有効性について詳細に検証するため，異なる4つの素性集合を用いたときの平均化パーセプトロンの分割結果の比較を行った（表）．表の1行目は使用した素性集合を表す．Basicは基本素性，ParaとTransはそれぞれ言い換え素性と翻字素性，は全ての素性集合を表す．この表より，言い換え素性と翻字素性の両方ともが分割精度向上に大きく貢献していることを確認することができた．いずれの場合においても，基本素性だけを使った場合と比較して，精度の向上は統計的に有意であった（p&lt;0.01，マクネマー検定）．次に，各素性の発火率について調査を行った．実験で用いたラベル付きデータには7709の構成語が含まれており，そのうち64.0%(4937/7709)は外部辞書に登録されていた．これに対して，単語対応付き翻字対に出現していた構成語の割合は64.0%(4935/7709)，外部辞書か単語対応付き翻字対のいずれかに出現していたものの割合は77.1%(5941/7709)であった．これにより，翻字素性を導入することによって，未知語の数が大幅に減少していることが確認された．一方，ラベル付きデータに含まれる構成語2-gramの数は2423であったが，それらに対して発火していた言い換え素性と翻字素性の割合は，それぞれ79.5%(1926/2423)と12.8%(331/2423)であった．これらの結果から，提案素性はいずれも精度向上に寄与しているものの，カバレージにはまだ改善の余地があることが分かった．続いて，素性の発火率と収集元であるブログデータの大きさの関係を調査した（図）．ここではブログデータの大きさとして，収集したブログ記事（UTF8エンコーディング）をgzipで圧縮したデータのサイズをギガバイト単位で表示している．この図から，大量のブログデータを使うことによって，高い発火率を実現できていることが確認できる．しかし，その一方で，データが増えるにつれて，発火率の向上の度合いは鈍りつつある．このことから，データを単純に増加させるだけでは，ここからの大幅な発火率の改善を期待することは難しく，言い換え規則の拡張などの方法も併せて検討していくことが今後重要になると考えられる．</subsection>
  <subsection title="誤りの分析">提案手法が分割を誤った事例を調べたところ，「アップロード」を「アップ」と「ロード」，「トランスフォーマー」を「トランス」と「フォーマー」に分割するなど，単語を過分割している事例が見られた．ここでの「アップ」や「トランス」は接頭辞であると考えられるため，これらの分割結果は形態論的分割(morphologicalsegmentation)としては正しいものであるかもしれないが，単語分割としては不適切であると考えられる．こうした過分割が発生する要因として，接辞と単語の曖昧性を指摘することができる．例えば「アップ」は，確かに接頭辞の1つであるが，文脈によっては「給料がアップする」のように独立した名詞として使われる場合もある．同じく「トランス」に対しても「トランス状態」のような名詞用法を考えることができる．このような曖昧性によって引き起こされる最も顕著な問題は，辞書素性（表におけるテンプレートID4）が過剰に発火することである．前述の過分割の事例においては，NAIST-jdicに「アップ」と「トランス」がともに名詞として登録されていたため，本来不適切な分割結果であるにも関わらず辞書素性が発火していた．これと同様の問題は，辞書素性だけでなく，逆翻字素性においても発生しうる．節で説明した単語対応付き翻字対の抽出手法は，原語が正しく分かち書きされていることを前提としていた．しかしながら，実際には接頭辞や接尾辞の前後に空白区切りを挿入しているテキストも存在するため，不適切な対応関係が学習されてしまう場合がある．表は上記の過分割結果に影響を与えたと思われる単語対応付き翻字対の一部である．この表から，「アップロード」と「トランスフォーマー」については，それぞれ原語との対応関係が適切に学習されていることが確認できる．しかしながら「アップローダー」と「トランスフォーム」については，原語が接頭辞の直後で分かち書きされていたため，不適切な単語対応が学習されていることが分かる．こうした対応付け結果から導出された逆翻字素性（この例では特に1-gram）は分割に悪影響を与えている可能性がある．翻字抽出の手法を改善することにより，こうした誤りを減少させることは，今後の課題の一つであると考えている．過分割が多くみられた別要因としてデータの偏りを考えることもできる．今回使用したデータの半数以上は構成語数が1つであったため，そもそも過分割が発生しやすい設定の実験になっていた可能性がある（節を参照）．現在のところ，当該タスクに対する別のデータセットを用意することは難しいため，この点をすぐに調査することはできないが，今後の研究の中で議論を深めていくべきであると考えられる．</subsection>
  <section title="おわりに">本論文では，言い換えと逆翻字を用いて，片仮名複合語の分割処理の精度を向上させる方法を提案した．提案手法により，大規模なラベルなしテキストを分割処理に利用することが可能となり，分割精度の低下の要因となる未知語の影響を軽減させることが可能となる．実験においては，8つのベースライン手法との比較を通じて，提案手法の有効性を実証的に示した．今後の課題としては，提案手法と既存の単語分割手法を融合した解析モデルの構築に取り組みたい．節においては，提案手法を後処理に利用可能であることについて言及したが，そうしたアドホックな方法は，学術的立場からは必ずしも満足のいくものではないと考えている．提案手法と既存の単語分割を組み合わせる方法としては，今回提案した素性を統計的な単語分割器に追加することなどが考えられるが，現時点ではその有効性について十分な検証を行うことができておらず，今後調査すべき課題であろう．また，近年では，教師なし学習による単語分割手法も盛んに研究されているが，そうした手法に言い換えや翻字の情報を取り入れることも興味深い問題である．これに加えて，本論文の中で提案したアイデアを一般化していくことも，今後重要な研究課題になると考えている．本論文では議論の対象を英語由来の片仮名複合名詞に限定していたが，同様の手法は，その他の片仮名語に対しても有効である可能性が高い．例えば，翻字素性は，英語以外の言語からの借用語に対しても有効に働くことが期待できる．また，言い換え素性は，和語や漢語の片仮名表記に対しても有効である可能性が高い（例えば「トンコツラーメン」に対する「トンコツのラーメン」などの言い換え）．さらに，言い換えを単語境界の認識に利用するという考え方は，複合名詞に限らず，単語分割処理一般に対しても適用できる可能性がある．今後はこうした方向についても研究を進めていきたい．document</section>
</root>
