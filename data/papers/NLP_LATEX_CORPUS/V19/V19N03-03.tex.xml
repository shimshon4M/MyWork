<?xml version="1.0" ?>
<root>
  <section title="Introduction">Parsingisoneofthefundamentalbuildingblocksofnaturallanguageprocessing,withapplicationsrangingfrommachinetranslationtoinformationextraction.However,whilestatisticalparsersachievehigherandhigheraccuraciesonin-domaintext,thecreationofdatatotraintheseparsersislabor-intensive,whichbecomesabottleneckforsmallerlanguages.Inaddition,itisalsoawellknownfactthataccuracyplummetswhentestedonsentencesofadifferentdomainthanthetrainingcorpus,andthatin-domaindatacanbeannotatedtomakeupforthisweakness.Inthispaper,weproposeadependencyparserforJapanesethathelpsamelioratetheseproblemsbyallowingfortheefficientdevelopmentoftrainingdata.Thisisdonethroughacombinationofanefficientcorpusannotationstrategyandanovelparsingmethod.WeusetheassumptionthatJapaneseisahead-finallanguagetosimplifydecodingbyconstrainingthesizeofthesearchspace.Forcorpusconstruction,weusepartialannotation,whichallowsanannotatortoskipannotationofunnecessaryedges,focusingtheireffortsonlyontheonesthatwillprovidethemaximalgainsinaccuracy.Whilepartialannotationhasbeenshowntobeaneffectiveannotationstrategyforanumberoftasks,traditionalparserssuchasthatofcannotbelearnedfrompartiallyannotateddata.Thereasonforthisisthattheyusestructuralpredictionmethodsthatmustbelearnedfromfullyannotatedsentences.However,anumberofrecentworkshavefoundthatitispossibletoignorestructureandstillachievecompetitiveaccuracyontaskssuchaspart-of-speech(POS)tagging.Similarly,recentworkondependencyparsinghasshownthattrainingconstraintscanberelaxedtoallowparserstobetrainedfrompartiallyannotatedsentences,withonlyasmallreductioninparsingaccuracy.Inthisapproachthescoringfunctionusedtoevaluatepotentialdependencytreesismodifiedsothatitdoesnotpenalizetreesconsistentwiththepartialannotationsusedfortraining.Ourformulationisbasedonanevenstrongerindependenceassumption,namelythatthescoreofeachedgeisindependentoftheotheredgesinthedependencytree.Whilethisdoeshavethepotentialtodecreaseaccuracy,ithasanumberofadvantagessuchastheabilitytousepartiallyannotateddata,fasterspeed,andsimpleimplementation.WeperformanevaluationoftheproposedmethodonaJapanesedependencyparsingtask.First,wecomparetheproposedmethodtoboth'sparserandadeterministicparser.Wefindthatdespitethelackofstructureinourpredictionmethod,theproposedmethodisstillabletoachieveaccuracysimilartothatof'sparser,whiletrainingandtestingspeedsaresimilartothoseofthedeterministicparser.Inaddition,weperformacase-studyoftheuseofpartialannotationinapracticalscenario,wherewehavedatathatfollowsasegmentationstandardthatdiffersfromtheonewewouldliketofollow.InJapanesedependencyparsing,traditionallyphrasesegments(bunsetsu)havebeenusedinsteadofwordsastheminimalunitforparsing,butthesesegmentsareoftentoolargeorunwieldyforapplicationssuchasinformationextractionandmachinetranslation.Inourcase-study,wedemonstratethatacorpuslabeledwithphrasedependenciescanbeusedasapartiallyannotatedcorpusinthedevelopmentofaword-basedparserthatismoreappropriatefortheseapplications.Theuseofaphrase-labeledcorpusallowsustoincreasetheaccuracyofaword-basedparsertrainedonasmallerword-labeleddatasetby2.75%.</section>
  <section title="Pointwise estimation for dependency parsing">Thisworkfollowsthestandardsettingofrecentworkondependencyparsing.Givenasequenceofwordsw=w_1,w_2,,w_nasinput,thegoalistooutputadependencytreed=d_1,d_2,,d_n,whered_ijwhentheheadofw_iisw_j.Weassumethatd_i=0forsomewordw_iinasentence,whichindicatesthatw_iistheheadofthesentence.</section>
  <subsection title="A pointwise dependency parser">Theparsingmodelwepursueinthispaperis'sedge-factoredmodel.Ascore,(i,d_i,w),isassignedtoeachedge(i.e.dependency)d_i,andparsingfindsadependencytree,d,thatmaximizesthesumofthescoresofalltheedgeswhereDisthesetofallpossiblespanningtreesfortheinputsentence.Itisknownthat,given(i,d_i,w)forallpossibledependenciesinasentence,dcanbecomputedbythemaximumspanningtreealgorithmsuchasChu-Liu/Edmonds'algorithm.Animportantdifferencefromisintheestimationof(i,d_i,w).appliedaperceptron-likealgorithmthatoptimizesthescoreofentiredependencytrees.However,westicktopointwiseprediction:(i,d_i,w)isestimatedforeachw_iindependently.Anyvarietyofmachine-learning-basedclassifierscanbeappliedtotheestimationof(i,d_i,w),becauseitisessentiallyann-classclassificationproblem.Wedefinetheedgescoreasaprobability,(i,d_i,w)=p(d_i),andestimatealog-linearmodel.Wecalculatetheprobabilityofadependencylabelingp(d_i=j)forawordw_ifromitscontext,whichisatuplex=w,t,i,wheret=t_1,t_2,,t_nisasequenceofPOStagsassignedtowbyatagger.Theconditionalprobabilityp(j|x)isgivenbythefollowingequation.Thefeaturevector=_1,_2,,_misavectorofnon-negativevaluescalculatedfromfeaturesonpairs(x,j),withcorrespondingweightsgivenbytheparametervector=_1,_2,,_m.Weestimatefromsentencesannotatedwithdependencies.Itshouldbenotedthattheprobabilityp(d_i)dependsonlyoni,j,andtheinputsw,t,whichensuresthatitisestimatedindependentlyforeachw_i.Becauseparameterestimationdoesnotinvolvecomputingd,wedonotapplythemaximumspanningtreealgorithmintraining.</subsection>
  <subsection title="Features">Ourcurrentimplementationusesthefollowingfeatures,bothindividuallyandascombinationfeatures,for.Thedistancej-ibetweenadependentwordw_ianditscandidateheadw_j.Thesurfaceformsw_iandw_j.Theparts-of-speechofw_iandw_j.Thesurfaceformsofuptothreewordstotheleftofw_iandw_j.Thesurfaceformsofuptothreewordstotherightofw_iandw_j.Theparts-of-speechofthewordsselectedfor.Theparts-of-speechofthewordsselectedfor.Tableshowsthevaluesofthesefeaturesforapartiallyannotatedexamplesentencewhereoneword,thecasemarkerは(),hasbeenannotatedwithitshead,theverb歓迎(welcomes).Usingpointwisepredictionratherthanstructuredpredictionhasthepotentialtohurtparsingaccuracy.However,ourmethodcanenjoygreaterflexibility,whichallowsfortrainingfrompartiallyannotatedcorporaaswillbedescribedinSection~.Italsosimplifiestheimplementationandreducesthetimenecessaryfortraining,whichisimportantasrecentworkonactivelearningforwordsegmentationandPOStagginghasshowntheimportanceoflearningspeedforactivelearningstrategies.</subsection>
  <subsection title="First-order and second-order features">'soriginalapproachiscalledafirst-orderformulationbecausefeaturesareonlydefinedoverthedependentandheadwordsformingasingleedge.ThemainfeaturesusedarethesurfaceformsandPOStagsofthedependentandhead,anddistancebetweenthem.Theyalsoincorporatelocalcontextinformationbydefiningfeaturesonwordstotheimmediateleftandrightofboththedependentandhead,forawindowsizeofthreewords.Similarly,theymakeuseofbroadercontextinformationbydefiningfeaturesonthePOStagsofwordsthatoccurbetweenthedependentandhead.laterextendedthefirst-orderapproachoftoasecond-orderapproach,whereinformationaboutadjacentedgesisalsousedasfeatures.Inthisnewformulation,thescoreofthetreeisfactoredintothesumofadjacentedgepairscoresinsteadofthesumofindividualedgescores.Becauseuptotwoadjacentdependencyedgesfromthesameheadareconsideredwhencomputinganedgepairscore,thishastheeffectofconditioningonthelastdependentchosenforthehead.Incontrasttothesecond-orderformulationdescribedabove,theproposedmethodstickstofirst-orderfeaturesbutreferstoalargerwindowofsurroundingwordsforboththedependentandhead.Uptothreewordsineachdirectionareconsidered,resultinginawindowsizeofsevenwords.Thisallowsustopickupcontextinformationregardlessofwhetheranadjacentedgeexistsforahead.showedthatthiskindofcontextinformationcanalsobeusefulforphrase-basedJapanesedependencyparsing.Therearethreemainmotivationsforourpointwiseapproach.First,wewishtoavoidfeaturesparsityinthetrainingdatabyrestrictingourselvestofirst-orderfeatures.Second,wewanttoenableourparsertobetrainedfrompartiallyannotatedcorpora,whereonlysomedependenciesinasentenceareannotated.Finally,weseektoreducetheamountoftimenecessaryfortraining.Wewillshowinsec:withFACthatforaJapanesedependencyparsingtask,theproposedmethodachievesreasonablygoodparsingaccuracy.</subsection>
  <subsection title="Solution search">ThetargetofourexperimentsiswrittenJapanese,whichisahead-finallanguage.Inlinewith,weassumethatinJapanesedependenciesgofromlefttorightandthateverywordexceptforthelastoneinasentencedependsonexactlyoneotherword.Thusweassumethatd_i&gt;iforallinandd_n=0.Thisassumptionreducesthemaximumspanningtreealgorithmtoasimpleralgorithm:foreachwordweselectthedependencywiththemaximumscore.Asthisnevercreatesaloopofdependencies,arecursiveprocessasinChu-Liu/Edmonds'algorithmisnotnecessary.Incontrasttowedonotmaketheassumptionthatdependenciesdonotcross,becauseeveninwrittenJapanesesuchdependenciesmayoccurininformalcontexts.Ourimplementationdoesnotenforcethisprojectivityconstraintondependencies,soitcanhandlenon-projectivedependenciesinthetrainingdatawhichsatisfythehead-finalassumption.Thishead-finalassumptiondoesnotholdforspokenJapaneseandlanguagessuchasEnglish,butitiseasytoextendourimplementationtohandlethesecases.Specifically,thiscanbedonebychangingtheconstraintonheadsfromd_i&gt;itod_iiandusingChu-Liu/Edmonds'algorithmtoensurethatnoloopsofdependenciesarecreatedwhilebuildingthemaximumspanningtreeforthesentence.Thisalgorithmhastheadditionalbenefitofhandlingalltypesofnon-projectivedependencies.Becausetheproposedmethodforlearningfeatureweightsfrompartiallyannotateddatadoesnotdependontheparsingalgorithm,differentparsingalgorithmscouldalsobeused,forexampletoenforceprojectivityconstraints.</subsection>
  <subsection title="Japanese dependency parsing">alsoproposedaprobabalisticparserforJapanesewhichusestheassumptionthatedgescanbeestimatedindependently.Theirapproachusesbunsetsu(chunks)insteadofwordsandislimitedtoprojectivedependencies,butisotherwisesimilartotheproposedmethod.Thekeydifferenceishowcontextinformationisusedintheirfeatureset.Duringbothtrainingandparsing,informationaboutdependenciesforchunksbetweenacandidatedependentandheadisusedfordeterminingwhetheradependencyexistsbetweenthem.Thesearecalled``dynamicfeatures''becausetheyareupdateddynamicallyduringparsingasdependenciesinthesentenceareestimated,incontrastto``static''featureslikePOStags,whichdependonlyontheinputstringanddonotchange.Whilefeaturesbasedonsurroundingdependenciesaretrivialtouseduringtraining,suchfeaturesaredifficulttouseduringparsingbecausethestructureforthedependencytreeisnotknown.Dynamicfeaturesareupdatedasthedependencytreestructureforthesentenceisbuiltincrementally,allowingdependenciesthathavebeenfinalizedtobeusedasinformationforthosethathavenot.concludethatmodelswiththesedynamicfeaturesconsistentlyoutperformthosewithoutthem.Incontrast,ourapproachdoesnotuseestimatedvaluessuchasthesedynamicfeatureswhendeterminingwhetheradependencyexistsbetweenagivenpairofwordswhenparsing.Instead,ourfeaturesetusesalargernumberofstaticfeaturestocapturecontextinformation.Whenevaluatingapotentialdependencybetweenadependentandaheadword,surfaceformsandPOStagsinawindowofsevenwordsforbothareusedasfeatures.Dependenciesareestimatedindependentlytoenabletrainingfrompartialannotation.Because'smodelassumesthatedgesareindependentofeachother,itistheoreticallypossibletoadaptitsothatitcanusepartiallyannotatedtrainingdata.Dynamicfeaturesbasedonchunksbetweenacandidatedependentandheadchunkareanimportantpartoftheirapproach,sotoannotateasingledependencyforapairofchunksanannotatorwouldhavetoannotatetheheadsforchunksinbetweenthem.Thismakespartialannotationtime-consumingforchunkswhicharenotadjacent.showedhowpartialannotationcanbeusedforJapanesedependencyparsing,butonlyconsideredpartialannotationsconsistingofadjacentchunksforthisreason.Inourmodelitissufficienttoannotateindividualdependenciesbetweenwords,soevenlong-distancedependenciesareeasytouseaspartialannotations.WeleavetheproblemoffindinganinformativecriterionforselectingannotationsforJapanesedependencyparsingasfuturework.</subsection>
  <section title="Domain adaptation for dependency parsing">Assumingthatthecostofannotationcorrespondsroughlytothenumberofannotationsperformed,outofallpossibleannotationstohaveannotatorsperformforatargetdomaincorpuswewanttoselecttheoneswhichprovidethegreatestbenefittoaccuracywhentraining.Thehighcostofannotationworkistheprimarymotivationforthisapproach.</section>
  <subsection title="Partial annotation for a parser">Inthecontextofdependencyparsing,partialannotationreferstoannotatingonlycertaindependenciesbetweenwordsinasentence.Dependencieswhichareassumedtohavelittletonovaluefortrainingareleftunannotated.Tableshowsanexampleofapartiallyannotatedsentencethatcanbeusedastrainingdatabyoursystem.Beforetextcanbeannotatedwithdependencies,itmustfirstbetokenizedandlabeledwithPOStags.WeassumethattheresultsofthistokenizationandPOStaggingareaccurateenoughthatweneedtomanuallyannotateonlythedependenciesbetweenthetokenizedwords.</subsection>
  <subsection title="Learning feature weights from partial annotations">Asexplainedinsec:pointwisemst,edgescores,(i,d_i,w),areestimatedforeachw_iindependently.Thismeansthattheestimationof(i,d_i,w)requiresonlyagolddependencyofw_i,andtheotherdependenciesinasentencearenotnecessary.Thisallowsustolearnweightsforfeaturesfrompartiallyannotatedcorpora.Whentrainingdataincludesagolddependencythatw_idependsonw_j,adiscriminativeclassifiercanbetrainedbyregardingd_i=jasapositivesampleandd_i=j'wherej'jasnegativesamples.InthecaseofJapaneseparsing,becausej&gt;iforalld_i=j,negativesamplesared_i=j'wherej'jandj'&gt;i.Forexample,fromthepartialannotationgiveninTable,wecancreateatraininginstanceforw_2,は(subj.),wherethepositivesampleisd_2=8andthenegativesamplesared_2=3,4,,7,9.</subsection>
  <subsection title="Domain adaptation with a partially annotated training corpus">Asacasestudy,weshowhowpartialannotationcanbeusedasalow-costmethodofconvertingtheannotationstandardofanexistinglinguisticresource.Aswementionedinsec:introduction,traditionalframeworksforJapanesedependencyparsingarephrase-based.Manyexistingdependencycorporausephrasesastheunitofannotation,andtheseresourcesareavaluablepotentialsourceofdataforminingworddependencies.However,phrasedependenciesalonedonotprovideenoughinformationforanautomaticconversiontoworddependencies.Oneoftheadvantagesofourparseristhatitcanbetrainedonapartiallyannotatedcorpus,soifwecanderiveevensomeworddependenciesfromphrasedependencieswecanquicklyandeasilymakeuseofexistingresources.Totakeadvantageoftheselinguisticresources,wecreatedanumberofrulestoderiveword-baseddependencyannotationsfromphrase-basedannotations.Insteadoftryingtoconvertallphrasedependencies,wefocusedonheuristicsthatprovideonlyreliableworddependencies.Theword-baseddependencysetproducedbytheserulesisapartialannotationoftheoriginalcorpus.ForthedomainadaptationexperimentsdescribedinSection~,weusedthisprocedureontheNAISTTextCorpus(NTC)tocreateasmallpartially-annotatedtargetdomaincorpus.TheNTCconsistsofnewspaperarticlesfromtheMainichiShimbun.figure:phraseshowsanexamplesentencefromthiscorpusannotatedwithphrasedependencies.Toaidtheconstructionofconversionrules,wechosethreebroadcategoriesofwords---contentwords,functionwords,andpunctuationsymbols---thatprovidecluestothestructureofaphrase.Beforeweexplainourrules,wewillgiveashortexplanationofthesethreecategories.Wedefinedcontentwordsasnouns,verbs,adjectives,interjections,prenominaladjectives,suffixes,andprefixes.Functionwordsareauxiliaryverbs,particles,inflections,andconjunctions.Inthiscontext,punctuationsymbolsareboththeEnglishandJapaneseversionsofperiodandcommacharacters.Thesethreecategoriesareusedtodeterminephraseswhichcanbeminedforrelativelyaccurateworddependencies.figure:convshowsanexampleofhowtherulesexplainedbelowareusedtoderiveword-baseddependenciesfromphrase-baseddependenciesforthesentencegiveninfigure:phrase.Thefirsttworulesareinter-phraserules,whichareconcernedwiththerelationshipbetweenwordslocatedindifferentphrases.:Givenadependentphraseanditsheadphraseintheoriginalannotation,settheheadofthelastwordinthedependentphrasetothelastcontentwordintheheadphrase.Note,weonlyapplythisruleiftheheadphraseconsistsofacontentwordfollowedbyzeroormorefunctionwords,followedbyanoptionalpunctuationsymbol.:Settheheadofaleftparenthesis(orleftbracket)tothefirstrightparenthesis(orrightbracket)thatfollowsitinthesentence.Thelastfourrulesareintra-phraserulesthatareconcernedwiththedependenciesbetweenwordsinthesamephrase.Thefollowingruleswerefoundtobeeffective.2:Ifaphraseconsistsofzeroormorecontentwords,functionwords,orpunctuationsymbolsfollowedbyasequenceoftwofunctionwordsandapunctuationsymbol,thensettheheadofthefirstfunctionwordinthesequencetothesecondfunctionwordandtheheadofthesecondfunctionwordtothepunctuationsymbol.:Ifaphraseconsistsofzeroormorecontentwordsfollowedbyasequenceofacontentwordandafunctionword,thensettheheadofthecontentwordtothefunctionword.:IfawordthatisinflectedinJapanese(verb,auxiliaryverb,oradjective-typeadjectivesandna-typeadjectives.Bothtypesareinflected.)isfollowedbyaninflection,thefirstworddependsontheinflection.:Ifafunctionwordisfollowedbyapunctuationsymbol,settheheadofthefunctionwordtothepunctuationsymbol.</subsection>
  <section title="Evaluation">Asanevaluationofourparser,wemeasuredparsingaccuraciesofseveralsystemsontestcorporaintwodomains:isageneraldomaininwhichacorpusfullyannotatedwithwordboundaryanddependencyinformationisavailable,andtheotherisatargetdomainassuminganadaptationsituationinwhichonlyapartiallyannotatedcorpusisavailableforquickandlow-costdomainadaptation.</section>
  <subsection title="Experimental settings">Intheexperimentsweusedexamplesentencesfromadictionaryasthegeneral(source)domaindata,andbusinessnewspaperarticles(Nikkei),similartotheWallStreetJournal,forthetargetdomaintestset.Comparedtothedictionaryexamples,thenewspaperarticlesuseamoreformalwritingstyle,specializedvocabulary,andlongersentences.Thus,thedomainsofthetwocorporaaredifferentenoughtojustifydomainadaptationtechniques.Forthedomainadaptationexperiments,weusedthepartially-annotatedcorpusmentionedinSection~asatargetdomaintrainingcorpus.Thiscorpusconsistsofnewspaperarticlesthataresimilartothetargetdomaintestset.Usagesandspecificationsofthevariouscorporaareshownintable:corpus.Allthesentencesaresegmentedintowordsmanuallyandallthewordsareannotatedwiththeirheadsmanually,exceptforNTC-train.TheJapanesedataprovidedbytheCoNLLorganizersaretheresultofanautomaticconversionfromphrase(bunsetsu)dependencies.Foramoreappropriateevaluationwehavepreparedaword-baseddependencydataset.Thedependencieshavenolabelsbecausealmostallnounsareconnectedtoaverbwithacasemarkerandmanyimportantlabelsareobvious.ThewordsarenotannotatedwithPOStags,soweusedaJapanesePOStagger,KyTea,trainedonabout40~ksentencesfromtheBalancedCorpusofContemporaryWrittenJapanese(BCCWJ).Forthegeneraldomainexperimentswecomparedthefollowingsystems.Malt:'sMaltParser.Wechosetheprojectivearc-eageralgorithmandHEADoption(-pphead)toprojectivizethetrainingdatabecausethesesettingsfordetailsontheoptimalhyperparametersettingsforJapanese.WealsochoseLIBLINEAR(withMaltParser'sdefaultchoiceofamulti-classSVM)asthelearnerinsteadofLIBSVM,asrecommendedintheMaltParseroptimizationguideavailableathttp://www.maltparser.org/guides/opt/quick-opt.pdf(linksaccessedApril2012).achievedthebestperformanceforJapaneseontheCoNLL-Xsharedtask.1st-orderMST:'sMSTParser,usingtheoptionsforfirst-orderparsing,non-projectivedecoding,andk-bestparsesizewithk=1.Wechosethenon-projectivedecodingoptionforafairercomparisonwiththeproposedmethod,whichdoesnotenforceprojectivityconstraints.2nd-orderMST:Thesameas1st-orderMST,butwiththeoptionforsecond-orderparsing.EDA:Oursystem(``EasilyadaptableDependencyAnalyzer''),whichusespointwiseestimationandfirst-orderfeaturestoestimatedependencies.Weusedstochasticgradientdescentfortraining.</subsection>
  <subsection title="With a fully annotated training corpus">Forthefirstexperiment,wemeasuredtheaccuracyofeachsystemonanin-domaintestsetwhentrainingonafullyannotatedcorpus.Ourgoalistoseehowourmethodperformsincomparisontostate-of-theartmultilingualparserswhenparsingJapanese.Theresultsareshownintable:result-EHJ.Allsystemsachievehighaccuracyonthistask,andnodifferencesbetweensystemswerestatisticallysignificant(p&gt;0.05,accordingtoPearson's^2test).MaltandEDAhavesimilaraccuracy,whilebothvariationsofMSThaveonlyslightlyloweraccuracy.Ourmodelfactorsthescoreforadependencytreeintothesumofindividualedgescoresinthesamewayasthe1st-orderMSTmodel,soweexpectedtheirperformancetobeclose.ThericherfeaturesetofourmethodismostlikelythereasonforthesmalldifferenceinperformancebetweenEDAand1st-orderMST.ThisresultshowsthatourpointwiseapproachachievescomparableaccuracyonJapanesetothatofstate-of-theartparserswhileallowingformuchmoreflexibleuseoflanguageresources.Thisflexibilityisveryimportantinpracticalsituations.Incontrast,Maltand2nd-orderMSTbothuseahistory-basedfeatureset,whichincorporatesmorecontextthantheedge-factoredapproachesofEDAand1st-orderMST.InthecaseofMalt,thepartiallybuiltdependencystructureofasentenceisusedasfeatures,whicharesimilartothe``dynamicfeatures''usedbyanddiscussedinSection.AsdiscussedinSection,2nd-orderMSTfactorsthesentenceintoasetofedgepairsinsteadofindividualedges.Wealsomeasuredthetrainingtimeandtheparsingspeedofeachsystem.table:learningshowstheresults.Fromthistable,firstweseethatboth1st-orderand2nd-orderversionsofMSTaremuchslowerthanMalt,asiswellknown.2nd-orderMSTtakesmorethanthreetimesaslongtotrainas1st-orderMST,buttheirparsingspeedisalmostidentical.ThetrainingtimeofourmethodisinbetweenMaltandbothversionsofMST---whileitismuchslowerthantheshift-reducebasedMalt,thisresultshowsthatourmethodisfastenoughtobeusedforactivelearning.Trainingspeediscrucialforactivelearningbecausetheannotatormustwaitwhilethemodelisretrainedaftereachroundofannotation.demonstratedtheeffectivenessofthepointwiseapproachinarealisticactivelearningscenario.Theoreticallythetrainingtimeofourmethodisproportionaltothenumberofannotateddependencies.TheassumptionsoutlinedinSectionaremostlikelythemainreasonforthedifferenceintrainingtimesbetweenEDAandthetwoversionsofMST.Forotherlanguageswherepossibleheadscanbelocatedbothtotheleftandrightofaword,weexpecttrainingandparsingtimestoincrease.Ourpointwiseapproachcanbeextendedtohandletheselanguagesbychangingtheconstraintonheadsfromj&gt;itojiforalld_i=j.ThisisanimportantdirectionforfutureworknowthatwehaveconfirmedthatthisapproachiseffectiveforJapanese.Weperformedasecondexperimentinthegeneraldomaintomeasuretheimpactofthetrainingcorpussizeonparsingaccuracy.Tomakesmallertrainingcorpora,wesetafixednumberofdependencyannotationsandthensequentiallyselectedsentencesfromEHJ-trainuntilthedesirednumberofdependencyannotationswerecollected.Theresultsareshowninfigure:ACC.ForsmallertrainingcorporaMaltoutperformsallothersystems,butthedifferenceislesspronouncedwhenatleasthalfofthetrainingcorpusisused.Theproposedmethod'sperformancelagsbehindtheothersystemswhenlittletrainingdataisavailable,butiscomparablewhenatleasthalfofthetrainingdataisused.While2nd-orderMSToutperforms1st-orderMST,thedifferenceisnotpronounced.Thisisprobablybecausedependencyarcsinthisdatasetalwayspointtotheright---inastandarddependencyparsingtaskwherearcsmaygoineitherdirection,weexpect2nd-orderMSTtoconsistentlyoutperform1st-orderMST.</subsection>
  <section title="Comparison with a phrase-based dependency parser">Becausethephrase-basedapproachisthemostcommonlyusedinworkonJapanesedependencyparsing,wealsocomparedtheperformanceofourword-basedmethodtoatraditionalphrase-basedmethod,thecascadedchunkingapproachof.Adirectcomparisonisdifficultbecauseitwouldrequiredataannotatedwithbothwordandphrasedependencies.However,ifacorpusisannotatedwithworddependenciesandPOStags,andwehaveanassignmentofwordstophrases,itispossibletouseheuristicstoestimatethecorrespondingphrasedependencyannotations.Thisisbecausetheinformationprovidedbyworddependenciesismorefine-grainedthantheinformationprovidedbyphrasedependencies,andtheformercanbeseenasasupersetofthelatter.Phrasedependenciescanbeviewedasmodelingonlytherelationshipsamongeachphrase'skeywords,ignoringanydependencyinformationbetweenwordsinthesamephrase.figure:word_cabochashowsasentenceannotatedwithword-baseddependenciesandPOStagswhichwillbeusedtocreatephrasedependencyannotations.</section>
  <subsection title="Converting word dependencies to phrase dependencies">Theconversionisatwo-stepprocess:wefirstuseonlyPOStagstogroupwordsintophrases,andthenweusedependenciesbetweenwordsindifferentphrasestoassignphrasedependencies.ThesecondstepisstraightforwardbecauseofourassumptionthatJapaneseishead-final.Justasinthecaseofworddependencies,whenestimatingphrasedependenciesweonlyconsiderheadswhichoccurtotherightoftheirdependentsanddonotallownon-projectivedependencies.Thereforeasinglescanthroughallphrasesinsentenceorderissufficienttoassigntheirheads.Foragivenphrase,wesimplyscanthrougheachofitswordsinorder,checkingtoseeiftheword'sheadbelongstoadifferentphrasethanthecurrentone.Ifitdoes,wesetthecurrentphrase'sheadtotheonecontainingtheheadwordandthenbeginprocessingthenextphraseinthesentence.Westopprocessingwhenwereachthelastphrase,becausebyconventionitsheadistheroot.Thefirststepismoredifficultandrequiredustodevelopsomeheuristicrules.Toformulatetheserules,wemadeuseoftheideaoffunctionwordsandcontentwordsdescribedinsec:conversion_rules.Weusedthesamesetoffunctionwords,butweaddedpronounsandadverbstothesetofcontentwords.Thisisbecausewetookabottom-upapproachtobuildingphraseswhereweincrementallyaddwordstoaphrase,decidingwhetherornottoinsertaphraseboundaryaftereachword.Thebasicprocedureisasfollows.Webeginwithanemptyphraseandthenexamineeachwordinorder,consideringwhetherornottoaddittothecurrentphrase.WefirstchecktoseewhetherthePOStagforthewordbelongstothesetoffunctionwords.Iftheword'stagisnotinthesetoffunctionwords,weaddthewordtothecurrentphrase.Ifthewordisaprenominaladjective,adverb,conjunction,oradverbialnoun,weinsertaphraseboundaryaftertheword.Werefertothissetofwordsasboundarywordsbecausetheyarelikelytoindicateaphraseboundary.Ontheotherhand,ifthewordisinthesetoffunctionwordsweaddittothecurrentphraseandexaminethenextword,addingittothecurrentphraseonlyifitisalsoafunctionword.However,ifthenextwordisaboundarywordortherearenomorefunctionwordsremaininginthesentenceweinsertaphraseboundarybetweenthefirstwordandthenextword.Theprocedureoutlinedabovewillgiveusacoarse-grainedphrasesegmentationforthesentence,buttherearesomeedgecaseswhichrequirefurthersegmentationofthesephrases.Weidentifiedfourofthesecasesandcheckedeachphrasecreatedbytheoriginalphrasesegmentationagainstthem.Notethatinboththeinitialcoarse-grainedphrasesegmentationandthesubsequentfine-grainedsegmentation,beforeinsertingaphraseboundarywecheckthesurroundingwordstoavoidsplittingconstructionscommonlytreatedasasinglephraseinJapanese.Whenaphrasecontainsarightquoteorparenthesisfollowedbythecorrespondingleftquoteorparenthesis,insertaphraseboundarybetweenthem.Insertaphraseboundarybetweenafunctionwordthatisimmediatelyfollowedbyacontentword.Whenthereisanounorpronounimmediatelyfollowedbyaverb,insertaphraseboundarybetweenthem.Wedonotapplythisrulewhentheverbisする(todo)orarelatedverb,becausethisindicatesthatthenounisactingasaverbalnoun.Whendictionaryformoftheverbする(todo)isfollowedbyanoun,insertaphraseboundarybetweenthetwowords.figure:phrase_cabochashowstheresultofapplyingthestepsoutlinedabovetoconverttheworddependenciesshowninfigure:word_cabochatophrasedependencies.Afterthecoarse-grainedphrasesegmentationthewordsinphrases02and03areinitiallygroupedintoasinglephrase.Thisphrasecorrespondstocase(3)abovebecauseitcontainsthenoun全部(all)followedbyaverb,soaphraseboundaryisinsertedbetweenthesetwowords.</subsection>
  <subsection title="Evaluation on a phrase-based test set">Afterbothcorporawereannotatedwithphrasedependencies,weperformedanexperimenttomeasuretheunlabeledphrasedependencyaccuracyofthecascadedchunkingmethod(CC)andtheproposedmethod(EDA).First,weconvertedthegoldword-baseddependenciesforEHJ-testtophrase-baseddependencies.ForEDA,wetrainedaparserasdescribedinsec:withFACandthenconvertedtheparser'soutputontheword-basedversionofEHJ-testtophrase-baseddependencies.Thesameprocedurewasusedtoconvertboththegolddependenciesandtheparseroutputforthetestset,ensuringthatthephrasesegmentationwasconsistentbetweenthem.Itshouldbenotedthatthetrainingandtestdatasetsconsistofexamplesentencesfromadictionary,whichingeneralaremuchshorterthansentencesfromcommondomainssuchasnewspaperarticles.Thus,theresultsonthesetypesofdatasetsarelikelytodifferfromtheoneswereportbelow.ForCC,wefirstconvertedEHJ-traintophrase-baseddependenciesandthenused'simplementation,CaboCha(accessedNovember2011).,totrainadependencyparsingmodel.Wechose3asthedegreeofthepolynomialkernelsincethissettinghasbeendemonstratedtobeeffectiveforJapanesedependencyparsing.Wethenusedthatmodeltoparsethephrase-basedversionofEHJ-testcreatedfromthegoldworddependencies.ToensurethatthephrasesegmentationandPOStagswereconsistentforthetestset,wedidnotusethephrasesegmentationorPOStaggingfeaturesoftheirimplementation.'scascadedchunkingapproachusesthebasefeaturesetof'sprobabalisticmodel(discussedinSection)andaddsadditionaldynamicfeaturesbasedonthechunksthatmodifythedependentchunkandthechunkwhichtheheadchunkmodifies.Theyreportthatthecascadedchunkingmodelrequiresfewertrainingexamplesandisthusmuchfastertotrainthantheprobabalisticmodel,whichusesallcandidatedependenciesastrainingdata.Thisisbecausethecascadedchunkingmodelusesheuristicstopruneexceptionaldependencies(wherepossiblecorrectheadsarenotselectedbecausebetteroneexistsinthesamesentenceorbecauseofprojectivityconstraints)fromthetrainingdata.Thetrainingtimefortheproposedmethodistheoreticallylongerthanthatofthecascadedchunkingmethod,becauseitusesasmallerunit(wordsinsteadofchunks)andusesallcandidatedependenciesastrainingdatainthesamewayastheprobabalisticmodel.However,fortaskslikemachinetranslationwhichrequiresmallerunitsthanchunks,thefine-graineddependencyinformationofourapproachisworththeadditionaltrainingtime.Theresultsareshownintable:cabocha_comparison.ThoughEDA'sparsingspeedisreasonablyfast,CC'sismuchfaster.ItcanalsobeseenthatEDAoutperformsCCbyasmallmarginintermsofparsingaccuracy.Eventhoughseveralofthedependenciesbetweenwordsmaybeobvious,theword-baseddependencyannotationprovidesuswithricherinformationaboutthestructureofthesentencethanphrasedependencies.OnepossiblecauseforthedifferenceinaccuracybetweenCCandEDAisthePOStaggingstandard.CCisdesignedtouseadetailedsetoffine-grainedPOStags,wherebroadcategoriessuchasnounsandverbsarefurtherseparatedintoseveralsubcategories.Weuse'sPOStaggingstandard,whichdefinesbothcoarse-grainedandfine-grainedtags.However,weonlymakeuseofthecoarse-grainedtagsbecausethesearemorelikelytobeavailableinarealisticdomainadaptationsituation.Taggingwordswithfine-grainedtagsrequiresannotatorstohaveexperiencewiththetaggingstandardinadditiontodomainknowledge,whichlimitsthenumberofpotentialannotators.Stickingtocoarse-grainedtagsreducestheburdenonannotators.ThusCC'saccuracysuffersonthis``poor''featuresetbecausefine-grainedPOStagsarenotavailable.Anotherpossiblecauseforthedifferingaccuracymaybethedifferenceingranularitybetweenword-basedandphrase-basedsegmentation.Forthesametrainingdata,therewillbemoreexamplesofworddependenciesthanphrasedependencies.</subsection>
  <section title="Related work">Therehasbeenasignificantamountofworkonhowtoutilizein-domaindatatoimprovetheaccuracyofparsing.Themajorityofthisworkhasfocusedonusingunlabeleddataincombinationwithself-trainingorothersemi-supervisedlearningmethods.alsopresentworkonsuperviseddomainadaptation,althoughthisfocusesontheutilizationofanalready-existingin-domaincorpus.Therehasalsobeensomeworkonefficientannotationofdataforparsing.Mostpreviousworkfocusesonpickingefficientsentencestoannotateforparsing,butalsopresentamethodforusingpartiallyannotateddatawithdeterministicdependencyparsers,whichcanbetriviallyestimatedfrompartiallyannotateddata.Otherrecentworkhasshownhowboth'sMaltParserand'sMSTParsercanbeadaptedtousepartiallyannotatedtrainingdata.Traditionalparserssuchas'susestructuredpredictionmethods.showedthatlocalclassificationmethodscanbeusedtotrainstructuredpredictors.Theirapproachalsouses``dynamic''features,wherethepredictionsforsomesurroundingedgesareusedasfeatureswhenestimatingapossibleedgebetweenadependentandheadword.Ourparseralsomakesuseoflocalclassificationmethodsfortraining,butincontrasttowetakeapointwiseapproachbasedontheassumptionthatedgescorescanbeestimatedindependently.Thisworkfollowsinthethreadofand,whodemonstratedthattheseassumptionscanbemadewithoutasignificantdegradationinaccuracyforwordsegmentationandPOStagging.Herewedemonstratedthatthesameapproachcanbeusedfordependencyparsing.</section>
  <section title="Conclusion">Weintroducedaparserthatevaluatesthescoreforeachedgeinadependencytreeindependently,whichallowsfortheuseofpartiallyannotatedcorporaintraining.Wedemonstratedthattargetdomaindataannotatedinthiswaycanbecombinedwithavailablesourcedomaindatatoincreaseparsingaccuracyinthetargetdomain.Wealsoshowedhowpartialannotationcanbeleveragedtomakeuseofcorporaindifferentformatswhenafullconversionisnotfeasible.InourevaluationonaJapanesedependencyparsingtaskwefoundthatourparserdeliversaccuracycomparabletothatofstate-of-the-artdependencyparsersthatusemuchmorecomplexmodels,andhasparsingandtrainingspeedsthatarefastenoughtoallowforrapiddomainadaptation.Onaphrase-basedJapanesedependencyparsingtask,ourword-basedparserslightlyoutperformedatraditionalphrase-basedparser.Whileourparsercouldnotmatchthefastparsingspeedofthetraditionalone,thetrainingspeedsandaccuracyofbothwerecomparable.Theincreasedflexibilityofsimplerparsingmodelsoftencomesatthepriceofdecreasedaccuracy,buttheseresultsshowthatasimplemodelcanbeusedtoenableflexibledomainadaptationwithoutsacrificingaccuracy.workwassupportedbyGrant-in-AidforScientificResearchofthegovernmentofJapan(23500177).document</section>
</root>
