<?xml version="1.0" ?>
<root>
  <title>言語資源を活用した実用的な対訳表現抽出</title>
  <author>北村美穂子松本裕治</author>
  <jabstract>高精度の機械翻訳システムや言語横断検索システムを構築するためには，大規模な対訳辞書が必要である．文対応済みの対訳文書に出現する原言語と目的言語の単語列の共起頻度に基づいて対訳表現を自動抽出する試みは，対訳辞書を自動的に作成する方法として精度が高く有効な手法の一つである．本稿はこの手法をベースにし，文節区切り情報や対訳辞書などの言語知識を利用したり，抽出結果を人間が確認する工程を設けたりすることにより，高精度で，かつ，カバレッジの高い対訳表現抽出方法を提案する．また，抽出にかかる時間を削減するために，対訳文書を分割し，抽出対象とする文書量を徐々に増やしながら確からしい対訳表現から段階的に抽出していくという手法についても検討する．8,000文の対訳文書による実験では，従来手法は精度40%，カバレッジ79%であったのに対し，言語知識を利用した提案手法では，精度89%，カバレッジ85%と向上した．さらに人手による確認工程を設けることにより，精度が96%，カバレッジが85%と向上した．また，16,000文の対訳文書による実験では，対訳文書を分割しない方法では抽出時間が約16時間であったのに対し，文書を4分割する方法では，約9時間に短縮されたことを確認した．</jabstract>
  <jkeywords>対訳表現抽出,対訳文書,機械翻訳</jkeywords>
  <section title="はじめに">高精度の機械翻訳システムや言語横断検索システムを構築するためには，大規模な対訳辞書が必要である．特に，専門性の高い文書や時事性の高い文書を扱う場合には，専門用語や新語・造語に関する対訳辞書の有無が翻訳や検索の精度を大きく左右する．人手による対訳辞書の作成はコスト及び時間がかかる作業であり，できるだけ自動化されることが望ましい．このような課題に対処するため，対訳文書から対訳表現を自動的に抽出する手法が数多く提案されている．この中でも，文対応済みの対訳文書から共起頻度に基づいて統計的に対訳表現を自動抽出する手法は，精度が高く，対訳辞書を自動的に作成する方法として有効である．本稿では，その中の一つであるの手法をベースにし，従来手法の利点である高い抽出精度を保ちつつ，抽出できる対訳表現のカバレッジを向上させるために行った種々の工夫について論じ，その有効性を実験で示す．の5点である．これらを用いることで実用的な対訳表現抽出を行うことができる．(A)は，文節区切り情報や品詞情報を利用することにより，構文的に有り得ない表現が抽出候補にならないようにする．文節区切り情報の有効性は，既存の研究において確かめられているが，彼らは抽出の対象を自立語に限定している．提案手法では，各単語における文節内の位置情報と品詞情報を用いて抽出の対象を制限することで，自立語以外の語も抽出の対象とする．(B)，(C)では，共起頻度に基づいた統計的な値のみでは対訳かどうかが判断できない場合，対訳辞書や人手を利用して対訳か否かを判断する手法である．過去の研究では，対訳辞書は対訳文書から対訳関係にある単語ペアを見つけるための手がかりとして利用されることが多いが，本提案では，手がかりとするのではなく，統計的に抽出された対訳表現から適切な対訳表現だけを選り出すための材料として利用する．(D)では，原言語と目的言語の単語列間の対応関係の強さを示す尺度である対応度の評価式を改良する．対応度の計算には，一般に重み付きDice係数やLog-Likelihoodなどの評価式が用いられるが，我々は従来手法の実験結果を分析した結果，Dice係数やLog-Likelihoodの評価式に対して，多対多の対応数を考慮した負の重み付けを行うことが効果的であると判断し，評価式を改良した．(E)は抽出時間に関する課題を解決する．従来手法では10,000文以上からなる対訳文書を抽出対象とする場合，原言語と目的言語単語列の組み合わせが多数生成されるという課題があった．提案手法ではその組み合わせ数を削減するために，対訳文書を一定の単位に分割し，抽出対象とする文書の単位を徐々に増やしていきながら抽出するという方法を採用する．対象とする対訳文を1,000文，2,000文，…，10,000文と徐々に増やす度に，抽出された対訳表現に関わる単語列を除去していく．その結果，対象の文が10,000文に達した時の単語列の組み合わせ数は，直接10,000文を対象にした場合の組み合わせ数より少なくなり，抽出時間を短縮させることができる．以下，章では，従来手法における，原言語単語列と目的言語単語列間の対応度の計算方法と抽出アルゴリズムを説明する．章では，本稿が提案する種々の工夫を採用した改良手法について述べる．章では章に述べた各手法の評価実験を報告し，その結果を考察する．章では関連研究と比較し，章でまとめる．</section>
  <section title="従来手法"/>
  <subsection title="連続単語列間の対応度計算方法">原言語単語列w_oと目的言語単語列w_tの対応関係の強さを示す尺度として，対応度sim(w_o,w_t)を定義する．対応度は，原言語の単語列の出現回数，目的言語の単語列の出現回数，両者が同時に対訳文に同時に出現する回数で求められ，いくつかの計算方法が提案されている．従来手法では重み付きDice係数が用いられている．Dice係数はXとYの事象において，Xが発生する回数とYが発生する回数の和に対してXとYの事象が同時に出現する回数の割合で表す．さらに，同時出現回数の重みを与えたものを重み付きDice係数と呼び，これはXとYの相関関係だけでなく，出現回数も考慮に入れることができる．日本語単語列をw_J，英語単語列をw_E，w_Jの日本語文書中の出現回数をf_j，w_Eの英語文書中の出現回数をf_eとし，w_Jとw_Eが対訳文に同時に出現する回数をf_jeとすると，重み付きDice係数を用いた対応度は，以下の式で定義される．[sim(w_J,w_E)=(_2f_je)2f_jef_j+f_e]</subsection>
  <subsection title="抽出アルゴリズム">従来手法の基本的な考え方は，原言語文書と目的言語文書から抽出される連続単語列集合の全ての組合せに対して，節に述べた対応度を計算し，対応度の高い連続単語列ペアから順に対訳表現として抽出するという手法である．図の流れ図に従って，各処理を説明する．この手法の特徴は，連続単語列ペアの出現回数に対する閾値を設け，その閾値を満足する連続単語列ペアを対象にして対訳表現を抽出し，閾値を満足する連続単語列ペアがなくなれば，その閾値を徐々に下げていくという点にある．対応度と連続単語列ペアの出現回数は相関関係をもつように設定されているので，出現回数に対する閾値を設けて，その値を徐々に小さくしていくことで，対応度の高い連続単語列ペア，つまり確からしい連続単語列ペアから順に対訳表現を抽出することができる．また，閾値を下げていき，精度が保証されなくなる段階で，処理を終了することもできる．</subsection>
  <section title="提案手法">我々は節の従来手法に対して，(A)文節区切り情報や品詞情報の利用，(B)対訳辞書の利用，(C)複数候補の対訳表現が得られた場合の人手による選択，(D)多対多の対応数を考慮に入れた対応度評価式，(E)対訳文書の分割による漸進的な抽出，の5つの改良を行った．これらを改良した提案手法の処理の流れ図を図に示す．ステップの番号のカッコ内の数字は，の抽出アルゴリズムの各ステップの番号に対応している．図の四角の枠で囲まれたステップは，本提案で改良されたステップである．ステップ(1)-2では，形態素解析と同時に文節区切り処理を行い，ステップ(2)-1では，その結果を用いて文節を超えないように連続単語列を抽出する．ステップ(5)-1の対応度の計算では，改良された対応度の評価式を用いる．ステップ(1)-1，(5)-2，(5)-3-aは，辞書を参照する場合に適用されるステップである．辞書参照だけでなく，人手による確認も行う場合はステップ(5)-3-aではなく，ステップ(5)-3-bを用いる．対訳文書の分割による漸進的な抽出は，ステップ(1)-3で対象となる対訳文書をあらかじめ決められた単位に分割し，ステップ(7)-2で1単位ずつ追加することによって対象とする文書範囲を拡大していくという手法をとる．以下に，提案アルゴリズムの各処理を説明する．</section>
  <subsection title="文節区切り情報や品詞情報の利用">節の従来手法による抽出結果を分析すると「中心に多くの:willbecomethetarget」「その保有する膨大な:vastinventory」のように，構成する一部の単語の対応は正しいが，全体では間違っているという対訳表現が数多くみられた．このような例の多くは，英語や日本語の表現として意味をなさない不適切な単位であることが多かった．この課題を解決するため，不適切な対訳表現候補を生成しない工夫を施す．具体的には，以下の処理を行う．以下に具体例を示す．図は日本語文における文節区切り結果とそれに基づく連続単語列抽出の例である．形態素解析ツールによって区切られた形態素の区切りを``/''，係り受け解析ツールによって区切られた文節の区切りを``//''で表す．文節区切り情報を利用しない場合「する安全」「決議の諸」などの対訳表現として不適切な日本語表現が候補となるが，提案手法では文節境界の範囲を超える連続単語列を生成しないため，これらの表現は候補とならない．また，(b)の条件を課することによって，文節内の不適切な表現も生成されない．例えば「の」や「れる」のような一単語のみからなる助詞や助動詞は生成されないが「決議_の」や「満たす_れる」のように，助詞が名詞の後ろに位置する場合や，助動詞が動詞の後ろに位置する連続単語列は生成される．上記の処理をすることにより，提案手法では意味的にまとまりをなしていない文字列を除外することができる．</subsection>
  <subsection title="対訳辞書の利用">対訳辞書は，抽出精度向上のための有効な知識である．しかし，対訳辞書を手がかりとして抽出すると，対訳辞書に登録されていない専門用語などの表現が抽出されなくなる可能性がある．そこで，我々は，従来手法のアルゴリズムで抽出された対訳表現に対して，対訳辞書を参照することによって適切な対訳表現を選り出し，それらを優先的に抽出するという改良を行った．閾値Thにおいて対訳表現が抽出される限り，ステップ(4)から(6)の処理が繰り返されるが，何回目の処理かによって処理内容を変える．閾値Thでの処理が1回目の場合，対訳辞書を参照し，その候補が対訳関係にあると認められれば抽出する．対訳関係か否かの判断は，連続単語列を構成する英語と日本語の自立語単語の組合せにおいて1つでも対訳辞書登録語があれば，対訳関係にあると認定する．この理由は，対訳辞書登録語との完全一致する連続単語列ペアのみを対訳関係にあると認定すると，対訳関係にある連続単語列ペアはわずかとなり，辞書参照の効果が得られないためである．ステップ(6)では，抽出された対訳表現に関する連続単語列候補は削除される．閾値での処理が2回目以降では，この削除された状態で，対訳辞書を参照せずに対訳表現を抽出する．このように，対訳辞書の利用は適切な対訳表現を選り出す働きだけでなく，不適切な連続単語列候補を除外する役割も果たすことができる．</subsection>
  <subsection title="複数候補の対訳表現が得られた場合の人手による確認・選択">次に人手による確認・選択を考える．人手による確認・選択は，作業効率に見合った効果が得られるかどうかが重要である．その作業が時間や手間がかかるものであれば，最終結果を人手で取捨選択する作業と変わらない．我々は，ここでも従来手法の抽出アルゴリズムの性質を利用して，繰り返し処理の途中に人手による確認・選択作業を施す．具体的には，出現回数の閾値での1回目の処理では，辞書参照による対訳表現抽出を行い，2回目以降の処理において，ステップ(5)-1で候補とされた対訳表現候補全てに対して，人間が正しいかどうかを確認する．正しいと判断された候補は「対訳表現データベース」に登録され，一方，残りの対訳表現は「対訳表現除外データベース」に登録される．「対訳表現データベース」に登録された連続単語列ペアは，ステップ(6)の連続単語列候補の削減に利用される．一方「対訳表現除外データベース」に登録された連続単語列ペアは，ステップ(5)-2で対訳表現を抽出する時に参照され，対訳表現候補から必ず除外される．</subsection>
  <subsection title="多対多の対応数を考慮した対応度評価式">従来手法では対応度を評価する式として重み付きDice係数が用いられたが，提案手法では，重み付きDice係数と同様に，原言語と目的言語の単語列の同時出現回数と相関があるLog-Likelihoodを用いる．さらに，重み付きDice係数やLog-likelihoodに対して，多対多の対応数を考慮した改良を行う．Log-Likelihoodは，ある表現Xの出現が別の表現Yの出現にどの程度強く依存するかを調べるための確率論に基づいた尺度である．実際の出現事例においてXの出現がYに依存しないという仮説とYの出現/非出現に依存するという仮説の尤度比で表す．節のw_J,w_E,f_j,f_e.f_jeの前提に加えて，対訳文書が有する文数をf_allとすると，以下の式で定義される．sim(w_J,w_E)&amp;=&amp;f_jef_je+(f_e-f_je)(f_e-f_je)+(f_j-f_je)(f_j-f_je)&amp;&amp;+(f_all+f_je-f_e-f_j)(f_all+f_je-f_e-f_j)&amp;&amp;-f_ef_e-f_jf_j-(f_all-f_j)(f_all-f_j)-(f_all-f_e)(f_all-f_e)&amp;&amp;+(f_all)(f_all)eqnarray*small次に，多対多の対応数を考慮した対応度評価式について説明する．節の従来手法を用いて抽出した対訳表現を分析した結果，図(a)のように，日本語連続単語列と英語連続単語列が，対応度が同じで，かつ，多対多の関係で対応付けられている場合に誤りが多かった．一方，(b)の例のように，同じ対応度では1対1の対応関係しか持たない場合，その大半は正しかった．図(a)の現象は，一部の単語が異なり，残りの単語は全て共通である対訳文が複数存在した場合に起こる．その共通部分において組み合わされる日本語・英語連続単語列ペアは対応度が等しくなり，多対多の対応関係を有する対訳表現となる．上記に述べた多対多の関係を有する対訳表現の抽出を避けるために，Dice係数及びLog-likelihoodの対応度sim(w_J,w_E)に対して，原言語と目的言語の連続単語列が多対多の関係で対応付けられる場合にはその対応度の値が小さくなるような重み付けを与える．以降，この対応度をdsim(w_J,w_E)と表記し，多対多の対応数を考慮した対応度と呼ぶ．dsim(w_J,w_E)を以下のように定義する．[dsim(w_J,w_E)=sim(w_J,w_E)_2(fw_JE+fw_EJ)]sim(w_J,w_E)は従来のDice係数やLog-likelihoodによる対応度の値である．fw_JEは現段階のステップ(4)で生成された全ての連続単語列において，日本語単語列w_Jを有する連続単語列ペアの数であり，fw_EJは，英語単語列w_Eを有する連続単語列ペアの数である．上記の式は，日本語単語列w_Jと英語単語列w_Eからなる連続単語列ペアにおいて，w_Jが対応する英語単語列の数と，w_Eが対応する日本語単語列の数の和が大きいほどその値が小さくなるように設定されている．また，w_Eとw_Jが1対1で対応する場合の値はDice係数やLog-likelihoodから計算されるsim(w_J,w_E)の値と等しくなるように設定されている．</subsection>
  <subsection title="対訳文書の分割による漸進的な抽出">ステップ(5)の処理における英語と日本語の連続単語列の組み合わせ数は，対訳文書の文数が多くなるにしたがい増大する．この連続単語列候補の生成を抑えるために，文書分割による漸進的な抽出手法を提案する．まず，ステップ(1)-3で対象となる対訳文書をあらかじめ定められた文数の単位で分割し，まず1単位で，100%の精度が保証される出現回数まで抽出を繰り返す(ステップ(7)-1)．その単位での処理が終了すれば，さらに1単位を追加して抽出を繰り返す(ステップ(7)-2)．追加しながら処理を繰り返し行い，抽出対象が対訳文書全体に及べば処理を終了する．対象とする文を徐々に拡大することで，対象とする文数が少ない初期の段階で抽出された対訳表現に関する英語・日本語連続単語列を候補から除外することができる．これにより，抽出対象が拡大された時の連続単語列候補の生成を削減することができる．</subsection>
  <section title="実験および考察">3章に提案した各手法の有効性を評価するために，様々な設定の下での比較実験を行った．基本となる実験条件と評価指標を最初に説明し，実験結果及び考察を述べる．実験には読売新聞とTheDailyYomiuriの記事データからなる「日英新聞記事対応付け結果」の先頭から8,000文を利用した．それ以外の文書を対象とする場合は各実験結果に明記する．日本語形態素解析及び文節区切りは「茶筌」及び「南瓜」を用いた．英語形態素解析及び対訳辞書参照に利用した対訳辞書は，機械翻訳システムの形態素解析モジュール及び英日・日英辞書を利用した．この対訳辞書は，507,110ペアの対訳表現を持つ．英語は「Charniakパーザー」の係り受け解析結果と節に述べた方法に基づいて文節単位に区切った．評価は，精度とカバレッジを求めることにより行った．精度は，対訳表現抽出結果をの三段階で評価し，抽出総数に対する正解及び半正解の割合を百分率で求めた．以降に示す表では，半正解の割合を()内に示す．一方，カバレッジは，英語，日本語それぞれの文書において，[coverage(%)=(1-未抽出自立語総単語数文書中自立語総単語数)100]を計算し，その平均を求めた．上記式内の「未抽出自立語総単語数」とは，各文書から正解，半正解の対訳表現を除去した結果，残った自立語の総単語数である．また，自立語の総単語数だけでなく，自立語異なり単語数に対しても同様にカバレッジを求めた．以下の表から表では後者を（）内に示す．</section>
  <subsection title="従来手法との比較">最初に「文節区切り情報の利用」「対訳辞書の利用」「人手による確認」の改良効果を確かめるための実験を行った．表に結果を示す．表の|&lt;1&gt;|は従来手法，|&lt;2&gt;|は文節区切り情報を利用した場合(2.2節のアルゴリズムに3節の提案アルゴリズムのステップ(1)-2，(2)-1のみを適用した場合），|&lt;3&gt;|は文節区切り情報と対訳辞書を利用した場合（|&lt;2&gt;|に対して3節の提案アルゴリズムのステップ(1)-1，(5)-2を適用した場合），|&lt;4&gt;|は，さらに人手による確認を行った場合(|&lt;3&gt;|に対してステップ(5)-3-aの代わりにステップ(5)-3-bを適用した場合)の結果である．いずれも対応度の計算には重み付きDice係数を用いた．|&lt;4'&gt;|は重み付きDice係数の代わりに多対多の対応数を考慮したLog-likelihoodを用いた結果である．表中のf_min=2は，出現回数を2回までとして抽出した場合，f_min=1は出現回数を1回までとして抽出した場合の結果である．但し，f_min=1では，f_je=f_j=f_e=2における対応度より小さく，f_je=f_j=f_e=1における対応度以上の値を持つ対訳表現が抽出されるが，f_je=f_j=f_e=1の対訳表現は抽出精度が極めて低いため=f_j=f_e=1の時の対訳表現の精度は43%であった．，今回の実験では抽出対象から除外した．以降の実験も同様である．本結果から，提案手法|&lt;4&gt;|のf_min=2では97%の精度が得られることがわかる．さらにf_min=1では，従来手法|&lt;1&gt;|は40%であったが，提案手法|&lt;4&gt;|は93%であった．|&lt;1&gt;|から|&lt;4&gt;|へと文節区切り情報，辞書参照，人手確認という工夫を追加していくことにより，抽出総数は減少していくが，カバレッジの低下を伴わない．これは，提案手法が間違った対訳表現のみを除去するフィルタリングの働きとして効果的に機能していることを示している．</subsection>
  <subsection title="対応度の評価式の違いによる比較">次に対応度の評価式の違いによる結果を比較する．本結果を表に記す．ここでは，表|&lt;4&gt;|と同じ設定で，適用する評価式を変える．|&lt;1&gt;|は重み付きDice係数の評価式，|&lt;2&gt;|は多対多の対応数を考慮した重み付きDice係数の評価式，|&lt;3&gt;|はLog-likelihoodの評価式，|&lt;4&gt;|は多対多の対応を考慮したLog-likelihoodの評価式を用いた場合の結果である．重み付きDice係数|&lt;1&gt;|とLog-likelihood|&lt;3&gt;|の結果を比較した場合，f_min=2では精度の違いはあまりみられないが，f_min=1ではLog-likelihoodの方が良い結果が得られた．重み付きDice係数はf_min=2ではLog-likelihoodと同等の信頼性が得られるが，f_min=1のように対応度が低い場合ではその信頼性は低いと言える．これは，両者の評価式の性質の違いによる．重み付きDice係数は連続単語列の出現回数のみを利用した評価式であるため，少ない出現回数の場合の計算の信頼性は低くなる．一方，Log-likelihoodは，周辺頻度（出現/非出現の両方の回数）を利用した確率論に基づく評価式であるため，出現回数が少ない場合でも正確に対応度を求めることができると考えられる．次に，多対多の対応を考慮した場合としない場合を比較する．多対多の対応を考慮することによりf_min=2では，重み付きDice係数，Log-likelihoodともに3%向上した．しかし，f_min=1では，Dice係数では3%向上したものの，Log-likelihoodでは差がみられなかった．重み付きDice係数では，対応度の高低にかかわらず，評価式の欠点を補い，多対多の対応の考慮が有効に働いている．しかし，Log-likelihoodは，対応度が高い場合では信頼性の高い対訳表現の対応度を上げ，抽出精度を高める効果を発揮するが，対応度が低い場合では周辺頻度を利用した確率計算が有効に働いているため，評価式の欠点を補うほどの効果を得ることはできなかったと考えられる．</subsection>
  <subsection title="対訳文書の性質による影響">実験で用いた「日英新聞記事対応付け結果」は，対訳辞書を用いた対応度の計算結果に基づいて，英語の文と日本語の文の自動対応付けを行っており，その対応度の高い順に文が並び替えられている．したがって「日英新聞記事対応付け結果」の先頭部分には，対応が明らかな対訳語を多く含む対訳文が多いのに対して，後半になるほど対訳関係が不明瞭な対訳文が多くなる．この対訳文の性質が抽出精度にどれだけ影響を及ぼすかを調べた．表は「日英新聞記事対応付け結果」において，先頭から8,000文(先頭部|&lt;1&gt;|)，8,001文目から8,000文(中間部|&lt;2&gt;|)，24,001文目から8,000文(後部|&lt;3&gt;|)を対象として実験した結果である．また比較のため最右部に機械ではなく人手で対応付けた取引条件に関する対訳文書(以下，取引条件文とよぶ)の9,045文における結果を示す．なお，全ての実験は節の表の|&lt;4&gt;|の条件と同じ実験環境(文節区切り情報利用，辞書参照，多対多の対応数を考慮したLog-likelihood評価式を利用)で行った．文の対応度が高い文書ほど抽出精度は高い．また，取引条件文を用いた場合，カバレッジは新聞記事を用いたいずれの結果より高かったが，その精度は67%と劣っていた．この理由は，取引条件文は専門用語が多く，出現する用語が偏っているため，連続単語列の組み合わせに要する時間は少なくてすむが，その一方で，類似する文が多く，組み合わせの曖昧性が増えるため，低い対応度での精度は低くなったと考えられる．表の最下部に，抽出結果において，対訳辞書を参照することにより抽出された対訳表現(章の提案アルゴリズムのステップ(5)-2で抽出された対訳表現)とそれ以外の対訳表現(ステップ(5)-3-aで抽出された対訳表現)の精度及び抽出語数を記した．新聞記事では後部になるほど，対訳辞書では対訳関係が認められない対訳表現の割合が増え，その影響で全体の精度も低くなった．取引条件文では，専門用語が新聞記事に比べて多いため，対訳辞書では対訳関係が認められない対訳表現の割合が高く，精度が低い結果となった．一方，表には記載していないが，表|&lt;1&gt;|の設定で，対訳辞書を参照せず，対訳表現を抽出した．その抽出結果に対して，ステップ(5)-2と同じ手法を用いて，対訳表現と認められるものと，そうでないものに分類した結果，前者の精度は97%，後者の精度は71%となった．表|&lt;1&gt;|で，対訳辞書を参照することにより抽出された対訳表現の精度は97%，そうでない対訳表現の精度は77%であり，予想通りの効果が得られている．</subsection>
  <subsection title="文書分割における影響">図は，出現回数2回(f_min=2)における連続単語列の組み合わせ数が，対訳文数の増加によってどのように増加するかを示すグラフである．「従来手法」は従来手法(表|&lt;1&gt;|と同じ設定)の場合「文節区切り」は文節区切り情報を利用した場合(表1|&lt;2&gt;|と同じ設定)「分割」は従来手法に対して，文書分割の手法=3とした．のみを採用した場合（従来手法に対してステップ(1)-3，(7)-2を適用した場合）「分割+文節」は文書分割の手法と文節区切り情報を利用した場合「分割+文節+辞書」はさらに辞書を参照した場合の結果を示している．図5から，文節区切り情報の利用と文書分割は計算量の削減に寄与しており，この両方を用いることにより，より大きな削減効果が得られることがわかる．一方，辞書の参照は，組み合わせ数を削減する効果はない．この理由は，辞書参照によって正しいと認められた対訳表現に関係する候補を削除する働きはあるものの，辞書を参照しない場合でも同様の削除処理が行われているためである．さらに「分割+文節+辞書」の手法に対して，人手による確認工程を加えた手法について同様の実験を行ったが，その結果も「分割+文節+辞書」の結果とほぼ等しくなり，組み合わせ数の削減効果は見られなかった．一方，表は文書分割手法の分割数による比較結果である．|&lt;1&gt;|から|&lt;4&gt;|は，実験対象として8,000文の対訳文書を用いた場合，|&lt;5&gt;|から|&lt;8&gt;|は16,000文の対訳文書の場合であり，それぞれ8分割，4分割，2分割で等分割した場合，分割しなかった場合の結果を求めた．この表からわかることは次の3点にまとめられる．第一に，対訳文書の文数が多い方が文書分割の効果が大きい．第二に，分割が細かすぎると逆に処理時間が遅くなる．8,000文，16,000文共，対訳文書を4分割した時が最も速い結果となった．この理由は，分割数が多いと対応度の計算処理の繰り返し回数が増え，この繰り返し処理のオーバーヘッドによって遅くなったと考えられる．第三に，精度，カバレッジ共に，文書分割の影響を受けない．しかし，文書分割により抽出される順序が変わるため，抽出される対訳表現は若干異なっている．</subsection>
  <subsection title="文書サイズによる結果の比較">表は，対訳文書のサイズの違いによって精度がどのように変化するかを記した表である．この表から，カバレッジは文書のサイズに影響を受け，サイズが大きくなるほど高くなるが，精度はほとんど影響を受けず，ほぼ一定の値をとることがわかる．</subsection>
  <subsection title="その他の考察">最後に，節の表の|&lt;4'&gt;|の実験環境(文節区切り情報利用，辞書参照，人手確認有り，多対多の対応数を考慮したLog-likelihood評価式を利用)での抽出結果の例を表に示す．本手法を用いることにより「中・東欧諸国：theCEECs」のような辞書には存在しない多くの専門用語を抽出することができる．また，形態素解析では未知語になる「コモンハウス：thecommonhouse」の例のような単語も数多く抽出することができる．半正解の原因は「冷戦：thecold」「冷戦：war」のような文節区切りによる悪影響である．これは将来的には，原言語の単語列に対して複数の目的言語の単語列が対応付けられている場合には元の文を参照することにより正しい対訳表現に復元することができると考えている．一方，間違った原因は，対訳辞書参照による悪影響と，人手確認による誤りである．例えば「休息その他の:otherwork」の対訳表現は「その他:other」が対訳辞書に登録されているために抽出された．今回の実験では，連続単語列を構成する英語と日本語の自立語単語の組合せにおいて1つでも対訳辞書登録語があれば，対訳表現として抽出するようにした．しかし，上記の例では「その他」と``other''は登録語であるが「休息」と``work''は登録語ではない．将来的には，辞書参照の方法をより厳格にし，その連続単語列ペアが対訳辞書登録語によって過不足なく対応付けられる場合のみ対訳表現と判定する，または，利用する対訳辞書をより大規模なものにして完全一致でも辞書参照の効果が得られるようにするなどの工夫が求められる．また，大量の対訳表現の確認は作業者のミスを招く．「米国:Washington」は作業者のミスにより抽出された．対訳辞書の拡張，改良等により，できるだけ多くの信頼性の高い対訳表現を自動的に検知し，作業者の負担を軽減させることも必要である．最後に，触れておかねばならないのは，人手確認における作業コストである．抽出された対訳表現を翻訳辞書として利用するためには，最終的に抽出された対訳表現が正しいか否かを再確認し，選定する必要がある．処理途中に人手による確認を行わない場合では，表|&lt;1&gt;|の結果のように，f_min=2では3,796語，f_min=1では6,452語を抽出処理終了後に人手により確認し，正しい対訳表現のみを選択しなければならない．一方，処理途中に人手による確認を行う場合では，f_min=2では処理途中に681語，処理終了後に人手未確認分の2,886語，合計3,567語を確認し，正しい対訳表現のみを選択しなければならない．またf_min=1では，処理途中に2,084語，処理終了後に人手未確認分の4,353語，合計6,437語を確認し，正しい対訳表現のみを選択しなければならない．このように確認すべき語数においては有意な差はみられない結果となったが，表|&lt;1&gt;|と表|&lt;4'&gt;|の結果にみるように，処理途中に人手確認をした方が，最終的な精度が高く，正解語数も増えている．精度が高くなることにより，処理終了後の削除の手間も削減されることから，人手による確認工程を処理途中に設ける方法は，作業コスト削減の効果があるといえる．</subsection>
  <section title="関連研究">我々の手法の特徴は，言語資源を効果的に利用することにより，低出現回数の対訳表現を抽出することができるという点にある．言語資源を利用する手法には，，がある．一方，低出現回数の対訳表現を抽出する手法には，，がある．は，対訳辞書，品詞，語源情報，構文情報の4種類の情報によって，抽出すべき対訳表現をフィルタリングしている．フィルタリングという点では我々の手法と似ているが，異なる点は我々の手法は複数の単語列からなる対訳表現候補を抽出対象としているのに対し，Melamedの手法は，単語対応に限定している点である．単語対応の場合は，その組み合わせ数は少なく，言語資源の利用の際にも計算量を考慮する必要はない．しかし，任意長の長さの表現を対象にする場合，計算量をできるだけ抑え，資源を利用するような仕組みが必要となる．我々は，信頼性の高い対訳表現から段階的に抽出するという漸進的な手法を活かし，処理の途中に対訳辞書利用や人手介入を行うことにより任意長の対訳表現の抽出の際に起こりがちな計算量の問題を解決している．は，言語資源としてWebページのような大規模な生成側の単言語テキストや，トランスリタレーション(音表記)情報を利用している．Al-Onaizanらが対象にしている言語は，アラビア語と英語であり，両者のような異なる言語族の２言語を抽出対象とする場合，対応の規則を抽出することが難しく既存の言語知識をいかに効率良く利用するかが重要となる．この点では我々のアプローチと似ており，Al-Onaizanの手法は我々の手法にも応用することができる．例えば，我々の手法での人手確認の代わりにWeb上での検索を利用することができる．また，カタカナ表記語はトランスリタレーション情報を用いて，対応度を再評価する等が考えられる．一方，の手法は，3段階の学習モデルを用いることによって，対応度の精度を高めていきながら対訳表現を抽出する手法であり，低出現回数の対訳表現も抽出することができる．ある語とその訳語は常に一対一の関係にあるという前提や先頭文字種情報などの表層的な情報を学習モデルとして利用することにより，出現回数が1回の対訳表現でも精度良く抽出することができる．この手法は，統計モデルと表層的な言語特徴情報のみを利用し，辞書や形態素解析結果などの既存の言語知識を利用しないため，専門用語の抽出も可能である．しかし，上述した前提や先頭文字種情報は専門用語の翻訳の特徴であり，イディオムや慣用句などの抽出精度は下がる．我々の手法では形態素解析結果を利用するが，対訳表現として適切でない単語列を除去するために利用するに過ぎないので，表の「コモンハウス:thecommonhouse」等の専門用語も抽出することができる．佐藤は，最大エントロピー法やSVMを利用して，少ない文書でも高精度で抽出する方法を提案している．しかし，これらの手法は学習用の対訳文書が必要となる．また，抽出対象を句単位と限定することにより，検索対象を絞っている．最後に，は，我々の手法と同様，統計的係り受け解析結果を用いて漸進的な手法で対訳表現抽出を行う．しかし，彼らは文節を越えた構造的な対訳表現を抽出することを目的としているのに対し，我々は候補とする連続単語列を文法的に意味のある範囲に限定し抽出間違いを減らすことを目的とする．</section>
  <section title="おわりに">本稿は，文節区切り情報や対訳辞書を利用する，人手による確認工程を設ける，などの種々の手法を組み合わせることによって実用性を高めた対訳表現抽出手法を提案した．また，(1)従来手法との比較，(2)対応度の評価式の違いよる比較，(3)文書の性質の違いによる比較，(4)文書分割手法の違いによる比較，(5)文書サイズの違いによる比較，という5つの比較実験により，その効果を確認した．従来手法との比較実験では，文節区切り情報利用，辞書参照，人手確認という手法が，間違った対訳表現の抽出を排除するためのフィルタリングの機能を果たすことを確認した．対応度の評価式の違いによる実験では，対応度が低い場合(f_min=1の場合)の対訳表現の抽出には重み付きDice係数よりLog-likelihoodが優れており，多対多の対応数の考慮による改良は，重み付きDice係数には効果的だがLog-likelihoodには効果が小さいことがわかった．文書の性質，及び，文書サイズの違いによる比較実験では，精度は文書のサイズには影響を受けないが，文書の専門性の高さや使用されている単語数などの対訳文書の性質に影響を受けやすいことがわかった．一方，カバレッジは文書のサイズ，性質共に影響を受けやすいことがわかった．最後に，文書分割手法の違いによる実験では，文書分割は連続単語列の組み合わせ数を削減し，計算時間を短縮させることができるが，分割が細かすぎると，逆に繰り返し処理のオーバーヘッドを生じ，計算時間が長くなることがわかった．8,000文の対訳文書による実験では，従来手法では精度40%，カバレッジ79%であったのに対し，提案手法では人手による確認工程がある場合では精度96%，カバレッジ85%で抽出することができた．人手による確認を行わない場合でも，8,000文では，精度89%，カバレッジ85%で抽出することができる．我々が，完全自動でなく，半自動という立場をとり，精度を重視している理由の一つは，抽出結果を辞書として機械翻訳システムに直接利用することを想定しているためである．今後は，本手法で抽出した対訳表現を機械翻訳システムの辞書として利用し，機械翻訳支援機能として本手法を評価することを計画している．</section>
</root>
