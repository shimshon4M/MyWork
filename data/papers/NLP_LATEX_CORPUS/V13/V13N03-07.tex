    \documentstyle[jnlpbbl,amsmath,enumerate]{jnlp_j_b5_2e}
	

\setcounter{page}{151}
\setcounter{巻数}{13}
\setcounter{号数}{3}
\setcounter{年}{2006}
\setcounter{月}{7}
\受付{2005}{12}{28}
\採録{2006}{4}{16}

\setcounter{secnumdepth}{2}

\title{関連用語収集問題とその解法}
\authorC{佐々木 靖弘\affiref{KU} \and 
	佐藤 理史\affiref{NU} \and 
	宇津呂 武仁\affiref{TU}}
\headauthor{佐々木，佐藤，宇津呂}
\headtitle{関連用語収集問題とその解法}
\affilabel{KU}{京都大学大学院情報学研究科}
	{Graduate School of Informatics, Kyoto University}
\affilabel{NU}{名古屋大学大学院工学研究科}
	{Graduate School of Engineering, Nagoya University}
\affilabel{TU}{筑波大学大学院システム情報工学研究科}
	{Graduate School of Systems and Information Engineering, University of Tsukuba}
\jabstract{
本論文で提案する{\em 関連用語収集問題}は，与えられた専門用語に対し，それ
と強く関連する用語集合を求める問題である．
この問題を解くためには，ある用語が専門用語であり，かつ，入力用語と強く関連す
るかどうかを判定する方法が必要となる．
本研究では，ウェブのサーチエンジンのヒット数から計算したJaccard係数もしくは
$\chi ^2$統計量を用いて，この判定を行なう．
作成した関連用語収集システムは，候補語収集モジュールと関連用語選択
モジュールの2つのモジュールから構成される．
候補語収集モジュールは，サーチエンジンを利用して，入力用語が出現するウェ
ブページを収集し，それらのページから関連用語の候補語を収集する．
関連用語選択モジュールは，Jaccard係数あるいは$\chi ^2$統計量の値に基づき，
候補語の中から入力用語に強く関連する用語を選択する．
実験により，作成したシステムが入力用語に強く関連する十数語の専門
用語を収集できることが確かめられた．
}
\jkeywords{語彙，知識獲得，専門用語，関連用語，ウェブ}

\etitle{Related Term Collection}
\eauthor{Yasuhiro Sasaki\affiref{KU} 
\and Satoshi Sato\affiref{NU} 
\and Takehito Utsuro\affiref{TU}} 
\eabstract{
This paper proposes the {\em related term collection problem} and its
solution. 
The related term collection problem is defined as collecting a 
dozen of technical terms that are closely related to a given 
\textit{seed} term. 
In order to solve this problem, we use the Jaccard coefficient 
or the $\chi ^2$ statistics on the Web, which is 
calculated by the search engine hits, 
for measuring relatedness between the given seed term and a candidate term. 
These measures also verify that the candidate term is a technical
term. 
We have implemented a related term collection system, which consists of 
two modules.
The first module collects candidate terms 
from the web pages that are retrieved by a search engine. 
The second module selects the terms that are closely 
related to the given term by using one of the above two measures. 
Experimental results show that the system can collect a dozen of closely
related terms of the given term. 
}
\ekeywords{terminology, knowledge acquisition, technical terms, related terms, Web}

\begin{document}
\maketitle
\thispagestyle{empty}

\section{はじめに}

「ある用語を知る」ということは，その用語が何を意味し，どのような概念を
表すかを知ることである．それと同時に，その用語が他のどのような用語と関
連があるのかを知ることは非常に重要である．
特定の専門分野で使われる用語---{\bf 専門用語}---は，
その分野内で孤立した用語として存在することはない．
その分野で使われる他の用語に支えられ，その関連を土台として，はじめて意
味を持つ．それらの用語間の関連を把握することは，「その専門分野について
知る」ことでもある．

例えば，「自然言語処理」について知りたい場合を考えよう．まずは，「自然
言語処理」という用語が表す意味，すなわち，「自然言語---人間が使っている
ことば---を計算機で処理すること」を知ることが，その第一歩となる．それと
同時に，「自然言語処理」に関連する用語にはどのような用語があり，それら
がどのような意味を持つかを知ることは，「自然言語処理」という分野を知るよ
い方法である．

用語の意味を調べる方法は自明である．百科辞典や専門用語辞典を引くことに
よって，あるいは，ウェブのサーチエンジン等を利用することによって，比較
的容易に達成できる場合が多い．それに対して，ある用語に関連する用語集合
を調べる方法は，それほど自明ではない．上記の例の場合，好運にも「自然言
語処理」用語集のようなものが見つかれば達成できるが，そのような用語集が
多くの専門分野に対して存在するわけではない．

関連用語を知ることが専門分野の理解につながるということは，逆に言えば，
適切な関連用語集を作成するためには，その分野に関する専門知識が必要であ
るということである．事実，一つの専門分野が形成され成熟すると，しばしば，
その分野の専門用語集・辞典が編纂されるが，その編纂作業は，その分野の専
門家によって行なわれるのが普通である．その作業には，かなりの労力と時間
が必要であるため，商業的に成立しうる場合にしか専門用語集は作成されない
とともに，分野の進展に追従して頻繁に改定されることはまれである．

このような現状を補完する形で，色々な分野に対する色々なサイズの私家版的
用語集が作られ，ウェブ上に公開されている．このような現象は，相互に関連
する専門用語群を知りたいというニーズが存在し，かつ，専門用語集が表す総
体---分野---を知る手段として，実際に機能していることを示唆する．

関連する専門用語群を集めるという作業は，これまで，その分野の専門家が行
なうのが常であったわけであるが，この作業を機械化することはできないであ
ろうか．我々が頭に描くのは，例えば，「自然言語処理」という用語を入力す
ると，「形態素解析」や「構文解析」，あるいは「機械翻訳」といった，「自
然言語処理」の関連用語を出力するシステムである．このようなシステムが実
現できれば，ある用語に対する関連用語が容易に得られるようになるだけでな
く，その分野で使われる専門用語の集合を収集することが可能になると考えら
れる．

このような背景から，本論文では，与えられた専門用語から，それに関連する
専門用語を自動的に収集する方法について検討する．まず，第\ref{chap2}章
で，本論文が対象とする問題---{\bf 関連用語収集問題}---を定式化し，その
解法について検討する．第\ref{sec:system}章では，実際に作成した関連用語
収集システムについて述べ，第\ref{chap4}章で，そのシステムを用いて行なっ
た実験とその結果について述べる．第\ref{chap5}章では，関連研究について
述べ，最後に，第\ref{chap6}章で，結論を述べる．


\section{関連用語収集問題} \label{chap2}

\subsection{関連用語収集問題の定式化}

先に述べたように，我々が実現したいシステムは，例えば，「自然言語処理」
を入力すると，「形態素解析」，「構文解析」，「機械翻訳」などの「自然言
語処理」に関連する専門用語を出力するシステムである．このシステムが持つべ
き機能は，与えられた専門用語に対して，それと強く関連する専門用語を収集・
出力することであり，これは，以下のような問題として定式化することができる．

\vspace{2mm}
\begin{quote}
 \framebox{\parbox{0.6\textwidth}{
 {\bf 入力：} 専門用語$s$\\
 {\bf 出力：} $s$に強く関連する専門用語の集合 \\
\hspace{2cm} $T = \{t_1,t_2,...,t_n \}$ 
 }}
\end{quote}
\vspace{2mm}
以下では，入力の専門用語$s$を{\bf シードワード}と呼び，
$t_i$を$s$の関連用語と呼ぶ．

問題をこのように定式化すると，収集すべき用語$t$は，次の2つの条件を満た
ものであることが明確となる．
\begin{enumerate}
 \item 用語$t$は，専門用語である．
 \item 用語$t$は，シードワード$s$と強く関連する．
\end{enumerate}

以下では，これらの条件とその判定法について検討する．

\subsection{「専門用語」とは}\label{sec:term}

「専門用語とは何か」ということは，ターミノロジーの中心的問いの一つであ
る．KageuraとUmino\cite{kageura96atr_review}は，重要語抽出(automatic term
recognition; ATR)の手法を整理するために，専門用語を特徴づけるunithoodと
termhoodという2つの概念を提示している．
\begin{description}
\item[unithood] the degree of strength or stability of syntagmatic
combinations or collocations
\item[termhood] the degree that a linguistic unit is related to
domain-specific concepts
\end{description}
しかし，その後，影浦は，「専門用語とは何か」をより直接的に議論し
た論文\cite{kageura02terminology}において，次のような定義を与えている．
\begin{itemize}
\item 専門用語とは専門用語として使われるものである(p.~3)
\item 専門用語は，もっぱら／特権的に／主に，特定の専門分野で使われる語彙的
単位である(p.~6)
\end{itemize}
我々は，影浦の説得力あるこの論文に同意し，専門用語の定義としてこの定義
を採用する．

この定義を採用すると，「ある用語$t$が{\bf 専門用語であるか}どうか」は，
「用語$t$が{\bf 専門用語として使われているか}どうか」を判定することに
帰着される．より具体的に言えば，どのような現象が観察されれば「専門用語
として使われている」とみなすかを決めれば，この問題は決着することになる．

ある用語が「専門用語として使われている」とは，「ある集団の人々が，ある
分野の特定の意味内容を表すために，その用語を実際に使っていること」と考える．
このことは，特定分野のテキストに，その用語がしばしば現れることを要請す
る．そこで，
\begin{enumerate}
 \item ある特定の分野のテキストにおいて，一定数以上の使用が観察されることを，
その用語をその分野の専門用語とみなすための必要条件とする．
\end{enumerate}

ただし，上記の条件を満たす語がすべて，その分野の専門用語となるとは限
らない．なぜならば，多くの分野で広く用いられる語(一般語)も上記の条件
を満たすからである．十分に大きいテキストコーパスを用意すれば，分野の数
は十分に多くなるため，このような一般語のコーパス全体における頻度は，
特定の分野にしか現れない専門用語より大きくなるとともに，その分布は，分
野に依存せずほぼ一様になることが期待できる．そこで，
\begin{enumerate} \setcounter{enumi}{1}
 \item コーパス全体における高頻度語，あるいは，分野に依存せず一様に分布する
語は，一般語とみなす．（専門用語とはみなさない．）
\end{enumerate}
逆に言うならば，専門用語は，ある特定の分野に偏って出現する用語であり，
コーパス全体における出現頻度は，それほど高くないということである．

さて，ある分野のテキストにある用語がほとんど現れないからといって，その
用語を専門用語ではないと判断することはできない．なぜならば，他の分野の
専門用語である可能性が残されているからである．これに対して，コーパス全
体における出現頻度が極めて低い用語は，専門用語である可能性がない
\footnote{これらの用語は，十分な使用例が観察されない用語---臨時的に用
いられた用語，用語として定着しなかった用語，あるいは誤植---と考えるの
が適切である．}．そこで，
\begin{enumerate} \setcounter{enumi}{2}
 \item コーパス全体における出現頻度が極めて低い用語は，専門用語とはみなさない．
\end{enumerate}

以上をまとめると，本研究では\mbox{表\ref{tbl:term_class}}に示す3種類の
用語クラスを導入し，テキストコーパス全体および特定分野のテキストにおけ
る出現傾向を観察して，それぞれの用語の用語クラスを推定するということになる．

\begin{table}
 \begin{center}\small
  \caption{本研究における用語の分類}
  \label{tbl:term_class}
  \begin{tabular}{|l|p{16zw}|p{18zw}|}
   \hline
   & 使われ方 & 観察される現象 \\ \hline
専門用語 
& 
ある集団の人々が，ある分野の特定の意味内容を表すために，実際に使っている &
特定の分野のテキストにおいて一定数の使用が観察される
\\ \hline
一般語 & 分野を問わず広く一般に使われている & 
テキストコーパス全体における出現頻度が高い，あるいは，
出現頻度は，分野に依存せず，ほぼ一定である \\ \hline
その他の語
& 
あまり使われていない &
テキストコーパス全体における出現頻度が極めて低い \\ \hline
  \end{tabular}
 \end{center}
\end{table}

本研究では，テキストコーパスとして，ウェブを採用する．ウェブは，現時点
で利用可能な，最も大きなテキストコーパスであり，さまざまな分野の情報が，
さまざまな専門レベルで記述されている．それゆえ，ウェブは，テキストにお
ける用語の出現傾向を観察するテキストとして，最も適切であると考える．


\subsection{「関連する」とは}\label{sec:rel}

次の問題は，「関連する」をどのように捉えるかという問題である．
これには，大きく2つのアプローチがある．

第一のアプローチは，2つの用語間に，シンボリックな関連性の種類---いわゆ
る{\bf 関係}---を設定し，そのような関係があると考えられる用語対を，
「関連する」と考えるアプローチである．用語間あるいは概念間の関係として
どのようなものを設定すべきかということは，古くから多くの議論があったわ
けであるが，いくつかの基本的な関係を除いては，いまだに決着を見ていない．
おおよそ合意できる範囲は，情報検索用のシソーラス\cite{thesaurus}
で用いられる等価関係(同義・類義関係)と階層関係(上位・下位関係)であろう．

しかしながら，我々は，このアプローチを採用しない．なぜならば，我々が考
慮の対象としたい専門用語間の関連をカバーする，適切な関係の集合を前もっ
て定義することは困難だと考えるからである．例えば，「形態素解析」と「形
態素」は，我々が対象としたい関連の一例であるが，この2つの用語間の関係
は，等価関係でも階層関係でもない．敢えて言うならば，形態素解析という解
析処理を行なう際の処理単位が形態素であるので，これらの用語間の関係は，
「処理と処理単位の関係」と考えるのが適切であろう．この例は一例にすぎな
いが，関連する用語間の関係には多くの種類が存在し，それらを前もってすべて
列挙できると仮定することは，現実的ではない．

第二のアプローチは，「関連の強さ」という概念を持ち込み，それをアナログ
値として表現し，その値がある一定の閾値以上となる場合に2つの用語
は強く関連する，と考えるアプローチである．ここで，「関連の強さ」は，我々が
持っている用語間の親密度・関連度を数値化したものである．もちろん，これ
を直接測ることはできないので，テキストから実際に値を計算できる何らかの尺度
を利用することになる．このような尺度として，これまでに相互情報量
\cite{church89word_association}や対数尤度比
\cite{dunning93accurate_methods}など数多くの尺度が提案されている
\cite{manning99fsnlp}．これらの尺度は，「与えられたテキストコーパスにお
いて，2つの用語が強く共起するならば，それらの用語は関連する」という仮定
に基づいている．我々は，この第二のアプローチを採用し，関連度を推定するた
めのテキストコーパスとしてウェブを採用する．


\subsection{候補語の収集}\label{sec:candidate}

ここまでの議論で，収集すべき用語$t$の条件と判定方法の基本方針が定まっ
た．しかしながら，これらから導けることは，2つの用語$s$と$t$が与えられ
たとき，$t$が$s$の関連用語となっているかどうかを判定する方法だけである．
言い換えるならば，関連用語の候補となる集合が与えられれば，それらの候補の
うち，どの用語が関連用語であるかを判定することは可能であるが，そのために
は，関連用語の候補集合をどこからか持ってこなければならない．

理論的には，その集合は，日本語のあらゆる用語を集めた集合で良い．しかし
ながら，この解は明らかに現実的ではない．関連用語の候補集合の数を限定し，
実際的な時間で前節の条件チェックを実行できるようにする必要がある．

我々は，次のような集合を，関連用語の候補集合の要素として採用する．
\begin{quote}
ウェブにおいて，シードワード$s$の周囲に現れる単名詞および複合名詞
\end{quote}
これは，次のような経験的事実に基づいている．
\begin{itemize}
\item 
日本語の専門用語のほとんどは，単名詞，あるいは，複合名詞である．
\item
関連用語は，シードワードの周辺に現れることが多い．
\end{itemize}
具体的な候補語収集手順については，\mbox{\ref{sec:cand_collect}節}で述べ
る．


\section{関連用語収集システム}\label{sec:system}

前章の考えに基づき，関連用語収集システムを作成した．
作成したシステムの構成を\mbox{図\ref{fig:system}}に示す．
本システムは，(1)候補語収集，(2)関連用語選択，の2つのモジュールから構成
される．


\begin{figure}
\begin{center} \small
\begin{picture}(380,110)

\qbezier(150,20)(150,40)(190,40)
\qbezier(230,20)(230,40)(190,40)
\qbezier(150,20)(150,0)(190,0)
\qbezier(230,20)(230,0)(190,0)
\put(190,20){\makebox(0,0){ウェブ}}

\put(30,50){\makebox(0,0){入力}}
\put(350,50){\makebox(0,0){出力}}

\put(30,70){\oval(60,20)}
\put(30,70){\makebox(0,0){用語$s$}}
\put(60,70){\vector(1,0){20}}
\put(80,60){\framebox(60,20)}
\put(110,70){\makebox(0,0){候補語収集}}
\put(140,70){\vector(1,0){20}}
\put(190,70){\oval(60,20)}
\put(190,70){\makebox(0,0){候補語集合$X$}}
\put(220,70){\vector(1,0){20}}
\put(240,60){\framebox(60,20)}
\put(270,70){\makebox(0,0){関連用語選択}}
\put(300,70){\vector(1,0){20}}
\put(350,70){\oval(60,20)}
\put(350,69){\makebox(0,0)[b]{関連用語集合}}
\put(350,68){\makebox(0,0)[t]{$T$}}

\put(190,100){\makebox(0,0){関連用語収集システム}}
\put(70,50){\dashbox{5}(240,40)}

\put(110,20){\vector(0,1){40}}
\put(110,20){\vector(1,0){40}}

\put(270,20){\vector(0,1){40}}
\put(270,20){\vector(-1,0){40}}


\end{picture}
\end{center}
\caption{関連用語収集システムの構成}\label{fig:system}
\end{figure}

\subsection{候補語収集}\label{sec:cand_collect}

\mbox{\ref{sec:candidate}}節で述べたように，ウェブにお
いてシードワード$s$の周辺に現れる単名詞および複合名詞を，関連用語の候補
語とする．具体的手順を以下に示す．

\begin{enumerate}
 \item {\bf ウェブページの収集：}シードワード$s$に対して，「$s$とは」
       「$s$という」「$s$は」「$s$の」「$s$」という5種類のクエリをサーチ
       エンジンに入力し，得られたURLのそれぞれ上位$l$ページを取得する．
       さらに，それらのページに，シードワード$s$がアンカーテキストとなっているア
       ンカーが存在する場合は，そのアンカー先ページも取得する．
 \item {\bf 文の抽出：}それぞれのページを整形して文に分割し，用語$s$を含
		      む文およびその前後$m$文を抽出する．
 \item {\bf 名詞・複合名詞の抽出：}それぞれの文を
	JUMAN 5.0\footnote{http://www.kc.t.u-tokyo.ac.jp/nl-resource/juman.html}
	を用いて形態素解析し
       ，以下のパターンにマッチする単語列を名詞・複合名詞として抽出する．
\begin{align*}
 &[noun|adj\_stem|adv\_kanji|pre|suf\_noun|suf\_stem]+\\ 
 &\quad noun: \textrm{名詞， } adj\_stem: \textrm{形容詞語幹， }
  adv\_kanji: \textrm{漢字のみからなる副詞， }\\
 &\quad pre: \textrm{接頭辞， } suf\_noun: \textrm{名詞性接尾辞， }
 suf\_stem: \textrm{形容詞性名詞接尾辞語幹}
\end{align*}
\end{enumerate}

この手順のステップ(1)で，「とは」，「という」，「は」，「の」という4つの
付属語を付加したクエリを用いるのは，単純に「$s$」のみをクエリとするより
も，多様なウェブページを収集できるのではないか，という考えに基づいている．
また，ステップ(3)において，「漢字のみからなる副詞」を複合名詞の構成
要素に含めているのは，形態素解析器JUMANにおいて，ナ形容詞語幹(例えば，
「自然言語処理」の「自然」)を副詞と解析する例が多く見られたためである．

なお，上記の手順には，収集するページ数$l$と抽出する文数$m$の2つのパラメー
タが存在する．これらのパラメータを大きくすれば，収集される関連用語の候補
集合は大きくなり，最終的に得られる関連用語も増加する一方，計算に要する時
間は長くなる．現在は，予備的な実験に基づいて，$l=100$，$m=2$を採用してい
る．


\subsection{関連用語選択}\label{sec:select}

次に，こうして収集した候補集合$X$の中から，$s$の関連用語を選択する．具体
的には，ある尺度を用いてシードワード$s$と候補語$x \in X$の関連度を計算し，
その値が大きなものを関連用語として選択する．この選択を行なうための尺度と
して，我々は{\bf Jaccard係数}と{\bf $\chi^2$統計量}に着目する．なぜなら，
この2つの尺度は，関連の強さを表す尺度であると同時に，用語$x$が専門用語
であるかどうか(専門用語性)を測る尺度とみなすことができるからである．

まず，シードワード$s$と候補語$x$が出現するか否かによって，ウェブページ
集合全体を\mbox{表\ref{tbl:2by2}}のように4つの部分集合に分割する．
それぞれの部分集合のページ数を，それぞれ$a, b, c, d$で表すとき，
Jaccard係数と$\chi ^2$統計量は，次の式で与えられる．

\noindent {\bf Jaccard係数}
\begin{align}
 Jac(s,x) = \frac{a}{a+b+c} \label{eq:jac}
\end{align}
\noindent {\bf $\chi ^2$統計量}
\begin{align}
 \chi ^2(s,x) = \frac{n(ad-bc)^2}{(a+b)(c+d)(a+c)(b+d)} \label{eq:chi2}
\end{align}
ここで，$n$は全ウェブページ数($n=a+b+c+d$)である．

\begin{table}
 \begin{center}
  \caption{用語$s$と$x$の出現に対する，ウェブページの2$\times$2分割表}
  \label{tbl:2by2}
  \begin{tabular}{r|c|c}
   & $x$が現れる & $x$が現れない \\
   \hline
   $s$が現れる & a & b \\
   \hline
   $s$が現れない & c & d \\
  \end{tabular}
 \end{center}
\end{table}

これらの尺度は，用語$s$と$x$が共起すればするほど値が大きくなるため，
「用語$x$が$s$と関連するか」を測る尺度と考えることができる．同時に，以
下に述べる理由により，「用語$x$が専門用語であるか」を測る尺度にもなっ
ている．ここで，重要な点は，「シードワード$s$は専門用語である」という
前提が存在する点である．

\noindent {\bf Jaccard係数}

Jaccard係数は，2つの用語が共起する回数を，2つの用語の出現数で正規
化した尺度である．用語$s$が出現するウェブページは，近似的に，(用語$s$ 
で表現できるような)ある特定の分野のテキストとみなすことができるので，
Jaccard係数が大きい場合は，それらのテキストに$x$が多数出現するという
現象が観察されたと判断してよい．

さて，$x$の出現頻度($a+c$)が非常に高い場合を考えよう．シードワード$s$ 
は専門用語であるので，sが現れるページ数($a+b$)はそれほど多くない．その
ため，$s$と$x$が共起するページ数$a$は，それほど大きくなることはない．
この結果，$x$の出現頻度($a+c$)が非常に高い場合は，$Jac(s,x)$ の値は小
さくなることになる．

次に，$x$の出現頻度($a+c$) が非常に低い場合を考えよう．$s$は専門用語で
あるため，$s$が現れるページ数($a+b$)は，それなりの大きさであることが保
証されている．このため，\mbox{式(\ref{eq:jac})}の分母は，$c$の値が小さ
くなっても一定の大きさを維持するのに対し，分子は，$x$が現れるペー
ジ数($a+c$)が小さければ小さいほど，小さくなる．故に，$x$の出現頻度
($a+c$)が非常に低い場合は，$Jac(s,x)$の値は小さくなることになる．

\newpage 
\noindent {\bf $\chi ^2$統計量}

$\chi ^2$統計量は，用語の出現分布が$\chi ^2$分布に従うと仮定したときに，
2つの用語が互いに独立である(関連しない)という仮説を棄却することが可能
かどうかを検定する統計量である．この値が大きいということは，$s$と$x$の
出現分布が独立ではないということを意味する．つまり，特定の分野のテキス
ト($s$が出現するテキスト)に偏って出現している証拠となる．逆に，$x$が，
特定の分野に依存せず広くテキストコーパス全体に一様に出現する場合は，
$\chi ^2$統計量は小さくなる．

次に，$x$が現れるページ数($a+c$)が小さい場合を考えよう．
$x$が現れるページに$s$が現れる割合($\frac{a}{a+c}$)が一定だと仮定し，
$d$が$a,b,c$のいずれよりも十分に大きいと仮定する\footnote{シードワード
$s$は固定されているので，$s$が現れるページ数($a+b$)は，$x$にかかわらず一
定である．}．このとき，$x$が現れるページ数($a+c$)が小さくなるにつれて，
$\chi ^2(s,x)$の値は小さくなる．すなわち，用語$x$の出現頻度が極めて小さ
いとき，$\chi ^2$統計量は小さい値をとる．

これらを総合すると，$\chi ^2$統計量が大きい値をとるとき，$x$は特定の分
野のテキストに偏って出現しており，かつ，コーパス全体に対する出現頻度は
極めて低いということはない．つまり，特定の分野のテキストに，一定数の使
用が観察されたと判断してよい．

\bigskip

以上のように，Jaccard係数と$\chi ^2$統計量の特性より，これらの尺度が大
きい場合，専門用語に対して観察される現象が観察され，かつ，一般語およ
びその他の語に対して観察される現象が観察されなかったことを意味する．
つまり，これらの尺度は，専門用語性を測る尺度としても使用できる
\footnote{これらの尺度の値が小さいからといって，「$x$は専門用語ではな
い」と断定することはできない．シードワード$s$との関連度が低い他の専門
分野の用語である可能性もあるからである．}．よって，本研究では，Jaccard 
係数と$\chi ^2$統計量を，候補集合から関連用語を選択する尺度として採用
する．

ある用語が現れるウェブページの数は，その用語をウェブのサーチエンジン
のクエリとして入力したときに得られる検索結果のヒット数から，簡単に求める
ことができる．
用語$s$および$x$をクエリとしたときのサーチエンジンのヒット数を$hits(s)$
および$hits(x)$，$s$と$x$のAND検索のヒット数を$hits(s\&x)$で表すとすると，
\mbox{表\ref{tbl:2by2}}の$a,b,c$は以下のように求めることができる．
\begin{align*}
 a &= hits(s\&x)\\
 b &= hits(s)-hits(s\&x)\\
 c &= hits(x)-hits(s\&x)
\end{align*}

\mbox{表\ref{tbl:2by2}}の$d$は，$s$も$x$も共に現れないウェブページ
の数であり，サーチエンジンを用いて直接測定することはできない．
そこで，本研究では，サーチエンジンで検索可能な日本語ウェブページをウェブ
全体と考え，ウェブ全体のページ数$n$を，いくつかの助詞(「に」，「を」，
「は」，「も」，「が」)のヒット数に基づいて見積もる．
こうして得られた$n$から，$d$の値を計算する．


\section{実験と検討} \label{chap4}

作成した関連用語収集システムの性能を評価する実験を行なった．
まず最初に，システムの評価に使用する参照セットを作成した．
次に，この参照セットを利用して，関連用語選択モジュールと候補語収集モジュー
ルの評価実験を行なった．
最後に，システム全体の性能を評価する実験を行なった．
なお，すべての実験において，
サーチエンジンとしてgoo\footnote{http://www.goo.ne.jp/}を用いた．

\subsection{参照セット}\label{sec:reference}

ある一定の条件を満たす用語を抽出するタスクでは，出力すべき用語のセット
(正解セット)を準備し，システムの出力結果と正解セットを比較して，システ
ムの評価を行なうのが一般的である．用語を抽出する対象となるコーパスがあ
らかじめ与えられている場合には，そのコーパスからある基準(例えば，人間
の判断)に従って収集した用語を正解用語とすることにより，正解セットを作
成することが可能である．しかし，本研究の場合，あらかじめ与えられている
コーパスはウェブということになるが，ウェブ全体からシードワードの関連用
語を人手で収集することは非現実的であるため，網羅的な正解セットを準備す
ることは事実上不可能である．

システムを評価するもう一つの方法として，システムの出力結果を一つ一つ人
手でチェックするという方法も考えられる\footnote{この場合，再現率を評価
することはできない．}．しかし，本研究で正解とする用語は，特定の専門分
野においてシードワードと関連する専門用語であるため，正解か否かの判断に
はその分野の専門知識が不可欠である．このため，多くの分野に対して実験を
行なおうとすれば，多数の専門家の協力を仰ぐ必要があり，非常にコストがか
かる．

そこで，我々は，専門家の知識の代替として，専門家の手によって書かれた書
籍から参照セットと呼ぶ用語集を作成し，これを用いてシステムの評価を行な
うことにした．作成した参照セットの概要を\mbox{表\ref{tbl:ref}}に示す．
参照セットは，6つの専門分野に対してそれぞれ3つと，一般語に対する1つ
の，計19セットからなる．

\begin{table}
 \begin{center}\footnotesize
  \caption{作成した参照セット}\label{tbl:ref}
   \begin{tabular}{|l|rrr|l|}
    \hline
    分野 & $|R_3|$ & $|R_2|$ & $|R_1|$  & 使用した書籍\\
    \hline
    自然言語処理 & 17 & 143 & 1337  & 
    \shortcite{nagao96nlp}，\shortcite{nagao98nlp}，\shortcite{tanaka99nlp}\\
    情報理論 & 33 & 108 & 744 & 
    \shortcite{hirasawa00it}，\shortcite{hirata03it}，\shortcite{yokoo04it}\\
    パターン認識 & 17 & 120 & 1352 & 
    \shortcite{toriwaki93pr}，\shortcite{ishii98pr}，\shortcite{nakagawa99pr}\\
    バイオインフォマティクス & 23 & 101 & 1077 & 
    \shortcite{mitaku03bio}，\shortcite{murakami03bio}，\shortcite{gojohbori03bio}\\
    マクロ経済学 & 63 & 244 & 1805 &
    \shortcite{akashi03macro}，\shortcite{wakita04macro}，\shortcite{fukuda05macro}\\
    ミクロ経済学 & 59 & 206 & 1076 &
    \shortcite{asada02micro}，\shortcite{yogo02micro}，\shortcite{ibori04micro}\\
    \hline
    一般語 & 50 & & & \shortcite{RSK} \\    \hline
    合計 & 262 & 922 & 7391 & \\ \hline
   \end{tabular}
 \end{center}
\end{table}

各専門分野の参照セットは，次の方法で作成した．
\begin{enumerate}
\item その分野の書籍を3冊用意する．
\item それぞれの書籍から巻末の索引語をすべて収集し，それぞれ索引語リストを作成する．
\item こうして得られた3つの索引語リストから，次の3つの用語集合を作成する．
\begin{description}
\item[$R_3$] 3冊の書籍で索引語となっている用語の集合．
\item[$R_2$] 2冊以上の書籍で索引語となっている用語の集合．
\item[$R_1$] いずれかの書籍で索引語となっている用語の集合．
\end{description}
\end{enumerate}
この手順から明らかなように，それぞれの分野の3つの参照セット間には，
次の関係が成り立つ．
\begin{equation}
R_3 \subseteq R_2 \subseteq R_1
\end{equation}

$R_3$に含まれる用語は，3冊の書籍すべてで索引語となっていた用語であり，
その分野の代表的な専門用語とみなすことができるだろう．これに対して，条
件を緩めた$R_2$，$R_1$は，その信頼度は下がるものの，すくなくとも，その
分野の専門用語の候補集合と考えてもよいであろう．このような考えに基づき，
これらの集合は，システムの性能を見積もるための参照解として使用できると
考えた．ただし，これらの集合を参照解として使用するということは，「特定の
分野の専門用語は，その分野の他の専門用語と必ず強く関連する」と仮定している
点に注意する必要がある．また，作成した参照セットは，その分野の専門用語
を網羅しているわけではない点にも注意が必要である．

一般語の参照セットは，小学生向けの国語辞典\cite{RSK}から，名詞50
語をランダムに選択することによって作成した．以下では，これを一般語$R_3$
と表記する．

なお，これらの参照セットに含まれる用語を，以下では参照用語と呼ぶ．


\subsection{関連用語選択モジュールの評価}\label{sec:ex_rel}

ここでは，参照セットを用いた人工的な設定下において，
関連用語選択モジュールが適切に機能するかどうかを調べた．
参照セットとしては，6つの専門分野の$R_3$と一般語$R_3$を用いた．
具体的には，以下の手順で行なった．
\begin{enumerate}
\item 
1つの専門分野の$R_3$を選ぶ．そこから専門用語を1つ選び，
シードワード$s$とする．その残りを$R_3^{-}$とする．
\item
$R_3^{-}$，上記で選択した専門分野以外の$R_3$，および，
一般語$R_3$に含まれる用語をすべて集め，これを
関連用語の候補語集合$X$とする．($|X|=261$である．\unskip)
\item
すべての$x \in X$に対して，$s$との関連度を計算し，関連度の大きい順に，
$X$の要素を並べる．最後に，上位20位までを取り出し，これを$T$とする．
\item
$|T \cap R_3^{-}|$を求める．
この数は，集合$T$に$s$と同じ分野の参照用語がいくつ含まれるかを表す．
\end{enumerate}
すなわち，ここでは，$s$と同じ分野の参照用語を仮想的な正解(関連用語)
とみなし，採用した尺度が，上位20位までに正解をどの程度出力するかを調べ
た．シードワードとしては，6つの専門分野からそれぞれ4用語ずつ，計24用語
を使用した．また，関連度を測る尺度としては，3.2節で述べたJaccard係数と
$\chi^2$統計量の2つの尺度の他に，比較のために，他の4つの尺度(共起頻
度，Dice係数，相互情報量，対数尤度比) に対しても結果を求めた．これらの
尺度とその計算式を\mbox{表\ref{tbl:relatedness}}に示す．この表の「略記」
は，本論文におけるそれぞれの尺度の略記法を示す．計算式の$a$，$b$，$c$，
$d$は
\mbox{表\ref{tbl:2by2}}に示したウェブページ数であり，$n$はウェブ全体のペー
ジ数である．実験では，$n=13600000$を用いた\footnote{この値は，助詞「に」，
「を」，「は」，「も」，「が」のいずれのヒット数よりも大きい値である．}．

\begin{table}
 \begin{center}\footnotesize
  \caption{比較尺度}\label{tbl:relatedness}
  \begin{tabular}{|l|l|l|}
   \hline
   尺度 & 略記 & 計算式 \\
   \hline
   Jaccard係数 & jac & $\frac{a}{a+b+c}$ \\
   $\chi ^2$統計量 & chi2 & $\frac{n(ad-bc)^2}{(a+b)(c+d)(a+c)(b+d)}$ \\\hline
   共起頻度 & cooc & $a$ \\
   Dice係数 & dice & $\frac{2a}{2a+b+c}$ \\
   相互情報量 & pmi & $\log \frac{an}{(a+b)(a+c)}$ \\
   対数尤度比 & llr & $a\log \frac{an}{(a+b)(a+c)}+
   b\log \frac{bn}{(a+b)(b+d)}+c\log \frac{cn}{(a+c)(c+d)}+
   d\log \frac{dn}{(b+d)(c+d)}$ \\
   \hline
  \end{tabular}
 \end{center}
\end{table}

実験結果を\mbox{表\ref{tbl:rel_result}}に示す．この表において，
$|R_3^{-}|$は，候補語集合に含まれる仮想的な正解の数を表す．

\begin{table}
 \begin{center}\footnotesize
  \caption{関連度上位20語に含まれる$R_3$の参照用語数}\label{tbl:rel_result}
  \begin{tabular}{|l|rr|rrrr|}
   \multicolumn{7}{l}{「自然言語処理」($|R_3^{-}|=16$)}\\
   \hline
   \multicolumn{1}{|c}{入力用語} & 
   \multicolumn{1}{|c}{jac} &
   \multicolumn{1}{c}{chi2} &
   \multicolumn{1}{|c}{cooc} &
   \multicolumn{1}{c}{dice} &
   \multicolumn{1}{c}{pmi} &
   \multicolumn{1}{c|}{llr} \\
   \hline
   自然言語処理 & 10 & {\bf 12} & 6 & 10 & 11 & 9 \\
   意味解析 & 12 & {\bf 14} & 7 & 12 & 12 & 10 \\
   形態素解析 & 11 & {\bf 14} & 6 & 11 & 13 & 10 \\
   構文解析 & 11 & {\bf 14} & 7 & 11 & {\bf 14} & 10 \\
   \hline
   \multicolumn{7}{l}{\tiny }\\[-5pt]
   \multicolumn{7}{l}{「情報理論」($|R_3^{-}|=32$)}\\
   \hline
   \multicolumn{1}{|c}{入力用語} & 
   \multicolumn{1}{|c}{jac} &
   \multicolumn{1}{c}{chi2} &
   \multicolumn{1}{|c}{cooc} &
   \multicolumn{1}{c}{dice} &
   \multicolumn{1}{c}{pmi} &
   \multicolumn{1}{c|}{llr} \\
   \hline
   情報理論 & 13 & 19 & 7 & 13 & {\bf 20} & 15 \\
   通信路容量 & {\bf 20} & {\bf 20} & 17 & {\bf 20} & {\bf 20} & {\bf 20} \\
   情報源符号化 & {\bf 20} & {\bf 20} & 15 & {\bf 20} & {\bf 20} & {\bf 20} \\
   エントロピー & 10 & {\bf 20} & 3 & 10 & {\bf 20} & 12 \\
   \hline
   \multicolumn{7}{l}{\tiny }\\[-5pt]
   \multicolumn{7}{l}{「パターン認識」($|R_3^{-}|=16$)}\\
   \hline
   \multicolumn{1}{|c}{入力用語} & 
   \multicolumn{1}{|c}{jac} &
   \multicolumn{1}{c}{chi2} &
   \multicolumn{1}{|c}{cooc} &
   \multicolumn{1}{c}{dice} &
   \multicolumn{1}{c}{pmi} &
   \multicolumn{1}{c|}{llr} \\
   \hline
   パターン認識 & 12 & {\bf 13} & 5 & 12 & 8 & 10 \\
   線形識別関数 & 10 & {\bf 14} & 13 & 10 & 10 & 13 \\
   部分空間法 & 11 & {\bf 13} & 11 & 11 & 10 & 11 \\
   特徴抽出 & 11 & {\bf 13} & 6 & 11 & 9 & 10 \\
   \hline
   \multicolumn{7}{l}{\tiny }\\[-5pt]
   \multicolumn{7}{l}{「バイオインフォマティクス」($|R_3^{-}|=22$)}\\
   \hline
   \multicolumn{1}{|c}{入力用語} & 
   \multicolumn{1}{|c}{jac} &
   \multicolumn{1}{c}{chi2} &
   \multicolumn{1}{|c}{cooc} &
   \multicolumn{1}{c}{dice} &
   \multicolumn{1}{c}{pmi} &
   \multicolumn{1}{c|}{llr} \\
   \hline
   バイオインフォマティクス & {\bf 16} & 15 & 8 & {\bf 16} & 15 & 14 \\
   相同性 & {\bf 18} & {\bf 18} & 16 & {\bf 18} & 17 & {\bf 18} \\
   スプライシング & {\bf 17} & {\bf 17} & 12 & {\bf 17} & {\bf 17} & 15 \\
   GenBank & {\bf 19} & {\bf 19} & 15 & {\bf 19} & {\bf 19} & {\bf 19} \\
   \hline
   \multicolumn{7}{l}{\tiny }\\[-5pt]
   \multicolumn{7}{l}{「マクロ経済学」($|R_3^{-}|=62$)}\\
   \hline
   \multicolumn{1}{|c}{入力用語} & 
   \multicolumn{1}{|c}{jac} &
   \multicolumn{1}{c}{chi2} &
   \multicolumn{1}{|c}{cooc} &
   \multicolumn{1}{c}{dice} &
   \multicolumn{1}{c}{pmi} &
   \multicolumn{1}{c|}{llr} \\
   \hline
   マクロ経済学 & 14 & 16 & 14 & 14 & {\bf 17} & 14 \\
   投資関数 & 15 & {\bf 17} & 15 & 15 & {\bf 17} & 15 \\
   有効需要 & 15 & {\bf 19} & 14 & 15 & 17 & 17 \\
   マネーサプライ & {\bf 20} & 19 & 17 & {\bf 20} & 18 & 18 \\
   \hline
   \multicolumn{7}{l}{\tiny }\\[-5pt]
   \multicolumn{7}{l}{「ミクロ経済学」($|R_3^{-}|=58$)}\\
   \hline
   \multicolumn{1}{|c}{入力用語} & 
   \multicolumn{1}{|c}{jac} &
   \multicolumn{1}{c}{chi2} &
   \multicolumn{1}{|c}{cooc} &
   \multicolumn{1}{c}{dice} &
   \multicolumn{1}{c}{pmi} &
   \multicolumn{1}{c|}{llr} \\
   \hline
   ミクロ経済学 & 15 & {\bf 19} & 7 & 15 & {\bf 19} & 13 \\
   無差別曲線 & {\bf 20} & {\bf 20} & 13 & {\bf 20} & 18 & 18 \\
   限界効用 & {\bf 20} & {\bf 20} & 11 & {\bf 20} & 16 & 17 \\
   需要曲線 & 19 & {\bf 20} & 11 & 19 & {\bf 20} & 18 \\
   \hline
  \end{tabular}
 \end{center}
\end{table}

\mbox{表\ref{tbl:rel_result}}から，次のことが観察される．
\begin{enumerate}
\item
Jaccard係数(jac)と$\chi^2$統計量(chi2)のどちらの尺度を用いた場合でも，
シードワードと同じ専門分野の専門用語が上位に集まっている．
\end{enumerate}

このことから，これらの尺度は関連用語選択の尺度として適切であり，これら
の尺度を用いた関連用語選択モジュールは適切に機能することが確認できた．

しかしながら，同時に，次の事実が観察される．
\begin{enumerate} \setcounter{enumi}{1}
\item 
シードワードと同じ専門分野の専門用語のすべてが，上位に集まるわけではない．
\end{enumerate}

例えば，「バイオインフォマティクス」の分野では，それぞれのシードワード
に対して，同じ専門分野の用語が22個，候補語集合(261個)の中に含まれて
いるのだが，上位20位に入らない語が存在した．このことは，本システムは，
一つのシードワードから，それが属す専門分野の専門用語を網羅的に収集する
能力を持たないことを意味する．

参照セットによる評価が拠り所にしている仮定「特定の分野の専門用語は，そ
の分野の他の専門用語と必ず強く関連する」は，現実には強すぎる仮定である．そ
のため，参照セットによる評価は，あくまでもシステムの性能の目安を知るた
めのものであり，万能ではない点に注意する必要がある．

一方，使用した6つの尺度に対しては，以下のような事実が観察される．
\begin{enumerate} \setcounter{enumi}{2}
\item 
$\chi^2$統計量(chi2)が最も安定して良い結果を示している．
\item 
相互情報量(pmi)，Jaccard係数(jac)およびDice係数(dice)，対数尤度比(llr)
も比較的良い結果を示している．これらの尺度と
$\chi^2$統計量との差は，それほど大きくない．
\item 
共起頻度(cooc)は他の5つの尺度と比較して，明らかに性能が劣っている．
\end{enumerate}

上記の結果は，Jaccard係数と$\chi^2$統計量の2つの尺度以外に，比較のた
めに用いた3つの尺度，すなわち，Dice係数，相互情報量，対数尤度比も，関
連用語選択の尺度となり得る可能性を持つことを示唆する．

このうち，Dice係数は，Jaccard係数とほとんど同じ尺度のため，考慮の対象か
ら除外する．また，相互情報量は，2つの用語の共起の割合が等しい場合，用
語の出現頻度が低ければ低いほど関連度が高くなるという性質をもつ
\cite[pp.~182]{manning99fsnlp}ため，専門用語性を測る尺度として不適切であ
る
\footnote{\mbox{\ref{sec:term}節}および\mbox{\ref{sec:select}節}で述べたように，
本研究では，極めて頻度が低い用語は専門用語とは見なさない．}．
対数尤度比は，専門用語性を判定するという意味づけが難しく，かつ，計算式が
複雑なので，特にこの尺度を採用すべきだという積極性に欠ける．
以上の理由により，我々は，関連用語選択の尺度として，
Jaccard係数と$\chi^2$統計量の2つの尺度を採用するという方針を
堅持することとし，以降の実験では，この2つの尺度のみを用いることにした．

\subsection{候補語収集モジュールの評価}\label{sec:ex_cand}

次に，参照セットを用いて候補語収集モジュールの性能を評価する
実験を行なった．実験の手順は次のとおりである．
\begin{enumerate}
\item 
1つの専門分野の$R_3$を選ぶ．
その中から専門用語を1つ選び，シードワード$s$とする．その残りを$R_3^{-}$とする．
\item
(1)で選択した専門分野と同じ専門分野の$R_2$，$R_1$
から，それぞれシードワード$s$を除去した集合$R_2^{-}$，$R_1^{-}$を作成する．
\item
シードワード$s$を候補語収集モジュールに与え，候補語集合$X$を得る．
\item 
$|X \cap R_3^{-}|$，$|X \cap R_2^{-}|$，$|X \cap R_1^{-}|$を計算する．
これらの値は，3つの参照セットのそれぞれに対して，候補語集合$X$に，シー
ドワードと同じ分野の参照用語がどれだけ含まれているかを表す．

\end{enumerate}

前節と同じ入力用語24語に対して，上記の手順を適用した結果を
\mbox{表\ref{tbl:cand_result}}に示す．
この表では，$R_3^{-}$，$R_2^{-}$，$R_1^{-}$に対する
結果を $R_3^{-}/R_2^{-}/R_1^{-}$ という形式で示している．

\begin{table}
 \begin{center}\footnotesize
  \caption{候補語に含まれる参照用語数}\label{tbl:cand_result}
   \begin{tabular}{|l|rr|cc|}
    \multicolumn{5}{l}{「自然言語処理」($|R_i^{-}|=16/142/1336$)} \\
    \hline
    \multicolumn{1}{|c}{入力用語} & 
    \multicolumn{1}{|c}{$|X|$} &
    \multicolumn{1}{c|}{$|X\cap R_i^{-}|$} &
    \multicolumn{1}{c}{$\frac{|X\cap R_i^{-}|}{|R_i^{-}|}$} & 
    \multicolumn{1}{c|}{$\frac{|X\cap R_i^{-}|}{|X|}$}
    \\
    \hline
    自然言語処理 & 2250 & 7/22/ 87 & .44/.15/.07 & .00/.01/.04\\
    意味解析 &  1408 & 11/27/100 & .69/.19/.07 & .00/.02/.07\\
    形態素解析 & 2022 & 6/24/ 87 & .38/.17/.07 & .00/.01/.04\\
    構文解析 &  2726 & 12/30/114 & .75/.21/.09 & .00/.01/.04\\
    \hline
    \multicolumn{5}{l}{\tiny }\\[-5pt]
    \multicolumn{5}{l}{「情報理論」($|R_i^{-}|=32/107/743$)} \\
    \hline
    \multicolumn{1}{|c}{入力用語} & 
    \multicolumn{1}{|c}{$|X|$} &
    \multicolumn{1}{c|}{$|X\cap R_i^{-}|$} &
    \multicolumn{1}{c}{$\frac{|X\cap R_i^{-}|}{|R_i^{-}|}$} &
    \multicolumn{1}{c|}{$\frac{|X\cap R_i^{-}|}{|X|}$}
    \\
    \hline
    情報理論 &  2677 & 14/28/64  & .44/.26/.09 & .01/.01/.02\\
    通信路容量  & 835 & 9/17/40  & .28/.16/.05 & .01/.02/.05\\
    情報源符号化  & 843 & 15/27/55 & .47/.25/.07 & .02/.03/.07\\
    エントロピー  & 4176 & 5/17/40 & .16/.16/.05 & .00/.00/.01\\
    \hline
    \multicolumn{5}{l}{\tiny }\\[-5pt]
    \multicolumn{5}{l}{「パターン認識」($|R_i^{-}|=16/119/1351$)} \\
    \hline
    \multicolumn{1}{|c}{入力用語} & 
    \multicolumn{1}{|c}{$|X|$} &
    \multicolumn{1}{c|}{$|X\cap R_i^{-}|$} &
    \multicolumn{1}{c}{$\frac{|X\cap R_i^{-}|}{|R_i^{-}|}$} &
    \multicolumn{1}{c|}{$\frac{|X\cap R_i^{-}|}{|X|}$}
    \\
    \hline
    パターン認識 &  2354 & 8/33/95 & .50/.28/.07 & .00/.01/.04\\
    線形識別関数 &  252 & 7/17/40 & .44/.14/.03 & .03/.07/.16\\
    部分空間法 &  537 & 7/11/23 & .44/.09/.02 & .01/.02/.04\\
    特徴抽出 &  1658 & 6/24/66 & .38/.20/.05 & .00/.01/.04\\
    \hline
    \multicolumn{5}{l}{\tiny }\\[-5pt]
    \multicolumn{5}{l}{「バイオインフォマティクス」($|R_i^{-}|=22/100/1076$)} \\
    \hline
    \multicolumn{1}{|c}{入力用語} & 
    \multicolumn{1}{|c}{$|X|$} &
    \multicolumn{1}{c|}{$|X\cap R_i^{-}|$} &
    \multicolumn{1}{c}{$\frac{|X\cap R_i^{-}|}{|R_i^{-}|}$} &
    \multicolumn{1}{c|}{$\frac{|X\cap R_i^{-}|}{|X|}$}
    \\
    \hline
    {\footnotesize バイオインフォマティクス} & 5037 & 10/34/130 & 
    .45/.34/.12 & .00/.00/.03\\
    相同性  & 3139 & 14/41/122 & .64/.41/.11 & .00/.01/.04\\
    スプライシング &  3216 & 12/31/ 99 & .55/.31/.09 & .00/.01/.03\\
    GenBank &  1355 & 13/31/ 75 & .59/.31/.07 & .01/.02/.06\\
    \hline
    \multicolumn{5}{l}{\tiny }\\[-5pt]
    \multicolumn{5}{l}{「マクロ経済学」($|R_i^{-}|=62/243/1804$)} \\
    \hline
    \multicolumn{1}{|c}{入力用語} & 
    \multicolumn{1}{|c}{$|X|$} &
    \multicolumn{1}{c|}{$|X\cap R_i^{-}|$} &
    \multicolumn{1}{c}{$\frac{|X\cap R_i^{-}|}{|R_i^{-}|}$} &
    \multicolumn{1}{c|}{$\frac{|X\cap R_i^{-}|}{|X|}$}
    \\
    \hline
    マクロ経済学 &  1872 & 22/55/116 & .35/.27/.06 & .01/.03/.06\\
    投資関数 &  1142 & 16/41/100 & .26/.17/.06 & .01/.04/.09\\
    有効需要 &  3243 & 33/88/200 & .53/.36/.11 & .01/.03/.06\\
    マネーサプライ &  1872 & 29/84/207 & .47/.35/.11 & .02/.04/.11\\
    \hline
    \multicolumn{5}{l}{\tiny }\\[-5pt]
    \multicolumn{5}{l}{「ミクロ経済学」($|R_i^{-}|=58/205/1075$)} \\
    \hline
    \multicolumn{1}{|c}{入力用語} & 
    \multicolumn{1}{|c}{$|X|$} &
    \multicolumn{1}{c|}{$|X\cap R_i^{-}|$} &
    \multicolumn{1}{c}{$\frac{|X\cap R_i^{-}|}{|R_i^{-}|}$} &
    \multicolumn{1}{c|}{$\frac{|X\cap R_i^{-}|}{|X|}$}
    \\
    \hline
    ミクロ経済学 &  1934 & 19/51/105 & .33/.25/.10 & .01/.03/.05\\
    無差別曲線 &  864 & 18/46/ 80 & .31/.22/.07 & .02/.05/.09\\
    限界効用 &  1628 & 27/57/112 & .47/.28/.10 & .02/.04/.07\\
    需要曲線 &  1411 & 20/60/108 & .34/.29/.10 & .01/.04/.08\\
    \hline
   \end{tabular}
 \end{center}
\end{table}

この結果から，次のことが観察される．
\begin{enumerate}
\item
本モジュールで収集される候補語の数は，おおよそ800〜3000である．
ただし，800未満の場合も存在する．
\end{enumerate}

収集された候補語の数が特に少なかったシードワードは，「線形識別関
数(252個)」である．この語は，そもそもウェブでのヒット数が少なく
($hits(\text{`線形識別関数'})=130$)，収集されるウェブページ数が少ない．
このことが，少数の候補語しか得られない原因となっている．経験的には，収
集される候補語の数は，シードワードのヒット数と正の相関がある．

\begin{enumerate} \setcounter{enumi}{1}
\item 
本モジュールが収集した候補語集合の中には，
シードワードと同じ専門分野の参照用語が含まれている．
参照セット$R_3$を用いた評価では，
平均的に14語程度，$R_3$の4割強が候補語集合に含まれている．
\end{enumerate}

本モジュールが候補語を収集する範囲は，非常に限定されている(シードワー
ドを中心とした前後2文)にもかかわらず，一定量の参照用語を収集すること
に成功している．これは，本モジュールが有効に機能していることを示している．

\begin{enumerate} \setcounter{enumi}{2}
\item 
参照セットを$R_2$，$R_1$に変更して参照用語数を拡大すると，
候補集合に含まれる参照用語の数は増加し，$R_1$では，数十から百を越える参
照用語を含むようになる．しかしながら，参照用語全体に対する比率
($\frac{|X\cap R_i^{-}|}{|R_i^{-}|}$)は減少する．
\end{enumerate}

このことは，候補語集合には，その分野の代表的な専門用語以外の専門用語も
含まれていることを示している．これは望ましい性質である．

しかし同時に，候補語集合は，その分野の専門用語を網羅的に含んでいるわ
けではないことを示している．つまり，候補語収集モジュールも，関連用語選
択モジュールと同様，一つのシードワードから当該分野の専門用語を網羅的に
収集する能力はないということである．

\begin{enumerate} \setcounter{enumi}{3}
\item 
参照セット$R_1$を用いた場合，収集した候補語に対する参照用語の割合(参
照用語の「密度」)は，平均的に6\%程度である．
\end{enumerate}

この「密度」は十分に高いとは言えない．システムの効率化のためには，この
密度を高めることが必要である．候補語集合には多数の一般語が含まれるた
め，既存の国語辞書等を用いて一般語を排除する方法が有効だと考えられる．


\subsection{システム全体の評価}\label{sec:ex_total}

\subsubsection{参照セットを用いた評価}

ここでは，実際にシステム全体を動作させ，適切な関連用語が収集できるかど
うかを参照セットを利用して調べた．具体的には，前節の実験で収集した候補
語とシードワードとの関連度(Jaccard係数，$\chi ^2$統計量)を計算し，関連
度上位$N(=10, 20, 30)$語に，シードワードと同じ分野の参照用語がどれだけ
含まれるかを，$R_3$，$R_2$，$R_1$のそれぞれの参照セットに対して調べた．
その結果を\mbox{表\ref{tbl:total_result}}に示す．

\begin{table}
 \begin{center}\scriptsize
  \caption{システム全体での評価}\label{tbl:total_result}
  \begin{tabular}{|l|c||cc|cc|cc|}
   \multicolumn{8}{l}{「自然言語処理」 }\\
   \hline
   \multicolumn{2}{|c||}{} &
   \multicolumn{2}{c}{$N=10$} & 
   \multicolumn{2}{|c}{$N=20$} &
   \multicolumn{2}{|c|}{$N=30$} \\
   \multicolumn{1}{|c}{入力用語} & 
   \multicolumn{1}{c||}{$|X\cap R_i^{-}|$} &
   \multicolumn{1}{c}{jac} &
   \multicolumn{1}{c}{chi2} &
   \multicolumn{1}{|c}{jac} &
   \multicolumn{1}{c}{chi2} &
   \multicolumn{1}{|c}{jac} &
   \multicolumn{1}{c|}{chi2} \\
   \hline
   自然言語処理 &7/22/87&{\bf 3}/{\bf 4}/{\bf 6}&2/2/3&{\bf 4}/{\bf 7}/{\bf 11}&{\bf 4}/5/ 6&{\bf 4}/{\bf 8}/{\bf 13}&{\bf 4}/5/ 9 \\ 
   意味解析 &11/27/100&{\bf 4}/{\bf 5}/{\bf 8}&2/3/4&{\bf 5}/{\bf 7}/{\bf 11}&{\bf 5}/6/{\bf 11}&{\bf 6}/{\bf 8}/{\bf 14}&5/6/12 \\ 
   形態素解析 &6/24/87&{\bf 2}/{\bf 2}/{\bf 2}&1/1/1&{\bf 4}/{\bf 5}/ {\bf 8}&2/3/ 4&{\bf 4}/{\bf 5}/ {\bf 8}&{\bf 4}/{\bf 5}/ {\bf 8} \\ 
   構文解析 &12/30/114&{\bf 4}/{\bf 4}/{\bf 7}&1/1/4&{\bf 5}/{\bf 5}/ {\bf 9}&2/2/ 5&{\bf 5}/{\bf 6}/{\bf 10}&{\bf 5}/{\bf 6}/ 9 \\ 
   \hline
   \multicolumn{8}{c}{\tiny }\\[-5pt]
   \multicolumn{8}{l}{「情報理論」 }\\
   \hline
   \multicolumn{2}{|c||}{} &
   \multicolumn{2}{c}{$N=10$} & 
   \multicolumn{2}{|c}{$N=20$} &
   \multicolumn{2}{|c|}{$N=30$} \\
   \multicolumn{1}{|c}{入力用語} & 
   \multicolumn{1}{c||}{$|X\cap R_i^{-}|$} &
   \multicolumn{1}{c}{jac} &
   \multicolumn{1}{c}{chi2} &
   \multicolumn{1}{|c}{jac} &
   \multicolumn{1}{c}{chi2} &
   \multicolumn{1}{|c}{jac} &
   \multicolumn{1}{c|}{chi2} \\
   \hline
   情報理論 & 14/28/64 & 2/5/5 & {\bf 5}/{\bf 6}/{\bf 6} & 4/ 7/ 9 & {\bf 7}/ {\bf 9}/{\bf 10} & 6/ 9/11 & {\bf 8}/{\bf 12}/{\bf 14} \\ 
   通信路容量 & 9/17/40 & {\bf 5}/5/6 & {\bf 5}/{\bf 6}/{\bf 7} & {\bf 6}/ {\bf 7}/ {\bf 9} & {\bf 6}/ {\bf 7}/ 8 & {\bf 7}/ {\bf 8}/{\bf 14} & {\bf 7}/ {\bf 8}/11 \\ 
   情報源符号化 & 15/27/55 & {\bf 6}/{\bf 7}/{\bf 9} & 5/{\bf 7}/8 & {\bf 7}/{\bf 10}/{\bf 14} & {\bf 7}/{\bf 10}/13 & 8/{\bf 13}/{\bf 19} & {\bf 9}/{\bf 13}/18 \\ 
   エントロピー & 5/17/40 & 0/0/{\bf 1} & 0/0/0 & {\bf 1}/ {\bf 1}/ {\bf 2} & 0/ 0/ 0 & {\bf 1}/ {\bf 1}/ {\bf 2} & 0/ 0/ 1 \\ 
   \hline
   \multicolumn{8}{c}{\tiny }\\[-5pt]
   \multicolumn{8}{l}{「パターン認識」 }\\
   \hline
   \multicolumn{2}{|c||}{} &
   \multicolumn{2}{c}{$N=10$} & 
   \multicolumn{2}{|c}{$N=20$} &
   \multicolumn{2}{|c|}{$N=30$} \\
   \multicolumn{1}{|c}{入力用語} & 
   \multicolumn{1}{c||}{$|X\cap R_i^{-}|$} &
   \multicolumn{1}{c}{jac} &
   \multicolumn{1}{c}{chi2} &
   \multicolumn{1}{|c}{jac} &
   \multicolumn{1}{c}{chi2} &
   \multicolumn{1}{|c}{jac} &
   \multicolumn{1}{c|}{chi2} \\
   \hline
   パターン認識 & 8/33/95 & {\bf 2}/{\bf 3}/{\bf 6} & 0/2/3 & {\bf 3}/{\bf 4}/{\bf 11} & 1/3/ 7 & {\bf 3}/5/{\bf 15} & 2/{\bf 6}/11 \\ 
   線形識別関数 & 7/17/40 & 1/4/{\bf 9} & {\bf 2}/{\bf 5}/{\bf 9} & {\bf 3}/{\bf 6}/{\bf 12} & {\bf 3}/{\bf 6}/11 & {\bf 3}/{\bf 6}/{\bf 14} & {\bf 3}/{\bf 6}/13 \\ 
   部分空間法 & 7/11/23 & {\bf 1}/{\bf 1}/{\bf 2} & {\bf 1}/{\bf 1}/{\bf 2} & {\bf 1}/{\bf 1}/ {\bf 3} & {\bf 1}/{\bf 1}/ 2 & {\bf 1}/{\bf 1}/ {\bf 3} & {\bf 1}/{\bf 1}/ {\bf 3} \\ 
   特徴抽出 & 6/24/66 & {\bf 1}/{\bf 3}/{\bf 6} & 1/2/3 & {\bf 1}/{\bf 3}/ {\bf 8} & {\bf 1}/{\bf 3}/ {\bf 8} & {\bf 1}/3/10 & {\bf 1}/{\bf 4}/{\bf 11} \\ 
   \hline
   \multicolumn{8}{c}{\tiny }\\[-5pt]
   \multicolumn{8}{l}{「バイオインフォマティクス」 }\\
   \hline
   \multicolumn{2}{|c||}{} &
   \multicolumn{2}{c}{$N=10$} & 
   \multicolumn{2}{|c}{$N=20$} &
   \multicolumn{2}{|c|}{$N=30$} \\
   \multicolumn{1}{|c}{入力用語} & 
   \multicolumn{1}{c||}{$|X\cap R_i^{-}|$} &
   \multicolumn{1}{c}{jac} &
   \multicolumn{1}{c}{chi2} &
   \multicolumn{1}{|c}{jac} &
   \multicolumn{1}{c}{chi2} &
   \multicolumn{1}{|c}{jac} &
   \multicolumn{1}{c|}{chi2} \\
   \hline
   {\tiny バイオインフォマティクス} & 10/34/130 & 0/0/{\bf 2} & 0/0/{\bf 2} & 0/{\bf 2}/ {\bf 7} & 0/0/ 3 & {\bf 1}/ {\bf 3}/{\bf 10} & 0/2/ 6 \\ 
   相同性 & 14/41/122 & {\bf 2}/{\bf 4}/{\bf 6} & 1/1/4 & {\bf 2}/4/ 8 & {\bf 2}/{\bf 5}/ {\bf 9} & {\bf 2}/ {\bf 5}/{\bf 12} & {\bf 2}/{\bf 5}/{\bf 12} \\ 
   スプライシング & 12/31/99 & {\bf 3}/{\bf 5}/{\bf 8} & 1/1/2 & {\bf 3}/{\bf 5}/{\bf 10} & {\bf 3}/4/ 6 & {\bf 3}/ {\bf 6}/{\bf 11} & {\bf 3}/5/ 8 \\ 
   GenBank & 13/31/75 & {\bf 3}/4/4 & {\bf 3}/{\bf 5}/{\bf 6} & {\bf 4}/{\bf 8}/{\bf 11} & {\bf 4}/{\bf 8}/10 & {\bf 6}/{\bf 12}/{\bf 17} & 4/8/12 \\
   \hline
   \multicolumn{8}{c}{\tiny }\\[-5pt]
   \multicolumn{8}{l}{「マクロ経済学」 }\\
   \hline
   \multicolumn{2}{|c||}{} &
   \multicolumn{2}{c}{$N=10$} & 
   \multicolumn{2}{|c}{$N=20$} &
   \multicolumn{2}{|c|}{$N=30$} \\
   \multicolumn{1}{|c}{入力用語} & 
   \multicolumn{1}{c||}{$|X\cap R_i^{-}|$} &
   \multicolumn{1}{c}{jac} &
   \multicolumn{1}{c}{chi2} &
   \multicolumn{1}{|c}{jac} &
   \multicolumn{1}{c}{chi2} &
   \multicolumn{1}{|c}{jac} &
   \multicolumn{1}{c|}{chi2} \\
   \hline
   マクロ経済学 & 22/55/116 & 0/{\bf 1}/{\bf 2} & 0/0/1 & {\bf 1}/{\bf 2}/ {\bf 4} & 0/1/ 2 & {\bf 2}/ {\bf 4}/ {\bf 7} & 0/ 1/ 3 \\ 
   投資関数 & 16/41/100 & {\bf 5}/{\bf 6}/{\bf 7} & 4/5/5 & 5/{\bf 8}/{\bf 13} & {\bf 6}/{\bf 8}/{\bf 13} & {\bf 6}/{\bf 10}/18 & {\bf 6}/{\bf 10}/{\bf 19} \\ 
   有効需要 & 33/88/200 & {\bf 3}/{\bf 4}/{\bf 7} & 2/3/3 & {\bf 5}/{\bf 9}/{\bf 14} & 4/6/ 9 & {\bf 9}/{\bf 17}/{\bf 23} & 6/12/17 \\ 
   マネーサプライ & 29/84/207 & {\bf 3}/{\bf 3}/{\bf 8} & 2/2/6 & {\bf 5}/{\bf 6}/{\bf 11} & 4/5/ 9 & {\bf 5}/ {\bf 9}/{\bf 15} & {\bf 5}/ 8/{\bf 15} \\ 
   \hline
   \multicolumn{8}{c}{\tiny }\\[-5pt]
   \multicolumn{8}{l}{「ミクロ経済学」 }\\
   \hline
   \multicolumn{2}{|c||}{} &
   \multicolumn{2}{c}{$N=10$} & 
   \multicolumn{2}{|c}{$N=20$} &
   \multicolumn{2}{|c|}{$N=30$} \\
   \multicolumn{1}{|c}{入力用語} & 
   \multicolumn{1}{c||}{$|X\cap R_i^{-}|$} &
   \multicolumn{1}{c}{jac} &
   \multicolumn{1}{c}{chi2} &
   \multicolumn{1}{|c}{jac} &
   \multicolumn{1}{c}{chi2} &
   \multicolumn{1}{|c}{jac} &
   \multicolumn{1}{c|}{chi2} \\
   \hline
   ミクロ経済学 & 19/51/105 & 0/{\bf 2}/{\bf 3} & 0/1/1 & 1/ 6/ 8 & {\bf 2}/ {\bf 7}/ {\bf 9} & 2/ 7/{\bf 12} & {\bf 3}/ {\bf 9}/11 \\ 
   無差別曲線 & 18/46/80 & {\bf 6}/{\bf 6}/{\bf 8} & 5/5/6 & {\bf 10}/{\bf 13}/{\bf 16} & 9/10/14 & 10/{\bf 14}/{\bf 20} & {\bf 11}/{\bf 14}/{\bf 20} \\ 
   限界効用 & 27/57/112 & {\bf 7}/{\bf 7}/{\bf 9} & 4/4/6 & {\bf 12}/{\bf 13}/{\bf 17} & 8/ 9/12 & {\bf 13}/{\bf 16}/{\bf 22} & 11/15/20 \\ 
   需要曲線 & 20/60/108 & {\bf 5}/{\bf 8}/{\bf 9} & {\bf 5}/{\bf 8}/{\bf 9} & {\bf 10}/{\bf 16}/{\bf 17} & 7/11/14 & {\bf 10}/18/21 & {\bf 10}/{\bf 19}/{\bf 22} \\ 
   \hline
  \end{tabular}
 \end{center}
\end{table}

この結果から，次のことが観察される．
\begin{enumerate}
\item 
参照セット$R_3$を用いた場合，
候補語集合に平均的に14個程度の参照用語が含まれているが，
そのうち関連度上位10位に含まれるのは，2〜3個程度である．
\item 
参照セット$R_1$を用いた場合，
候補語集合に平均的に100個弱の参照用語が含まれおり，
関連度上位10位に6個程度，上位20位に10個程度，上位30位に13個程度
の参照用語が含まれている．
\end{enumerate}

厳しい条件(参照セット$R_3$)ではそれほど性能が出ていないが，制限を緩
めた参照セット($R_1$)では，上位30位の半数程度が参照用語となっている．
このことは，本システムは，「ある分野の代表的な用語から，同じ分野の専門
用語を収集する」のではなく，「ある専門用語から，それと強く関連す
る比較的狭い範囲の専門用語を収集する」ことに長けていることを示唆する．
本システムは，このようなタスクにおいては，有効に機能していると考えられ
る．この点については，次の主観的評価のところで再度確認する．

\begin{enumerate} \setcounter{enumi}{2}
\item 
与えるシードワードによっては，参照用語をほとんど収集することができない
場合が存在する．
\end{enumerate}

本実験で極端に性能が悪かったのは，「エントロピー」と「部分空間法」をシー
ドワードとした場合である．エントロピーは「情報理論」分野以外(例えば
「熱力学」)でも用いられる専門用語である．本システムは，シードワードの
みを入力とするため，複数の分野で使われる専門用語に対して，関連用語を分
野毎に出力する能力を持たない．そのため，システムが出力する用語に，複数
の分野の専門用語が混在することになる．本実験では，「エントロピー」に対
する参照用語は「情報理論」の用語に限られるため，システムの出力に含まれ
る参照用語数は相対的に小さくなる．

一方，「部分空間法」は比較的広がりを持たない専門用語であり，それに強く
関連する専門用語が，そもそも参照セットにあまり存在していない．このこと
が上記の結果をもたらしていると考えられる．

\begin{enumerate} \setcounter{enumi}{3}
\item 
Jaccard係数と$\chi^2$統計量の2つの尺度では，使用する参照セットや
$N$の大小にかかわらず，大半のシードワードにおいて，
Jaccard係数の方が良い結果が得られている．
\end{enumerate}

つまり，Jaccard係数と$\chi^2$統計量の優劣が，\mbox{\ref{sec:ex_rel}節}の実験と
逆転している．この点についても，主観的評価のところで考察する．

\subsubsection{主観的評価}

最後に，「自然言語処理」分野について，主観的評価を行なった．具体的には，
それぞれのシードワードに対して得られた関連度上位30位までの用語を，
\begin{description}
\item[専門用語性] 該当用語は専門用語として適切か
\item[関連性] 該当用語はシードワードと強く関連しているか
\end{description}
という2つの観点でそれぞれ3段階の評点を付与した．
\begin{description}
 \item[A] 専門用語として適切である／シードワードと強く関連している
 \item[B] どちらともいえない(判断しかねる)
 \item[C] 専門用語として不適切である／シードワードと強く関連していない
\end{description}

2名の評価者(著者のうちの2名)が独立にこの評価を行ない，最終的に，
2名の評価者が専門用語性と関連性の両方でいずれもAと判定した用語を，
シードワードの関連用語(正解)とみなした．

上記の主観的評価の結果を\mbox{表\ref{tbl:subjective_evl}}に示す．この
表では，参照セットを用いた場合の結果も併せ，それぞれの正解の数を
「$R_3/R_2/R_1/\mbox{主観的評価}$」の形式で示した．

\begin{table}
 \begin{center}\scriptsize
  \caption{「自然言語処理」分野に対する主観的評価}\label{tbl:subjective_evl}
  \begin{tabular}{|l||rr|rr|rr|}
   \hline
   \multicolumn{1}{|c||}{} &
   \multicolumn{2}{c}{$N=10$} & 
   \multicolumn{2}{|c}{$N=20$} &
   \multicolumn{2}{|c|}{$N=30$} \\
   \multicolumn{1}{|c||}{入力用語} & 
   \multicolumn{1}{c}{jac} &
   \multicolumn{1}{c}{chi2} &
   \multicolumn{1}{|c}{jac} &
   \multicolumn{1}{c}{chi2} &
   \multicolumn{1}{|c}{jac} &
   \multicolumn{1}{c|}{chi2} \\
   \hline
   自然言語処理 & {\bf 3}/{\bf 4}/{\bf 6}/ {\bf 7} & 2/2/3/{\bf 7} & {\bf 4}/{\bf 7}/{\bf 11}/{\bf 16} & {\bf 4}/5/ 6/14 & {\bf 4}/{\bf 8}/{\bf 13}/{\bf 24} & {\bf 4}/5/ 9/20 \\
   意味解析 & {\bf 4}/{\bf 5}/{\bf 8}/{\bf 10} & 2/3/4/6 & {\bf 5}/{\bf 7}/{\bf 11}/{\bf 18} & {\bf 5}/6/{\bf 11}/14 & {\bf 6}/{\bf 8}/{\bf 14}/{\bf 25} &  5/6/12/17 \\
   形態素解析 & {\bf 2}/{\bf 2}/{\bf 2}/ {\bf 9} & 1/1/1/7 & {\bf 4}/{\bf 5}/ {\bf 8}/{\bf 17} & 2/3/ 4/11 & {\bf 4}/{\bf 5}/ {\bf 8}/{\bf 20} & {\bf 4}/{\bf 5}/ {\bf 8}/16 \\
   構文解析 & {\bf 4}/{\bf 4}/{\bf 7}/ {\bf 8} & 1/1/4/7 & {\bf 5}/{\bf 5}/ {\bf 9}/{\bf 12} & 2/2/ 5/{\bf 12} & {\bf 5}/{\bf 6}/{\bf 10}/{\bf 17} & {\bf 5}/{\bf 6}/ 9/{\bf 17} \\
   \hline
  \end{tabular}
 \end{center}
\end{table}

この結果から，次のことが観察される．
\begin{enumerate}
\item
主観的評価で正解と判定された用語数は，参照セット$R_1$を正解とした
場合の用語数より，いずれの場合も多い．
\item
Jaccard係数を用いた場合，上位10位では8〜9個程度，
上位20位では16個程度，上位30位では22個程度の正解が含まれている．
\end{enumerate}

これらの事実は，参照セットを用いたシステムの評価は，過小評価となっているこ
とを示している．
\mbox{\ref{sec:reference}節}で述べたように，
参照セットはその分野の専門用語を網羅的に集めたものではない．そのため，
参照セットに含まれなくてもその分野の専門用語として認められ，かつ，シー
ドワードと強く関連するような用語が数多く存在する．上記の結果は，これらの用
語を関連度上位に収集できているということを意味し，システムが有効に機能
していることを示している．

\begin{enumerate}\setcounter{enumi}{2}
\item
すべての場合において，Jaccard係数を用いた方が，$\chi ^2$統計量を用いた
場合より同等か良い結果を示している．
\end{enumerate}

この点については，最後に考察する．


\begin{table}
 \begin{center}\scriptsize
  \caption{「自然言語処理」に対するシステムの出力}\label{tbl:nlp_result}
  \begin{tabular}{|l|rr|rr|rr|cc|}
   \hline
   \multicolumn{1}{|c|}{} &
   \multicolumn{2}{c|}{} &
   \multicolumn{2}{c|}{$Jac(s,t)$} &
   \multicolumn{2}{c|}{$\chi ^2(s,t)$} &
   \multicolumn{2}{c|}{評価} \\
   \multicolumn{1}{|c|}{$t$} &
   \multicolumn{1}{c}{$hits(t)$} &
   \multicolumn{1}{c|}{$hits(s\&t)$} &
   \multicolumn{1}{c}{順位} &
   \multicolumn{1}{c|}{スコア} &
   \multicolumn{1}{c}{順位} &
   \multicolumn{1}{c|}{スコア} &
   \multicolumn{1}{c}{参照} &
   \multicolumn{1}{c|}{主観} \\
   \hline
   言語処理 & 19700 & 9950 &  1 & 0.505 & 1 & $6.86\times 10^6$ & & $\surd$ \\
   自然言語 & 20300 & 9950 & 2 & 0.490 & 2 & $6.66\times 10^6$ & 1 & $\surd$\\
   自然言語処理技術 & 1020 & 1020 & 3 & 0.103 & 3 & $1.39\times 10^6$  &  & \\
   形態素解析 & 5570 & 1270 & 4 & 0.089 & 7 & $3.94\times 10^5$  & 3 & $\surd$ \\
   形態素 & 8440 & 1460 & 5 & 0.086 & 9 & $3.43\times 10^5$  & 3 &$\surd$ \\
   コーパス & 15400 & 1740 & 6 & 0.074 & 12 & $2.66\times 10^5$  & 2 &$\surd$ \\
   構文解析 & 8190 & 1220 & 7 & 0.072 & 13 & $2.46\times 10^5$  & 3 &$\surd$ \\
   言語処理学会 & 1500 & 734 & 8 & 0.069 & 5 & $4.90\times 10^5$  &  &$\surd$ \\
   音声言語 & 9490 & 892 & 9 & 0.048 & 26 & $1.13\times 10^5$  & 1 & \\
   言語情報 & 9200 & 859 & 10 & 0.047 & 27 & $1.08\times 10^5$  &  & \\
   \hline
   機械学習 & 3750 & 612 & 11 & 0.047 & 21 & $1.35\times 10^5$  &  & $\surd$\\
   言語理解 & 3550 & 595 & 12 & 0.046 & 22 & $1.35\times 10^5$  & 1 & $\surd$\\
   自然言語処理研究会 & 406 & 406 & 13 & 0.041 & 4 & $5.55\times 10^5$  &  & $\surd$\\
   意味解析 & 1610 & 430 & 14 & 0.039 & 16 & $1.56\times 10^5$  & 3 & $\surd$\\
   知識表現 & 2190 & 440 & 15 & 0.038 & 24 & $1.20\times 10^5$  & 1 & $\surd$\\
   パターン認識 & 9370 & 699 & 16 & 0.038 &  &  &  & \\
   情報抽出 & 3110 & 462 & 17 & 0.037 &  &  & 2 & $\surd$ \\
   意味論 & 13900 & 799 & 18 & 0.035 &  &  &  & $\surd$\\
   人工知能 & 60100 & 2330 & 19 & 0.034 & 25 & $1.20\times 10^5$    &  & $\surd$\\
   機械翻訳 & 29500 & 1290 & 20 & 0.034 &  &  & 2 & $\surd$ \\
   \hline
   知識処理 & 2640 & 406 & 21 & 0.033 &  &  &  & $\surd$\\
   自動要約 & 1110 & 355 & 22 & 0.033 & 17  & $1.55\times 10^5$  &  & $\surd$\\
   知識ベース & 6310 & 520 & 23 & 0.033 &   & &  & $\surd$\\
   言語理論 & 3450 & 425 & 24 & 0.033 &   & & 1 & $\surd$\\
   自然言語処理学 & 305 & 305 & 25 & 0.031 & 6 & $4.17\times 10^5$  &  & \\
   人工知能学会誌 & 1750 & 334 & 26 & 0.029 &  &  &  & \\
   長尾真 & 2480 & 351 & 27 & 0.029 &  &  &  & $\surd$\\
   認知科学 & 17000 & 759 & 28 & 0.029 &  &  &  & $\surd$\\
   曖昧性 & 3210 & 365 & 29 & 0.029 &  &  & 2 & $\surd$\\
   対話システム & 2370 & 341 & 30 & 0.028 &  &  &  &$\surd$ \\
   \hline
   自然言語処理学講座 & 266 & 266 &  &  &  8 & $3.63\times 10^5$  &  & \\
   自然言語処理システム & 237 & 236 &  &  &  10 & $3.21\times 10^5$  &  & $\surd$\\
   自然言語処理研究 & 213 & 213 &  &  &  11 & $2.91\times 10^5$  &  & \\
   自然言語処理入門 & 123 & 123 &  &  &  14 & $1.68\times 10^5$  &  & \\
   計算言語学 & 461 & 234 &  &  & 15  & $1.62\times 10^5$  &  & $\surd$\\
   言語資源 & 438 & 222 &  &  & 18 & $1.54\times 10^5$  &  & $\surd$\\
   JAIST & 265 & 170 &  &  & 19 & $1.49\times 10^5$  &  & \\
   音声言語処理 & 621 & 253 &  &  & 20 & $1.41\times 10^5$  &  & $\surd$\\
   自然言語処理研究室 & 95 & 95 &  &  & 23 & $1.30\times 10^5$  &  & \\
   奥村学 & 473 & 189 &  &  & 28 & $1.03\times 10^5$  &  & $\surd$\\
   アルゴリズム & 127000 & 3080 &  &  & 29 & $9.70\times 10^4$  &  & \\
   テキスト自動要約 & 221 & 124 &  &  & 30 & $9.49\times 10^4$  &  & $\surd$\\
   \hline
  \end{tabular}
 \end{center}
\end{table}

「自然言語処理」をシードワードとしたときのシステムの出力(上位30語)と
その評価を
\mbox{表\ref{tbl:nlp_result}}に示す．この表において，「$Jac(s,t)$順位」は，
Jaccard係数を用いた場合の用語$t$の順位，「$\chi ^2(s,t)$順位」は，
$\chi ^2$統計量を用いた場合の用語$t$の順位である．順位の空欄は，その尺
度でその用語が上位30語に入らなかったことを示す．また，「参照」の数字は
参照セット$R_i$に含まれる場合の$i$の最大値を示す．「主観」欄のチェックは，
主観的評価において関連用語と判定されたことを示す．

この結果からも，本システムが得意とするのは，「ある専門用語(シードター
ム)から，それと強く関連する比較的狭い範囲の専門用語を収集する」
というタスクであることが確認できる．「自然言語処理」はひとつの分野を統
括する用語であるため，「形態素解析」や「構文解析」といった代表的用語が
得られているが，「茶筌」や「文脈自由文法」といった，それぞれのサブ分野
の専門用語は得ることができない．

我々は，このようなシステムの特性を問題視しない．逆に，好ましい特性と考
える．なぜならば，狭い範囲でも強く関連する用語を得ることができるのであ
れば，それを再帰的に適用することによって，関連用語集合を段階的に拡大し
ていくことができるからである．事実，「茶筌」や「文脈自由文法」といった
専門用語は，それぞれ「形態素解析」や「構文解析」をシードワードとしたと
き，本システムは，これらの用語をその関連用語として出力することができる．

\mbox{表\ref{tbl:nlp_result}}において，
$\chi ^2$統計量の上位30位以内に含まれ，かつ，Jaccard 
係数では30位以内に含まれなかった用語に見られる特徴として，「自然言語処
理学講座」や「自然言語処理研究」などの用語内にシードワードを含む
用語がある．これらの用語は，シードワードを含むため，サーチエンジンによ
るAND検索では，シードワードと100\%共起するが，用語自身のヒット数
$hits(t)$はやや小さい用語である．既に述べたように，Jaccard係数も$\chi
^2$統計量も，共起の割合が同じであれば，頻度が低い用語ほどスコアは小さ
くなるが，両者を比較すると，$\chi ^2$統計量の方が，低頻度語に高いスコ
アを与える傾向がある．この差が，現実の状況において，2つの尺度の優劣の
逆転現象をもたらす要因となっている．しかしながら，その差はそれほど大きく
ないため，どちらの尺度を用いるかはシステムの利用者に委ねるという立場を
結論とした．

\section{関連研究} \label{chap5}

本研究は，一つの用語から，それに関連する用語集合を収集するという問題を
扱っている．これを，特定分野の用語集合を収集する方法とみなせば，
その関連研究は{\bf 重要語抽出}となる．また，これを，用語間の関連性の推定
とみなせば，{\bf トピックワードグラフ生成}や
{\bf 特定の関係を持つ用語対の自動獲得}と関連する．


\subsection{重要語抽出}

重要語抽出は，与えられた文書(または文書集合)から，その文書の内容を代表
するような重要語を抽出・列挙する技術である．その最も重要な要素は，用語
の重要性を測る尺度であり，tf.idf，C-value
\shortcite{frantzi98cvalue_ncvalue}，FLR \shortcite{nakagawa03flr}や
Term Representativeness \shortcite{hisamitsu00representativeness}など
の尺度が提案されている．

重要語抽出の技術の主な応用は，情報検索のための索引語の抽出である．しか
し，ある特定の分野を代表するような文書集合を集め，この文書集合に重要語
抽出を適用すれば，そこで得られる重要語集合は，その分野の専門用語集合の
候補と考えることができる．このような見方においては，重要語抽出の研究と
本研究は，強く関連する．

重要語抽出技術を用いた専門用語抽出と，本研究の大きな違いは，入出力の違
いである．前者の入出力は，文書集合と専門用語集合(多数)であるのに対し，
後者の入出力は，専門用語と専門用語集合(少数)である．この違いは，出力
する用語の選択に用いる尺度の違いをもたらす．すなわち，前者は，特定の文
書集合における用語の重要度を測る尺度を使用するのに対し，後者は，2つの
用語間の関連度を測る尺度を使用する．

重要語抽出技術を用いた専門用語抽出の大きな問題点は，特定の分野を代表す
るような文書集合を作成することが難しいという点にある．本研究では，入力
を一つの専門用語(シードワード)に限定することによって，この問題を回避
している．しかしながら，その代償として，1回の収集ではそれほど多くの関
連用語(専門用語)を収集することはできない．この新たな問題は，収集され
た用語をシードワードとして，再帰的に関連用語を収集する方法によりある程
度解決できると考えられる．


\subsection{トピックワードグラフ生成}

検索システムDualNAVI \shortcite{niwa99dualnavi}におけるトピックワード
グラフは，検索文書集合を代表するトピックワード集合をユーザーに提示する
方法として提案されたもので，トピックワードを節点，2つのトピックワード
間の関連をリンクとするグラフである．このグラフの生成過程のうち，グラフ
のリンクの作成，すなわち，関連するトピックワードの決定は，ある用語に対
して関連する用語を決定するという側面において，本研究と強く関連する．

上記のリンク作成は，与えられたトピックワード集合の各要素に対して，
最も強く関連するトピックワードを決定することによって行なわれる．
このとき使われる関連性の尺度は，$\frac{a}{a+c}$に相当するような用語の共起
に基づいている．ただし，$a$や$c$を計算する対象は，検索された文書集合である．

このことから分かるように，トピックワード間の関連推定と本研究の大きな違
いは，前者が，ある特定の文書集合(検索された文書集合)における関連性の
推定問題を対象としているのに対し，後者は，文書集合に依存しない，より一
般的な(辞書的な)関連性の推定問題を対象としている点である．この違いは，
前者の目的が文書検索支援であるのに対し，
我々の最終目的は特定分野の用語集の自動編纂であるという違いから来ている．


\subsection{特定の関係を持つ用語対の自動獲得}

与えられたコーパスから，ある特定の関係を持つ語のペアを抽出することは，
上位・下位関係，類義関係などを対象として，比較的よく研究されてきている．
例えば，上位・下位関係の獲得では，上位・下位関係を表す特定の文型パター
ンを用いる方法
\cite{hearst92acquisition_hyponyms}や，HTML文書のリスト構造を利用する方法
\cite{shinzato05html}などが提案されている．
また，類義関係の獲得では，それぞれの語に対して，特定の文脈情報をコーパ
スから抽出し，それらをクラスタリングすることによって，類義語を発見する
方法などが提案されている\cite{hindle90noun_classification,lin98automatic_retrieval}．
これらの方法で得られる「関係」は，特定の文書に依存しない(辞書やシソーラ
スに記述すべきような)一般的な「関係」であり，そのような関係にある語の組
を求めるという側面において，本研究と共通の側面を持つ．ただし，これらの研
究が主に対象としているのは，一般語や固有名詞であり，専門用語ではない．ま
た，使用している技法も大きく異なる．


\section{おわりに} \label{chap6}

本論文では，従来，専門家の手によって行なわれていた「特定の専門分野で用いら
れる専門用語群を収集する」という作業を機械化するための方法として，
入力として与えられた専門用語(シードワード)に強く関連する用語をウェブから
自動的に収集する手法を提案した．

これを実現するために，まず，関連用語収集問題を定式化し，この問題を解くた
めには，ある用語が，(1)専門用語であり，(2)シードワードと関連すること，を
判定する必要があることを論じた．本研究では，専門用語が特定の専門文書には
数多く現れるが，その他の文書にはほとんど現れないこと，および，関連する2
つの用語は，文書において強く共起することに着目し，このような条件を満たす
用語を関連用語として収集することとした．
具体的には，Jaccard係数と$\chi^2$統計量が，これらの条件判定に利用できる
ことを定性的に示し，ウェブのサーチエンジンのヒット数からこれらの尺度を計
算し，関連用語を収集するシステムを構築した．
参照セットを用いた評価，および，主観的な評価により，本システムが入力の専
門用語に強く関連する十数語の専門用語を収集できることを示した．

ある分野の専門用語を収集することは，専門用語集の編纂の第1ステップであ
る．本研究で提案した方法は，一つのシードワードから少数の関連用語を出力
する能力しか持たないが，これを再帰的に適用することによって，多くの関連
用語を収集することが可能である．こうして得られる専門用語集合から，最終
的に見出しとすべき用語の集合を決定すれば，専門用語集の自動編纂が達成で
きると考えられる．

本論文では，日本語を対象とした関連用語収集システムについてのみ述べたが，
既に，フランス語\shortcite{xavier05french_rtc}，
英語を対象とした同様のシステムが実現されている．
これらのシステムと日本語関連用語収集システムを組み合わることによって，
特定分野の対訳用語集の自動編纂を実現することが可能となる\shortcite{EACL06}．

\medskip

\acknowledgment

本研究に対して，多くの有益なコメントを下さった，東京大学大学院教育学研究科
の影浦峡助教授に深く感謝する．


\bibliographystyle{jnlpbbl}
\begin{thebibliography}{}

\bibitem[\protect\BCAY{Aitchison \BBA\ Gilchrist}{Aitchison \BBA\
  Gilchrist}{1987}]{thesaurus}
Aitchison, J.\BBACOMMA\  \BBA\ Gilchrist, A. \BBOP 1987\BBCP.
\newblock {\Bem Thesaurus Construction: A Practical Manual\/} (2nd \BEd).
\newblock Aslib.

\bibitem[\protect\BCAY{明石}{明石}{2003}]{akashi03macro}
明石茂生 \BBOP 2003\BBCP.
\newblock \Jem{マクロ経済学}.
\newblock 中央経済社.

\bibitem[\protect\BCAY{浅田}{浅田}{2002}]{asada02micro}
浅田統一郎 \BBOP 2002\BBCP.
\newblock \Jem{ミクロ経済学の基礎}.
\newblock 中央経済社.

\bibitem[\protect\BCAY{Church \BBA\ Hanks}{Church \BBA\
  Hanks}{1990}]{church89word_association}
Church, K.~W.\BBACOMMA\  \BBA\ Hanks, P. \BBOP 1990\BBCP.
\newblock \BBOQ Word Association Norms, Mutual Information, and
  Lexicography\BBCQ\
\newblock {\Bem Computational Linguistics}, {\Bbf 16}  (1), pp.~22--29.

\bibitem[\protect\BCAY{Dunning}{Dunning}{1993}]{dunning93accurate_methods}
Dunning, T. \BBOP 1993\BBCP.
\newblock \BBOQ Accurate Methods for the Statistics of Surprise and
  Coincidence\BBCQ\
\newblock {\Bem Computational Linguistics}, {\Bbf 19}  (1), pp.~61--74.

\bibitem[\protect\BCAY{Frantzi, Ananiadou, \BBA\ Tsujii}{Frantzi
  et~al.}{1998}]{frantzi98cvalue_ncvalue}
Frantzi, K.~T., Ananiadou, S., \BBA\ Tsujii, J. \BBOP 1998\BBCP.
\newblock \BBOQ The C-value/NC-value Method of Automatic Recognition for
  Multi-word Terms\BBCQ\
\newblock In {\Bem Proceedings of the Research and Advanced Technology for
  Digital Libraries, Second European Conference (ECDL'98)}, \BPGS\ 585--604.

\bibitem[\protect\BCAY{福田 照山}{福田\JBA 照山}{2005}]{fukuda05macro}
福田慎一，照山博司 \BBOP 2005\BBCP.
\newblock \Jem{マクロ経済学・入門} (3 \JEd).
\newblock 有斐閣.

\bibitem[\protect\BCAY{五條堀}{五條堀}{2003}]{gojohbori03bio}
五條堀孝\JED\ \BBOP 2003\BBCP.
\newblock \Jem{生命情報学}.
\newblock シュプリンガー・フェアラーク東京.

\bibitem[\protect\BCAY{Hearst}{Hearst}{1992}]{hearst92acquisition_hyponyms}
Hearst, M.~A. \BBOP 1992\BBCP.
\newblock \BBOQ Automatic Acquisition of Hyponyms from Large Text Corpora\BBCQ\
\newblock In {\Bem Proceedings of the 14th International Conference on
  Computational Linguistics (COLING'92)}, \BPGS\ 539--545.

\bibitem[\protect\BCAY{Hindle}{Hindle}{1990}]{hindle90noun_classification}
Hindle, D. \BBOP 1990\BBCP.
\newblock \BBOQ Noun Classification from Predicate Argument Structures\BBCQ\
\newblock In {\Bem Proceedings of the 28th Annual Meeting of the Association
  for Computational Linguistics (ACL'90)}, \BPGS\ 268--275.

\bibitem[\protect\BCAY{平澤}{平澤}{2000}]{hirasawa00it}
平澤茂一 \BBOP 2000\BBCP.
\newblock \Jem{{\small 情報数理シリーズA-6} 情報理論入門}.
\newblock 培風館.

\bibitem[\protect\BCAY{平田}{平田}{2003}]{hirata03it}
平田廣則 \BBOP 2003\BBCP.
\newblock \Jem{情報理論のエッセンス}.
\newblock 昭晃堂.

\bibitem[\protect\BCAY{Hisamitsu, Niwa, \BBA\ Tsujii}{Hisamitsu
  et~al.}{2000}]{hisamitsu00representativeness}
Hisamitsu, T., Niwa, Y., \BBA\ Tsujii, J. \BBOP 2000\BBCP.
\newblock \BBOQ A Method of Measuring Term Representativeness\BBCQ\
\newblock In {\Bem Proceedings of 18th International Conference on
  Computational Linguistics (COLING-2000)}, \BPGS\ 320--326.

\bibitem[\protect\BCAY{井堀}{井堀}{2004}]{ibori04micro}
井堀利宏 \BBOP 2004\BBCP.
\newblock \Jem{入門ミクロ経済学} (2 \JEd).
\newblock サイエンス社.

\bibitem[\protect\BCAY{石井, 上田, 前田, 村瀬}{石井\Jetal }{1998}]{ishii98pr}
石井健一郎，上田修功，前田英作，村瀬洋 \BBOP 1998\BBCP.
\newblock \Jem{わかりやすいパターン認識}.
\newblock オーム社.

\bibitem[\protect\BCAY{Kageura \BBA\ Umino}{Kageura \BBA\
  Umino}{1996}]{kageura96atr_review}
Kageura, K.\BBACOMMA\  \BBA\ Umino, B. \BBOP 1996\BBCP.
\newblock \BBOQ Methods of Automatic Term Recognition: A Review\BBCQ\
\newblock {\Bem Terminology}, {\Bbf 3}  (2), pp.~259--289.

\bibitem[\protect\BCAY{影浦}{影浦}{2002}]{kageura02terminology}
影浦峡 \BBOP 2002\BBCP.
\newblock \JBOQ 「専門用語の理論」に関する一考察\JBCQ\
\newblock \Jem{情報知識学会誌}, {\Bbf 12}  (1), pp.~3--12.

\bibitem[\protect\BCAY{Lin}{Lin}{1998}]{lin98automatic_retrieval}
Lin, D. \BBOP 1998\BBCP.
\newblock \BBOQ Automatic Retrieval and Clustering of Similar Words\BBCQ\
\newblock In {\Bem Proceedings of COLING/ACL-98}, \BPGS\ 768--774.

\bibitem[\protect\BCAY{Manning \BBA\ Sch\"utze}{Manning \BBA\
  Sch\"utze}{1999}]{manning99fsnlp}
Manning, C.~D.\BBACOMMA\  \BBA\ Sch\"utze, H. \BBOP 1999\BBCP.
\newblock {\Bem Foundations of Statistical Natural Language Processing}.
\newblock The MIT Press.

\bibitem[\protect\BCAY{美宅 榊}{美宅\JBA 榊}{2003}]{mitaku03bio}
美宅成樹，榊佳之\JEDS\ \BBOP 2003\BBCP.
\newblock \Jem{{\small 応用生命科学シリーズ9} バイオインフォマティクス}.
\newblock 東京化学同人.

\bibitem[\protect\BCAY{村上 古谷}{村上\JBA 古谷}{2003}]{murakami03bio}
村上康文，古谷利夫\JEDS\ \BBOP 2003\BBCP.
\newblock \Jem{バイオインフォマティクスの実際}.
\newblock 講談社.

\bibitem[\protect\BCAY{長尾, 佐藤, 黒橋, 角田}{長尾\Jetal }{1996}]{nagao96nlp}
長尾真，佐藤理史，黒橋禎夫，角田達彦 \BBOP 1996\BBCP.
\newblock \Jem{{\small 岩波講座 ソフトウェア科学 15} 自然言語処理}.
\newblock 岩波書店.

\bibitem[\protect\BCAY{長尾, 黒橋, 佐藤, 池原, 中野}{長尾\Jetal
  }{1998}]{nagao98nlp}
長尾真，黒橋禎夫，佐藤理史，池原悟，中野洋 \BBOP 1998\BBCP.
\newblock \Jem{{\small 岩波講座 言語の科学 9} 言語情報処理}.
\newblock 岩波書店.

\bibitem[\protect\BCAY{中川, 森, 湯本}{中川\Jetal }{2003}]{nakagawa03flr}
中川裕志，森辰則，湯本紘彰 \BBOP 2003\BBCP.
\newblock \JBOQ 出現頻度と連接頻度に基づく専門用語抽出\JBCQ\
\newblock \Jem{自然言語処理}, {\Bbf 10}  (1), pp.~27--45.

\bibitem[\protect\BCAY{中川}{中川}{1999}]{nakagawa99pr}
中川聖一 \BBOP 1999\BBCP.
\newblock \Jem{{\small 情報科学コアカリキュラム講座} パターン情報処理}.
\newblock 丸善.

\bibitem[\protect\BCAY{Niwa, Iwayama, Hisamitsu, Nishioka, Takano, Sakurai,
  \BBA\ Imaichi}{Niwa et~al.}{1999}]{niwa99dualnavi}
Niwa, Y., Iwayama, M., Hisamitsu, T., Nishioka, S., Takano, A., Sakurai, H.,
  \BBA\ Imaichi, O. \BBOP 1999\BBCP.
\newblock \BBOQ Interactive Document Search with {\it DualNAVI}\BBCQ\
\newblock In {\Bem Proceedings of the 1st NTCIR Workshop on Research in
  Japanese Text Retrieval and Term Recognition}, \BPGS\ 123--130.

\bibitem[\protect\BCAY{Robitaille, Sasaki, Tonoike, Sato, \BBA\
  Utsuro}{Robitaille et~al.}{2006}]{EACL06}
Robitaille, X., Sasaki, Y., Tonoike, M., Sato, S., \BBA\ Utsuro, T. \BBOP
  2006\BBCP.
\newblock \BBOQ Compiling French-Japanese Terminologies from the Web\BBCQ\
\newblock In {\Bem Proceedings of the 11th Conference of the European Chapter
  of the Association for Computational Linguistics (EACL-06)}, \BPGS\ 225--232.

\bibitem[\protect\BCAY{Robitaille, Sato, \BBA\ Utsuro}{Robitaille
  et~al.}{2005}]{xavier05french_rtc}
Robitaille, X., Sato, S., \BBA\ Utsuro, T. \BBOP 2005\BBCP.
\newblock \BBOQ Automatic Collection of Related Terms in French\BBCQ\
\newblock \Jem{言語処理学会第11回年次大会発表論文集}, \BPGS\ 891--894.

\bibitem[\protect\BCAY{新里，鳥澤}{新里，鳥澤}{2005}]{shinzato05html}
新里圭司，鳥澤健太郎 \BBOP 2005\BBCP.
\newblock \JBOQ HTML文書からの単語間の上位下位関係の自動獲得\JBCQ\
\newblock \Jem{自然言語処理}, {\Bbf 12}  (1), pp.~125--150.

\bibitem[\protect\BCAY{田中}{田中}{1999}]{tanaka99nlp}
田中穂積（監修） \BBOP 1999\BBCP.
\newblock \Jem{自然言語処理—基礎と応用—}.
\newblock 電子情報通信学会.

\bibitem[\protect\BCAY{田近}{田近}{2001}]{RSK}
田近洵一\JED\ \BBOP 2001\BBCP.
\newblock \Jem{例解小学国語辞典〈ワイド版〉}.
\newblock 三省堂.

\bibitem[\protect\BCAY{鳥脇}{鳥脇}{1993}]{toriwaki93pr}
鳥脇純一郎 \BBOP 1993\BBCP.
\newblock \Jem{認識工学—パターン認識とその応用—}.
\newblock コロナ社.

\bibitem[\protect\BCAY{脇田}{脇田}{2004}]{wakita04macro}
脇田成 \BBOP 2004\BBCP.
\newblock \Jem{マクロ経済学のナビゲーター} (2 \JEd).
\newblock 日本評論社.

\bibitem[\protect\BCAY{余語}{余語}{2002}]{yogo02micro}
余語將尊 \BBOP 2002\BBCP.
\newblock \Jem{現代ミクロ経済学}.
\newblock 慶應義塾大学出版会.

\bibitem[\protect\BCAY{横尾}{横尾}{2004}]{yokoo04it}
横尾英俊 \BBOP 2004\BBCP.
\newblock \Jem{情報理論の基礎}.
\newblock 共立出版.

\end{thebibliography}

\newpage 

\begin{biography}
\biotitle{略歴}
\bioauthor{佐々木 靖弘}{
2003年京都大学工学部電気電子工学科卒業．
2005年同大学大学院情報学研究科修士課程知能情報学専攻修了．
2006年同博士課程退学．
}
\bioauthor{佐藤 理史}{
1983年京都大学工学部電気工学第二学科卒業．
1988年同大学院工学研究科博士課程電気工学第二専攻研究指導認定退学．
京都大学工学部助手，北陸先端科学技術大学院大学情報科学研究科助教授，
京都大学大学院情報学研究科助教授を経て，2005年より名古屋大学大学院工学研究科教授．
工学博士．自然言語処理，情報の自動編集等の研究に従事．
}
\bioauthor{宇津呂 武仁}{
1989年京都大学工学部電気工学第二学科卒業．
1994年同大学大学院工学研究科博士課程電気工学第二専攻修了．
京都大学博士（工学）．
奈良先端科学技術大学院大学情報科学研究科助手，
豊橋技術科学大学工学部情報工学系講師，
京都大学大学院情報学研究科講師を経て，2006年より
筑波大学大学院システム情報工学研究科助教授．
自然言語処理の研究に従事．
}

\bioreceived{受付}
\bioaccepted{採録}

\end{biography}

\end{document}

