



\documentstyle[epsf,jnlpbbl]{jnlp_j}

\setcounter{page}{3}
\setcounter{巻数}{2}
\setcounter{号数}{3}
\setcounter{年}{1995}
\setcounter{月}{7}
\受付{1995}{5}{6}
\再受付{1995}{7}{8}
\採録{1995}{9}{10}

\setcounter{secnumdepth}{2}

\title{言語資源を活用した実用的な対訳表現抽出}
\author{北村 美穂子\affiref{OKI} \and 松本 裕治\affiref{NAIST}}

\headauthor{北村 美穂子・松本 裕治}
\headtitle{言語資源を活用した実用的な対訳表現抽出}

\affilabel{OKI}{沖電気工業株式会社 研究開発本部}
{Corporate Research \& Development Center, Oki Electric Industry Co., Ltd.}

\affilabel{NAIST}{奈良先端科学技術大学院大学 }
{Nara Institute of Science and Technology}

\jabstract{
  高精度の機械翻訳システムや言語横断検索システムを構築するためには，大規模な
  対訳辞書が必要である．文対応済みの対訳文書に出現する原言語と目的言語の単語
  列の共起頻度に基づいて対訳表現を自動抽出する試みは，対訳辞書を自動的に作成
  する方法として精度が高く有効な手法の一つである．本稿はこの手法をベースにし，
  文節区切り情報や対訳辞書などの言語知識を利用したり，抽出結果を人間が確認す
  る工程を設けたりすることにより，高精度で，かつ，カバレッジの高い対訳表現抽
  出方法を提案する．また，抽出にかかる時間を削減するために，対訳文書を分割し，
  抽出対象とする文書量を徐々に増やしながら確からしい対訳表現から段階的に抽出
  していくという手法についても検討する．
  8,000文の対訳文書による実験では，従来手法は精度40\%，カバレッジ79\%であった
  のに対し，言語知識を利用した提案手法では，精度89\%，カバレッジ85\%と向上した．
  さらに人手による確認工程を設けることにより，精度が96\%，カバレッジが85\%と向
  上した．また，16,000文の対訳文書による実験では，対訳文書を分割しない方法では
  抽出時間が約16時間であったのに対し，文書を4分割する方法では，約9時間に短縮さ
  れたことを確認した． }

\jkeywords{対訳表現抽出, 対訳文書, 機械翻訳}

\etitle{Practical Translation Pattern Acquisition \\
with Utilizing Language Resources}
\eauthor{Mihoko Kitamura\affiref{OKI} \and Yuji Matsumoto\affiref{NAIST}} 

\eabstract{ High-quality MT systems and cross-lingual information
  retrieval systems need large-sized translation dictionaries.
  Automatic extraction of translation patterns from parallel corpora
  is an efficient and accurate way to automatically develop
  translation dictionaries, and various approaches have been proposed
  to achieve this. This paper presents a practical translation pattern
  extraction method where translation patterns based on co-occurrence
  frequency of word sequences between English and Japanese can be
  greedily extracted, and manual confirmation or extra linguistic
  resources, such as chunking information and translation
  dictionaries, can be also effectively combined with.  　This paper
  examines the method of extracting probable translation patterns in
  incremental steps by gradually enlarging a unit of segmentalized
  corpus, in order to reduce the time spent on pattern extraction.
  　Our experiments using 8,000 sentences showed that the proposed
  method achieved an accuracy of 89\% for coverage of 85\% while the
  existing method achieved only an accuracy of 40\% for coverage of
  79\%, and this was further improved to an accuracy of 96\% for
  coverage of 85\% when combined with manual confirmation. Our
  experiments using 16,000 sentences showed that the method of
  dividing a corpus in quarters could reduce the extraction time to 9
  hours while the nondividing method required 16 hours.  }

\ekeywords{translation pattern extraction, parallel corpus, machine translation}

\begin{document}
\maketitle


\section{はじめに}
\label{はじめに}

高精度の機械翻訳システムや言語横断検索システムを構築するためには，
大規模な対訳辞書が必要である．特に，専門性の高い文書や時事性の高い
文書を扱う場合には，専門用語や新語・造語に関する対訳辞書の有無が翻
訳や検索の精度を大きく左右する．

人手による対訳辞書の作成はコスト及び時間がかかる作業であり，できる
だけ自動化されることが望ましい．このような課題に対処するため，対訳
文書から対訳表現を自動的に抽出する手法が数多く提案されている．この
中でも，文対応済みの対訳文書から共起頻度に基づいて統計的に対訳表現
を自動抽出する手法は，精度が高く，対訳辞書を自動的に作成する方法と
して有効である\cite[など]{北村97,山本2001,佐藤2002,佐藤2003}．

本稿では，その中の一つである\cite{北村97}の手法をベースにし，従来
手法の利点である高い抽出精度を保ちつつ，抽出できる対訳表現のカバレッジ
を向上させるために行った種々の工夫について論じ，その有効性を実験で示す．

\begin{description}
\item[(A)] 文節区切り情報や品詞情報の利用 
\item[(B)] 対訳辞書の利用 
\item[(C)] 複数候補の対訳表現が得られた場合の人手による確認・選択 
\item[(D)] 多対多の対応数を考慮に入れた対応度評価式 
\item[(E)] 対訳文書の分割による漸進的な抽出
\end{description}

\noindent
の5点である．これらを用いることで実用的な対訳表現抽出を行うことができる．

(A)は，文節区切り情報や品詞情報を利用することにより，構文的に有り得ない表現が
抽出候補にならないようにする．文節区切り情報の有効性は，
既存の研究\cite{Yamamoto-Matsumoto:2003}において確かめられているが，
彼らは抽出の対象を自立語に限定している．提案手法では，各単語における文節内の
位置情報と品詞情報を用いて抽出の対象を制限することで，自立語以外の語も抽出の
対象とする．

(B)，(C)では，共起頻度に基づいた統計的な値のみでは対訳かどうかが判断できない
場合，対訳辞書や人手を利用して対訳か否かを判断する手法である．過去の研究\cite{佐藤2003}
では，対訳辞書は対訳文書から対訳関係にある単語ペアを見つけるための手がかりと
して利用されることが多いが，本提案では，手がかりとするのではなく，統計的に抽
出された対訳表現から適切な対訳表現だけを選り出すための材料として利用する．

(D)では，原言語と目的言語の単語列間の対応関係の強さを示す尺度である{\bf 対応度}
の評価式を改良する．対応度の計算には，一般に重み付きDice係数やLog-Likelihood
などの評価式が用いられるが，我々は従来手法\cite{北村97}の実験結果を分析した
結果，Dice係数やLog-Likelihoodの評価式に対して，多対多の対応数を考慮した負の重
み付けを行うことが効果的であると判断し，評価式を改良した．

(E)は抽出時間に関する課題を解決する．従来手法では10,000文以上からなる対訳文書を
抽出対象とする場合，原言語と目的言語単語列の組み合わせが多数生成されるという
課題があった．提案手法ではその組み合わせ数を削減するために，対訳文書を一定の単位に
分割し，抽出対象とする文書の単位を徐々に増やしていきながら抽出するという方法を
採用する．対象とする対訳文を1,000文，2,000文，…，10,000文と徐々に増やす度に，
抽出された対訳表現に関わる単語列を除去していく．その結果，対象の文が10,000文に
達した時の単語列の組み合わせ数は，直接10,000文を対象にした場合の組み合わせ数より
少なくなり，抽出時間を短縮させることができる．

以下，\ref{従来}章では，従来手法\cite{北村97}における，原言語単語列と目的言語単語列間の
対応度の計算方法と抽出アルゴリズムを説明する．\ref{提案}章では，本稿が提案する種々の
工夫を採用した改良手法について述べる．\ref{実験}章では\ref{提案}章に述べた各手法の
評価実験を報告し，その結果を考察する．\ref{関連研究}章では関連研究と比較し，
\ref{まとめ}章でまとめる．

\section{従来手法}
\label{従来}

\subsection{連続単語列間の対応度計算方法}
\label{連続単語列間の対応度計算方法}

原言語単語列 $ w_{o} $ と目的言語単語列 $ w_{t} $ の対応関係の強さを
示す尺度として，対応度$sim(w_{o},w_{t})$を定義する．対応度は，原言語の単語列の出現回数，目的言語の
単語列の出現回数，両者が同時に対訳文に同時に出現する回数で求められ，いくつかの
計算方法が提案されている\cite{Matsumoto-Utsuro:2000}．

従来手法では重み付きDice係数が用いられている．Dice係数はXとYの事象に
おいて，Xが発生する回数とYが発生する回数の和に対してXとYの事象が同時に出現する
回数の割合で表す．さらに，同時出現回数の重みを与えたものを重み付きDice係数と呼び，
これはXとYの相関関係だけでなく，出現回数も考慮に入れることができる．日本語単語列を
$ w_{J} $，英語単語列を $ w_{E} $，$ w_{J} $の日本語文書中の出現回数を
$ f_{j} $，$ w_{E} $の英語文書中の出現回数を$ f_{e} $とし，
$ w_{J} $と$ w_{E} $が対訳文に同時に出現する回数を$ f_{je} $とすると，
重み付きDice係数を用いた対応度は，以下の式で定義される\cite{北村97}．
  
\[ sim(w_{J},w_{E})=(\log_{2}f_{je}) \cdot \frac{2f_{je}}{f_{j}+f_{e}}  \]

\subsection{抽出アルゴリズム}
\label{抽出アルゴリズム}

従来手法の基本的な考え方は，原言語文書と目的言語文書から抽出される
連続単語列集合の全ての組合せに対して，\ref{連続単語列間の対応度計算方法}節に
述べた対応度を計算し，対応度の高い連続単語列ペアから順に対訳表現として抽出する
という手法である．図\ref{基本アルゴリズム}の流れ図に従って，各処理を説明
する\footnote{この手法で対象とする対訳文書は言語に依存しないが，以下では
日本語と英語の対訳文書を対象にして説明する．}．

\begin{figure}[t]
  \begin{center}
    \epsfxsize=9cm \epsfbox{./figure/figure1.ps}
  \end{center}
  \caption{基本アルゴリズムの流れ図}
  \label{基本アルゴリズム}
\end{figure}

\vspace{5mm}

\begin{description}
\item[(1)形態素解析:] 対訳文書 $(E,J)$ を構成する日本語文書及び英語文書中の
各対訳文を形態素解析する．形態素解析された対訳文書 $(E,J)$ 中の各対訳文を 
$(es,js)$とする．

\item[(2)連続単語列の抽出:] 各対訳文 $(es,js)$ に対して英語連続単語列集合 $EWS$ と
日本語連続単語列集合 $JWS$ を抽出し，$ (EWS,JWS) $ を連続単語列データベースに登録する．
ここでの連続単語列とは，連続単語列の構成単語数の最大値を $l_{max} $とした場合の
$ n \leq l_{max} $ の条件を満たす単語n-gramである．\ref{実験}節の実験では
$ l_{max}=10 $を用いた．

\item[(3)出現回数閾値設定:] (4)から(6)の処理で抽出対象とする連続単語列の出現回数の
最低値を閾値 $ Th $ とし，その初期値である $ f_{max} $ を代入する．4節の実験では，
$ f_{max} $ は，抽出された英語及び日本語連続単語列の出現回数の最高値の1/2の値に
設定した．なお，(4)から(6)の処理は，対訳表現が抽出されなくなるまで閾値 $Th$ を
変えずに繰り返される．対訳表現が抽出されなくなれば(7)で $Th$ を下げ，再び(4)から
(6)が繰り返される．

\item[(4)出現回数の数え上げ:] 上記(2)の全ての $(EWS, JWS)$ に対して，閾値$Th$回以上
出現する英語連続単語列($ews$ とする)と日本語連続単語列($jws$ とする)を抽出する．

\item[(5)対応度の計算・対訳表現の抽出:] (4)で抽出された全ての$ews$，$jws$に対して，
\ref{連続単語列間の対応度計算方法}節の評価式にしたがって対応度を計算し，
以下のa),b)の処理を行う．

\begin{description}
\item[a)] $ews$において最大の対応度をもつ $jws'$ を探す．$jws$ において最大の対応度を
もつ $ews'$を探す．
\item[b)] $ews$と$ews'$及び$jws$と$jws'$が同じで，かつ，$jws$と$ews$の対応度が\\
$ sim(jws,ews)\geq \log_{2}Th $ の条件を満たすならば，連続単語列ペア $(ews,jws)$ を
対訳表現とみなして，対訳表現データベースに登録する．
\end{description}

b)の条件の右辺は評価式(ここでは $(\log_{2}f_{je})\cdot \frac{2f_{je}}{f_{j}+f_{e}}$) に
$Th=f_{je}=f_{j}=f_{e}$を代入することにより求められる．
出現回数が閾値 $Th$ の場合，対応度が最高となるのは $Th=f_{je}=f_{j}=f_{e}$ の時である．
したがって，出現回数が閾値 $Th$ より小さい場合，$Th=f_{je}=f_{j}=f_{e}$ の時の
対応度の最高値を超えることはない．この条件を課すことで，閾値 $Th$ を下げた次の段階で
得られる対応度の最高値は，前段階で抽出される対訳表現の対応度の値を越えないことが
保証される．

\item[(6)連続単語列データベースにおける候補の削減:] 以降の処理で新たな対訳表現を
抽出候補としたいため，既に抽出した対訳表現，つまり，(5)で「対訳表現データベース」に
登録された対訳表現 $(ews,jws)$ に関連する連続単語列を連続単語列データベースから
削除する．削除の方法は，連続単語列データベース内の $(EWS,JWS)$ に同時に出現する 
$ews$ と同一の部分をもつ英語連続単語列 $ews$-$ex$ ($ews$と$ews$-$ex$が同一である
場合も含む)と，$jws$と同一の部分をもつ日本語連続単語列 $jws$-$ex$ ( $jws$と
$jws$-$ex$が同一である場合も含む)を$(EWS,JWS)$から削除する．$ews$でなく$ews$と
同一の部分をもつ英語連続単語列 $ews$-$ex$を削除する理由は，$ews$-$ex$は$ews$を
元にして抽出された連続単語列であるためである．$jws$-$ex$を削除する理由も同様である．
(5)において対訳表現が抽出されたならば (4)の処理に戻る．抽出されなければ (7)に進む．

\item[(7)閾値低下による候補拡大:] $ Th > f_{min}$であれば閾値$Th$の値を1つ減らして
(4)から(7)の処理を繰り返す．$Th=f_{min}$ であれば処理を終了する．
$f_{min}$ は抽出対象とする連続単語列の最低出現回数であり，4節の実験では $f_{min}=2$又は
$f_{min}=1$に設定した．

\end{description}

\vspace{5mm}

\noindent
この手法の特徴は，連続単語列ペアの出現回数に対する閾値を設け，その閾値を満足する連
続単語列ペアを対象にして対訳表現を抽出し，閾値を満足する連続単語列ペアがなくなれば，
その閾値を徐々に下げていくという点にある．対応度と連続単語列ペアの出現回数は
相関関係をもつように設定されているので，出現回数に対する閾値を設けて，その値を徐々に
小さくしていくことで，対応度の高い連続単語列ペア，つまり確からしい連続単語列ペアから
順に対訳表現を抽出することができる．また，閾値を下げていき，精度が保証されなくなる
段階で，処理を終了することもできる．

\section{提案手法}
\label{提案}

我々は\ref{従来}節の従来手法に対して， (A)文節区切り情報や品詞情報の利用，
(B)対訳辞書の利用，(C)複数候補の対訳表現が得られた場合の人手による選択，
(D)多対多の対応数を考慮に入れた対応度評価式，(E)対訳文書の分割による漸進的な抽出，
の5つの改良を行った．これらを改良した提案手法の処理の流れ図を
図\ref{提案アルゴリズム}に示す．ステップの番号のカッコ内の数字は，
\ref{抽出アルゴリズム}の抽出アルゴリズムの各ステップの番号に対応している．
図\ref{提案アルゴリズム}の四角の枠で囲まれたステップは，本提案で改良された
ステップである．ステップ(1)-2では，形態素解析と同時に文節区切り処理を行い，
ステップ(2)-1では，その結果を用いて文節を超えないように連続単語列を抽出する．
ステップ(5)-1の対応度の計算では，改良された対応度の評価式を用いる．
ステップ(1)-1，(5)-2，(5)-3-aは，辞書を参照する場合に適用されるステップである．
辞書参照だけでなく，人手による確認も行う場合はステップ(5)-3-aではなく，ステップ
(5)-3-bを用いる．対訳文書の分割による漸進的な抽出は，ステップ(1)-3で対象となる
対訳文書をあらかじめ決められた単位に分割し，ステップ(7)-2で1単位ずつ追加する
ことによって対象とする文書範囲を拡大していくという手法をとる．
以下に，提案アルゴリズムの各処理を説明する．

\begin{figure}[t]
  \begin{center}
    \epsfxsize=9cm \epsfbox{./figure/figure2.ps}
  \end{center}
  \caption{対訳表現抽出方法の流れ図}
  \label{提案アルゴリズム}
\end{figure}

\vspace{5mm}

\begin{description}
\item[(1)-1 対訳辞書の登録:] 既存の対訳辞書を「対訳辞書データベース」に登録する．

\item[(1)-2 形態素解析・文節区切り解析:] 対訳文書 $(E,J)$ を構成する日本語文書及び
英語文書の各対訳文において形態素解析及び文節区切り解析\footnote{英語には「文節」と
いう単位がないため，それに相当する単位を導入する．\ref{文節情報利用}節を参照のこと．}
を行う．形態素解析及び文節区切り解析された各対訳文を$(es,js)$とする． 

\item[(1)-3 対訳文書抽出対象の分割・設定:] 対訳文書 $(E,J)$ を$n$個に分割し，\\
$(E_{1},J_{1}),(E_{2},J_{2}),...,(E_{n},J_{n})$とする．
現時点での分割文書番号 $x$に $1$ をセットする．

\item[(2)-1連続単語列の抽出:] 対訳文書$(E_{x},J_{x})$ 中の各対訳文 $(es,js)$ に対して，
(1)-2の文節区切り解析結果を利用して，文節の境界を超えない範囲で英語単語列の集合 
$EWS$ と日本語単語列の集合 $JWS$ を抽出する．各対訳文 $(es,js)$における$(EWS,JWS)$を
連続単語列データベースに追加する．

\item[(2)-2連続単語列ＤＢにおける候補の削減:] $x=1$であれば，(3)の処理に進む．\\
$x > 1 $であれば，現時点で「対訳表現データベース」に登録されている対訳表現$(ews,jws)$に
関連する連続単語列を連続単語列データベースから削除する．削除の方法は，連続単語列
データベース内の$(ews,jws)$に同時に出現する $ews$ と同一の部分をもつ英語連続単語列 
$ews$-$ex$($ews$と$ews$-$ex$が同一である場合も含む)と，$jws$と同一の部分をもつ
日本語連続単語列$jws$-$ex$($jws$と$jws$-$ex$が同一である場合も含む)を$(EWS,JWS)$から
削除する．(これは，ステップ(6)と同じ処理である．)

\item[(3) 出現回数の閾値の設定:] (4)から(6)の処理で抽出対象となる連続単語列の
出現回数の最低値を閾値$Th$とし，その初期値 $f_{max}$を代入する．
\ref{実験}節の実験での$f_{max}$の値は\ref{抽出アルゴリズム}節の
抽出アルゴリズムに準ずる． 

\item[(4) 出現回数の数え上げ:] 上記(2)-2の全ての $(EWS,JWS)$ に対して，$Th$ 回以上
出現する英語連続単語列($ews$ とする)と日本語連続単語列($jws$ とする)を抽出する． 

\item[(5)-1対応度の計算: ] 抽出された全ての $ews$，$jws$に対して，
\ref{多対多評価式}節の計算方法にしたがって対応度を計算し，以下のa),b)の処理を行う．

  \begin{description}
  \item[a)] $ews$において最大の対応度をもつ$jws'$を探す．
$jws$において最大の対応度をもつ$ews'$を探す．
  \item[b)] $ews$ と$ews'$及び$jws$と$jws'$が同じであり，以下の条件を満たしており，
かつ，連続単語列ペア$(ews,jws)$が「対訳表現除外データベース」に登録されていない
ならば，連続単語列ペア$(ews,jws)$を対訳表現候補とする．\\

{\bf 対応度にDice係数を用いる場合:} $sim(jws,ews) \leq \log_{2} Th $

{\bf 対応度にLog-Likelihoodを用いる場合:}
\[ sim(jws,ews) \geq f_{all}\log f_{all}-(f_{all}-Th)\log(f_{all}-Th)-Th\log Th \]
  \end{description}
             
上記の条件を課す理由は，\ref{従来}節の従来手法の抽出アルゴリズムの(5)と同じである．
本処理は閾値$Th$において複数回繰り返されるが，閾値 において1回目の処理であれば，
(5)-2に進む．2回目以降の処理であれば，(5)-3-aまたは(5)-3-bに進む．

\item[(5)-2 対訳辞書参照による対訳表現の抽出(1回目の処理):] (5)-1で候補とされた
連続単語列ペア$(ews,jws)$において，$ews$を構成する英語自立語単語の集合と，$jws$を
構成する日本語自立語単語の集合を取り出し，両集合間での組合せにおいて，少なくとも
1つの組合せが「対訳辞書データベース」に登録されているならば，連続単語列ペア
$(ews,jws)$を対訳表現と認定し「対訳表現データベース」に登録する．

\item[(5)-3-a 対訳表現の抽出(2回目以降の処理):](5)-1で候補とされた対訳表現候補$(ews,jws)$を
全て対訳表現と認定し「対訳表現データベース」に登録する．

\item[(5)-3-b 人手による対訳表現の確認・選択(2回目以降の処理):] (5)-1で候補とされた
対訳表現候補を対訳表現候補$(ews,jws)$を対応度の高いものから順に並べ替える．
それをファイルに格納し，そのファイルを作業者に提示する．作業者は各対訳表現候補を確認し，
間違った対訳表現候補をファイルから削除する．全ての確認を終了し，ファイルが保存されると，
ファイルに残された候補は「対訳表現データベース」に登録され，ファイルから削除された候補は
「対訳表現除外データベース」に登録される．

\item[(6) 連続単語列DBにおける候補の削減:] (5)-2,(5)-3-a,(5)-3-bで「対訳表現データベース」に
登録された全ての対訳表現 $(ews,jws)$に関連する連続単語列を連続単語列データベースから削除する．
削除の方法は，連続単語列データベース内の$(EWS,JWS)$に同時に出現する$ews$と同一の部分をもつ
英語連続単語列$ews$-$ex$($ews$と$ews$-$ex$が同一である場合も含む)と，$jws$と同一の部分を
もつ日本語連続単語列$jws$-$ex$($jws$と$jws$-$ex$が同一である場合も含む)を$(EWS,JWS)$から
削除する．(5)において対訳表現が抽出されたならば(4)の処理に戻る．抽出されなければ，
ステップ(7)-1に進む．

\item[(7)-1 閾値低下による候補の拡大:] $Th > f_{merge} $であれば，閾値$Th$の値を
1つ下げ，(4)から(7)の処理を繰り返す．
$Th > f_{merge} $でなければ，ステップ(7)-2に進む．
$f_{merge}$は100\%の精度が得られる出現回数の最低値であり，\ref{実験}節の実験では
予備実験の結果から$ f_{merge}=3 $に設定した．

\item[(7)-2 対訳文書対象範囲の拡大:] $ x \neq n $，つまり，抽出対象が全対訳文書で
ないならば，$x$に $1$ を加え，(2)から(7)の処理を繰り返す．一方，$x=n$ かつ 
$Th > f_{min}$であれば，閾値の値を下げて(4)から(7)の処理を繰り返す．$x=n$かつ
$Th=f_{min}$であれば，全処理を終了する．$f_{min}$ は抽出対象とする連続単語列の
最低出現回数であり，\ref{実験}節の実験では，従来手法と同様，$f_{min}=2$又は
$f_{min}=1$を用いた．

\end{description}

\subsection{文節区切り情報や品詞情報の利用}
\label{文節情報利用}

\ref{連続単語列間の対応度計算方法}節の従来手法による抽出結果を
分析すると\footnote{実験データは，先行研究\cite{北村97}で
使用された「取引条件表現辞典例文\cite{石上:1992}を利用した．
\ref{文書性質の実験}節の実験結果の分析も同様である．}「中心に多くの:will become 
the target」「その保有する膨大な:vast inventory」のように，構成する一部の単語の
対応は正しいが，全体では間違っているという対訳表現が数多くみられた．このような
例の多くは，英語や日本語の表現として意味をなさない不適切な単位であることが
多かった．この課題を解決するため，不適切な対訳表現候補を生成しない工夫を施す．
具体的には，以下の処理を行う．

\begin{description}

\item[(a)] 候補となる連続単語列を抽出する時(ステップ(2)-1)，文節区切り情報を用いて，
文節境界の範囲を超える連続単語列候補を生成しない．なお，英語には，文節という単位が
ないため，動詞句，名詞句，前置詞句，副詞句の4つの文の構成成分を文節とする．

\item[(b)] あらかじめ設定された連続単語列内の位置及び品詞の条件をもつ連続単語列は
生成しない．これは，その条件を規則として表現し，その規則に適合する連続単語列は
生成しないことで実現している．現時点での品詞レベルの規則数は英語25規則，日本語45規則，
単語(見出し)レベルの規則数は，英語201規則，日本語309規則である．
\end{description}


\begin{figure}[t]
  \begin{center}
    \epsfxsize=9cm \epsfbox{./figure/figure3.ps}
  \end{center}
  \caption{文節区切りを利用した連続単語列の抽出}
  \label{文節区切り利用連続単語列抽出}
\end{figure}


\noindent
以下に具体例を示す．図\ref{文節区切り利用連続単語列抽出}は日本語文における文節区切り
結果とそれに基づく連続単語列抽出の例である．形態素解析ツールによって区切られた形態素の
区切りを ``/''，係り受け解析ツールによって区切られた文節の区切りを ``//''で表す．
文節区切り情報を利用しない場合「する安全」「決議の諸」などの対訳表現として不適切な
日本語表現が候補となるが，提案手法では文節境界の範囲を超える連続単語列を生成しない
ため，これらの表現は候補とならない．また，(b)の条件を課することによって，文節内の
不適切な表現も生成されない．例えば「の」や「れる」のような一単語のみからなる
助詞や助動詞は生成されないが「決議\_の」や「満たす\_れる」のように，助詞が名詞の
後ろに位置する場合や，助動詞が動詞の後ろに位置する連続単語列は生成される．

上記の処理をすることにより，提案手法では意味的にまとまりをなしていない文字列を除外する
ことができる．

\subsection{対訳辞書の利用}

対訳辞書は，抽出精度向上のための有効な知識である．しかし，対訳辞書を手がかりとして
抽出すると，対訳辞書に登録されていない専門用語などの表現が抽出されなくなる可能性が
ある．そこで，我々は，従来手法のアルゴリズムで抽出された対訳表現に対して，
対訳辞書を参照することによって適切な対訳表現を選り出し，それらを優先的に抽出すると
いう改良を行った．

閾値$Th$において対訳表現が抽出される限り，ステップ(4)から(6)の処理が繰り返されるが，
何回目の処理かによって処理内容を変える．閾値$Th$での処理が1回目の場合，対訳辞書を
参照し，その候補が対訳関係にあると認められれば抽出する．対訳関係か否かの判断は，
連続単語列を構成する英語と日本語の自立語単語の組合せにおいて1つでも対訳辞書登録語が
あれば，対訳関係にあると認定する．この理由は，対訳辞書登録語との完全一致する
連続単語列ペアのみを対訳関係にあると認定すると，対訳関係にある連続単語列ペアは
わずかとなり，辞書参照の効果が得られないためである．

ステップ(6)では，抽出された対訳表現に関する連続単語列候補は削除される．閾値での処理が
2回目以降では，この削除された状態で，対訳辞書を参照せずに対訳表現を抽出する．

このように，対訳辞書の利用は適切な対訳表現を選り出す働きだけでなく，不適切な連続単語列
候補を除外する役割も果たすことができる．

\subsection{複数候補の対訳表現が得られた場合の人手による確認・選択}

次に人手による確認・選択を考える．人手による確認・選択は，作業効率に見合った効果が
得られるかどうかが重要である．その作業が時間や手間がかかるものであれば，最終結果を
人手で取捨選択する作業と変わらない．我々は，ここでも従来手法の抽出アルゴリズムの性質を
利用して，繰り返し処理の途中に人手による確認・選択作業を施す．

具体的には，出現回数の閾値での1回目の処理では，辞書参照による対訳表現抽出を行い，2回目
以降の処理において，ステップ(5)-1で候補とされた対訳表現候補全てに対して，人間が正しいか
どうかを確認する．正しいと判断された候補は「対訳表現データベース」に登録され，一方，
残りの対訳表現は「対訳表現除外データベース」に登録される．

「対訳表現データベース」に登録された連続単語列ペアは，ステップ(6)の連続単語列候補の
削減に利用される．一方「対訳表現除外データベース」に登録された連続単語列ペアは，
ステップ(5)-2で対訳表現を抽出する時に参照され，対訳表現候補から必ず除外される．
　
\subsection{多対多の対応数を考慮した対応度評価式}
\label{多対多評価式}

従来手法では対応度を評価する式として重み付きDice係数が用いられたが，提案手法では，
重み付きDice係数と同様に，原言語と目的言語の単語列の同時出現回数と相関がある
Log-Likelihoodを用いる．さらに，重み付きDice係数やLog-likelihoodに対して，多対多の
対応数を考慮した改良を行う．

Log-Likelihood\cite{Dunning:1993,Matsumoto-Utsuro:2000}は，ある表現Xの出現が
別の表現Yの出現にどの程度強く依存するかを調べるための確率論に基づいた尺度である．
実際の出現事例においてXの出現がYに依存しないという仮説とYの出現/非出現に依存する
という仮説の尤度比で表す．\ref{連続単語列間の対応度計算方法}節の
$w_{J}$,$w_{E}$,$f_{j}$,$f_{e}$.$f_{je}$の前提に加えて，対訳文書が有する
文数を$f_{all}$とすると，以下の式で定義される．

\begin{small}
\begin{eqnarray*}
sim(w_{J},w_{E})&=& f_{je} \log f_{je} + (f_{e}-f_{je}) \log(f_{e}-f_{je}) + (f_{j}-f_{je})\log(f_{j}-f_{je})\\
                 &&+(f_{all}+f_{je}-f_{e}-f_{j})\log(f_{all}+f_{je}-f_{e}-f_{j}) \\
                 &&- f_{e}\log f_{e}-f_{j}\log f_{j}- (f_{all}-f_{j})\log(f_{all}-f_{j})-(f_{all}-f_{e})\log(f_{all}-f_{e})\\
                 &&+(f_{all})\log(f_{all})
\end{eqnarray*}
\end{small}


\begin{figure}[t]
  \begin{center}
    \epsfxsize=9cm \epsfbox{./figure/figure4.ps}
  \end{center}
  \caption{基本アルゴリズムによる抽出結果の例}
  \label{抽出結果の例}
\end{figure}

\noindent
次に，多対多の対応数を考慮した対応度評価式について説明する．\ref{従来}節の
従来手法を用いて抽出した対訳表現を分析した結果，図\ref{抽出結果の例}(a)のように，
日本語連続単語列と英語連続単語列が，対応度が同じで，かつ，多対多の関係で対応付け
られている場合に誤りが多かった．
一方，(b)の例のように，同じ対応度では1対1の対応関係しか持たない場合，その大半は
正しかった．図\ref{抽出結果の例}(a)の現象は，一部の単語が異なり，残りの単語は全て
共通である対訳文が複数存在した場合に起こる．その共通部分において組み合わされる
日本語・英語連続単語列ペアは対応度が等しくなり，多対多の対応関係を有する対訳表現
となる．

上記に述べた多対多の関係を有する対訳表現の抽出を避けるために，Dice係数及び
Log-likelihoodの対応度 $sim(w_{J},w_{E})$に対して，原言語と目的言語の連続単語列が
多対多の関係で対応付けられる場合にはその対応度の値が小さくなるような重み付けを与える．
以降，この対応度を $dsim(w_{J},w_{E})$と表記し，{\bf 多対多の対応数を考慮した対応度}
と呼ぶ．$dsim(w_{J},w_{E})$を以下のように定義する．

\[ dsim(w_{J},w_{E})= \frac{sim(w_{J},w_{E})}{\log_{2}(fw_{J \rightarrow E}+fw_{E \rightarrow J})} \]

$sim(w_{J},w_{E})$ は従来のDice係数やLog-likelihoodによる対応度の値である．
$fw_{J \rightarrow E}$は現段階のステップ(4)で生成された全ての連続単語列において，
日本語単語列$w_{J}$を有する連続単語列ペアの数であり，$fw_{E \rightarrow J}$は，
英語単語列 $w_{E}$を有する連続単語列ペアの数である．

上記の式は，日本語単語列$w_{J}$と英語単語列$w_{E}$からなる連続単語列ペアにおいて，
$w_{J}$が対応する英語単語列の数と，$w_{E}$が対応する日本語単語列の数の和が大きいほど
その値が小さくなるように設定されている．また，$w_{E}$と$w_{J}$が1対1で対応する場合の
値はDice係数やLog-likelihoodから計算される$sim(w_{J},w_{E})$の値と等しくなるように
設定されている．

\subsection{対訳文書の分割による漸進的な抽出}

ステップ(5)の処理における英語と日本語の連続単語列の組み合わせ数は，対訳文書の文数が
多くなるにしたがい増大する．この連続単語列候補の生成を抑えるために，文書分割による
漸進的な抽出手法を提案する．

まず，ステップ(1)-3で対象となる対訳文書をあらかじめ定められた
文数の単位\footnote{何文単位で分割するのが適切かは、\ref{文書分割実験}節の
文書分割による影響の項で議論する．}で分割し，
まず1単位で，100\%の精度が保証される出現回数まで抽出を繰り返す(ステップ(7)-1)．
その単位での処理が終了すれば，さらに1単位を追加して抽出を繰り返す(ステップ(7)-2)．
追加しながら処理を繰り返し行い，抽出対象が対訳文書全体に及べば処理を終了する．

対象とする文を徐々に拡大することで，対象とする文数が少ない初期の段階で抽出された
対訳表現に関する英語・日本語連続単語列を候補から除外することができる．これにより，
抽出対象が拡大された時の連続単語列候補の生成を削減することができる．

\section{実験および考察}
\label{実験}

3章に提案した各手法の有効性を評価するために，様々な設定の下での比較実験を行った．
基本となる実験条件と評価指標を最初に説明し，実験結果及び考察を述べる．

実験には読売新聞とThe Daily Yomiuriの記事データからなる「日英新聞記事対応付け
結果」\cite{内山:2003}の先頭から8,000文を利用した．それ以外の文書を対象とする場合は
各実験結果に明記する．

日本語形態素解析及び文節区切りは「茶筌\footnote{http://chasen.aist-nara.ac.jp/}」
及び「南瓜\footnote{http://cl.aist-nara.ac.jp/~taku-ku/software/cabocha/}」を用いた．
英語形態素解析及び対訳辞書参照に利用した対訳辞書は，
機械翻訳システム\cite{Kitamura-Murata:2003}の形態素解析モジュール及び英日・日英辞書を
利用した．この対訳辞書は，507,110ペアの対訳表現を持つ．英語は
「Charniakパーザー\footnote{http://ftp.cs.brown.edu/pub/nlparser/}」の
係り受け解析結果と\ref{文書分割実験}節に述べた方法に基づいて文節単位に区切った．

評価は，精度とカバレッジを求めることにより行った．精度は，対訳表現抽出結果を

\vspace{2mm}

\begin{description}
\item[正解:] 対訳表現をそのまま辞書として登録できる
\item[半正解:] 対訳表現のどちらか一方の一部の表現を削除すれば辞書として登録できる
\item[不正解:] 正解及び半正解以外
\end{description}

\vspace{2mm}

\noindent
の三段階で評価し，抽出総数に対する正解及び半正解の割合を百分率で求めた．
以降に示す表では，半正解の割合を()内に示す．

一方，カバレッジは，英語，日本語それぞれの文書において，


\[ coverage(\%) = ( 1- \frac{未抽出自立語総単語数}{文書中自立語総単語数}) \cdot 100 \]

\noindent
を計算し，その平均を求めた．上記式内の「未抽出自立語総単語数」とは，各文書から正解，
半正解の対訳表現を除去した結果，残った自立語の総単語数である．また，自立語の総単語数
だけでなく，自立語異なり単語数に対しても同様にカバレッジを求めた．以下の
表\ref{従来手法}から表\ref{文書サイズ}では後者を（）内に示す．

\subsection{従来手法との比較}
\label{従来手法の実験}

\begin{table}[t]
\caption{従来手法との比較}
\label{従来手法}
\begin{center}
\begin{tabular}{l|l||r|r|r|r|r}
\hline
\multicolumn{2}{c||}{} 
& \multicolumn{4}{c|}{Dice} 
& \multicolumn{1}{c}{d-Loglike} \\\cline{3-7}
\multicolumn{2}{c||}{} 
& \multicolumn{1}{c|}{\verb|<1>|従来}
& \multicolumn{1}{c|}{\verb|<2>|文節}
& \multicolumn{1}{c|}{\verb|<3>|辞書}
& \multicolumn{1}{c|}{\verb|<4>|人手}
& \multicolumn{1}{c}{\verb|<4'>|人手} \\
\hline
          & 合計    & 3,033   & 3,057   & 2,981   & 2,847   & 3,527\\
          & 正解数  & 2,686   & 2,808   & 2,832   & 2,770   & 3,447\\
$f_{min}$ & 半正解数&   110   &    83   &    82   &    64   &    64\\\cline{2-7}
$=2$      & 精度    & 88\%(92)& 91\%(94)& 95\%(97)& 97\%(99)& 98\%(100) \\
         &カバレッジ& 79\%(12)& 80\%(15)& 80\%(15)& 80\%(15)& 82\%(13)  \\\cline{2-7}
     &計算機実行時間&   6h34m &  3h40m  & 4h17m   & 4h19m   & 41m       \\
       &人手作業時間&     --- &     --- &     --- &   24m   & 21m       \\ \hline

          & 合計    & 16,784  & 16,276  & 10,276  & 7,274   & 6,250     \\
          & 正解数  &  6,766  &  7,293  &  6,996  & 6,821   & 5,993     \\
$f_{min}$ & 半正解数&    434  &    391  &    335  &   221   &   151     \\ \cline{2-7}
$=1$      & 精度    & 40\%(42)& 44\%(47)& 68\%(71)& 93\%(96)&96\%(98)\\
         &カバレッジ& 85\%(16)& 86\%(19)& 86\%(19)& 85\%(19)&85\%(19)\\ \cline{2-7}
     &計算機実行時間&  11h47m & 4h15m   & 4h49m   & 4h51m   & 1h06m  \\
       &人手作業時間&     --- &     --- &     --- & 2h07m   & 1h57m  \\ 
\hline
\end{tabular}
\end{center}
\end{table}

最初に「文節区切り情報の利用」「対訳辞書の利用」「人手による確認」の
改良効果を確かめるための実験を行った．表\ref{従来手法}に結果を示す．表\ref{従来手法}の
\verb|<1>|は従来手法，\verb|<2>|は文節区切り情報を利用した場合(2.2節のアルゴリズムに
3節の提案アルゴリズムのステップ(1)-2，(2)-1のみを適用した場合），\verb|<3>|は
文節区切り情報と対訳辞書を利用した場合（\verb|<2>|に対して3節の提案アルゴリズムの
ステップ(1)-1，(5)-2を適用した場合），\verb|<4>|は，さらに人手による確認を行った場合
(\verb|<3>|に対してステップ(5)-3-aの代わりにステップ(5)-3-bを適用した場合)の結果である．
いずれも対応度の計算には重み付きDice係数を用いた．\verb|<4'>|は重み付きDice係数の
代わりに多対多の対応数を考慮したLog-likelihoodを用いた結果である．表中の$f_{min}=2$は，
出現回数を2回までとして抽出した場合，$f_{min}=1$は出現回数を1回までとして
抽出した場合の結果である．但し，$f_{min}=1$では，$f_{je}=f_{j}=f_{e}=2$における対応度
より小さく，$f_{je}=f_{j}=f_{e}=1$における対応度以上の値を持つ対訳表現が抽出されるが，
$f_{je}=f_{j}=f_{e}=1$の対訳表現は抽出精度が極めて低いため\footnote{表\ref{従来手法}
\verb|<3>|と同様の設定で実験を行った結果，$f_{je}=f_{j}=f_{e}=1$の時の
対訳表現の精度は43\%であった．}，
今回の実験では抽出対象から除外した．以降の実験も同様である．

本結果から，提案手法\verb|<4>|の$f_{min}=2$では97\%の精度が得られることがわかる．さらに
$f_{min}=1$では，従来手法\verb|<1>|は40\%であったが，提案手法\verb|<4>|は93\%であった．
\verb|<1>|から\verb|<4>|へと文節区切り情報，辞書参照，人手確認という工夫を
追加していくことにより，抽出総数は減少していくが，カバレッジの低下を伴わない．
これは，提案手法が間違った対訳表現のみを除去するフィルタリングの働きとして効果的に
機能していることを示している．


\subsection{対応度の評価式の違いによる比較}
\label{評価式違いによる実験}

次に対応度の評価式の違いによる結果を比較する．本結果を表\ref{対応度評価式}に記す．
ここでは，表\ref{従来手法}\verb|<4>|と同じ設定で，適用する評価式を変える． 

\begin{table}[t]
\caption{対応度評価式の違いによる比較}
\label{対応度評価式}
\begin{center}
\begin{tabular}{l|l||r|r|r|r}
\hline
\multicolumn{2}{c||}{} 
& \multicolumn{1}{c|}{\verb|<1>|Dice}
& \multicolumn{1}{c|}{\verb|<2>|d-Dice}
& \multicolumn{1}{c|}{\verb|<3>|Loglike}
& \multicolumn{1}{c} {\verb|<4>|d-Loglike}\\
\hline
          & 合計    & 2,981   & 2,934   & 3,791   & 3,796   \\
          & 正解数  & 2,832   & 2,875   & 3,563   & 3,682   \\
$f_{min}$ & 半正解数&    82   &    30   &   152   &    76   \\\cline{2-6}
$=2$      & 精度    & 95\%(97)& 98\%(99)& 94\%(98)& 97\%(99)\\
         &カバレッジ& 80\%(15)& 78\%(13)& 83\%(16)& 81\%(13)\\
\hline
          & 合計    & 10,276  &  9,957  &  6,412  & 6,452   \\
          & 正解数  &  6,996  &  7,040  &  5,740  & 5,764   \\
$f_{min}$ & 半正解数&    335  &    333  &    222  &   236   \\ \cline{2-6}
$=1$      & 精度    & 68\%(71)& 71\%(74)& 89\%(93)& 89\%(93)\\
         &カバレッジ& 86\%(19)& 86\%(19)& 85\%(19)& 85\%(19)\\
\hline
\end{tabular}
\end{center}
\end{table}

\verb|<1>|は重み付きDice係数の評価式，\verb|<2>|は多対多の対応数を考慮した
重み付きDice係数の評価式，\verb|<3>|はLog-likelihoodの評価式，\verb|<4>|は
多対多の対応を考慮したLog-likelihoodの評価式を用いた場合の結果である．

重み付きDice係数\verb|<1>|とLog-likelihood\verb|<3>|の結果を比較した場合，$f_{min}=2$
では精度の違いはあまりみられないが，$f_{min}=1$ではLog-likelihoodの方が良い結果が
得られた．重み付きDice係数は$f_{min}=2$ではLog-likelihoodと同等の信頼性が得られるが，
$f_{min}=1$のように対応度が低い場合ではその信頼性は低いと言える．これは，両者の評価式の
性質の違いによる．重み付きDice係数は連続単語列の出現回数のみを利用した評価式であるため，
少ない出現回数の場合の計算の信頼性は低くなる．
一方，Log-likelihoodは，周辺頻度（出現/非出現の両方の回数）を利用した確率論に
基づく評価式であるため，出現回数が少ない場合でも正確に対応度を求めることができると
考えられる．

次に，多対多の対応を考慮した場合としない場合を比較する．多対多の対応を考慮することにより
$f_{min}=2$では，重み付きDice係数，Log-likelihoodともに3\%向上した．しかし，
$f_{min}=1$では，Dice係数では3\%向上したものの，Log-likelihoodでは差がみられなかった．
重み付きDice係数では，対応度の高低にかかわらず，評価式の欠点を補い，多対多の対応の
考慮が有効に働いている．しかし，Log-likelihoodは，対応度が高い場合では信頼性の
高い対訳表現の対応度を上げ，抽出精度を高める効果を発揮するが，対応度が低い場合では
周辺頻度を利用した確率計算が有効に働いているため，評価式の欠点を補うほどの効果を
得ることはできなかったと考えられる．

\subsection{対訳文書の性質による影響}
\label{文書性質の実験}

\begin{table}[t]
\caption{文の位置及び文書の違いによる比較}
\label{文書の違い}
\begin{center}
\begin{tabular}{l|l||r|r|r||r}
\hline
\multicolumn{2}{c||}{} 
& \multicolumn{1}{c|}{\verb|<1>|先頭部}
& \multicolumn{1}{c|}{\verb|<2>|中間部}
& \multicolumn{1}{c||}{\verb|<3>|後部}
& \multicolumn{1}{c} {\verb|<4>|取引}\\
\hline
\multicolumn{2}{l||}{対訳文数}
                    & \multicolumn{3}{c||}{8,000} & 9,045  \\ \hline
\multicolumn{2}{l||}{英語単語総数}
                    & 175,768 & 193,284 & 200,707& 158,652 \\
\multicolumn{2}{l||}{(英語出現単語数)}
                    &(7,687)  & (8,866) & (9,355)& (2,746) \\ \hline
\multicolumn{2}{l||}{日本語単語総数}
                    & 209,709 & 221,119 & 228,949& 222,737 \\
\multicolumn{2}{l||}{(日本語出現単語数)}
                    &(9,853)  & (12,012)&(13,000)& (3,052) \\ \hline

          & 合計    & 3,796   & 3,631   & 3,490   & 2,980   \\
          & 正解数  & 3,682   & 3,485   & 3,350   & 2,741   \\
$f_{min}$ & 半正解数&    76   &   109   &   105   &    90   \\\cline{2-6}
$=2$      & 精度    & 97\%(99)& 96\%(99)& 96\%(99)& 92\%(95)\\
         &カバレッジ& 81\%(13)& 76\%(12)& 75\%(11)& 92\%(22)\\
         &所要時間  &   42m   & 1h29m   & 3h21m   &   43m   \\\cline{2-6}
\hline
          & 合計    &  6,452  &  6,588  &  6,676  & 4,631   \\
          & 正解数  &  5,764  &  5,640  &  5,613  & 3,113   \\
$f_{min}$ & 半正解数&    236  &    335  &    351  &   271   \\ \cline{2-6}
$=1$      & 精度    & 89\%(93)& 86\%(91)& 84\%(89)& 67\%(73)\\
         &カバレッジ& 85\%(19)& 81\%(16)& 78\%(15)& 94\%(34)\\ \cline{2-6}
         &所要時間  & 1h06m   & 4h58m   & 7h58m   &  1h26m  \\ \cline{2-6}
     &辞書登録語率  & 63\%    & 61\%    & 59\%    & 39\%    \\ \cline{2-6}
    &辞書登録語精度 & 97\%    & 96\%    & 96\%    & 92\%    \\
    &正解数/総数    & 3,924/4,054 & 3,832/4,000 & 3,755/3,917 & 1,657/1,798\\ \cline{2-6}
   &辞書未登録語精度& 77\%        & 70\%        & 67\%        & 52\%       \\
    &正解数/総数    & 1,840/2,398 & 1,808/2,588 & 1,858/2,759 & 1,466/2,833\\
\hline
\end{tabular}
\end{center}
\end{table}

実験で用いた「日英新聞記事対応付け結果」\cite{内山:2003}は，対訳辞書を用いた対応度の
計算結果に基づいて，英語の文と日本語の文の自動対応付けを行っており，その対応度の
高い順に文が並び替えられている．したがって「日英新聞記事対応付け結果」の
先頭部分には，対応が明らかな対訳語を多く含む対訳文が多いのに対して，後半になるほど
対訳関係が不明瞭な対訳文が多くなる．この対訳文の性質が抽出精度にどれだけ影響を
及ぼすかを調べた．表\ref{文書の違い}は「日英新聞記事対応付け結果」において，
先頭から8,000文(先頭部\verb|<1>|)，8,001文目から8,000文(中間部\verb|<2>|)，24,001文目から
8,000文(後部\verb|<3>|)を対象として実験した結果である．また比較のため最右部に
機械ではなく人手で対応付けた取引条件に関する対訳文書\cite{石上:1992}
(以下，取引条件文とよぶ)の9,045文における結果を示す．なお，全ての実験は
\ref{評価式違いによる実験}節の表\ref{従来手法}の\verb|<4>|の条件と同じ
実験環境(文節区切り情報利用，辞書参照，多対多の対応数を考慮したLog-likelihood評価式を
利用)で行った．

文の対応度が高い文書ほど抽出精度は高い．また，取引条件文を用いた場合，カバレッジは
新聞記事を用いたいずれの結果より高かったが，その精度は67\%と劣っていた．この理由は，取引
条件文は専門用語が多く，出現する用語が偏っているため，連続単語列の組み合わせに
要する時間は少なくてすむが，その一方で，類似する文が多く，組み合わせの曖昧性が
増えるため，低い対応度での精度は低くなったと考えられる．

表\ref{文書の違い}の最下部に，抽出結果において，対訳辞書を参照することにより抽出された
対訳表現(\ref{提案}章の提案アルゴリズムのステップ(5)-2で抽出された対訳表現)とそれ以外の対訳表現
(ステップ(5)-3-aで抽出された対訳表現)の精度及び抽出語数を記した．新聞記事では後部に
なるほど，対訳辞書では対訳関係が認められない対訳表現の割合が増え，その影響で全体の精度も
低くなった． 
取引条件文では，専門用語が新聞記事に比べて多いため，対訳辞書では対訳関係が
認められない対訳表現の割合が高く，精度が低い結果となった．

一方，表\ref{文書の違い}には記載していないが，表\ref{文書の違い}\verb|<1>|の設定で，対訳
辞書を参照せず，対訳表現を抽出した．その抽出結果に対して，ステップ(5)-2と同じ手法を用いて，
対訳表現と認められるものと，そうでないものに分類した結果，前者の精度は97\%，後者の精度は
71\%となった．
表\ref{文書の違い}\verb|<1>|で，対訳辞書を参照することにより抽出された対訳表現の精度は97\%，
そうでない対訳表現の精度は77\%であり，予想通りの効果が得られている．

\subsection{文書分割における影響}
\label{文書分割実験}

図\ref{グラフ}は，出現回数2回($f_{min}=2$)における連続単語列の組み合わせ数が，対訳文数の
増加によってどのように増加するかを示すグラフである．


\begin{figure}[t]
  \begin{center}
    \epsfxsize=13cm \epsfbox{./figure/figure5.eps}
  \end{center}
  \caption{出現回数2回における対訳表現候補対の総数}
  \label{グラフ}
\end{figure}

「従来手法」は従来手法 (表\ref{従来手法}\verb|<1>|と同じ設定)の場合「文節区切り」は
文節区切り情報を利用した場合(表1\verb|<2>|と同じ設定)「分割」は従来手法に対して，
文書分割の手法\footnote{4,000文までは500文単位で分割し，それ以上は4,000文単位で
分割した．また，$f_{merge}=3$とした．}のみを採用した場合（従来手法に対してステップ(1)-3，(7)-2を
適用した場合）「分割+文節」は文書分割の手法と文節区切り情報を利用した場合
「分割+文節+辞書」はさらに辞書を参照した場合の結果を示している．

図5から，文節区切り情報の利用と文書分割は計算量の削減に寄与しており，この両方を用いる
ことにより，より大きな削減効果が得られることがわかる．一方，辞書の参照は，組み合わせ数を
削減する効果はない．この理由は，辞書参照によって正しいと認められた対訳表現に関係する
候補を削除する働きはあるものの，辞書を参照しない場合でも同様の削除処理が行われている
ためである．さらに「分割+文節+辞書」の手法に対して，人手による確認工程を加えた手法に
ついて同様の実験を行ったが，その結果も「分割+文節+辞書」の結果とほぼ等しくなり，組み
合わせ数の削減効果は見られなかった． 


\begin{table}[t]
\caption{文書分割手法の分割数による比較}
\label{文書分割手法}
\begin{center}
\begin{tabular}{l|l||r|r|r|r|r|r|r|r}
\hline
\multicolumn{2}{c||}{} 
& \multicolumn{1}{c|}{\verb|<1>|}
& \multicolumn{1}{c|}{\verb|<2>|}
& \multicolumn{1}{c|}{\verb|<3>|}
& \multicolumn{1}{c|}{\verb|<4>|}
& \multicolumn{1}{c|}{\verb|<5>|}
& \multicolumn{1}{c|}{\verb|<6>|}
& \multicolumn{1}{c|}{\verb|<7>|}
& \multicolumn{1}{c} {\verb|<8>|}\\
\cline{3-10}
\multicolumn{2}{c||}{} 
& \multicolumn{4}{c|}{8,000文}
& \multicolumn{4}{c}{16,000文}\\
\cline{3-10}
\multicolumn{2}{c||}{} 
& \multicolumn{3}{c|}{分割有}
& \multicolumn{1}{c|}{\raisebox{-1.5ex}[0cm][0cm]{分割無}}
& \multicolumn{3}{c|}{分割有}
& \multicolumn{1}{c}{\raisebox{-1.5ex}[0cm][0cm]{分割無}}\\
\cline{3-5}\cline{7-9}
\multicolumn{2}{c||}{} 
& \multicolumn{1}{c|}{8分割}
& \multicolumn{1}{c|}{4分割}
& \multicolumn{1}{c|}{2分割}
& \multicolumn{1}{c|}{}
& \multicolumn{1}{c|}{8分割}
& \multicolumn{1}{c|}{4分割}
& \multicolumn{1}{c|}{2分割}
& \multicolumn{1}{c}{}\\
\hline
& 合計    & 3,830   & 3,813   & 3,793   & 3,796 & 6,124   & 6,124   & 6,076   & 6,124   \\
& 正解数  & 3,715   & 3,698   & 3,679   & 3,682 & 5,879   & 5,880   & 5,833   & 5,879   \\
$f_{min}$ 
& 半正解数&    77   &    76   &    76   &    76 &   183    &  186   &   181   &  149    \\
\cline{2-10}
$=2$      
&\raisebox{-1.5ex}[0cm][0cm]{精度}    
          & 97\%& 97\%& 97\%& 97\%&96\%& 96\%& 96\%& 96\%\\
&         & (99)& (99)& (99)& (99)&(99)& (99)& (99)& (98)\\\cline{2-10}
&\raisebox{-1.5ex}[0cm][0cm]{カバレッジ}
          & 81\%& 81\%& 81\%& 81\%&81\%  & 81\% & 81\% & 81\% \\
&         & (13)& (13)& (13)& (13)&(12)  &(12)  &(13)  &(13)  \\ \cline{2-10}
&所要時間 &  49m&  31m&  36m&  42m& 7h47m& 6h45m& 8h48m&10h21m\\
\hline

& 合計    & 6,556  &  6,461  &  6,465  & 6,452 & 10,654  &  10,581 &  10,549  & 10,473 \\
& 正解数  &  5,834 &  7,040  &  5,753  & 5,764 &  9,375  &  9,306  &  9,283  &  9,206  \\ 
$f_{min}$ 
& 半正解数&    262 &    231  &    255  &   236 &    426  &    439  &    422  &   437   \\ \cline{2-10}
$=1$      
&\raisebox{-1.5ex}[0cm][0cm]{精度} 
          & 89\%& 89\%& 89\%& 89\%& 88\%& 88\%& 88\%& 88\%\\
&         & (93)& (93)& (93)& (93)& (92)& (92)& (92)& (92)\\\cline{2-10}
&\raisebox{-1.5ex}[0cm][0cm]{カバレッジ}
          & 85\%& 85\%& 85\%& 85\%& 85\%& 85\%& 85\%& 85\%\\
&         & (19)& (19)& (19)& (19)& (18)& (18)& (18)&(18)\\ \cline{2-10}
&所要時間 &1h15m &1h00m &1h01m &1h06m &9h42m&9h04m&11h45m&16h28m\\
\hline
\end{tabular}
\end{center}
\end{table}

一方，表\ref{文書分割手法}は文書分割手法の分割数による比較結果である．
\verb|<1>|から\verb|<4>|は，実験対象として8,000文の対訳文書を用いた場合，
\verb|<5>|から\verb|<8>|は16,000文の対訳文書の場合であり，それぞれ8分割，4分割，2分割で
等分割した場合，分割しなかった場合の結果を求めた．この表からわかることは次の3点に
まとめられる．第一に，対訳文書の文数が多い方が文書分割の効果が大きい．第二に，分割が
細かすぎると逆に処理時間が遅くなる．
8,000文，16,000文共，対訳文書を4分割した時が最も速い結果となった．この理由は，分割数が
多いと対応度の計算処理の繰り返し回数が増え，この繰り返し処理のオーバーヘッドによって
遅くなったと考えられる．第三に，精度，カバレッジ共に，文書分割の影響を受けない．
しかし，文書分割により抽出される順序が変わるため，抽出される対訳表現は若干異なっている．
　
\subsection{文書サイズによる結果の比較}

\begin{table}[t]
\caption{文書サイズの違いによる比較}
\label{文書サイズ}
\begin{center}
\begin{tabular}{l|l||r|r|r|r|r|r|r}
\hline
\multicolumn{2}{c||}{} 
& \multicolumn{1}{c|}{\verb|<1>|}
& \multicolumn{1}{c|}{\verb|<2>|}
& \multicolumn{1}{c|}{\verb|<3>|}
& \multicolumn{1}{c|}{\verb|<4>|}
& \multicolumn{1}{c|}{\verb|<5>|}
& \multicolumn{1}{c|}{\verb|<6>|}
& \multicolumn{1}{c}{\verb|<7>|}\\
\multicolumn{2}{c||}{} 
& \multicolumn{1}{c|}{500}
& \multicolumn{1}{c|}{1,000}
& \multicolumn{1}{c|}{2,000}
& \multicolumn{1}{c|}{4,000}
& \multicolumn{1}{c|}{8,000}
& \multicolumn{1}{c|}{16,000}
& \multicolumn{1}{c}{32,000}\\
\hline

& 合計    & 332  & 455   & 975 & 2,123 & 3,631   & 5,319 & 14,395  \\
& 正解数  & 312  & 437   & 936 & 2,038 & 3,485   & 5,106 & 13,819  \\
$f_{min}$ 
& 半正解数&   6  &   9   &  29 &    64 &   109   &   159 &   288   \\
\cline{2-9}
$=2$      
&\raisebox{-1.5ex}[0cm][0cm]{精度}    
          & 94\%& 96\%& 96\%& 96\%&96\%& 96\%& 96\% \\
&         & (96)& (98)& (99)& (99)&(99)& (99)& (98) \\\cline{2-9}
&\raisebox{-1.5ex}[0cm][0cm]{カバレッジ}
          & 66\%& 68\%& 70\%& 73\%&76\% & 79\% & 80\% \\
&         & (14)& (13)& (11)& (12)&(12) &(12)  &(12)  \\ \cline{2-9}
&所要時間 & 35sec &2m &5m   &13m  &1h29m&16h41m&28h53m\\
\hline
& 合計    & 633 &1,284&2,230&4,048&6,588&10,704&17,368\\
& 正解数  & 550 &1,200&1,935&3,437&5,640&9,186 &14,551\\ 
$f_{min}$ 
& 半正解数&  29 &   37&  109&  200&  335&   555&  894 \\ \cline{2-9}
$=1$      
&\raisebox{-1.5ex}[0cm][0cm]{精度} 
          & 87\%& 88\%& 87\%& 85\%& 86\%&  86\%& 84\%\\
&         & (91)& (93)& (92)& (90)& (91)&  (91)& (89)\\ \cline{2-9}
&\raisebox{-1.5ex}[0cm][0cm]{カバレッジ}
          & 72\%& 73\%& 76\%& 75\%& 81\%& 83\%& 84\% \\
&         & (18)& (17)& (16)& (15)& (16)& (16)& (17)\\ \cline{2-9}
&所要時間 &  1m &  3m &  6m &  16m&4h58m&22h36m&52h35m\\
\hline
\end{tabular}
\end{center}
\end{table}

表\ref{文書サイズ}は，対訳文書のサイズの違いによって精度がどのように変化するかを記した表である\footnote{この実験では，\ref{文書性質の実験}節にみられる対訳文書の位置による影響を少なくするために，全てのサイズにおいて32,000文の中間部の文を対象とした．また，32,000文は文書分割($n=4$)の手法を採用した．}．
この表から，カバレッジは文書のサイズに影響を受け，サイズが大きくなるほど高くなるが，精度は
ほとんど影響を受けず，ほぼ一定の値をとることがわかる．

\subsection{その他の考察}


\begin{table}[t]
\caption{対訳表現抽出例}
\label{対訳表現抽出例}
\begin{center}
\begin{tabular}{c||ll|r}
\hline
評価 & \multicolumn{1}{|c}{日本語} & \multicolumn{1}{c}{英語} & \multicolumn{1}{|c}{対応度} \\ 
\hline
      & 中　・　東欧　諸国     & the CEECs            & 11.48 \\
 正解 & 洗練　する　られる     & sophisticated        &  7.61 \\
      & コモ　ン　ハウス       & the common house     &  6.62 \\
\hline
      & 冷戦                   & war                  & 48.45 \\
半正解& 冷戦                   & the cold             & 44.14 \\
      & に従って               & in accordance        &  1.82 \\
\hline
      & 米国                   & Washington           & 48.45 \\
不正解& いまだに               & have yet             & 44.14 \\
      & 休息　その他　の       & other work           &  6.62 \\
\hline
\end{tabular}
\end{center}
\end{table}

最後に，\ref{従来手法の実験}節の表\ref{従来手法}の\verb|<4'>|の実験環境(文節区切り情報
利用，辞書参照，人手確認有り，多対多の対応数を考慮したLog-likelihood評価式を利用)での
抽出結果の例を表\ref{対訳表現抽出例}に示す．本手法を用いることにより「中・東欧諸国：the CEECs」
のような辞書には存在しない多くの専門用語を抽出することができる．また，形態素解析では
未知語になる「コモンハウス： the common house」の例のような単語も数多く抽出することができる．
半正解の原因は「冷戦：the cold」「冷戦：war」のような文節区切りによる悪影響である．
これは将来的には，原言語の単語列に対して複数の目的言語の単語列が対応付けられている場合には
元の文を参照することにより正しい対訳表現に復元することができると考えている．

一方，間違った原因は，対訳辞書参照による悪影響と，人手確認による誤りである．
例えば「休息その他の:other work」の対訳表現は「その他:other」が対訳辞書に
登録されているために抽出された．今回の実験では，連続単語列を構成する英語と日本語の
自立語単語の組合せにおいて1つでも対訳辞書登録語があれば，対訳表現として抽出するようにした．
しかし，上記の例では「その他」と ``other''は登録語であるが「休息」と ``work''は登録語
ではない．
将来的には，辞書参照の方法をより厳格にし，その連続単語列ペアが対訳辞書登録語によって
過不足なく対応付けられる場合のみ対訳表現と判定する，または，利用する対訳辞書をより
大規模なものにして完全一致でも辞書参照の効果が得られるようにするなどの工夫が求められる．

また，大量の対訳表現の確認は作業者のミスを招く．「米国:Washington」は作業者の
ミスにより抽出された．対訳辞書の拡張，改良等により，できるだけ多くの信頼性の高い対訳表現を
自動的に検知し，作業者の負担を軽減させることも必要である．

最後に，触れておかねばならないのは，人手確認における作業コストである．抽出された
対訳表現を翻訳辞書として利用するためには，最終的に抽出された対訳表現が正しいか否かを
再確認し，選定する必要がある．

処理途中に人手による確認を行わない場合では，表\ref{文書の違い}\verb|<1>|の結果のように，
$f_{min}=2$では3,796語，$f_{min}=1$では6,452語を抽出処理終了後に人手により確認し，
正しい対訳表現のみを選択しなければならない．
一方，処理途中に人手による確認を行う場合では，$f_{min}=2$では処理途中に681語，処理終了後に
人手未確認分の2,886語，合計3,567語を確認し，正しい対訳表現のみを選択しなければならない．
また$f_{min}=1$では，処理途中に2,084語，処理終了後に人手未確認分の4,353語，合計6,437語を確認し，
正しい対訳表現のみを選択しなければならない．

このように確認すべき語数においては有意な差はみられない結果となったが，
表\ref{文書の違い}\verb|<1>|と表\ref{従来手法}\verb|<4'>|の結果にみるように，処理途中に
人手確認をした方が，最終的な精度が高く，正解語数も増えている．精度が高くなることにより，
処理終了後の削除の手間も削減されることから，人手による確認工程を処理途中に設ける方法は，
作業コスト削減の効果があるといえる．

\section{関連研究}
\label{関連研究}

我々の手法の特徴は，言語資源を効果的に利用することにより，低出現回数の対訳表現を
抽出することができるという点にある．言語資源を利用する手法には，\cite{Melamed:1995}，
\cite{Al-Onaizan-Kevin:2002} がある．一方，低出現回数の対訳表現を抽出する手法には，
\cite{Moore:2003}，\cite{佐藤2002,佐藤2003}がある．

\cite{Melamed:1995} は，対訳辞書，品詞，語源情報，構文情報の4種類の情報によって，抽出
すべき対訳表現をフィルタリングしている．フィルタリングという点では我々の手法と
似ているが，異なる点は我々の手法は複数の単語列からなる対訳表現候補を抽出対象と
しているのに対し，Melamedの手法は，単語対応に限定している点である．単語対応の
場合は，その組み合わせ数は少なく，言語資源の利用の際にも計算量を考慮する必要は
ない．しかし，任意長の長さの表現を対象にする場合，計算量をできるだけ抑え，資源を
利用するような仕組みが必要となる．我々は，信頼性の高い対訳表現から段階的に抽出する
という漸進的な手法を活かし，処理の途中に対訳辞書利用や人手介入を行うことにより
任意長の対訳表現の抽出の際に起こりがちな計算量の問題を解決している．

\cite{Al-Onaizan-Kevin:2002}は，言語資源としてWebページのような大規模な生成側の
単言語テキストや，トランスリタレーション(音表記)情報を利用している．Al-Onaizanらが
対象にしている言語は，アラビア語と英語であり，両者のような異なる言語族の２言語を
抽出対象とする場合，対応の規則を抽出することが難しく既存の言語知識をいかに効率良く
利用するかが重要となる．この点では我々のアプローチと似ており，Al-Onaizanの手法は
我々の手法にも応用することができる．例えば，我々の手法での人手確認の代わりに
Web上での検索を利用することができる．また，カタカナ表記語はトランスリタレーション情報を
用いて，対応度を再評価する等が考えられる．

一方，\cite{Moore:2003}の手法は，3段階の学習モデルを用いることによって，対応度の精度を
高めていきながら対訳表現を抽出する手法であり，低出現回数の対訳表現も抽出することが
できる．ある語とその訳語は常に一対一の関係にあるという前提や先頭文字種情報などの
表層的な情報を学習モデルとして利用することにより，出現回数が1回の対訳表現でも
精度良く抽出することができる．この手法は，統計モデルと表層的な言語特徴情報のみを
利用し，辞書や形態素解析結果などの既存の言語知識を利用しないため，専門用語の抽出も
可能である．しかし，上述した前提や先頭文字種情報は専門用語の翻訳の特徴であり，
イディオムや慣用句などの抽出精度は下がる．我々の手法では形態素解析結果を利用するが，
対訳表現として適切でない単語列を除去するために利用するに過ぎないので，表\ref{対訳表現抽出例}の
「コモンハウス:the common house」等の専門用語も抽出することができる．

佐藤は，最大エントロピー法\cite{佐藤2002}やSVM\cite{佐藤2003}を利用して，少ない文書でも
高精度で抽出する方法を提案している．しかし，これらの手法は学習用の対訳文書が必要となる．
また，抽出対象を句単位と限定することにより，検索対象を絞っている．

最後に， \cite{山本2001}は，我々の手法と同様，統計的係り受け解析結果を用いて
漸進的な手法で対訳表現抽出を行う．しかし，彼らは文節を越えた構造的な対訳表現を
抽出することを目的としているのに対し，我々は候補とする連続単語列を文法的に意味のある範囲に
限定し抽出間違いを減らすことを目的とする．

\section{おわりに}
\label{まとめ}

本稿は，文節区切り情報や対訳辞書を利用する，人手による確認工程を設ける，などの種々の
手法を組み合わせることによって実用性を高めた対訳表現抽出手法を提案した．また，
(1)従来手法との比較，(2)対応度の評価式の違いよる比較，(3)文書の性質の違いによる比較，
(4)文書分割手法の違いによる比較，(5)文書サイズの違いによる比較，という5つの比較実験により，
その効果を確認した．

従来手法との比較実験では，文節区切り情報利用，辞書参照，人手確認という手法が，間違った
対訳表現の抽出を排除するためのフィルタリングの機能を果たすことを確認した．対応度の
評価式の違いによる実験では，対応度が低い場合($f_{min}=1$の場合)の対訳表現の抽出には重み付き
Dice係数よりLog-likelihoodが優れており，多対多の対応数の考慮による改良は，重み付き
Dice係数には効果的だがLog-likelihoodには効果が小さいことがわかった．文書の性質，及び，
文書サイズの違いによる比較実験では，精度は文書のサイズには影響を受けないが，文書の
専門性の高さや使用されている単語数などの対訳文書の性質に影響を受けやすいことがわかった．
一方，カバレッジは文書のサイズ，性質共に影響を受けやすいことがわかった．最後に，文書
分割手法の違いによる実験では，文書分割は連続単語列の組み合わせ数を削減し，計算時間を
短縮させることができるが，分割が細かすぎると，逆に繰り返し処理のオーバーヘッドを生じ，
計算時間が長くなることがわかった．

8,000文の対訳文書による実験では，従来手法では精度40\%，カバレッジ79\%であったのに対し，
提案手法では人手による確認工程がある場合では精度96\%，カバレッジ85\%で抽出することができた．
人手による確認を行わない場合でも，8,000文では，精度89\%，カバレッジ85\%で抽出することができる．

我々が，完全自動でなく，半自動という立場をとり，精度を重視している理由の一つは，
抽出結果を辞書として機械翻訳システムに直接利用することを想定しているためである．
今後は，本手法で抽出した対訳表現を機械翻訳システム\cite{Kitamura-Murata:2003}の
辞書として利用し，機械翻訳支援機能として本手法を評価することを計画している．


\acknowledgment

本研究は、通信・放送機構平成17年度基盤技術研究促
進制度に係る研究開発課題「多言語標準文書処理システ
ムの研究開発」の一環として行われている。


\bibliographystyle{jnlpbbl}
\begin{thebibliography}{}

\bibitem[\protect\BCAY{Al-Onaizan \BBA\ K.}{Al-Onaizan \BBA\
  K.}{2002}]{Al-Onaizan-Kevin:2002}
Al-Onaizan, Y.\BBACOMMA\  \BBA\ K., K. \BBOP 2002\BBCP.
\newblock \BBOQ Translating Named Entities Using Monolingual and Bilingual
  Resources\BBCQ\
\newblock In {\Bem Proceedings of 40th Annual Meeting of the Association for
  Computational Linguistics (ACL-2002)}, \BPGS\ 400--408.

\bibitem[\protect\BCAY{Dunning}{Dunning}{1991}]{Dunning:1993}
Dunning, T. \BBOP 1991\BBCP.
\newblock \BBOQ Accurate methods for statistics of surprise and
  coincidence\BBCQ\
\newblock {\Bem Computational Linguistics}, {\Bbf 19}  (1), 61--74.

\bibitem[\protect\BCAY{石上}{石上}{1992}]{石上:1992}
石上進 \BBOP 1992\BBCP.
\newblock \Jem{取引条件表現法辞典電子ブック版第1巻物品取引}.
\newblock 国際事業開発株式会社.

\bibitem[\protect\BCAY{北村\JBA 松本}{北村\JBA 松本}{1997}]{北村97}
北村美穂子\JBA  松本裕治 \BBOP 1997\BBCP.
\newblock \JBOQ {対訳コーパスを利用した対訳表現の自動抽出}\JBCQ\
\newblock \Jem{情報処理学会論文誌}, {\Bbf 38}  (4), 727--736.

\bibitem[\protect\BCAY{Kitamura \BBA\ Murata}{Kitamura \BBA\
  Murata}{2003}]{Kitamura-Murata:2003}
Kitamura, M.\BBACOMMA\  \BBA\ Murata, T. \BBOP 2003\BBCP.
\newblock \BBOQ Practical Machine Translation System allowing Complex
  Patterns\BBCQ\
\newblock In {\Bem Proceedings of MT Summit IX}, \BPGS\ 232--239.

\bibitem[\protect\BCAY{Matsumoto \BBA\ Utsuro}{Matsumoto \BBA\
  Utsuro}{2000}]{Matsumoto-Utsuro:2000}
Matsumoto, Y.\BBACOMMA\  \BBA\ Utsuro, T. \BBOP 2000\BBCP.
\newblock \BBOQ Lexical Knowledge Acquisition\BBCQ\
\newblock In {\Bem Handbook of Natural Language Processing}, \BPGS\ 563--610.
  Marcel Dekker.

\bibitem[\protect\BCAY{Melamed}{Melamed}{1995}]{Melamed:1995}
Melamed, I. \BBOP 1995\BBCP.
\newblock \BBOQ Automatic Evaluation and Uniform Filter Cascades for Inducing
  N-best translation lexicons\BBCQ\
\newblock In {\Bem Proceedings of 3rd Annual Workshop on Very Large Corpora
  (WVLC-95)}, \BPGS\ 184--198.

\bibitem[\protect\BCAY{Moore}{Moore}{2003}]{Moore:2003}
Moore, R. \BBOP 2003\BBCP.
\newblock \BBOQ Learning Translations of Named-Entity Phrases from Parallel
  Corpora\BBCQ\
\newblock In {\Bem Proceedings of 16th Conference of the European Chapter of
  the Association for Computational Linguistics (EACL-2003)}, \BPGS\ 259--266.

\bibitem[\protect\BCAY{佐藤健吾\JBA 斉藤博昭}{佐藤健吾\JBA
  斉藤博昭}{2002}]{佐藤2002}
佐藤健吾\JBA  斉藤博昭 \BBOP 2002\BBCP.
\newblock \JBOQ {最大エントロピー法を用いた対訳表現の抽出}\JBCQ\
\newblock \Jem{自然言語処理}, {\Bbf 9}  (1), 101--115.

\bibitem[\protect\BCAY{佐藤健吾\JBA 斉藤博昭}{佐藤健吾\JBA
  斉藤博昭}{2003}]{佐藤2003}
佐藤健吾\JBA  斉藤博昭 \BBOP 2003\BBCP.
\newblock \JBOQ {サポートベクターマシンを用いた対訳表現の抽出}\JBCQ\
\newblock \Jem{自然言語処理}, {\Bbf 10}  (4), 109--124.

\bibitem[\protect\BCAY{内山将夫\JBA 井佐原均}{内山将夫\JBA
  井佐原均}{2003}]{内山:2003}
内山将夫\JBA  井佐原均 \BBOP 2003\BBCP.
\newblock \JBOQ {日英新聞の記事および文を対応付けるための高信頼性尺度}\JBCQ\
\newblock \Jem{自然言語処理}, {\Bbf 10}  (4), 201--220.

\bibitem[\protect\BCAY{山本\JBA 松本}{山本\JBA 松本}{2001}]{山本2001}
山本薫\JBA  松本裕治 \BBOP 2001\BBCP.
\newblock \JBOQ {統計的係り受け解析結果を用いた対訳表現抽出}\JBCQ\
\newblock \Jem{情報処理学会論文誌}, {\Bbf 42}  (9), 2239--2247.

\bibitem[\protect\BCAY{Yamamoto \BBA\ Matsumoto}{Yamamoto \BBA\
  Matsumoto}{2003}]{Yamamoto-Matsumoto:2003}
Yamamoto, K.\BBACOMMA\  \BBA\ Matsumoto, Y. \BBOP 2003\BBCP.
\newblock \BBOQ Extracting Translation Knowledge from Parallel Corpora\BBCQ\
\newblock In {\Bem Recent Advances in Example-based Machine Translation},
  Kluwer Text, Speech and Language Technology Series, \BCH~13, \BPGS\ 365--396.
  Kluwer Academic Publishers.

\end{thebibliography}


\begin{biography}
\biotitle{略歴}
\bioauthor{北村 美穂子}{
1987年奈良女子大学理学部生物学科卒業．
1995年奈良先端科学技術大学院大学情報科学研究科博士前期課程終了．
2004年同大学院博士後期課程修了．
現在、沖電気工業株式会社 研究開発本部に勤務し，機械翻訳の研究に従事．
工学博士．情報処理学会，ACL各会員．
}
\bioauthor{松本 裕治}{
1977年京都大学工学部情報工学科卒業．
1979年同大学大学院工学研究科修士課程情報工学専攻修了．
同年電子技術総合研究所入所．1984〜85年英国インペリアルカレッジ客員研究員．
1985〜87年(財)新世代コンピュータ技術開発機構に出向．
京都大学助教授を経て，1993年より奈良先端科学技術大学院大学教授，現在に至る．
工学博士．
情報処理学会，人工知能学会，日本ソフトウェア科学会，認知科学会，AAAI，ACL，ACM各会員．
}

\bioreceived{受付}
\biorevised{再受付}
\bioaccepted{採録}

\end{biography}

\end{document}

