    \documentclass[japanese]{jnlp_1.4}
\usepackage{jnlpbbl_1.1}
\usepackage[dvips]{graphicx}
\usepackage{amsmath}
\usepackage{hangcaption_jnlp}
\usepackage{udline}
\setulminsep{1.2ex}{0.2ex}
\let\underline


\Volume{16}
\Number{1}
\Month{January}
\Year{2009}

\received{2008}{6}{2}
\revised{2008}{7}{11}
\rerevised{2008}{10}{14}
\accepted{2008}{10}{20}

\setcounter{page}{101}

\jtitle{辞書からの上位語情報抽出とオントロジー自動生成}
\jauthor{鈴木　　敏\affiref{Author_1}}
\jabstract{
辞書の定義文を基にした上位語情報の抽出手法を提案し，その結果に基づく単語オン
トロジーの自動生成を試みた．
提案するのは再帰的語義展開による情報抽出手法である．
本手法では定義文を再帰的に展開し，巨大な単語集合として定義文を再定義する．
このとき，定義文中に上位語が含まれるという仮定を利用すれば，
非常に多くの単語を上位語候補とすることができる．
この手法では上位語となる尤もらしさの指標を得ることができるため，
これを利用して多数の候補の中から上位語を効率よく選択できるようになる．
本手法を適用した上位語抽出実験では，構文解析を用いた既存手法
を上回る精度を示した．
更に本論文では，取り出された上位語情報を用いて単語オントロジーの自動生成を試みた．
自動生成の手法はまだ完全なものではないが，実験結果は上位語情報の有用性を示すものであり，今後のオントロジー自動生成の可能性を示している．
}
\jkeywords{辞書，上位語，オントロジー，自動生成}

\etitle{Information Extraction of Hypernyms and Ontology \\
	from Dictionaries}
\eauthor{Satoshi Suzuki\affiref{Author_1}} 
\eabstract{
This paper proposes a method for information extraction of hypernyms
from dictionaries, and presents a result of automatic construction of
word ontology based on the extracted information.
The method recursively expands word definitions
to get much larger word sets, 
which will be candidates of hypernyms of the headwords.
At the same time, this method gives likelyhood of the candidates for hypernyms,
which is useful for selecting hypernyms from the candidates.
Computational experiments showed that the proposed method gives better results
than an existing method, which regards HEAD as hypernyms by parsing the explanatory notes.
Additionally, we tryed to create a word ontology with the resulting hypernyms.
This method is still underconstruction, but the results showed effectiveness
of the resulting hypernyms, and showed possibility of 
entirely automatic construction of word ontology.
}
\ekeywords{dictionary, hypernym, ontology, information extraction}

\headauthor{鈴木}
\headtitle{辞書からの上位語情報抽出とオントロジー自動生成}


\affilabel{Author_1}{日本電信電話株式会社 NTTコミュニケーション科学基礎研究所}{NTT Communication Science Laboratories, NTT corporation}



\begin{document}
\maketitle


\section{まえがき}
\label{sec:intro}

単語オントロジーは自然言語処理の基礎データとして様々な知識処理技術に利用され
ており，その重要性は年々高まっている． 


現在広く知られている日本語オントロジーとしては，例えば日本語語彙大系\cite{goitaikeij}等が挙げ
られる．
日本語語彙大系は人手により編集された大規模オントロジーであり，
約3,000の意味カテゴリーを木構造状に分類し，
約40万語を各意味カテゴリーに割当てている．
しかしながら，これらは翻訳への適用を主な目的として作成されており，
利用目的によっては必ずしも適切な分類とはならない．
言い換えれば，オントロジーは利用目的に応じて異なるものが求められるのである．

ところが，オントロジーの作成には膨大な労力が必要であり，
また，言葉が日々進化するものであることを考えると，特定目的に応じた
オントロジー作成を人手で行うことは現実的に不可能である．
従って，オントロジーの生成は自動化されることが望まれる．

 そこで本論文ではオントロジー自動生成手法の検討を行う．
技術的検討を行う上では特定目的のオントロジー生成よりも，むしろ一般の
オントロジーを取り扱う方が検証を行いやすい．
従って，本論文ではオントロジー自動生成の第一歩として，日本語語彙大系のような
一般的なオントロジーの自動生成を目的とし，検討を進めることとする．

オントロジーは単語の意味的関連性を表すものであり，
この点からみると，基礎となるデータは共起情報を与えるコーパスよりも単語の意味を
直接定義している辞書（国語辞典）の方が適していると考えられる．

辞書を用いた関連性抽出の例を挙げると，例えば，鶴丸らは辞書の定義文のパターン抽出により上位語の同定が可能であることを示している\cite{tsurumaru1991}．
また，オントロジーの自動獲得の試みも行われており，例えばNicholsらは定義文中
に上位語が含まれているという仮定の下での単語階層化手法を提案している\cite{Nichols:Bond:2005}．

上記の手法は，定義文を構文解析し，その主辞を上位語とするものであるが，
必ずしも定義文の主辞が上位語であるとは限らないため，決定論的に
上位語を決めてしまうとオントロジー生成時に矛盾を引き起こすことになる．
従って，決定論的に上位語を決めるのではなく，順位づけられた上
位語候補を取り出すことが望まれる．
しかしながら，辞書の短い定義文からこれを行うことは
難しい．

一方で，上位語を抽出する方法として，コーパスから''is-a''構造等を取り出すと
いう方法がある． 
この方法は統計量が大きければ信頼性の高い情報が得られる一方，基本的な単語
が網羅される保証はなく，単語の偏りが起こる可能性が高い． 
また，Snowらは''is-a''構造を
持つデータを利用してオントロジーを構築する手法を提案しているが
\cite{Snow06}，この手法は既存のオントロジーに単語を追加する手法としては有
効であるが，オントロジーの骨格をゼロから作り上げることには向いていない． 

このように，上位語抽出とオントロジー構築にはそれぞれの課題が
あり，
オントロジーを生成するためには，
上位語の抽出方法と上位語候補を用いたオントロジー生成手法とを分けて
考えるべきであり，まず適切な上位語情報を抽出することが重要である．
以上の点から，本論文では，順位付け可能な上位語情報を取り出し，そ
の情報を利用した最適化学習によりオントロジーを生成することを目指す．


ところで，鈴木は辞書の定義文を再帰的に展開することで
カバー率の非常に大きい単語類似度計算手法を提案している\cite{Suzuki2,Suzuki3j}．
この方法によると，辞書の定義文を仮想的に巨大な単語集合と見なすことができ，
各単語の出現頻度は確率として与えられるため，上位語候補の不足を解決できる
可能性がある．

そこで本論文では上位語情報の抽出を主な目的とし，
辞書の定義文を巨大な単語集合として再定義することにより
上位語侯補を増やすという手法を試みる．
提案手法では，定義文中に上位語が含まれるという前堤を保ちつつ，大きな単
語集合の中から上位語候補を確率的指標を伴った形でリストアップする．
即ち，辞書の定義文を基に，上位語の尤もらしさを数値として表す手
法を提案する． 
更に，この上位語候補情報を利用したオントロジー自動生成も試みる．
本論文に示す自動生成手法は簡易的なものであるが，
前述の上位語候補情報の効果を確認するには非常に有効である．

以下，確率モデルによる定義文の拡張方法を簡単に説明し，
この手法により
一般的な国語辞典から上位語候補が確率的指標と共
に取り出せることを示す． 
また同時に，従来手法との比較も行い，その有効性を検証する．
次に，この指標の利用例としてオントロジー自動生成手法を
提案し，この手法に上記指標を適用した結果を示す．


\section{辞書からの上位語情報抽出}
\label{sec:hyperinfo}

まずはじめに，辞書から単語の上位語情報を取り出すことを考える．
ここで言う上位語情報とは，上位語と相関のある数値情報のことであり，上位語を一
意に決定するものではない．

一般に定義文中に同じ単語が複数回現れることは稀なので，そのま
までは単語間で差を付けることはできない．
そこで，定義文を再帰的に展開し，拡張した定義文の中での出現頻度の差を利用
することにする．
この再帰的展開は適当な回数の展開を設定しても良いが，
本稿は上位語抽出方法として鈴木による再帰的語義展開手法
\cite{Suzuki2,Suzuki3j}を利用することにする．
この手法によれば，定義文を無限に再帰的に展開することにより巨大な仮想定義文を
生成し，そこから頻度情報を取り出すことができる．
辞書の定義文が見出語に意味を与えるためのものであるとすれば，上位語は単語
に意味付けするために非常に重要な要素である．
従って，この仮想定義文中には上位語が高い頻度で出現することを期待できる．


\subsection{再帰的語義展開}
\label{sec:method}


再帰的語義展開の基本的な考えは，定義文中の単語頻度を再帰的に展開し，より多く
の単語からなる定義文を作成するということである． 

辞書（国語辞典）は見出語と定義文の組合せから成り立っている．
定義文は単語の集合であり，これを見出語の集合とみなせば，一つの定義文を
複数の定義文の集合として再定義することができる．
ところが，このような展開は無限に続いてしまう．
従って，展開された定義文中の単語数も無限になり，頻度計算は一般に不可能に
なる．
しかしながら，
定義文の展開を行なう毎に一定の割合でその影響が小さくなるとす
れば，無限に展開された定義文の影響は元の定義文に比べて微小になる．
このとき，定義文の影響力は語義展開の回数に従う等比数列として表すことが
できる．
同時に，様々な深さまで展開した定義文の集合体を考え，その総和を無限級数として計算すると必ず有限な値となる．
これにより，定義文の集合体中の単語頻度も有限になり，計算可能となる．
これらを確率モデルに置き換えることで，無限の展開を含めた定義文の集合体か
ら単語の出現確率を取り出すことが可能になり，拡張された定義文として再定義
することができる．

計算の概要を以下にまとめる．
以下，$n$回展開された定義文を$n$次の定義文と呼ぶ．
また，辞書中の定義文を0次定義文とする．

まず，見出語$w_i$と0次定義文中の単語$w_j$の関係は$P(w_j^{(0)}|w_i)$と表すものとする．
ここで，$w^{(n)}$は$n$次の定義文中の単語$w$を表す．
従って，確率$P(w_j^{(0)}|w_i)$は，見出語$w_i$の0次定義文中に現れる$w_j$の出現確率である．

この表記を用いると，各見出語に関する0次定義文中の単語の出現確率は
\begin{equation}
A=
\begin{bmatrix}
 {P\left( {w_1^{(0)}|w_1} \right)} & \cdots &\cdots &{P\left( {w_1^{(0)}|w_m} \right)} \\
 {P\left( {w_2^{(0)}|w_1} \right)}&\ddots &{}&{} \\
 \vdots &{}&\ddots &{} \\
 {P\left( {w_m^{(0)}|w_1} \right)}&{}&{}&{P\left( {w_m^{(0)}|w_m} \right)}
\end{bmatrix}
\label{eq:matrix}
\end{equation}
の列ベクトルとして表される．
ここで，$m$は辞書中の見出語の数である．
行列$A$の各要素$P(w_j^{(0)}|w_i)$は見出語$w_i$の
定義文中の単語頻度$N_i(w)$を用いて
\begin{equation}
 P(w_j^{(0)}|w_i)=\frac{N_i(w_j^{(0)})}{\sum_{all\;k}N_i(w_k^{(0)})}
\label{eq:frequency}
\end{equation}
と書ける．
このとき，全ての列ベクトルは，要素の合計が$1$であり，確率表現となっている．
さらに，この表記に従うと，$n$次定義文は$A^{n+1}$により表される．

目的とする定義文の集合体$C$は，
語義展開の度に定義文の影響が一定の割合$a$で減少すると仮定すると，
\pagebreak
\begin{equation}
  C=(1-a)(A+aA^2+\cdots+a^{n-1}A^n+\cdots)
\label{eq:Ca}
\end{equation}
と書ける．
ここで，係数$1-a$は正規化のための定数である．

式(\ref{eq:Ca})は無限級数の計算から
\begin{equation}
 (I-aA)C=(1-a)A
\label{eq:target}
\end{equation}
と書け，
線型計算により解を求めることができる．

計算により得られる行列$C$は，列ベクトルの各要素の合計が必ず1となり，
確率として扱うことが出来る．
$C$の$(j,i)$要素を$P(w_j^*|w_i)$と書くと，$w^*$は定義文の集合体の
中の単語を意味することになる．
以下，この定義文の集合体を拡張定義文と呼ぶことにする．
すなわち，$P(w_j^*|w_i)$は拡張定義文中の単語の確率頻度である．



\subsection{拡張定義文}
\label{subsec:expand}

上記の手法を実際に国語辞典\cite{gakkenj}に適用した結果を以下に示す．
前処理として，扱う単語を一般名詞とサ変名詞に限定（形態素解析は茶筌
\cite{chasenj}を利用）し，語義の区別はせず，語義文と例文をまとめて見出語の定義文とした．
その結果，43,915語の見出語と，平均約7語の0次定義文を得た．
以下，$a=0.9$で行った実験結果を例に詳細を記す．

まず，式(\ref{eq:matrix})(\ref{eq:frequency})から確率行列$A$を計算した．
これはスパースな43,915次元の正方行列である．
次に式(\ref{eq:target})から線形ライブラリ$CLAPACK$\cite{clapack}を利用して$C$を求めた．
このときの計算精度は32~bit長の浮動小数点演算で，有効桁は$10^{-7}$までとした．
この結果得られた列ベクトルの非ゼロの値を持つ次元数は平均約33,000であった．
すなわち，平均約33,000語の仮想定義文ができたことになる．

表\ref{tab:expand}に0次定義文と拡張定義文との比較を示す．
まず，見出語「通信」に関しては，0次定義文では「通信」が頻度4，他の13語
が等しく頻度1で，「通信」のみが突出して頻度が高い．
一方，拡張定義文では全ての単語の確率頻度が異なっており，順位づけすることができ
る．
また，頻度1位の「通信」と2位の「人」との差は相対的に小さくなっている．
さらに，0次定義文に現れていなかった「物事」「一つ」が拡張定義文では上
位の頻度で現れている．
定義文中の語数は，0次定義文では14語だったものが，拡張定義文では
32,182語に大幅に増えている．

\begin{table}[t]
  \caption{0次定義文と拡張定義文の比較}
  \label{tab:expand}
\begin{center}
\input{05table01.txt}
\end{center}
\end{table}

同様に，見出語「傍受」に関しては，0次定義文では5単語が等しく頻度1で現れているのに対し，拡張定義文では順位が付けられ，中でも「通信」が相対的に大きな頻度を示している．
また，見出語「通信」の場合と同様に「人」「物事」「自分」「行動」といった一般的な単語が現れているほか，「電線」「有線」「発信」といった見出語と関連の深い特徴的な単語が現れている．

本手法を用いると，辞書全体でよく使われる単語，即ち一般的な単語が上
位に現れやすくなる．
この性質により，等しい頻度の単語でも，より一般的な語が拡張定義文中の上位に現れる傾向があり，場合によっては0次定義文に現れない単語が確率頻度最大となることもある．





\subsection{上位語情報としての評価}
\label{subsec:eval-hyper}

見出語$w_i$の拡張定義文中に上位語$w_j$があるとすれば，上位語$w_j$は見出語$w_i$を説明するた
めに非常に重要な単語であるため，その確率頻度$P(w_j^*|w_i)$は高いことが予
想される．
従って，見出語$w_i$の上位語は，その拡張定義文の中から確率頻度
$P(w_j^*|w_i)$が高い順に尤もらしいと考えることができる．
この仮説を検証するために，日本語語彙大系\cite{goitaikeij}を正解データとした検
証実験を行った．
対象としたのは，前節で用いた43,915語のうち，日本語語彙大系と表記が一致する
39,982語である．

\ref{sec:intro}節で記したように，
日本語語彙大系は意味的上下関係を表した約3,000のカテゴリーからなるオ
ントロジーと各カテゴリーに割当てられた合計約40万語からなる大規模語彙データである．
ただし，単語の割当てに関しては翻訳への適用を主な目的として作成されているため，
オントロジーとしては必ずしも正しいとは限らない．
そこで，本論文では，まずはじめに日本語語彙大系をオントロジー検証のための正解データとして適切に利用する方法を検討し，その後，提案手法に対する評価を行うことにする．

まず，日本語語彙大系のオントロジーとしての性質を調べるため，既存手法による
上位語抽出結果と，
その結果を人手により修正した正解データ\footnote{
この正解デー
タはLexeedに上位語を付与することを目的として作成されたものであり，上位語
はLexeedの見出語の中から選ばれている．
{但し，この修正は，正解データとして一般性があるものを目指したものではなく，あくまでも既存手法の評価のための修正であることに注意を要する．
実際，修正に際しては，既存手法による結果をできるだけ残す方針で
編集されており，かなりのバイアスがかかっている．また，このため，上位語の
数は定義文中の文（主辞）の数に等しい．}
}
を利用した．
この既存手法は，Nicholsらにより提案された，辞書定義文の構文解析により得られた
主辞を上位語とみなす手法である\cite{Nichols:Bond:2005}．
ただし，ここで用いた辞書(Lexeed \cite{lexeed})は前節の実験で用いた辞書（学研国語辞典\cite{gakkenj}）とは異なる．

これらのデータを利用して，日本語語彙大系の意味カテゴリー間の関
係を上位語の評価指標としての妥当性という観点から評価した．
評価方法は，次に示す3種類をそれぞれ上位語の正解データとして精度を計算するものである．
\begin{enumerate}
\item 見出語の含まれる意味カテゴリーの直接上位カテゴリー中の全単語
\item 直接上位カテゴリーを除く全ての上位カテゴリー（間接上位カテゴリー）中の全単語
\item 見出語と同一カテゴリーに属する全単語
\end{enumerate}

評価結果を表\ref{tab:head}に示す．

\begin{table}[b]
 \begin{center}
  \caption{日本語語彙大系カテゴリーの上位語集合としての妥当性評価}
  \label{tab:head}
\input{05table02.txt}
\end{center}
\end{table}

人手修正データの評価結果を見ると，直接上位カテゴリーよりもむしろ
同一カテゴリーの単語に上位語が多く含まれていることがわかる．
人手修正による精度の向上も同一カテゴリーの方が大きく，上位語の多くはここに
集まっていると考えられる．
逆に，間接上位カテゴリーでは，人手修正による精度向上率は$-24.5\%$であり，
精度を大きく下げている．
この結果は，間接上位カテゴリーは
上位語以外の単語を多く含んでおり，間違った結果を
正解と判断している率が非常に高いことを示している．
{言い替えると，間接上位カテゴリーは再現率が非常に低く，
一方，直接上位カテゴリーと同一カテゴリーには従来手法のような構文解析手法
では取り出しにくい上位語が集まる傾向があるといえる．
ここで，人手修正データに強いバイアスが掛っている（脚注参照）点を考慮すると，
実際に修正を加えられた人手修正データの方がより信頼性の高いデータであることに気附く．
従って，精度の絶対値ではなく，向上率を重視したほうが信頼性が高いと考えられる．
よって，本論文においては，日本語語彙大系を上位語の評価として使う場合には，
同一カテゴリーあるいは直接上位カテゴリーを正解とみなして評価を行うことにする．
特に同一カテゴリーによる評価を重視する方向で検討をすすめることとする．
}


この結果を考慮して，提案手法による上位語情報の抽出結果を日本語語彙大系の
直接上位カテゴリーおよび同一カテゴリーを正解データとして評価した．
その結果を図\ref{fig:hyper-above}, \ref{fig:hyper-same}に示す．
図\ref{fig:hyper-above}は直接上位カテゴリーを正解とした場合，
図\ref{fig:hyper-same}は同一カテゴリーを正解とした場合である．
それぞれの図には，横軸に拡張定義文中の確率頻度を降順に並べた順位を，
縦軸に正解データに対する精度をとり，全ての見出語に関する統計量でプロットしている．
黒丸は前述の人手修正による正解データを，白丸は同じく前述の既存手法による結果
である．
太い実線は再帰的展開を行わない場合，即ち$a=0$の結果である．
ここでは同一頻度のものは任意に順位を割当てた．
破線，細い実線，鎖線はそれぞれ$a=0.1,0.5,0.9$の場合の結果を示している．

\begin{figure}[b]
\vspace{-1\baselineskip}
\begin{center}
\includegraphics{16-1ia5f1.eps}
\end{center}
\caption{出現頻度順位と精度（日本語語彙大系直接上位カテゴリーによる評価）}
\label{fig:hyper-above}
\end{figure}

\begin{figure}[t]
\begin{center}
\includegraphics{16-1ia5f2.eps}
\end{center}
\caption{出現頻度順位と精度（日本語語彙大系同一カテゴリーによる評価）}
\label{fig:hyper-same}
\end{figure}

図\ref{fig:hyper-above}の順位1位を比較すると，既存手法の精度$0.0399$
に対して$a=0.1$では$0.0402$であり，提案手法の精度が上回っている．
また，$a=0.5,0.9$でもそれぞれ$0.0389,0.0378$
であり，ほぼ同等の結果を得ている．
ただし，人手修正による正解データの精度$0.0510$には及んでいない．
一方，図\ref{fig:hyper-same}の順位1位での比較では，既存手法の精度$0.2170$
に対して$a=0.1,0.5,0.9$のそれぞれで$0.2628,0.2915,0.2932$と大きく上回る結果を
出している．
さらに，$a=0.5,0.9$は人手修正による正解データの精度$0.2793$をも僅かに上
回っている．\footnote{
既存手法との比較は異なる辞書を基にして計算された結果の比較であるた
め微小な差異は意味を持たないが，どちらの辞書も国語辞典であり，同じ性質の
文章であるためその影響は非常に小さいと考えられる．
特に，図\ref{fig:hyper-same}の結果は
人手修正データとの相対比較という観点からみても大きな差があり，十分意味を持つも
のである．}

また，再帰的展開前の結果($a=0$)と展開後の結果($a=0.1,0.5.0.9$)とを
比較してみると，
展開後の結果は，展開前に比べて順位1位の精度が上り，順位2,3位以下では精度が下っている．
これは，再帰的展開により，低い順位にあった上位語が高い順位に移動したことを
示しており，再帰的展開の効果を明確に表す結果である．

以上の結果から，提案手法は既存手法と同等以上の精度を持ち，評価方法によっては
手作業にも劣らない精度を出せることが示された．
さらに，上位語候補として1語あるいは数語しか提示できない既存手法に比べて，提案手法では順位2位以下の情報を多量に持っているため，アプリケーションへの応用の際に
これらの情報が有効に働くことを期待できる．

\section{オントロジーの生成}
\subsection{学習モデル}

\ref{sec:hyperinfo}節で得られた上位語情報を利用して，単語オント
ロジーの自動生成を試みた．
以下は，今回検証したモデルである．

まず，目的とするオントロジーは上位語が下位語を意味的に包含するものである．
従って，下位語は上位語の意味を要素として含まなければならない．
逆に，下位語は上位語以外の単語の意味を要素として含んではいけないことになる．

いま，あるオントロジーが存在し，その中の単語$C$の上位語が$A,B$である場合を考える（図\ref{fig:ont-model}(a)参照）．
このとき，$C$の持つ意味は$A,B$及び$C$自身により特徴づけられると考える．
ところが，$C$は拡張定義文において様々な単語から成り立っている．
そこで，単語の意味空間の集合としての見出語の意味空間を考える
（図\ref{fig:ont-model}(b)参照）．

\begin{figure}[b]
\vspace{-0.5\baselineskip}
\begin{center}
\includegraphics{16-1ia5f3.eps}
\end{center}
\caption{オントロジー学習モデル}
\label{fig:ont-model}
\end{figure}

拡張定義文中の各単語は見出語を構成する要素（部分空間）であると考え，
見出語自身とその上位語のみが，その見出語を特徴づける意味要素
として有効であると仮定する．
即ち，
\begin{quote}
オントロジー上に単語$C$の上位語として単語$A,B$のみが存在する場合，
このオントロジーにおける$C$の意味は拡張定義文中の
$A,B,C$で構成される部分空間に限定され，拡張定義文中の他の単語は無視される．
そして，これらの単語の確率頻度の合計が大きいほど，見出語の本来の意味が再現される
\end{quote}
と考える．
この再現率がオントロジー全体で高いほど，良いオントロジーとなる．


これにオントロジー上の距離の要素を加味し，
単語の確率頻度（\ref{sec:method}節参照）を用いて，
上位語を$A,B,C,D,\cdots$としたときの
再現率を
\begin{equation}
 P'(w)=b_AP(w_A^*|w)+b_BP(w_B^*|w)+b_CP(w_C^*|w)+b_DP(w_D^*|w)+\cdots
\end{equation}
と書くことにする．
ただし，$b_x$は単語$w_x^*$の木構造の頂点からのトポロジカルな距離に従う定数($\sum_xb_x=1$)である．
以下，この$P'(w)$を意味再現率と呼ぶ．

この仮定の下で，あるオントロジー$T$が存在している場合，$T$が存在する確からし
さは全単語の意味再現率の積，即ち，
\begin{equation}
\label{eq:prob_t}
 P(T)=\prod_{w_x}^{all\; words} P'(w_x)
\end{equation}
で表すことができる．
\pagebreak
つまり，全ての単語がより良く元の意味を再現できている状態がオントロジー
の存在が最も安定している状態であると考える．

計算を簡単にするため，式(\ref{eq:prob_t})の対数をとれば，
\begin{equation}
\begin{aligned}[b]
 L(T) & = \log P(T)\\
  & = \sum_{w_x}^{all\; words}\log P'(w_x) \\
  & = \sum_{w_x}^{all\; words}\log \sum_{w_y^*}^{all\; hypernyms}b_y P(w_y^*|w_x)
\end{aligned}
\end{equation}
となる．
この$L(T)$を最大化することにより，前述の仮定の下で
の最適な単語オントロジーが取り出せる． 



\subsection{計算機実験}

本論文では，計算コストを抑えるため，最適化アルゴリズムを各単語の意味再
現率最大化と，その組合せとしての全体の最適化の2段階に分離して行った． 
即ち，
\begin{enumerate}
 \item $\forall w_x,\ P'(w_x)=\sum_{w_y^*}^{all\; hypernyms}b_y P(w_y^*|w_x)$ の最大化
 \item  $L(T)=\sum_{w_x}^{all\; words}\log P'(w_x)$ の最大化
\end{enumerate}
を交互に行うことで，近似的に最適化を行った．
オントロジー上の距離に関するパラメータ$b_y$は，上位語の木構造の最上位
からの階層の深さを$n$としたとき，$b_y \propto z^n, z=0.1,0.5,0.9,0.99$の4種類とした．

学習の前提として，オントロジー上での各単語の直接上位語は1語に限定し，
自身を上位語とすることを許可している．
自身が上位語となる場合は，当該単語は木構造の最上位に位置するものと考える．
これらの前提の下，学習の初期状態は全ての単語が自身を上位語とする状態にあるものとし，学習を行った．

具体的な学習手順は，次の通りである．
\begin{enumerate}
\item 見出語を1つ選ぶ．
\item 上位語候補を$N$個選ぶ．
\item 各上位語候補に対して$P'(w_x)$を計算し，最大となる上位語を決定する．
\item 上記に対し$L(T)$を計算し，値が増加すれば上位語を置換，それ以外なら元に戻す．
\end{enumerate}
を全見出語に関し変化がなくなるまで繰り返す．
ただし，$N$は事前に与えられる定数であり，今回の実験では$N=100$とした．



\subsection{結果}

\begin{figure}[p]
\begin{center}
\includegraphics{16-1ia5f4.eps}
\end{center}
\caption{獲得されたオントロジーの例（「通信」の上位語／下位語）}
\label{fig:tree-word}
\end{figure}

以下，$a=0.1$の場合を例に，結果の詳細を記す．

学習の結果得られたオントロジーの一部を図\ref{fig:tree-word}に示す．
「通信」の全上位語および全下位語の構造を表示している\footnote{
上位語の他の下位語，即ち，「兄弟」「従兄弟」等の関係にあ
たる単語は全て省略している．}
．
(a)はオントロジー上の距離に関するパラメータ$b(z=0.1)$を用いて学習した結果である．
同様に(b), (c), (d)はそれぞれ$b(z=0.5,0.9,0.99)$のときの結果である．
この値が大きくなる程，上位語の影響は階層の離れた下位語まで届くため，深い
木構造が期待できる．
逆にこの値が小さいと，浅い木構造ができることが期待される．
今回の実験では，独立した木構造の数は，(a) 1687，(b) 1472，(c) 788，(d) 142
であった．
(a), (b), (c)では「通信」は木構造の最上位に位置しているが，(d)では「人」を
頂点とする巨大な木構造の一部であり，多くの上位語の下に位置している．
これらの結果から，
パラメータ$z$の値が増加するに従い木構造の階層がより深く大きくなるのが確認できる．
また，図\ref{fig:tree-word}を詳細にみると，上下関係が特定の意味関係で統一されているとは
言い難いものの，関連のある言葉が集まり木構造を構築していることは確認できる．

次に，生成されたオントロジーの精度を，\ref{subsec:eval-hyper}節同様，
日本語語彙大系のカテゴリー間の上下関係との一致度を測ることにより調べた． 
評価方法は\ref{subsec:eval-hyper}節の結果を考慮して，2種類の正解データを設定した．
一つは，見出語の含まれる意味カテゴリーの直接上位カテゴリーを
正解とするもの，
もう一つは，見出語の含まれる意味カテゴリーと同一カテゴリーを
正解とするものである．
これら正解カテゴリー内の単語のいずれかに，生成したオントロジーから得られる
上位語が一致すれば正解とみなした．

この手法による評価を，オントロジー上での上位語の距離に対する精度としてプロット
したのが図\ref{fig:eval-direct}および図\ref{fig:eval-all}である．
横軸は上位語のトポロジカルな距離で，直接上位語であれば``1''，さらにその上位語
であれば``2''というように，オントロジー上の距離を表している．
縦軸は精度である．
図の各線は，学習時のパラメータ$b(z=0.1,0.5,0.9,0.99)$のそれぞれの結果をプロットしたものである．
また，白丸はオントロジー生成学習前（拡張定義文中の出現頻度最大の単語を正解とみなした場合）の精度を，黒丸は人手で修正したデータの
精度（表\ref{tab:head}参照）を表している．

\begin{figure}[t]
\begin{center}
\includegraphics{16-1ia5f5.eps}
\end{center}
\caption{オントロジー上の上位語の精度（直接上位カテゴリーによる評価）}
\label{fig:eval-direct}
\end{figure}
\begin{figure}[t]
\begin{center}
\includegraphics{16-1ia5f6.eps}
\end{center}
\caption{オントロジー上の上位語の精度（同一カテゴリーによる評価）}
\label{fig:eval-all}
\vspace{-1.5\baselineskip}
\end{figure}

全体的に，学習時のパラメータ$z$が小さいほど距離1での精度が高く，また，距離が大きくなるに従い急激に精度を落している．
$z$が小さいと，生成された木構造の階層が浅いため大きな距離の上位語は
非常に少くなり，
逆に，$z$が大きいと，木構造の階層が深くなり，多くの上位語を同時に学習するため
直接上位語の精度が犠牲になると考えられる．
ただし，この差は僅かであり，距離2以上では逆転するものもあるので，
$z$を如何に設定すべきかは更なる検討が必要である．

さらに，これらの結果を学習の前後で比較してみる．
図\ref{fig:eval-direct}の
距離が1（直接上位語）の場合を見ると，オントロジー生成学習前（図では白丸）よりも後の方が大きく精度を下げている．
人手修正データの場合（図では黒丸）と比べると，半分程度の精度である．
ここで人手修正データの性質を考える．
このデータはオントロジーを意識して作成されたものではないので，
上位語にさらにその上位語を積上げる手法をとっても木構造にはならず，
ループ状につながってしまう．
従って，木構造にするには，いくつかの上位語を変える必要があり，
多少の精度低下が起る．
この点を考慮すると，学習前後で精度が低下することは妥当であると思われる．

一方，図\ref{fig:eval-all}では距離が1の各値はオントロジー生成学習前（図
の白丸）および人手修正データ（図の黒丸）と比べて大きく精度を上げている．
\ref{subsec:eval-hyper}節の結果を考慮すれば同一カテゴリーによる評価がより重要であるとも考えられるが，
同義語など他の要素が影響している可能性も考えられる．
この点に関しても更なる検討が必要である．

以上の結果は，$a=0.5,0.9$の場合でも同様の傾向がみられる．

上述のように，オントロジーとしての十分な評価を下すためには
更に検証を加える必要があるが，
上記の実験結果は計算により得られた上位語情報を学習によって木構造に組
み上げる方法論の妥当性を示唆している．
また，再帰的展開の影響が小さい$a=0.1$での結果が学習前から大きく改善したことは，
辞書から得たカバー率の高い上位語情報が有効に活用されていることを
表すものである．


\section{むすび}

本論文では辞書の定義文から上位語情報を取り出し，検証を行った．
取り出した上位語情報はカバー率が非常に高いという特徴をもっており，
精度の検証では既存の手法を上回る結果を示した．
また，上位語情報を利用したオントロジー生成手法を提案し，
上位語情報のカバー率の高さが有効に働いていることを示した．

提案したオントロジー生成手法はまだ簡易的なものであるが，パラメータに従い
様々な深さの階層をもつオントロジーを生成できる．
ただし，その精度の評価方法に関しては更なる検討が必要であることも
明らかになった．

今後の課題としては，第一に評価手法の確立が挙げられる．
今回は正解データとして日本語語彙大系のみを利用したが，様々な辞書等を組合せて，
より精度の高い評価手法を確立したいと考えている．
更に，学習の最適化手法の改良に関しても今後の課題である．
今回用いた最適化手法は簡易的なもので，極めて局所的な最適解しか得
られない．
この点を改善すれば，今後の更なる精度の向上も期待できる．

また，今回の実験では語義の曖昧性については考慮せず，1表記につき1語義とし
て実験を行ったが，定義文中の語義曖昧性を排除した辞書
（例えば Lexeed\cite{lexeed}）を利用することにより，語義レベルの
上位語抽出，オントロジー生成が可能である．
今回は特殊な辞書であるLexeedの利用は避けたが，これの再帰的展開への適用については
現在検討を進めているところである． 


\bibliographystyle{jnlpbbl_1.3}
\begin{thebibliography}{}

\bibitem[\protect\BCAY{Anderson, Bai, Bischof, Blackford, Demmel, Dongarra,
  Du~Croz, Greenbaum, Hammarling, McKenney, \BBA\ Sorensen}{Anderson
  et~al.}{1999}]{clapack}
Anderson, E., Bai, Z., Bischof, C., Blackford, S., Demmel, J., Dongarra, J.,
  Du~Croz, J., Greenbaum, A., Hammarling, S., McKenney, A., \BBA\ Sorensen, D.
  \BBOP 1999\BBCP.
\newblock {\Bem {LAPACK} Users' Guide\/} (Third \BEd).
\newblock Society for Industrial and Applied Mathematics, Philadelphia, PA.

\bibitem[\protect\BCAY{Nichols \BBA\ Bond}{Nichols \BBA\
  Bond}{2005}]{Nichols:Bond:2005}
Nichols, E.\BBACOMMA\ \BBA\ Bond, F. \BBOP 2005\BBCP.
\newblock \BBOQ Acquiring Ontologies Using Deep and Shallow Processing\BBCQ\
\newblock In {\Bem 11th Annual Meeting of the Association for Natural Language
  Processing}, \mbox{\BPGS\ 494--498}\ Takamatsu.

\bibitem[\protect\BCAY{Snow, Jurafsky, \BBA\ Ng}{Snow et~al.}{2006}]{Snow06}
Snow, R., Jurafsky, D., \BBA\ Ng, A.~Y. \BBOP 2006\BBCP.
\newblock \BBOQ Semantic Taxonomy Induction from Heterogenous Evidence\BBCQ\
\newblock In {\Bem Proceedings of 44th Annual Meeting of the ACL}, \mbox{\BPGS\
  801--808}. Association for Computational Linguistics, ACL.

\bibitem[\protect\BCAY{Suzuki}{Suzuki}{2003}]{Suzuki2}
Suzuki, S. \BBOP 2003\BBCP.
\newblock \BBOQ Probabilistic Word Vector and Similarity based on
  Dictionaries\BBCQ\
\newblock {\Bem Computational Linguistics and Intelligent Text Processing
  Lecture Notes in Computer Science (In proceedings of CICLing2003)}, {\Bbf
  \textmd{N 2588}}, \mbox{\BPGS\ 564--574}.

\bibitem[\protect\BCAY{池原\JBA 宮崎\JBA 白井\JBA 横尾\JBA 中岩\JBA 小倉\JBA
  大山\JBA 林}{池原\Jetal }{1997}]{goitaikeij}
池原悟\JBA 宮崎正弘\JBA 白井諭\JBA 横尾昭男\JBA 中岩浩巳\JBA 小倉健太郎\JBA
  大山芳史\JBA 林良彦 \BBOP 1997\BBCP.
\newblock \Jem{日本語語彙大系}.
\newblock 岩波書店.

\bibitem[\protect\BCAY{鶴丸\JBA 竹下\JBA 伊丹\JBA 柳川\JBA 吉田}{鶴丸\Jetal
  }{1991}]{tsurumaru1991}
鶴丸弘昭\JBA 竹下克典\JBA 伊丹克企\JBA 柳川俊英\JBA 吉田将 \BBOP 1991\BBCP.
\newblock \JBOQ 国語辞典情報を用いたシソーラスの作成について\JBCQ\
\newblock \Jem{情報処理学会　自然言語研究会}, {\Bbf \textmd{1991-NL-}83}.

\bibitem[\protect\BCAY{金田一\JBA 池田}{金田一\JBA 池田}{1988}]{gakkenj}
金田一春彦\JBA 池田弥三朗 \BBOP 1988\BBCP.
\newblock \Jem{学研国語大辞典第二版}.
\newblock 学習研究社.

\bibitem[\protect\BCAY{鈴木}{鈴木}{2005}]{Suzuki3j}
鈴木敏 \BBOP 2005\BBCP.
\newblock \JBOQ 辞書に基づく単語の再帰的語義展開\JBCQ\
\newblock \Jem{情報処理学会論文誌}, {\Bbf 46}  (2), \mbox{\BPGS\ 624--630}.

\bibitem[\protect\BCAY{松本\JBA 北内\JBA 山下\JBA 平野\JBA 松田\JBA 高岡\JBA
  浅原}{松本\Jetal }{2000}]{chasenj}
松本裕治\JBA 北内啓\JBA 山下達雄\JBA 平野善隆\JBA 松田寛\JBA 高岡一馬\JBA
  浅原正幸 \BBOP 2000\BBCP.
\newblock \JBOQ 日本語形態素解析システム『茶筌』 version 2.2.1 使用説明書\JBCQ.
\newblock http://chasen.aist-nara.ac.jp/.

\bibitem[\protect\BCAY{笠原\JBA 佐藤\JBA 田中\JBA 藤田\JBA 金杉\JBA
  天野}{笠原\Jetal }{2004}]{lexeed}
笠原要\JBA 佐藤浩史\JBA 田中貴秋\JBA 藤田早苗\JBA 金杉友子\JBA 天野成昭 \BBOP
  2004\BBCP.
\newblock \JBOQ 「基本語意味データベース:Lexeed」の構築(辞書,コーパス)\JBCQ\
\newblock \Jem{情報処理学会研究報告. 自然言語処理研究会報告}, {\Bbf 2004}  (1),
  \mbox{\BPGS\ 75--82}.

\end{thebibliography}

\begin{biography}
\bioauthor{鈴木　　敏（正会員）}{
昭和42年生．
平成2年東京大学教養学部基礎科学科卒．
同年NTT入社．
統計的学習理論，物体認識に関わる計算モデル，自然言語処理のための
 学習モデルの研究に従事．
平成4〜9年ATR人間情報通信研究所研究員．
平成6年日本神経回路学会研究賞受賞．
}

\end{biography}


\biodate

\end{document}
