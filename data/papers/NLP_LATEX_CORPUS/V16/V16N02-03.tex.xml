<?xml version="1.0" ?>
<root>
  <section title="Introduction">Semanticsimilarityofwordsisoneofthemostimportantlexicalknowledgefornaturallanguageprocessing,havingabroadrangeofapplicationssuchasqueryexpansionforinformationretrieval,wordsensedisambiguation,andthesaurusconstruction.Tomeasuresemanticrelatednessofwords,aconceptcalleddistributionalsimilarityhasbeenwidelyused.Itmeasuresthesimilarityoftwowordsbythecommonalityofcontextsthewordsshare,basedonthedistributionalhypothesis,whichstatesthatsemanticallysimilarwordstendtosharesimilarcontexts.Conversely,twowordsthatshareacertainamountofsimilarcontextsarelikelytobesemanticallysimilar.Anumberofresearcheswhichutilizeddistributionalsimilarityhavebeenconducted,includingandmanyothers.Althoughtheyhavebeensuccessfulinacquiringrelatedwords,thisinvolvesvariousparameterssuchassimilaritymeasuresandweightfunctions.AsWeedsetal.pointedout,``itisnotatallobviousthatoneuniversallybestmeasureexistsforallapplication,''thustheparametersmustbetunedbyhandinanad-hocmanner,dependingonthetask.Severalresearchershavecomparedanumberofweightfunctionsandsimilaritymeasures,buttheyendedupwithvariedresults.Thefactthatnotheoreticbasisisgiventothedistributionalsimilaritysettingismakingthemattermoredifficult.Ontheotherhand,ifwepayattentiontolexicalknowledgeacquisitioningeneral,avarietyofsystemswhichutilizedpatternsarefoundintheliterature.Inherlandmarkpaperinthefield,Hearstutilizedsyntacticpatternssuchas``suchXasY''and``YandotherX,''andextractedthehypernym/hyponymrelationofXandY.RiloffandShepherdandRoarkandCharniakappliedthisideatoextractionofwordswhichbelongtothesamecategories,utilizingwordco-occurrenceinacontextwindoworsyntacticrelationssuchasconjunctionsandappositives.Theadvantageofsyntacticpatternsisthattheycanbeeasilyconvertedtofeatures,andmachinelearningapproachcanbeusedtodiscriminatetherelationsthatthewordshave.Snowetal.,forexample,tookthisapproachandbuilthypernymclassifiersbasedondependencypatterns.Someresearchershavestartedtousethesetwoindependentapproaches,distributionalsimilarityandsyntacticpatternstogether.Linetal.firstcollecteddistributionallysimilarwords,i.e.,relatedwords,thenfilteredoutantonymsandcohyponyms(wordsthatsharethesamehypernym)usingsyntacticpatternssuchas``fromXtoY''and``eitherXorY.''Thisapproachpartiallysolvedtheproblemthatdistributionallysimilarwordstendtoincludenotonlysynonymsbutalsoantonymsandcohyponyms.Mirkinetal.triedtointegratethesetwoapproaches,byusingthedistributionalsimilarityasafeaturealongwithotherpattern-basedfeatures,andconstructingaclassifiertodetectlexicalentailmentrelations.Althoughtheyreportedthattheirclassification-basedsystemsuccessfullyimprovedtheperformance,itonlyachievespartialintegrationandwasstillrelyingonanindependentmoduletocomputethesimilarity.Thisconfigurationinheritsalargeportionofdrawbacksofthesimilarity-basedapproachesmentionedabove.Toachievefullintegrationofbothapproaches,wesupposethatre-formalizationofthesimilarity-basedapproachwillbeessential,aspattern-basedlexicalknowledgeacquisitionisenhancedwiththesupervisedlearning.Inthispaper,weproposeanovelapproachtoautomaticsynonymidentificationbasedonsupervisedlearningtechniquewhichdoesnotuseanykindofpredeterminedsimilaritymeasures.Thecontributionofthispaperisthree-fold.Firstly,were-formalizesynonymacquisitionasaclassificationproblem:onewhichclassifieswordpairsintosynonym/non-synonymclasses,withoutdependingonasinglevalueofdistributionalsimilarity.Inthisway,thesynonymclassificationcanbenefitfromvariousmachinelearningtechniquesincludingthekerneltrick.Secondly,inadditiontoconventionalcommonfeatures,weproposeanewsetoffeatures,i.e.,distributionalfeatures,whichcorrespondtothedegreeofcommonalityofindividualcontexttypessharedbywordpairs.Weshowintheexperimentstheireffectivenessinclassifyingsynonympairs.Thirdly,weproposefully-integratedsynonymclassifiersbasedonbothcommon/distributionalfeaturesandpattern-basedfeatures,whichhavealsobeenusedforlexicalknowledgeacquisitiontasks.Wefinallybuildeightclassifiersbasedondistributional,common,and/orpattern-basedfeatures.Intheexperiments,theirperformancesarecomparedintermsofprecisionandrecallofsynonymacquisition,andthedifferencesofactuallyacquiredsynonymsaretobeclarified.Therestofthispaperisorganizedasfollows:afterintroducingrelatedworkinSection2,webrieflyreviewtheconventionaldistributionalsimilaritymodelinSection3.InSection4,weproposetheclassification-basedapproachtosynonymacquisition,andtheconstructiondetailsofcommonanddistributionalfeaturesaredescribed.Theothertypeoffeatures,pattern-basedfeatures,aredefinedinthefollowingSection5.Finallywebuildeighttypesofsynonymclassifiers,andcomparetheirperformancesinSection6.Section7concludesthispaper.</section>
  <section title="Related Work"/>
  <subsection title="Distributional Similarity">AswementionedinSection1,thereareanumberofresearchesregardingdistributionalsimilarity.However,thesemethodsareallunsupervisedapproaches,thusappropriateweightsandsimilaritymeasureshavetobefixedbeforehand,andtheydependlargelyontheactualtasktowhichdistributionalsimilarityisapplied.Forthisreason,measuresandweightshavebeenextensivelycomparedsofar,withvariedresults.WeedsandWier'sworkisworthattentionhere---theyproposednewmeasures,namely,precisionandrecall,consideringthecontextmatchingprocessasinformationretrieval.Theyalsoproposedacombinedapproach,whichisgeneralizationofseveraldistributionalsimilaritymeasureswithtunablemeta-parameters.Thesemetaparameterswereoptimizedonadevelopmentset,andtheproposedmodeloutperformedspecificconventionalmodels.Althoughwecanconsiderthisasastepforwardtoafullysuperviseddistributionalsimilarity-basedapproach,itdoesnotuseanymachinelearning-basedtechniquesaswedointhisstudy.</subsection>
  <subsection title="Pattern-based Lexical Knowledge Acquisition">Lexico-syntacticpatternsinsentenceshavelongbeenused,notonlyforsynonymacquisitiontasksbutalsoforlexicalknowledgeacquisitiontasksingeneral.Hearstpioneeredthisfiled,extractinghypernym/hyponymrelationsusingsyntacticpatternssuchas``suchXasY''and``YandotherX.''Evenco-occurrenceinasinglesentenceorinawindowcanberegardedasagenerictypeof``patterns.''RiloffandShepherdusedwordco-occurrencewithinawindowforextractionofsemanticcategoriesofwords.RoarkandCharniakusedsyntacticrelationssuchasconjunctionsandappositivestoextractthesamekindsofsemanticcategories.Lexicalpatternshavealsobeenintenselyusedforbootstrapping-basedlexicalacquisitionmethods.Inordertoextractsemanticlexicons,i.e.,unaryrelationsofsemanticallyrelatedwords,thealgorithmreliedoncontextualevidenceextractedfromcorpora,e.g.``subjectwasarrested'',``murdereddirectobject'',and``collaboratedwithppobject''forextractingtermsbelongingtothecategory.TheEspressoalgorithmalsoutilizedpatternsextractedfromcorpora,butitwasdesignedforbinaryrelationextraction,anditdifferedfromBasiliskinthatitexploitedgenericpatterns,e.g.,``XisaY''forhypernym/hyponymrelationsand``XofY''formeronymrelations.Whatisworthattentionhereisthatsupervisedmachinelearningiseasilyincorporatedwithsyntacticpatterns.Forexample,Snowetal.furtherextendedHearst'sideaandbuilthypernymclassifiersbasedonmachinelearningandsyntacticpattern-basedfeatures,withsuccess.</subsection>
  <subsection title="Classification Approach">Supervisedclassificationapproachtolexicalknowledgeacquisitionisgatheringmoreandmoreattentionsthesedays.Duplicaterecorddetectionhasbeenanimportantprobleminthedatabasefield,andBilenkoandMooneyproposedlearnablestringdistancemeasuresforthisproblem.OneofthemeasurestheyproposedisbasedonthevectorspacemodelandSVMclassification,usingthefeaturesofwordpairsbuiltfromcommonfeaturesofindividualvectors.Thiscorrespondstothe``commonfeatures''describedinthispaper.YuandAgichteinworkedonthedetectionofsynonymousnamesofgenesandproteins,utilizingapartiallysupervisedapproach,Snowball,andafullysupervisedapproachbasedonaSVMclassifier,usingcontextualpatterns(i.e.,connectingstring,asdoneinEspresso).ConnorandRothisalsoworthmentioninghere,whodealtwithcontextsensitivelexicalparaphrasingusingaglobal,unsupervised,bootstrappedlearningapproach.Theirfocusisonhowtodealwithcontextsensitivityandhowtoutilizelocalcontext,anditdoesnotdirectlycompetewithourmethod---thefeatureconstructionintroducedherecanbeusedtogetherwiththeirframework.</subsection>
  <subsection title="Context Extraction">Inthefirststepofthemodel,thecontextsofwordsareextractedfromcorpora,basedonthepreviouslymentioneddistributionalhypothesis.Varioustypesofcontextualinformationhavebeenproposedfordistributionalsimilaritycomputation,includingsurroundingwords,dependencyorcasestructure,anddependencypath.Amongtheseweadoptdependencystructureasthecontextofwordssinceitisthemostwidelyusedanditisshowntobewell-performingcontextualinformationinthepaststudies,althoughtheapproachordiscussiondescribedinthispaperisnotlimitedtospecificcontextualinformationofchoice.InthispaperthesophisticatedparserRASPToolkit2isutilizedtoextractthiskindofwordrelations.Weusethefollowingexampleforillustrationpurposes:RASPoutputstheextracteddependencystructureasn-aryrelationsasfollows,whosegraphicalrepresentationisshowninFigure:(ncsubjhavelibrary_)(dobjhavecollection)(detcollectiona)(ncmod_collectionlarge)(iobjcollectionof)(dobjofbook)(ncmod_bookby)(dobjbyauthor)(detauthorsuch)(ncmod_authoras)...verbatimWhiletheRASPoutputsaren-aryrelationsingeneral,whatweneedhereisco-occurrencesofwordsandtheircontexts,soweextractthesetofco-occurrencesofstemmedwordsandcontextsbytakingoutthetargetwordfromtherelationandreplacingtheslotbyanasterisk``*'':library-(ncsubjhave*_)library-(det*The)collection-(dobjhave*)collection-(det*a)collection-(ncmod_*large)collection-(iobj*of)book-(dobjof*)book-(ncmod_*by)book-(ncmod_*classic)author-(dobjby*)author-(det*such)...verbatimSummingalltheseupproducestherawco-occurrencecountN(w,c)ofwordwandcontextc.</subsection>
  <subsection title="Co-occurrence Weighting">Asthesecondstep,thecontexttypesextractedabovecanbeweightedinvariousways,includingtf.idfand^2statistics,andmanyothersareproposed,butinthispaperweemploypointwisemutualinformation(PMI)becauseitperformedthebestamongvariousweightingfunctionsinapreliminaryexperiment.UsingPMI,theweightoftheco-occurrence(w_i,c_j)ofawordw_iandacontexttypec_jisdefinedas:whereP(w_i,c_j)istheprobabilityofthewordw_ico-occurringwiththecontextc_j.ThisprobabilityisempiricallycalculatedasP(w_i,c_j)=N(w_i,c_j)/_w,cN(w,c)usingtheco-occurrencefrequencyN(w_i,c_j)obtainedfromcorpora.Afterthisweightingstep,eachwordcanberepresentedasavectorwhoseelementscorrespondtotheweights:whereMisthenumberofcontexttypes.Therearetwothingstonotehere:whenN(w_i,c_j)=0andPMIcannotbedefined,thenwesimplydefinewgt(w_i,c_j)=0.Also,becauseithasbeenshownthatnegativePMIvaluesworsenthedistributionalsimilarityperformance,webindthePMIsothatwgt(w_i,c_j)=0ifPMI(w_i,c_j)&lt;0.</subsection>
  <subsection title="Similarity Calculation">Inthethirdstepofthismodel,thesimilaritybetweenwordsarecalculatedusingsomesimilarity/distancemeasures.Thereareavarietyofsimilaritymeasuresproposed,suchascosinesimilarity,Jaccardcoefficient,andconfusionprobability.Weemployedcosinesimilarity,again,consideringthepreliminaryresult.Itdefinesthesimilarityofwordw_1andw_2as:Inthesynonymdetectiontask,athresholdcanbesetonthesimilarity,andthewordpairswhosesimilaritiesareabovethisthresholdaremarkedassynonympairs.HowtooptimallysetthisthresholdisdescribedinSection6.2.</subsection>
  <section title="Pairwise Classification">Thissectiondescribesadifferentapproachtosynonymacquisition,i.e.,pairwisesynonymclassification.Theproblemformalizationisasfollows.Firstly,wedealwithwordpairs,insteadofindividualwords,astheinstancesofclassification.Wethenpreparefeaturesf_1,...,f_Mconstructedasdescribedbelow.Thetargetclassyisdefinedsuchthaty=1ifthepairistruesynonyms,i.e.,similarwordsnotincludingrelatedwords,hypernyms/hyponyms,antonyms,etc.,andy=0ifnot.Pairswithy=1arecalledpositiveexamples,whilepairswithy=0arecallednegativeexamplesinthispaper.Withthesedefined,theproblemisformalizedaspairwisebinaryclassificationwhichassignstheclassesy0,1tothewordpairsbasedonthefeaturesf_1,...,f_Mofthepair.Weadopttwoschemestoconstructfeaturesintheclassificationtask:commonfeaturesanddistributionalfeatures,andthedetailisgivenbelow.</section>
  <subsection title="Common Features">Asdescribedabove,theconventionaldistributionalsimilarityisanunsupervisedmodelheavilydependentonweightingfunctionsandsimilarity/distancemeasures,whichmustbecarefullyhand-tunedbecausethechoicedirectlyandconsiderablyaffectstheperformance.Tocircumventthis,inthissectionwere-formalizethesynonymacquisitionasasupervised,pairwiseclassificationmodelwhichdoesnotdependonconventionalsimilaritymeasures.Inthedistributionalsimilaritymodel,thesimilarityiscomputedforapairofwords,say,xandz,basedonthefeaturesassignedtoindividualwords.Wecallthesefeatureswordfeaturesinthefollowing.Thepairwiseclassificationmodel,ontheotherhand,directlydealswithwordpairs,say,p=(x,z),asthetargettoclassify.Thefeaturevaluesarecomputedfromwordfeaturesofthepairwords,anddirectlyassignedtothepairs.Wecallthesefeaturespairfeatures,asopposedtowordfeatures.Inthefollowing,wesupposethateachwordisrepresentedasavectorofwordfeaturesas:x&amp;=[x_1x_2x_M]^T,&amp;x_j=wgt(x,c_j),z&amp;=[z_1z_2z_M]^T,&amp;z_j=wgt(z,c_j).alignThispairwiseclassificationapproachwasusedforseverallexicalrelationidentificationtasksuchas,butnotforsynonymacquisitionsofartothebestofauthors'knowledge.Thefirstoneofthetwofeatureconstructionmethodsweadoptedistheonewhichconstructspairfeaturesbytakingthesimpleproductsofcommonwordfeatures,asusuallydoneinpairwiseclassificationtasks.Thisschemeconstructsthefeaturevectorp_cfeatforthepairp=(x,z)as:f^C_j(x,z)&amp;=wgt(x,c_j)wgt(z,c_j)p_cfeat&amp;=[f^C_1(x,z)f^C_2(x,z)f^C_M(x,z)]^T.alignThisfeatureconstructioncanbeinterpretedasfollows---insteadofusingasingle,integratedvalueofthesummationinEquation(),thesummationisexpandedandalltheproductsinthesummationareusedasindividualfeatures,exceptforthenormalizationfactorsinthedenominator.Inthisway,wecanexpectthatappropriateweightsareoptimallyassignedbylearningalgorithmsandcontributingfeatureswouldbeencouraged,increasingtheacquisitionperformance.</subsection>
  <subsection title="Distributional Features">Thepairfeaturesdonotnecessarilyhavetobeconstructedfromindividualwordfeatures,butdirectlyfromco-occurrenceprobabilitiesobtainedfromcorpora.Theotherschemeweproposehereistodirectlyassignpairfeatures,whichwecalldistributionalfeatures,tothepairpas:Thevalueofdistributionalfeaturesf^D_j(x,z)isdeterminedsothatitrepresentsthedegreeofcommonalityofcontextc_jsharedbythewordpair(x,z).Pointwisetotalcorrelation,oneofthegeneralizationsofpointwisemutualinformation,canmeasurethisinformationamount,andweadoptthisasthefeaturevalue:Theadvantageofthisfeatureconstructionisthat,giventheindependenceassumptionbetweenwordxandz,thefeaturevalueiseasilycalculatedasthesimplesumoftwocorrespondingpointwisemutualinformationweightsas:wherethevalueofPMI(x,c_j),whichisalsotheweightswgt(x,c_j)assignedfordistributionalsimilarity,iscalculatedasshowninEquation().Notethatthefeaturevaluef^D_j(x,z)inEquation()isfiniteonlywhenthecontextc_jco-occurswithbothxandz.Inpractice,however,weusedthedefinitioninEquation()withtheindependenceassumption,andthevalueoff^D_j(x,z)isfiniteevenforthecasewherethecontextc_jdoesnotco-occurwithbothxandz,i.e.,PMI(x,c_j)=0orPMI(z,c_j)=0.Noticethat,aslongasPMIisusedastheweightingfunctions,thesetwofeatureconstructionschemesdifferonlyinthattheformerusestheproducts,asshowninEquation(),whilethelatterusesthesum.Therefore,whentheweightsarePMI,Equation()canberewrittenas:Intheexperiments,wecomparetheperformanceofcommonfeaturesanddistributionalfeatures.</subsection>
  <section title="Pattern-based Features">Thissectiondescribeshowtoextractandconstructtheothertypeoffeatures---syntacticpatternsextractedfromsentences.</section>
  <subsection title="Syntactic Pattern Extraction">Syntacticpatternsareusuallysimplestringpatternswithwordslots,suchas``XsuchasY''and``YandotherX,''whichdirectlyexpresstherelationbetweentwowords.Forlexicalrelationextraction,suchpatternsareoftenconvertedintowebsearchqueriesandissuedtowebsearchenginestoobtaintheoccurrencecountofthepatterns,asdonein.Althoughsuchstring-basedpatternsaresimpletoimplementandaccurate,theyoftenlackrobustnessandgeneralitybecauselinguisticvariationsandnoiseinthesentencemaypreventaccuratematchingofthosepatterns.Thisproblemcanbeaddressedbyconsideringricherdefinitionofsyntacticpatterns,i.e.,onesbasedondependencystructureofsentences.FollowingSnowetal.'sdefinition,inthispaperwedefinethesyntacticpatternofwordsw_1,w_2astheconcatenationofthewordsandrelationlabelswhicharelocatedonthedependencypathfromw_1tow_2,notincludingw_1andw_2themselves.Forexample,thesyntacticpatternofwordauthorsandinFigureisdobj:by:ncmod,whilethatofauthorsandHerrickis-of:as:dobj-of:and:conj-of.Noticethat,althoughnotshowninthefigure,everyrelationedgehasareverseedgeasitscounterpart,withthedirectionoppositeandthepostfix``-of''attachedtothelabel.Forexample,thedobjedgewhichgoesfromtohashasitsreverse,dobj-of,whichgoesfromhastocollection.Thisallowstofollowtherelationsinreverse,increasingtheflexibilityandexpressivepowerofpatterns.Intheexperiment,welimitedthemaximumlengthofsyntacticpathtofive,meaningthatwordpairshavingsixormorerelationsinbetweenweredisregarded.Also,weconsideredconjunctionshortcutstocapturethelexicalrelationsmoreprecisely,followingSnowetal..Thismodificationcutsshorttheconjedgeswhennounsareconnectedbyconjunctionssuchasandand.Afterthisshortcut,thesyntacticpatternbetweenauthorsandHerrickisncmod-of:as:dobj-of,whichusedtobe-of:as:dobj-of:and:conj-of.ThesyntacticpatternbetweenandShakespeareisconj-and,whichisanewlyintroducedspecialsymmetricrelation,indicatingthatthenounsaremutuallyconjunctional.TheseconjunctionshortcutsareshowninFig.asdottedlines.Snowetal.alsoproposedsatellitelinks,i.e.,singledependingnodesattachedattheendofpaths,butwedidnotincludethisbecausetheygreatlyincreasethesparsenessofthepair-featurerelationanditdidnothelptoimprovetheperformance,atleastinthepreliminaryexperimenttargetingatthecurrentsynonymacquisitiontask.Afterextractingsyntacticpatterns,wereplacedsomeofthewordsinthepatternsbycategoricallabelstogeneralizewordsbycategoriesandtoavoidthesparsenessoftheextractedpatterns.ThisoperationincludesreplacingalloccurrencesofpronounswithaspeciallabelNPandcardinalnumberswithMCandsoon.ThisreplacementisconductedbasedonthePoStagsofRASP2outputs,andallthereplacingoperationsweemployedarelistedinTable.Forexample,ifwehaveHerrick:dobj:asasapatternandthePoStagofthewordHerrickisNP1,thepatternisreplacedwithNP:dobj:as.</subsection>
  <subsection title="Feature Construction">Afterthecorpusisanalyzedandpatternsareextracted,thepatternbasedfeaturef^P_k(w_1,w_2),whichcorrespondstothesyntacticpatternp_k,isdefinedastheconditionalprobabilityofobservingp_kgiventhatthewordsw_1,w_2bothappearinasentence.Thisdefinitionissimilartoandiscalculatedas:Topreventthefrequencycountsfromvaryingtoomuch,wetookthelogarithmoftheoriginalcountobtainedfromthecorpus,i.e.,n'(w_1,w_2,p_k)=(n(w_1,w_2,p_k)+1),wheren(w_1,w_2,p_k)isthefrequencyofpatternp_kco-occurringwithwordsw_1andw_2,andn'(w_1,w_2)=(n_s(w_1,w_2)+1)+_p_kn'(w_1,w_2,p_k),wheren_s(w_1,w_2)isthenumberoftimeswordsw_1andw_2co-occurinanysentencewithouthavingspecificrelationsbetweenthem(i.e.,nodependencyrelationswithalengthof5orlessareconnectingthem).Althoughtheformalresultsarenotshowninthispaper,takinglogarithmshowedabout1%increaseonF-1measurebasisonaverage.Mirkinetal.alsousedthreeadditionalfeaturesbesidestheoneweexplainedhere---aggregatedconditionalpatternprobability,conditionallist-patternprobability,andrelationdirectionratio.Asfortheconditionallist-patternprobability,wedidnotusethisasanadditionalfeature,sincethepatternconstructionmethodweemployedhereisdependencyrelation,whichwesupposeispowerfulenoughtocaptureliststructuresaswell.Asfortheaggregatedconditionalpatternprobabilityandrelationdirectionratio,weincludedthemasadditionalfeaturesandcomparedtheperformancesbutfailedtoobserveanysignificantimprovementinapreliminaryexperiment.Inourcase,everyrelationedgehasitscounterpartandthevalueoftherelationdirectionratioisalways1,becauseitistheratiooftheprobabilityofapair(w_1,w_2)havinganysyntacticrelationsbetweenthemtothatoftheinversedpair(w_2,w_1).Besides,itisessentiallydesignedfortheirasymmetricsettingsoflexicalentailmentdetectiontaskanditthereforecannotbeeffectiveforoursymmetricsettinginthefirstplace.</subsection>
  <section title="Experiments"/>
  <subsection title="Synonym Classifiers">Nowthatwehaveallfeaturesavailable,webuildeightsynonymclassifiersandcomparetheirperformances.AlltheclassifiersbuiltarelistedinTable,alongwiththeirdescriptionsandthefeaturesused.Outoftheseeightclassifiers,theDSIMclassifieristheonlyunsupervisedmethod,wherecosineisusedtomeasurethepairsimilarity.Athresholdissetonthesimilarityandbinaryclassificationisperformedbasedonwhetherthesimilarityisaboveorbelowofthegiventhreshold.HowtooptimallysetthisthresholdisdescribedlaterinSection6.2.TheremainingsevenclassifiersareallSVM-trained,supervisedmethods.PATusesonlypattern-basedfeatures,anditisessentiallythesameasSnowetal.'smethod.DSIM-PATisasemi-integratedclassifier,anditroughlycorrespondstoMirkinetal.'sapproach.CFEAT-PAT,DFEAT-PAT,andALLarefullyintegratedclassifierswhichweproposeinthispaper.</subsection>
  <subsection title="Experimental Settings"/>
  <subsubsection title="Corpus and Preprocessing">Asforthecorpus,NewYorkTimessection(1994)ofEnglishGigaword,consistingofapprox.46,000documents,922,000sentences,and30millionwords,wasanalyzedbyRASP2toobtainword-contextco-occurrences.Thisyielded100,000ormorecontexttypes,thusweappliedfeatureselectionandreducedthedimensionality.Firstly,wesimplyappliedfrequencycut-offtofilteroutanywordsandcontextswithlowfrequency.Morespecifically,anywordswsuchthat_cN(w,c)&lt;_wandanycontexttypescsuchthat_wN(w,c)&lt;_cwereremoved.Thefirstthreshold,thewordcut-offfrequency_w,isrelativelyeasytofix,becauseitdoesnotdirectlyaffecttheclassification.Thesecondthreshold,thecontextcut-offfrequency,islesseasytohandle,becauseitdirectlyaffectstheclassificationperformance.Weempiricallyinvestigatedtheperformancechange,andapreliminaryexperimentindicatedthattheperformanceactuallyincreasedalmostlinearlyasweincreased_cuntil_c=5,butafterthispointon,itbegantodecrease.Consideringthisresult,weset_w=5and_c=5,wheretheperformancechangewaskeptwithin2%rangewhilethecontexttypesarecut-downbymorethan70%(from145,358to39,750innumber).WethenappliedDF(documentfrequency)thresholdingtofurtherreducethedimensionalityofvectors.ThecontexttypeswiththelowestvaluesofDFwereremoveduntil10%oftheoriginalcontextsremained.Thisfeatureselectionisshowntokeeptheperformancelossatminimum,anditreducedtheperformanceonlyby8%inourcase.Asaresult,thisprocessleftatotalof3,975contexttypes,i.e.,featuredimensionality.Wealsoappliedthefeatureselectiontopattern-basedfeaturestoavoidhighsparseness---onlysyntacticpatternswhichoccurredmorethanorequaltoseventimeswereused.Thenumberofsyntacticpatterntypesleftafterthisprocessis17,964.</subsubsection>
  <subsubsection title="Supervised Learning">Trainandtestsetswerecreatedasfollows:firstly,thenounslistedintheLongmanDefiningVocabulary(LDV)werechosenasthetargetwordsofclassification.Then,alltheLDVpairswhichco-occurmorethanorequaltothreetimeswithanyofthesyntacticpatterns,i.e.,(w_1,w_2)|w_1,w_2LDV,_pn(w_1,w_2,p)3wereclassifiedintosynonym/non-synonymclasses.Totestwhetherornotagivenwordpair(w_1,w_2)isasynonympair,threeexistingthesauriwereconsulted:Roget'sThesaurus,CollinsCOBUILDThesaurus,andWordNet.Theunionofsynonymsobtainedwhentheheadwordislookedupasanounisusedastheanswerset,exceptforwordsmarkedas``idiom,''``informal,''``slang''andphrasescomprisedoftwoormorewords.Thepair(w_1,w_2)ismarkedassynonymsifandonlyifw_2iscontainedintheanswersetofw_1,orw_1iscontainedinthatofw_2.Allthepositive-markedpair,aswellasrandomlychosenoneoutoffivenegative-markedpairs,werecollectedastheexamplesetE.Thisrandomselectionistoavoidextremebiastowardthenegativeexamples.TheexamplesetEendedupwith2,148positiveand13,855negativeexamples,withtheirratiobeingapprox.6.45.TheexamplesetEwasthendividedintofivepartitionstoconductfive-foldcrossvalidation,ofwhichfourpartitionswereusedforlearningandonefortesting.SVM^lightwasadoptedformachinelearning,andweusedthelinearandRBF(radialbasisfunctions)kernels.Theparameters,i.e.,thesimilaritythresholdofDSIMclassifier,gammaparameteroftheRBFkernel,andthecost-factorjofSVM,i.e.,theratiobywhichtrainingerrorsonpositiveexamplesoutweigherrorsonnegativeones,wereoptimizedusingoneofthefive-foldcrossvalidationtrain-testpairasadevelopmentsetonthebasisofF-1measure.Theoptimalvaluesweregrid-searchedonj=6,7,8,9,10forthelinearkernel,andj=6,7,8,9,10and=0.001,0.005,0.01,0.05,0.1fortheRBFkernel.Theperformancewasevaluatedfortheotherfourtrain-testpairsandtheaveragevalueswererecorded.HeretheexamplesetconstructionstatedaboveseemsgreatlydependentontheLDV.Ideally,theexamplesetshouldberandomlycreatedoutofaunlimitedsetofwords.However,theopentestdescribedinSection6.4showsthatthesupervisedclassifiersperformwellevenfornon-LDVwords,suggestingthatLDV-trainedmodelsarenotdependentonLDVbutgeneralenoughtobeappliedtoout-of-LDVsynonymclassification.</subsubsection>
  <subsection title="Performance Comparison">Theperformances,i.e.,precision,recall,andF-1measure,ofthesevenclassifierswereevaluatedandshowninTable.DSIMclassifierisunsupervised,whiletheothersixclassifiersaretrainedusingboththelinearandRBFkernels.Firstly,whenwecompareDSIMandDSIM-PAT,thelattershowedslightincreaseonrecall,butnotonprecision,andF-1measuredidnotimproveevenwhentheRBFkernelisused.Thismeansthat,eventhoughintegratingpattern-basedfeaturesdidincreasetherecall,wecouldnotconfirmthepositiveresultofMirkinetal.'sintegratedapproachforthissynonymacquisitiontask.Ontheotherhand,whencommonordistributionalfeaturesareusedinsteadofdistributionalsimilarity,weobserveddrasticperformanceimprovement---bothprecisionandrecallimprovedandF-1measureincreasedbymorethan50%,comparingDSIMandCFEAT/DFEAT.TheimprovementisgreaterforDFEATthanforCFEAT,especiallywhentheRBFkernelisused.FurtheraddingPATfeatures(CFEAT-PATandDFEAT-PAT)tocommon/distributionalfeaturesincreasedtheperformanceslightlyforCFEAT,butnotforDFEATatall.Thisimpliesthatthediscriminativeabilityofdistributionalfeaturesweresohighthatpattern-basedfeatureswerealmostredundant.ThereasonofthedrasticimprovementofCFEAT/DFEATisthat,asfaraswespeculate,thesupervisedlearningmayhavefavorablyworkedtocausethesameeffectasautomaticfeatureselectiontechnique.Featureswithhighdiscriminativepowermayhavebeenautomaticallypromoted.InthedistributionalsimilaritysettingandMirkinetal.'sintegratedapproach,incontrast,thecontributionsofcontexttypesareuniformlyfixed.Furthermore,usingtheRBFkernel(aswellassomeotherkernelssuchaspolynomialkernels)correspondstomappingtheinputvectorstoahigher-dimensionalspace,whichincludeshigherdegreesofcombinationsofinputfeatures.Throughthisprocess,theclassifiersmighthavebeenabletofindusefulfeaturecombinationsfordiscrimination,andthisledtothesuperiorperformanceoftheRBFkernelcomparedtothelinerone.Tonote,theperformanceofPATwasthelowest,reflectingthefactthatsynonympairsrarelyoccurinthesamesentence,makingtheidentificationusingonlysyntacticpatterncluesevenmoredifficult.Onthecontrary,Mirkinetal.'staskistheautomaticacquisitionoflexicalentailmentrelations,whichareessentiallyasymmetric.Itisforthesamereasonthataddingpattern-basedfeaturestootherfeaturesshowedlittlecontributiontoclassificationperformanceexceptforCFEAT.Wefurthercomparedtheperformanceoftheall-in-oneclassifierALLwiththatofotherclassifiers.Itgreatlyincreasedprecisioncomparedtoalltheothermethods,evenapprox.5%higherthanCFEAT/DFEAT.Ontheotherhand,itdidnotachieveashighrecallasCFEAT/DFEATdid,buttheoverallF-1measureimprovedto4%higherthanDFEATwiththelinearkerneland1%higherwiththeRBFkernel.AlthoughALLachievedoneofthehighestprecision/F-1levels,noticethatitrequiresmorethantwicethesizeofthefeaturespacecomparedtotheothers,anditiscomputationallyextensive.Therefore,thetrade-offbetweencomputationalcostandperformancegainshouldbetakenintoconsiderationinpractice.</subsection>
  <subsection title="Extracted Synonyms">Inthesecondpartoftheexperiment,wefurtherinvestigatedwhatkindofsynonymswereactuallyacquiredbytheclassifierswebuilt.Thisisanopentestwhichtargetsattheextractionofnon-LDVwords.ThereasonwhyweconductedthisisthatwecannotruleoutthepossibilitythatthehighperformanceoftheDFEAT-basedmethodsseeninthepreviousexperimentwassimplyduetotheratherlimitedtargetwordsettings.Therestoftheexperimentalsettingwasalmostthesameasthepreviousone---weusedthesamemodeltrainedontheexamplesetE.TheRBFkernelwasagainusedforSVM-basedclassifiers.Theonlydifferenceisthatthetestsetalsoincludednon-LDVwords,i.e.,allthewordswhichappearedmorethan_wtimesinthecorpus.BecauseincludingPATfeaturesdidnotyieldasignificantdifferenceinthepreviousexperimentsexceptforCFEAT,weonlypaidattentiontotheDSIM,CFEAT,andDFEATclassifiers.Thetaskhereisactuallytoranksynonyms,nottoclassifythem,andthisisdonebyfirstlyfixingoneword,whichwecallqueryword,andthencomputingsimilaritiesofalltheotherwordsinthevocabularyandrankingthembythesimilarityinadescendingorder.TheSVM-basedclassifierscannotcomputethesimilaritydirectly,soweusedthevalueofdecisionfunctionofSVM,i.e.,thedistancefromthemaximum-marginhyperplane.Wechosethethreequerywordsinthissectionfromnon-LDVwords,andthisguaranteesthatthisisanopentest(rememberthattheexamplesetEconsistsonlyofLDVwords).Thefirstcaseisthetop10synonymsacquiredforthewordopera,whichareshowninTable.ThisisacasewheretheoutputsofDSIM,CFEAT,andDFEATarealmostasgoodasothers.Inadditiontothethreequerywordslistedinthissection,wemanuallychecked15querywords,andfoundoutthatthiswasthemostcommoncase.HereevenDSIMperformsquitewell,andsodotheothers.TheresultofCFEATseemstoincludethefewernumberofrelevantwords,butitdoesinclude``interesting''relatedwordsinstead,suchasoratorioandrepertory.ThismayexplaintheslightlylowerprecisionandhigherrecallofCFEATcomparedwithDFEAT.Tableshowsthesynonymsfortheword.Thisisanothercase,whereDSIMperformedpoorly.CFEATalsofailedtoidentifyappropriatesynonyms,althoughitsrankingisslightlydifferentfromDSIM's.Ontheotherhand,DFEATsuccessfullylistedmanyrelatedwordstocontinent.Alongwiththepreviousexperiment,thisresultsuggeststhesuperiorityofDFEATcomparedtoDSIM.Finally,Tableshowsthesynonymsforthewordpurse.Thisisyetanothercase,whereallthethreeclassifiershavedifficultyfindingcorrectsynonyms.TheresultofDSIMisapparentlyfilledwithanumberofunrelatedwords,perhapsexceptforthewordcamisole.Wesuspectthatthisisbecausethewordpurseisunlikelytoappearinmostnewsarticles,andevenwhenitdoes,theuseofthewordcanbedifferentfromthenormalusage.Still,CFEATbeganlistingsomerelatedwordssuchaspantsuit,tunic,andkimono,althoughthenumberisnotlarge.DFEATalsomanagedtoextractsomethingyouwear(e.g.,hat,andbag)andthisisdefinitelyastepinarightdirection.Thisresultalsoshowstherobustnessoftheclassification-basedapproachestosynonymacquisition.</subsection>
  <subsection title="Error Anaysis">Tofurtherinvestigatethecauseoferrorstheclassifiersmade,welookedintothemisclassifiedexamplesinthetestset.BecauseweusedDFEAT'sbestresultinthefollowinganalysis,theexamplesbelowcanbetheoneswhichalloftheclassifiersfinddifficultyinclassifying.Firstly,wepayattentiontofalsenegatives,i.e.,examplesforwhichtheannotatedanswerispositive(synonym)buttheoutputoftheclassifierwasnot(non-synonym).Thepair(word,news)isoneofthem.Aswecaneasilyguess,thereasonofmisclassificationisthatthesearesynonymsonlyinlimitedcases(e.g.spreadtheword).Ineffect,wenoticethatthesetwowordsdonotsharesignificantamountofcontext,exceptfor(ncsubjleak*_)(bothwordscanbethesubjectoftheverbleak),(ncsubjspread*obj)(bothwordscanbethesubjectoftheverbspreadinapassivesentence),andsoon.Wefoundsomeotherpairs,suchas(damage,cost)and(level,standard),whichfallintothiscategory.Wealsofoundanothercategoryoffalsenegatives,whichwecall-of-domainwords.Thisoneincludespairssuchas(soul,body)and(path,road),andthesepairsmayhavebeenmisclassifiedbecausetheyconsistofwordsthataregeneralenoughbutnotexactlyrelatedtonewsarticles,andasufficientamountofcontextwasn'tobtainedfromthecorpusweused.Addinggeneral-domaincorporacouldsolvethisproblem.Secondly,welookedintofalsepositives,i.e.,examplesforwhichtheannotatedanswerisnegative(non-synonym)buttheoutputoftheclassifierwasnot(synonym).Surprisingly,wefoundoutthatmanyoffalsepositiveswereactuallyrelatedinsomesense.Forexample,itincludedpairssuchas(stage,part),(,mind),and(corner,end),whichinfactsharealotofcontexttypes---(det*latter),(ncmod_*early),(ncmod_*crucial),etc.for(stage,),(ncmodposs*people),(ncmodposs*everyone),(ncmod_*suspicious),etc.for(eye,mind),and(ncmod_*opposite),(ncmod_*upper),(dobjnear*),etc.for(corner,end).Althoughthesearenotexactsynonyms,theclassifierdidn'tcompletelyfailtocapturerelatedwords,whichcouldbecandidatesforsynonyms.Thenegativeexamplesare``contaminated''withsuchrelatedwordsbecausewecreatedthembyensuringthattheyarenotincludedinthethesauriweused,butthisdoesnotnecessarilyensurethattheyarenotevenrelated.Furtherrefiningthetrainsetbymanuallyensuringthattheydon'tincludeanyrelatedpairscaneliminatesuchfalsepositives.Ingeneral,theperformanceandthebehaviorofourclassifierscouldbegreatlyaffectedbythetrainsetsused---forexample,weexpectthatitispossibletoextractexactsynonymsbyconstrainingthetrainsettoincludeonlythe``tightly''relatedwords.Thisalsomightenabletoexcludeantonymsandcohyponymsfromextractedrelatedterms,asLinetal.did.Theeffectofreplacingthetrainsetsshouldbeinvestigatedinthefuture.</subsection>
  <subsection title="Effective Features">Wearenownaturallyinterestedinwhichcommon/distributionalfeatures,i.e.,whichoff^C_1,...,f^C_Morf^D_1,...,f^D_M,werethemostusefulforthetask.Therecanbeanumberofmethodstorankorselectfeaturesformachinelearning,butherewesimplyevaluatedtheeffectivenessofafeature(orpossiblyasetoffeatures)byremovingthetargetfeaturesandre-evaluatetheoverallperformanceoftheclassifier.Thebiggerthedifferenceoftheperformanceis,themorelikelytheremovedfeaturesaretobeimportant.However,itisimpracticaltoinvestigatetheindividualcontributionofeverysingledistributional/commonfeatureinthiswaybecausetherearesomany(3,975innumber)ofthem.Thereforewefirstlyinvestigatethecontributionoffeaturecategories,i.e.,setsoffeaturesgroupedbytheirsyntacticrelations.Inthisexperiment,weevaluatedthedifferenceofperformance(F-1)comparedtotheclassifierperformancewithallthefeatures,bytakingoutonecategoryatatime.WeusedtheDSIMclassifierandthelinearkernelbecausetheRBFkernelconsidersthehigherdegreeofcorrelationbetweenfeaturesaswementionedinSection6.3andthismightmakeitdifficulttoevaluatethecontributionsoffeaturesindividually.Figureshowstheresultofthisexperiment.TheamountofF-1decreaseofthefeaturecategoryisshown,alongwiththenumberofthecontexttypesthatthecategorycontains.Contextcategorieswith10orfewercontexttypesareomittedhere.Itclearlyshowsthatthecategoriesdobj,ncmod,and,ncmod,andncsubjstandfordirectobject,non-clausalmodification,andnon-clausalsubject,respectively.demonstratethebestperformance.Thesearealsothecategorieswhichincludethehighestnumbersofcontexttypes,andthissuggeststhatcategorieswithmorecontexttypestendtoachievehigherperformance.Indeed,thereisahighcorrelation(=0.95)betweentheperformancedecreaseandthenumberofcontexttypes,anditisnotthequalitybutthesizeofthecategoriesthatlargelydeterminesitscontribution.Wethenrandomlyextractedfivecontexttypesassamplesfrom,ncmod,andncsubjandevaluatedtheircontributionsinthesamewayasweinvestigatedthecontributionofcontextcategories.TheresultisshowninTable,wheretheamountofF-1decreaseandthefrequencyofthecontexttypeintheexamplesetareshown.Althoughthetwohighestvaluesineachcategory,whichareemphasizedinboldface,suggestthathigh-frequencyfeaturestendtocontributetotheperformancemore,thecorrelationislessevident(=0.43)thanthatofcontextcategoriesandsomeofthefeatures,e.g.,ncmod:_:rightandncmod:_:citydonotperformwellinspiteoftheirhighfrequency.Theresultimpliesthatthespecificityofcontexttypescorrelateswiththeirperformance,becausencmod:_:*:righthasquiteabroadmeaning,whilencmod:_:*:roaddoesnot.Furtherinvestigationonfeaturetypesandtheirperformancecontributionisthefuturework.</subsection>
  <section title="Conclusion">Inthispaper,weproposedanovelapproachtoautomaticsynonymidentificationbasedonsupervisedlearningtechniqueandnewcontext-basedfeatures.Wefirstlyre-formalizedthesynonymacquisitiontaskasaclassificationproblem,andthenproposedcontext-basedfeaturescalleddistributionalfeatures,asopposedtocommonfeatureswhichhavebeenusedintheconventionalmethods.Thisformalizationenabledtousebothcontext-basedfeaturesandpattern-basedfeaturesandtobuildfullyintegratedclassifiers.Intheexperiments,webuilteightclassifiersbasedondistributional,common,and/orpattern-basedfeatures.Theexperimentalresultsshowedthatcontext-basedfeaturesshowedagreatincrease(morethan60%onF-1measure)comparedtodistributionalsimilarity-basedmethods.Ontheotherhand,pattern-basedfeatureswereonlypartiallyeffectivewhencombinedwithcommonfeatureswhereaswithdistributionalfeaturestheyweresimplyredundant.IncludingallthefeaturesdidincreaseprecisionandF-1measure,attheexpenseofaslightreductionofrecallandhighcomputationalcomplexity.TheimpactofthisstudyisthatitmakesunnecessarytocarefullychoosesimilaritymeasuressuchascosineandJaccard's.Instead,featurescanbedirectlygiventosupervisedlearningmethodsrightaftertheirconstruction.Therearestillsomeissuestoaddressasthecurrentapproachisonlyinitsinfancy.Forexample,theformalizationofdistributionalfeaturesrequiresfurtherinvestigation.Althoughweadoptedtotalcorrelationthistime,therecouldbesomeotherconstructionmethodswhichmayshowhigherperformance.Anothercontributionofthisstudyisthatitintroducedsupervisedlearningapproachestosynonymacquisition.Thismakesitpossibletousevarioustechniquesproposedinthemachinelearningfield.Forexample,theuseofsophisticatedkernelssuchasfeatureconjunctionisworthcarefulconsideration,andhowtheyaffectthesynonymacquisitionperformanceshouldbeinvestigatedinthefuture.document</section>
</root>
