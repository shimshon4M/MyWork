<?xml version="1.0" ?>
<root>
  <jtitle>擬似確率的単語分割コーパスによる言語モデルの改良</jtitle>
  <jauthor>森信介小田裕樹</jauthor>
  <jabstract>言語モデルの分野適応において，適応対象の分野の単語境界情報のない生コーパスの有効な利用方法として，確率的単語分割コーパスとしての利用が提案されている．この枠組では，生コーパス中の各文字間に単語境界が存在する確率を付与し，それを用いて単語n-gram確率などが計算される．本論文では，この単語境界確率を最大エントロピー法に基づくモデルによって推定することを提案する．さらに，確率的単語分割コーパスを従来の決定的に単語に分割されたコーパスで模擬する方法を提案し，言語モデルの能力を下げることなく計算コストが大幅に削減できることを示す．</jabstract>
  <jkeywords>言語モデル，確率的単語分割，音声認識，仮名漢字変換</jkeywords>
  <subsection title="単語n-gram頻度">確率的単語分割コーパスに対して単語n-gram頻度が以下のように定義される．</subsection>
  <section title="はじめに">一般的な分野において精度の高い単語分割済みコーパスが利用可能になってきた現在，言語モデルの課題は，言語モデルを利用する分野への適応，すなわち，適応対象分野に特有の単語や表現の統計的振る舞いを的確に捉えることに移ってきている．この際の標準的な方法では，適応対象のコーパスを自動的に単語分割し，単語n-gram頻度などが計数される．この際に用いられる自動単語分割器は，一般分野の単語分割済みコーパスから構築されており，分割誤りの混入が避けられない．特に，適切に単語分割される必要がある適応対象分野に特有の単語や表現やその近辺において誤る傾向があり，単語n-gram頻度などの信頼性を著しく損なう結果となる．上述の単語分割誤りの問題に対処するため，確率的単語分割コーパスという概念が提案されている．この枠組では，適応対象の生コーパスは，各文字の間に単語境界が存在する確率が付与された確率的単語分割コーパスとみなされ，単語n-gram確率が計算される．従来の決定的に自動単語分割された結果を用いるより予測力の高い言語モデルが構築できることが確認されている．また，仮名漢字変換や音声認識においても，従来手法に対する優位性が示されている．確率的単語分割コーパスの初期の論文では，単語境界確率は，自動分割により単語境界と推定された箇所で単語分割の精度（例えば0.95）とし，そうでない箇所で1-とする単純な方法により与えられている．実際には，単語境界が存在すると推定される確率は，文脈に応じて幅広い値を取ると考えられる．例えば，学習コーパスからはどちらとも判断できない箇所では1/2に近い値となるべきであるが，既存手法では1に近いか，0に近い1-とする他ない．この問題に加えて，既存の決定的に単語分割する手法よりも計算コスト（計算時間，記憶領域）が高いことが挙げられる．その要因は2つある．1つ目は，期待頻度の計算に要する演算の種類と回数である．通常の手法では，学習コーパスは単語に分割されており，これを先頭から単語毎に順に読み込んで単語辞書を検索して番号に変換し，対応する単語n-gram頻度をインクリメントする．単語辞書の検索は，辞書をオートマトンにしておくことで，コーパスの読み込みと比較して僅かなオーバーヘッドで行える．これに対して，確率的単語分割コーパスにおいては，全ての連続するn個の部分文字列（L文字）に対して，L+1回の浮動小数点数の積を実行して期待頻度を計算し，さらに1回の加算を実行する必要がある（subsection:EF参照）．2つ目の要因は，学習コーパスのほとんど全ての部分文字列が単語候補になるため，語彙サイズが非常に大きくなることである．この結果，単語n-gramの頻度や確率の記憶領域が膨大となり，個人向けの計算機では動作しなくなるなどの重大な制限が発生する．例えば，本論文で実験に用いた44,915文の学習コーパスに出現する句読点を含まない16文字以下の部分文字列は9,379,799種類あった．このうち，期待頻度が0より大きい部分文字列と既存の語彙を加えて重複を除いた結果を語彙とすると，そのサイズは9,383,985語となり，この語彙に対する単語2-gram頻度のハッシュによる記憶容量は10.0~GBとなった．このような時間的あるいは空間的な計算コストにより，確率的単語分割コーパスからの言語モデル構築は実用性が高いとは言えない．このことに加えて，単語クラスタリングや文脈に応じた参照履歴の伸長などのすでに提案されている様々な言語モデルの改良を試みることが困難になっている．本論文では，まず，確率的単語分割コーパスにおける新しい単語境界確率の推定方法を提案する．さらに，確率的単語分割コーパスを通常の決定的に単語に分割されたコーパスにより模擬する方法を提案する．最後に，実験の結果，言語モデルの能力を下げることなく，確率的単語分割コーパスの利用において必要となる計算コストが大幅に削減可能であることを示す．これにより，高い性能の言語モデルを基礎として，既存の言語モデルの改良法を試みることが容易になる．</section>
  <section title="確率的単語分割コーパスからの言語モデルの推定">確率的言語モデルを新たな分野に適応する一般的な方法は，適応分野のコーパスを用意し，それを自動的に単語分割し，単語の頻度統計を計算することである．この方法では，単語分割誤りにより適応分野のコーパスにのみ出現する単語が適切に扱えないという問題が起こる．この解決方法として，適応分野のコーパスを確率的単語分割コーパスとして用いることが提案されている．この節では，確率的単語分割コーパスからの確率的言語モデルの推定方法について概説する．</section>
  <subsection title="確率的単語分割コーパス">確率的単語分割コーパスは，生コーパスC_r（以下，文字列x_1^n_rとして参照）とその連続する各2文字x_i,x_i+1の間に単語境界が存在する確率P_iの組として定義される．最初の文字の前と最後の文字の後には単語境界が存在するとみなせるので，i=0,;i=n_rの時は便宜的にP_i=1とされる．確率変数X_iを[X_i=.]とし(P(X_i=1)=P_i,;P(X_i=0)=1-P_i)，各X_0,X_1,,X_n_rは独立であることが仮定される．文献の実験で用いられている単語境界確率の推定方法は次の通りである．まず，単語に分割されたコーパスに対して自動単語分割システムの境界推定精度を計算しておく．次に，適応分野のコーパスを自動単語分割し，その出力において単語境界であると判定された点ではP_i=とし，単語境界でないと判定された点ではP_i=1-とする．後述する実験の従来手法としてこの方法を採用した．</subsection>
  <section title="最大エントロピー法による単語境界確率の推定">この節では，最大エントロピー法による単語分割器を単語境界確率の推定に用いる方法について述べる．</section>
  <subsection title="単語境界確率の推定">日本語の単語分割の問題は，入力文の各文字間に単語境界が発生するか否かを予測する問題とみなせる．つまり，文x=xmに対して，x_ix_i+1の間が単語境界であるか否かを表すタグt_iを付与する問題とみなす．付与するタグは，単語境界であることを表すタグEと，非単語境界であることを表すタグNの2つのタグからなる．各文字間のタグがこのいずれかであるかは，単語境界が明示されたコーパスから学習された点推定の最大エントロピーモデル(MEmodel;maximumentropymodel)により推定する．その結果，より高い確率を与えられたタグをその文字間のタグとし，単語境界を決定する．すなわち，以下の式が示すように，最大エントロピーモデルにより，単語境界と推定される確率が非単語境界と推定される確率より高い文字間を単語境界とする．[t_i=.]これにより，入力文を単語に分割することができる．本論文では，以下のように，タグt_iの出現確率を確率的単語分割コーパスにおける単語境界確率P_iとして用いることを提案する．P_i=P_ME(t_i=E|x)displaymathこれにより，注目する文字の周辺のさまざまな素性を参照し，単語境界確率を適切に推定することが可能になる．</subsection>
  <subsection title="参照する素性">後述する実験においては，x_ix_i+1の間に注目する際の最大エントロピーモデルの素性としては，x_i-1^i+2の範囲の文字n-gramおよび字種n-gram(n=1,2,3)をすべて用いた．ただし，以下の点を考慮している．素性として利用するn-gramは，先頭文字の字種がその前の文字の字種と同じか否か，および，末尾文字の字種がその次の文字の字種と同じか否かの情報を付加して参照する^i+3の範囲のn-gram(n=3,4,5)を参照する．．素性には注目する文字間の位置情報を付加する．たとえば，文字列「文字列を単語に分割する」の「語」「に」の文字間の素性は，-単+|,+語|-,-|に-,|-分+,-単語|-,+語|に-,-|に分+,-単語|に-,+語|に分+,-K+|,+K|-,-|H-,|-K+,-KK|-,+K|H-,-|HK+,-KK|H-,+K|HK+,となる．「|」は注目する文字間を表す補助記号であり，「+」と「-」は前後の文字が同じ字種である(+)か否(-)かを表す補助記号である．「H」と「K」は字種の平仮名と漢字を表している．なお，実験においては，パラメータ数を減らすために，学習データで2回以上出現する素性のみを用いた．また，最大エントロピーモデルのパラメータ推定には，GISアルゴリズムを使用した．</subsection>
  <section title="疑似確率的単語分割コーパス">確率的単語分割コーパスに対する単語n-gram頻度は，高いコストの計算を要する．また，確率的単語分割コーパスは，頻度計算の対象となる単語や単語断片（候補）を多数含む．ある単語n-gramの頻度の計算に際しては，その単語の文字列としてのすべての出現に対して，頻度のインクリメントではなく，複数回の浮動小数点演算を実行しなければならない．この計算コストにより，より長い履歴を参照する単語n-gramモデルや単語クラスタリングなどの言語モデルの改良が困難になっている．上述の困難を回避する方法として，単語分割済みコーパスで確率的単語分割コーパスを近似する方法を提案する．具体的には，確率的単語分割コーパスに対して以下の処理を最初の文字から最後の文字まで(1in_r)行なう．文字x_iを出力する．0以上1未満の乱数r_iを発生させP_iと比較する．r_i&lt;P_iの場合には単語境界記号を出力し，そうでない場合には何も出力しない．これにより，確率的単語分割コーパスに近い単語分割済みコーパスを得ることができる．これを疑似確率的単語分割コーパスと呼ぶ．上記の方法では，文字列としての出現頻度が低い単語n-gramの頻度が確率的単語分割コーパスと疑似確率的単語分割コーパスにおいて大きく異なる可能性がある．そもそも，出現頻度が低い単語n-gramの場合，単語分割が正しいとしても，その統計的振る舞いを適切に捉えるのは困難であるが，近似によって誤差が増大することは好ましくない．従って，この影響を軽減するために，上記の手続きをN回行ない，その結果得られるN倍の単語分割済みコーパスを単語n-gram頻度の計数の対象とすることとする．このときのNを本論文では倍率と呼ぶこととする．疑似確率的単語分割コーパスは，一種のモンテカルロ法となっている．モンテカルロ法によるd次元の単位立方体上[0,d]^d上の定積分I=_[0,1]^df(x)dxの数値計算法では，単位立方体[0,d]^d上の一様乱数xNを発生させてI_N=_i=1^Nf(x_i)とする．このとき，誤差|I_N-I|は次元dによらずに1/Nに比例する程度の速さで減少することが知られている．疑似確率的単語分割コーパスにおける単語n-gram頻度の計算はこの特殊な場合であり，nの値や文字数によらずに1/FNに比例する程度の速さで減少する．ここでFは単語n-gramの文字列としての頻度である．</section>
  <section title="評価">単語境界確率の推定方法の評価として，言語モデルの適応の実験を行なった．まず，適応対象文野の大きな生コーパスに既存手法と提案手法のそれぞれで単語境界確率を付与した．次に，その結果得られる確率的単語分割コーパスから単語2-gramモデルを推定し，これを一般分野の単語分割済みコーパスから推定された単語2-gramモデルと補間した．最後に，適応分野のテストコーパスに対して，予測力と仮名漢字変換の精度の評価を行なった．後者は，理想的な音響モデルを用いた場合の音声認識と考えることも可能である．この節では，実験の結果を提示し，評価を行なう．</section>
  <subsection title="実験の条件">実験に用いたコーパスは，「現代日本語書き言葉均衡コーパス」モニター公開データ（2008年度版）中の人手による単語分割の修正がなされている文（一般コーパス）と医療文書からなる適応対象のコーパスである．一般コーパスの各文は正しく単語に分割され，各単語に入力記号列（読み）が付与されている．これを10個に分割し，この内の9個を学習コーパスとし，残りの1個をテストコーパスとした（table:corpus参照）．自動単語分割器や単語境界確率の推定のための最大エントロピーモデルはこの学習コーパスから構築される．一方，適応対象のコーパスは大量にあるが，単語境界情報を持たない．この内の7,000文に入力記号列（読み）を付与しテストコーパスとし，残りを確率的単語分割コーパスとして言語モデルの学習に用いた（table:raw-corpus参照）．テストコーパスの内の1,000文には，単語境界情報も付与し，言語モデルの予測力の評価に用いた．</subsection>
  <subsection title="評価基準">確率的言語モデルの予測力の評価に用いた基準は，テストコーパスにおける単語あたりのパープレキシティである．まず，テストコーパスC_tに対して未知語の予測も含む文字単位のエントロピーHを以下の式で計算する．H=-1|C_t|_2_wC_tM_w,n(w)displaymathここで，M_w,n(w)は単語n-gramモデルによる単語列wの生成確率を，|C_t|はテストコーパスC_tの文字数を表す．次に，単語単位のパープレキシティを以下の式で計算する．PP=2^H|w|displaymathここで|w|は平均単語長（文字数）である．これらの計算に際しては，単語境界情報が付与された1,000文を用いた．仮名漢字変換の評価基準は，文字誤り率である．文字誤り率は=1-N_LCS/N_CORと定義される．ここで，N_CORは正解に含まれる文字数であり，N_LCSは各文を一括変換することで得られる最尤解と正解との最長共通部分列(LCS;LongestCommonSubsequence)の文字数である．</subsection>
  <subsection title="単語境界確率の推定方法の評価">単語境界確率の推定方法の差異を調べるために，以下の2つの確率的単語分割コーパスを作成しそれらから推定された単語2-gramモデルの能力を調べた</subsection>
  <subsection title="疑似確率的単語分割コーパスの評価">本論文のもう一つの論点は，単語分割済みコーパスによる確率的単語分割コーパスの近似である．この評価として，3種類の大きさ(1/1,1/2,1/4)の適応分野の疑似確率的単語分割コーパスから推定した言語モデルのテストコーパスに対するパープレキシティと文字正解率を複数の倍率(N=1,2,4,,256)に対して計算した．table:result2〜table:result4はその結果である．まず，自動分割の結果を決定的単語分割コーパスとして用いる場合についてである．これと，確率的単語分割コーパスとして用いる場合との比較では，文献の報告と同じように確率的単語分割により予測力が向上し，文献の報告と同じように仮名漢字変換の文字正解率も向上している．さらに，本論文で提案する倍率が1の疑似確率的単語分割は，決定的単語分割に対して，予測力と文字正解率の双方において優れていることが分る．倍率が1の疑似確率的単語分割と決定的単語分割の唯一の違いは，自動単語分割の際に単語境界確率を0.5と比較するか，0から1の乱数と比較するかであり，モデル構築の計算コストはほとんど同じである．にもかかわらず，予測力と文字正解率の双方が向上している点は注目に値するであろう．次に，確率的単語分割と疑似確率的単語分割の比較について述べる．倍率が1の場合は，予測力や文字正解率は，確率的単語分割コーパスから推定された言語モデルに対して少し低く，倍率を上げることによりこれらは確率的単語分割コーパスによる結果に近づいていくことがわかる．これは，疑似確率的単語分割がモンテカルロ法による数値演算の一種になっていることを考えれば当然の結果である．このことから，ある程度の倍率の疑似確率的単語分割コーパスは，確率的単語分割コーパスのよい近似となっているといえる．適応分野のコーパスの大きさに係わらず，倍率が256の場合の疑似確率的単語分割による結果は，確率的単語分割の結果とほぼ同じといえる．最後に，確率的単語分割と疑似確率的単語分割の計算コストの比較について述べる．確率的単語分割の語彙サイズは，適応対象の学習コーパスにおける期待頻度が0より大きい16文字以下の部分文字列と一般コーパスの語彙の合計9,383,985語であった．この語彙に対する単語2-gram頻度をハッシュ(BerkeleyDB4.6.21)を用いてファイルに出力すると10.0~GBとなった．これをRAMディスク上で計算するのに61147.45秒（約17時間）を要した．同じ計算機で，16倍の疑似確率的単語分割コーパスから単語2-gram頻度をRAMディスク上で計算すると，語彙サイズが46,777語であり，単語2-gram頻度のファイルサイズは9.98~MBであり，計算時間は1009.95秒（約17分）と約61分の1となった．疑似確率的単語分割コーパスを用いた場合には，倍率が256の場合でも20.2~MBと，ファイルサイズが大きくないので，現在の多くの計算機で主記憶上で計算が可能である（主記憶上での計算時間は303.29秒）．これに対して，確率的単語分割コーパスからの推定では，一部の計算機においてのみ主記憶上での計算が可能である．さらに，実験で用いた適応対象の分野のコーパスは44,915文と決して大きくはなく，適応分野によっては1桁か2桁ほど大きい学習コーパスが利用できることも十分考えられる．この場合には，確率的単語分割では2次記憶（RAMディスクかハードディスク）上での計算が避けられず，モデル作成にかかる計算時間の違いは非常に大きくなる．したがって，本論文で提案する疑似確率的単語分割は，この点から有用であると考えられる．疑似確率的単語分割において，どの程度の倍率がよいかは要求する精度と利用可能な計算機資源との兼ね合いである．例えば倍率が16の場合は，単語に分割された718,640文から言語モデルを推定することになる．モデル構築に要する計算時間は，決定的単語分割の場合の16倍程度であり，現在の計算機はこの大きさのコーパスを処理する能力が十分ある．したがって，疑似確率的単語分割により，単語3-gramモデルや可変長記憶マルコフモデル，あるいは言語モデルのための単語クラスタリングなどさらなる言語モデルの改善を容易に試みることが可能となる．</subsection>
  <section title="おわりに">本論文では，確率的単語分割コーパスにおける新しい単語境界確率の推定方法を提案した．実験の結果，提案手法により約11%のパープレキシティの減少と約3.1%の文字誤りの削減が確認された．さらに，確率的単語分割コーパスを通常の決定的単語分割コーパスにより模擬する方法を提案した．実験の結果，言語モデルの能力を下げることなく，確率的単語分割コーパスの利用において必要となる計算コストが削減可能であることを示した．</section>
</root>
