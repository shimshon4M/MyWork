    \documentclass[japanese]{jnlp_1.4}
\usepackage{jnlpbbl_1.1}
\usepackage[dvips]{graphicx}
\usepackage{amsmath}
\usepackage{hangcaption_jnlp}
\usepackage{udline}
\setulminsep{1.2ex}{0.2ex}
\let\underline


\Volume{16}
\Number{3}
\Month{July}
\Year{2009}

\received{2008}{11}{12}
\revised{2009}{2}{10}
\accepted{2009}{2}{16}

\setcounter{page}{25}


\jtitle{概念ベースとEarth Mover's Distanceを用いた文書検索}
\jauthor{藤江　悠五\affiref{Author_1} \and 渡部　広一\affiref{Author_1} \and 河岡　　司\affiref{Author_1}}
\jabstract{
近年，コンピュータとネットワークの発達に伴って，個人が扱える情報は膨大なものとなり，その膨大な情報の中から必要な情報を探し出すのは非常に困難となっている．既存の検索システムは基本的には表記のみを活用するため，意味的には同じ内容の検索でもユーザが入力する語によって検索結果が異なってしまう．そのためユーザが適切なキーワードを考えなければならない．そこで本稿では文書の意味を捉えた検索を実現するために単語の関連性にもとづいた文書間の類似性の定量化手法を提案する．具体的には概念ベースを用い単語間の関連性を求め，Earth Mover's Distanceにより文書間の類似度を計算する方法を提案する．また概念ベースに存在しない固有名詞や新語に対して，Web情報をもとに新概念として意味を定義し，概念ベースを自動的に拡張する方法を提案する．これら提案手法をNTCIR3-WEBによって他の手法と比較実験したところ，本手法が他手法に比べ良好な結果が得られた．
}
\jkeywords{情報検索，概念ベース，Earth Mover's Distance, NTCIR}

\etitle{Associative Document Retrieval Using Concept Base and Earth Mover's Distance}
\eauthor{Yugo fujie\affiref{Author_1} \and Hirokazu Watabe\affiref{Author_1} \and Tsukasa Kawaoka\affiref{Author_1}} 
\eabstract{
Recently the development of computers and networks makes amount of information huge. It is very difficult to find necessary information in the huge information. The existing retrieval system uses not the meaning of input words but the notation of them. Therefore, different words bring a defferent result of retreieval even if they have the same meaning. A user of the system has to consider the input words to search the necessary information. This paper proposes the quantification technique of the semantic distance between documents based on relevance of the word to realize the search that captured the meaning of the document. Concretely the related degree between words is calculated by concept-base and the resemblance degree between documents is calculated by Earth Mover's Distance. Besides this paper proposes method that no existence word on concept-base is defined as a concept based on Web information to expand concept-base automatically. Retrieval experiments using the NTCIR3-WEB in comparison with the other method have shown that our method is effective than other method.}
\ekeywords{Information Retrieval, Concept-base, Earth Mover's Distance, NTCIR}


\headauthor{藤江，渡部，河岡}
\headtitle{概念ベースとEarth Mover's Distanceを用いた文書検索}

\affilabel{Author_1}{同志社大学大学院工学研究科知識工学専攻}{Dept. of Knowledge Engineering \& Computer Sciences, Graduate School of Engineering Doshisha University}


\begin{document}
\maketitle


\section{はじめに}
一般家庭にもPC，ブロードバンドが普及し，ユーザは手軽に情報を収集できるようになってきている．しかし一方では，情報が過度に溢れ過ぎ，利用者の要求に合った情報を探し出す必要性が高まっている．その中で要求に適合した情報のみを選出するのではなく，情報をランキング付けして提示することも重要となっている．ランキング付けは，検索要求と検索対象との間の類似性や関連性をもとに行われ，これらを定量化することが求められる．その際，従来の情報検索でよく用いられているベクトル空間モデル\cite{Salton:75}などでは文書における単語の出現頻度や統計情報などを利用して検索要求と文書間の類似性を判断し，文書を選別している．このような手法は検索要求と文書内の各単語の表記が一致しない場合は関連性がないとの仮定にもとづいている．しかし，実際の文書において，語の表記が同じでも異なる意味を有したり（多義性），同じ意味でも語の表記が異なる場合（表記揺れ，同類義語）がある．さらに単語間には，互いに意味的な関連性を持って存在しており，表記だけを頼りに検索を行う手法ではユーザが入力する語によって検索結果が異なってしまう．そのためユーザが適切なキーワードを考えなければならない．その問題を解消するために，ユーザが入力したキーワードの意味を捉えた検索手法が必要である．

このような背景から，本研究では文書における意味を捉えた検索を実現すべく，単語の意味特徴を定義した概念ベース\cite{okumura:07}を用いた検索手法を提案する．概念ベースを用いることによって，単語の表記のみでの検索方式とは異なり，意味を捉えた検索が可能になる．つまり，ユーザの入力語の表記的揺らぎに影響されず，意味的近さを定量化できる手法である．具体的には，概念ベースによって単語間の意味的な関連性を0から1までの数値として算出する．そして，その値をもとに検索要求と検索対象との類似度を画像検索等の分野で注目されている距離尺度であるEarth Mover's Distance (EMD) \cite{Rubner:00}により求める方法を提案する．また，概念ベースに存在しない固有名詞や新語に対して，Webをもとに新概念として定義し概念ベースを自動的に拡張する手法を提案する．


\section{先行研究と本研究の位置付け}

体系的に整理された辞書であるWordNet \cite{Miller:95}を用いて単語間の距離を定義し，EMDにより文書間の類似度を定義する手法\cite{Wan:06}が提案されている．これにより単語の意味的な関連性に着目した情報検索が実現されている．また，単語の共起情報をもとに単語間の関連性を定義し，EMDにより文書間の類似度を定義する手法が提案されている\cite{yanagimoto:07}．この手法では，単語の共起情報を用いることにより，全ての単語間の関連性を定義し，文書間の類似度を定義することを実現している．しかしながらこれらの手法の問題点として，WordNetなどの整理された辞書を用いる場合は，索引語の全てが辞書に含まれる保証がなく，全ての索引語間の関連性を求めることができない可能性がある．共起情報を手がかりにした場合は，用いる文書集合の特性や容量の影響を大きく受け，正確に関連性を定義しているとは言い難い．

提案手法では，概念ベースを用いて索引語間の関連性を求め，さらに概念ベースに存在しない語においてはWebをもとに自動的に概念として定義する．これにより，単語間の関連性をより正確に定義し，さらにあらゆる新語に対応できる索引語の網羅性を実現する．


\section{基本事項}

\subsection{NTCIR3-WEB}

\begin{figure}[b]
\begin{center}
\includegraphics{16-3ia2f1.eps}
\caption{NTCIR3-WEBの検索課題の例}
\label{fig:example_of_problem}
\end{center}
\end{figure}

本研究では提案手法の効果を検証するため，日本での代表的な情報検索システム評価用のテストコレクションであるNTCIR\footnote{国立情報学研究所．http://research.nii.ac.jp/ntcir/ntcir-ws3/ws-ja.html}を用いた．NTCIRは文献データ集合，検索課題集合，各検索課題に対する文献の適合不適合判定からなるもので，同一のテストコレクションを利用することにより共通の基準で情報検索システムを評価することができるようにしたものである．その中でも，本研究では一般の利用者が実際に検索する環境に近いWeb検索用のテストコレクションであるNTCIR3-WEBを用いた．図\ref{fig:example_of_problem}にNTCIR3-WEBの検索課題の一例を示す．
検索課題にはNUM，TITLE，DESC，NARR，CONC，RDOC，USERの7つのフィールドが含まれているが，このうち標準的なフィールドはTITLE (title)，DESC (description)，NARR (narrative)，CONC (concept)の4つである．TITLEとは検索課題の内容を簡単に表したタイトル，DESCは検索する内容を文で記述したもの，NARRは検索する内容の詳細な説明，CONCは検索する内容を表すキーワードである．本研究では検索要求を文章で入力するシステムの開発を想定し，DESCのみを使用する．図\ref{fig:example_of_object}に検索対象（HTMLもしくはプレーンテキストファイル．言語は主に日本語と英語，ごく一部にその他の言語．100~GB）の一例を示す．実験には検索対象のタグを省いた全文を用いる．

\begin{figure}[t]
\begin{center}
\includegraphics{16-3ia2f2.eps}
\caption{NTCIR3-WEBの検索対象の例}
\label{fig:example_of_object}
\end{center}
\end{figure}


\subsection{索引語の取得と重み付け}

本研究は日本語での検索を想定している．日本語は英語などとは異なり，単語間に明確な区切りがない．そこで，文章から単語を切り出す必要がある．本研究では形態素解析器を用いて行う．

\subsubsection{形態素解析器}

日本語構造の制約を利用し，単語の切り出しや品詞を同定することを形態素解析という．例えば，形容詞は名詞の前に付くことができるという法則である．実際は，日本語の複雑さのため完全に単語を切り出すことは難しいが，代表的な形態素解析器である茶筌\footnote{奈良先端科学技術大学院大学．http://chasen-legacy.sourceforge.jp/}は様々な工夫により高い精度で単語を正しく切り出すことができる．本研究では単語の切り出しに茶筌を用い，「名詞」，「形容詞」，「動詞」を索引語として用いる．

\subsubsection{tf・idf}\label{tf_idf}

索引語に対する重み付けは，情報検索の分野で広く用いられているtf・idf重み付け\cite{Salton:88}を使用する．tf・idfによる重み付けとは，対象としている単語の頻度と網羅性に基づいた重み付け手法である．文書$d$における索引語$t$の重み$wd(t,d)$は以下の式\ref{eq:tfidf}によって得られる．
\begin{equation}
 wd(t,d)=tf(t,d){\times}idf(t)\label{eq:tfidf}
\end{equation}
$tf(t,d)$は文書$d$における索引語$t$の出現頻度である．ただし，$tf(t,d)$は文書長の影響を受けやすいため，本論文では以下の式\ref{eq:tf}に示す正規化手法を用いた．単語$t$の出現頻度を$tfreq(t,d)$，文書$d$に含まれる単語数を$tnum(d)$とする．
\begin{equation}
 tf(t,d)=\frac{\log(tfreq(t,d)+1)}{\log(tnum(d))}\label{eq:tf}
\end{equation}
また，$idf(t)$は文書数$N$と索引語$t$が出現する文書の数$df(t)$によって決まり，式\ref{eq:idf}によって定義される．
\begin{equation}
 idf(t)=\log\frac{N}{df(t)}+1\label{eq:idf}
\end{equation}

\subsubsection{不要語の削除}

\begin{figure}[t]
\begin{center}
\includegraphics{16-3ia2f3.eps}
\caption{検索課題idfによる不要語の削除}
\label{fig:example_of_word}
\end{center}
\end{figure}

本研究では不要語を削除するために評価データの実験に使用する検索対象の空間でのidf値をもとに検索対象内の不要語を削除した．不要語とは「する」や「こと」のような，どの文書にも出現し，文書を特定するために有効でない語を指す．検索対象の不要語削除の閾値は提案手法が適合判定レベルのLEVEL1においての評価(MAP)がidf値0から9の間で一番高くなるidf値5に設定した．適合判定LEVELと評価(MAP)などの評価方法については\ref{evaluation method}節で述べる．さらに検索課題空間でのidf値を利用し検索課題の索引語中の不要語を削除した．削除の例を図\ref{fig:example_of_word}に示す．これにより検索課題によく出現する「文書」や「様々」などの語を削除でき，検索課題として比較的意味があると考えられる「将来」，「歴史」，「地域」などを残すことができる．この閾値は目視により設定した．


\section{概念ベース}
\label{concept base}

概念ベースとは複数の国語辞書や新聞などから機械的に構築した単語（概念）とその意味特徴を表す単語（属性）の集合からなる知識ベースである．概念には属性とその重要性を表す重みが付与されている．概念ベースには約12万語の概念表記が収録されており，1つの概念に平均約30個の属性が存在する．
ある概念$A$は属性$a_i$とその重み$wc_i$の対の集合として，式\ref{eq:concept_base}で表される．
\begin{equation}
 A=\{(a_1,wc_1),(a_2,wc_2),\cdots,(a_i,wc_i),\cdots,(a_n,wc_n)\}
 \label{eq:concept_base}
\end{equation}
任意の一次属性$a_i$は，その概念ベース中の概念表記の集合に含まれている単語で構成されている．したがって，一次属性は必ずある概念表記に一致するため，さらにその一次属性を抽出することができる．これを二次属性と呼ぶ．概念ベースにおいて，「概念」は$n$次までの属性の連鎖集合により定義されている（図\ref{fig:concept_base}）．以下に，本研究で使用した大規模概念ベースの構築方法について述べる．

\begin{figure}[b]
\begin{center}
\includegraphics{16-3ia2f4.eps}
\caption{概念ベース}
\label{fig:concept_base}
\end{center}
\end{figure}


\subsection{概念ベースの構築方法}

まず，基本となる概念ベースを複数の国語辞書から構築する．概念は国語辞書の見出し語から，属性は見出し語の語義説明文の自立語から，その重みは自立語の出現頻度に基づいて決定される\cite{kasahara:97}．そして，属性信頼度（語に関する種々の知識から属性としての確からしさを定量化した値）により不適切な属性の削除を行い，信頼性の高い属性を抽出する\cite{kojima:02}．これらにより概念数約3万4千語で平均属性数が16個の基本となる概念ベースが構築される．

複数の国語辞書には約12万語の見出し語（辞書に収録されている約20万語語彙のうち見出し語として不適切な表記を除去した語）があり，辞書中の語義説明文だけでは属性が付与できない語が約9万語もあった．そこで，概念総数と属性数を拡張するために電子化新聞（毎日新聞，日本経済新聞）を用いて，各概念に対する共起語を属性候補として追加する．この際，もともと概念ベースに定義されている概念についても，同様に属性を取得する．その後，\ref{degree_of_association}節で説明する関連度計算により概念と属性の関連の強さを求め，その値と概念ベースの頻度情報をもとに属性の重み付けを行う．以上の処理により本研究で使用した概念数が約12万語，平均属性数約30個の大規模概念ベース\cite{okumura:07}が構築できる．


\section{概念ベースを用いた単語間の関連性の定量化}

概念ベースを用いた単語間の関連性の定量化は，基本的に語意の展開結果を利用し数値として表す．何次属性まで展開するか，どの属性を用いるかによって値が変わってくるため，状況に応じてどのように計算するかが問題になってくる．そこで本研究では二種類の方法を使い分ける．文書間の類似度を求めるための単語間の関連性の定量化には，概念ベースの一次属性までを使用する一致度を用い，概念ベースの自動拡張手法における単語間の関連性の定量化には，概念ベースの二次属性までを使用する関連度計算\cite{watabe:06}を使用する．二次属性までを使用する方法が，概念ベースを用いた単語間の関連性の定量化には一番有効であると報告されている\cite{watabe:06}．一次属性までしか展開しないと，関連が薄い概念同士の関連性を定量化できず，三次属性まで用いると概念とはかけ離れた語が属性となり，雑音として働くため精度が低下してしまう．本研究では，文書を概念と文書間の類似度を求めるための単語間の関連性の定量化には一次属性までしか展開しない一致度を用いる．これは文書を概念と見立てた場合，索引語が一次属性となり，索引語の属性が二次属性となる．つまり，索引語の二次属性まで展開すると文書を概念とした場合の三次属性まで展開したこととなり雑音が増加し，概念（文書）とはかけ離れた語が計算に使用されてしまうためである．


\subsection{一致度計算}\label{degree_of_match}

任意の概念$A$，$B$について，それぞれ一次属性を$a_i$，$b_j$とし，対応する重みを$u_i$，$v_j$とする．また，概念$A$，$B$の属性数を$L$個，
\pagebreak
$M$個$(L\le M)$とする．
\begin{gather*}
A=\{(a_i,u_i) \mid i=1〜L\} \\
B=\{(b_j,v_j) \mid j=1〜M\}
\end{gather*}
このとき，概念$A$，$B$の一致度$MatchWR(A,B)$を以下の式\ref{eq:MatchWR1}，\ref{eq:MatchWR2}で定義する．
\begin{gather}
 MatchWR(A,B)=\sum_{a_i=b_j}\min(u_i,v_j)
	\label{eq:MatchWR1}\\
 \min(\alpha,\beta)=
  \begin{cases} 
	\alpha & (\beta > \alpha) \\
	\beta & (\alpha \geq \beta)
  \end{cases}
	\label{eq:MatchWR2}. 
\end{gather}
ただし，各概念の重みの総和をそれぞれ1に正規化する．概念$A$，$B$の属性$a_i$，$b_j$に対し，$a_i$ =$ b_j$（概念$A$，$B$に共通する属性がある）となる属性があった場合，共通する属性の重みの共通部分，つまり，重みの小さい分だけ有効に一致すると考え，その合計を一致度とする．定義から明らかなように両概念の属性と重みの両方が完全に一致する場合には一致度は1.0となる．


\subsection{関連度計算}
\label{degree_of_association}

関連度計算は概念の二次属性間の一致度計算により求めた値をもとに概念間の関連性を数値として算出する．具体的には，計算する二つの概念の内，一次属性の数の少ない方の概念を$A$とし$(L\le M)$，概念$A$の一次属性を基準とする．
\[
 A=\{(a_1,u_1),(a_2,u_2),\cdots,(a_i,u_i),\cdots,(a_L,u_L)\}
\]

そして概念$B$の一次属性を，概念$A$の各一次属性との一致度$MatchWR(a_i,b_{xi})$の和が最大になるように並び替える．
\[
 B_x = \{(b_{x1},v_{x1}),(b_{x2},v_{x2}),\dots,(b_{xi},v_{xi}),\cdots,(b_{xL},v_{xL})\}
\]

これによって，概念$A$の一次属性と概念$B$の一次属性の対応する組を決める．対応にあふれた概念$B$の一次属性は無視する（この時点では組み合わせは$L$個）．
ただし，一次属性同士が一致する（概念表記が同じ）ものがある場合 ($a_i=b_j$) は，別扱いにする．これは概念ベースには約12万の概念表記が存在し，属性が一致することは稀であるという考えに基づく．
従って，属性の一致の扱いを別にすることにより，属性が一致した場合を大きく評価する．具体的には，対応する属性の重み$u_i$，$v_j$の大きさを重みの小さい方にそろえる．このとき，重みの大きい方はその値から小さい方の重みを引き，もう一度，他の属性と対応をとることにする．
例えば，$a_i=b_j$で$u_i=v_j+\alpha$とすれば，対応が決定するのは$(a_i,v_j)$と$(b_j,v_j)$であり，$(a_i,\alpha)$はもう一度他の属性と対応させる．このように対応を決めて，対応の取れた属性の組み合わせ数を$T$個とする．
\pagebreak
このとき，概念$A$，$B$の一致度$DoA(A,B)$を以下の式\ref{eq:DoA}により定義する．
\begin{equation}
 DoA(A,B)=\sum_{i=1}^T\{MatchWR(a_i,b_{xi}) \times (u_i+v_{xi}) \times (\min(u_i,v_{xi})/\max(u_i,v_{xi}))/2\}
	\label{eq:DoA}
\end{equation}

関連度の値は概念間の関連の強さを0〜1の間の連続値で表す．1に近づくほど関連が強い．概念$A$と$B$に対して関連度計算を行った例を表\ref{table:degree_of_association}に挙げる．
最後に，概念「机」と「椅子」を例に用いて，関連度の計算例を説明する．概念「机」と「椅子」の一次属性および二次属性を表\ref{tab:example_primary_attribute}，表\ref{table:example_secondary_attribute}に示す．
\begin{table}[b]
\begin{minipage}{0.45\textwidth}
\caption{関連度計算の例}
\label{table:degree_of_association}
\input{02table01.txt}
\end{minipage}
\begin{minipage}{0.45\textwidth}
\caption{概念「机」と「椅子」の一次属性}
\label{tab:example_primary_attribute}
\input{02table02.txt}
\end{minipage}
\end{table}
\begin{table}[b]
\caption{概念「机」と「椅子」の二次属性}
\label{table:example_secondary_attribute}
\input{02table03.txt}
\end{table}

まず，概念「机」と「椅子」の一致度の計算を行う．例えば，概念「机」の一次属性「学校」
と概念「椅子」の一次属性「木」は，「木造」という共通する属性を持っているため，一致度は
以下のように計算される．
\[
 MatchWR(学校,木)=\min(0.2,0.4)=0.2
\]
同様に全ての一次属性の組み合わせについて一致度を計算した結果を表\ref{table:example_dom_matrix}に示す．

\begin{table}[b]
\caption{概念「机」と「椅子」の一致度行列}
\label{table:example_dom_matrix}
\input{02table04.txt}
\end{table}

次に，関連度の計算を行う．関連度の計算は，まず属性が完全に一致している部分から行われる．続いて，一致度の大きい部分から順に対応を決める．この場合，表\ref{table:example_dom_matrix}から一次属性「勉強」と「勉強」，「学校」と「教室」，「本棚」と「勉強」の順に対応が決まることになる．結果，
関連度は次式のように計算される．
\begin{align*}
DoA(机,椅子)	& =1.0\times(0.3+0.3)\times(0.3/0.3)/0.2+0.4\times(0.6+0.3)\times(0.3/0.6)/2	\\
		& \quad	+0.1\times(0.1+0.2)\times(0.1/0.2)/2	\\
		& =0.3975
\end{align*}

本研究では\ref{addict}節で述べる概念ベースに属性として追加する概念と追加される側の概念との間の関連性の定量化に関連度計算を用いる．


\section{概念ベースの自動拡張手法}
\label{concept base auto expand}

概念ベースに存在しない語（未定義語）にも属性を与えなければ，未定義語と他の単語との関連性を求めることができない．そこで，現在考えうる最大の言語データであるWeb情報をもとに未定義語の概念化を行い，概念ベースに追加する方法を提案する．本章ではこの手法について説明する．


\subsection{未定義語の概念化}
\label{acquiring_attribute_of_unknown_word}

以下の手順により，未定義語の概念化を行うために，未定義語の属性とその重みをWebから獲得する．

\begin{enumerate}

\item 入力された未定義語をキーワードとして検索エンジン\footnote{検索エンジンはgoogleを用いた．http://www.google.co.jp}を用いて検索を行い，検索上位100件の検索結果ページの内容を取得する．

\item HTMLタグなど不要な情報を取り除いた文書群に対して，形態素解析を行い，自立語を抽出する．

\item 得られた自立語の中から概念ベースに存在する単語のみを未定義語の属性として抜き出す．

\item 得られた属性の頻度にWeb上の単語のidfを統計的に調べたもの (SWeb-idf) \cite{tuji:04}の値を掛け合わせたものを属性の重みとし，得られた重み順に並び替える．SWeb-idfについては次節で説明する．なお，SWeb-idfのデータベースに存在しない属性については，Web上にあまり存在しない単語と考え，SWeb-idf値の最大値を掛け合わせている．

\end{enumerate}

表\ref{table:attribute_unknown_word}に未定義語を概念化した例を示す．

\begin{table}[t]
\caption{未定義語「Gショック」，「クイニーアマン」の属性とその重み（一部）}
\label{table:attribute_unknown_word}
\input{02table05.txt}
\vspace{-1\baselineskip}
\end{table}


\subsection{SWeb-idf}
\label{SWeb-idf}

SWeb-idf (Statics Web-Inverse Document Frequency) とは，Web上の単語のidfを統計的に調べたidf値である．まず，無作為に選んだ固有名詞1,000語を作成する．
表\ref{table:proper_noun}に無作為に選択した
固有名詞の一部を示す．
この作成した1,000語に対して個々に検索エンジンで検索を行い，1語につき検索上位10件の検索結果ページの内容を取得する．
よって，得られた検索結果ページ数は10,000ページとなる．
この10,000ページから，複数の国語辞書や新聞などから概念（単語）を抽出した知識ベースで
ある概念ベースの収録語数である約12万語とほぼ同等の単語数が得られたことから，獲得した
10,000ページをWebの全情報情報空間とみなしている．そして，その中での単語のidf値を表すSWeb-idfは，式\ref{eq:SWeb-idf}で求められる．
\begin{equation}
 SWeb \verb|-| idf(t)=\log\frac{N}{df(t)}\hspace{2em}(N=10000)\label{eq:SWeb-idf}
\end{equation}


\begin{table}[b]
\begin{minipage}[t]{0.6\textwidth}
\caption{SWeb-idfの作成に用いた固有名詞（一部）}
\label{table:proper_noun}
\input{02table06.txt}
\end{minipage}
\begin{minipage}[t]{0.3\textwidth}
\caption{SWeb-idfの例}
\label{table:SWeb-idf}
\input{02table07.txt}
\end{minipage}
\end{table}

これらにより得られた単語とそのidf値をデータベースに登録した．
なお$df(t)$項は，全文書空間（10,000ページ）に出現する概念$t$のページ数である．
獲得したSWeb-idfの値の例を表\ref{table:SWeb-idf}に挙げる．
なお，固有名詞の選び方を変えてもSWeb-idfの値に大きな変化は見られないという報告がなされている\cite{tuji:04}．


\subsection{属性内出現頻度を用いた重み付け手法}

未定義語の属性の重みはSWeb-idfによっても求まるが，Web情報と概念ベースでは語の頻度情報が異なり重みの値も変わるため，Web情報の重みをそのまま用い概念ベースに追加すると概念ベースの頻度情報に歪みが生じる．よってSWeb-idfは未定義語の属性候補を獲得する時にのみ用い，未定義語の属性の重み付けには使用せず，概念ベースに未定義語を追加する場合には概念ベースの頻度情報を用いて重み付けを行う必要がある．そこで，未定義語の属性に対して重みを付与する方法として，概念ベースの属性空間を考慮した重み付け手法を提案する．
概念に付与された属性は，特徴を表す語であるため，概念の説明文書であると捉えることが出来る．この文書空間内での属性の出現頻度を，概念に対する属性の確からしさだと考える．
例として「個人情報」の属性を表\ref{table:1st_and_2nd_attribute_of_new_concept}に示す．網掛けのセルに一次属性，各網掛けの下方のセルにはその二次属性を示している．

\begin{table}[b]
\caption{新概念「個人情報」の一次，二次属性}
\label{table:1st_and_2nd_attribute_of_new_concept}
\input{02table08.txt}
\end{table}

個人情報という概念を特徴付ける一次属性には，「個人，情報，識別，…」という属性が存在する．これは，「個人を識別することができる情報を指す」という文書であると捉えることができる．このように，概念に対する$n$次属性空間はその概念についての説明文書の集合だとみなすことができる．この$n$次属性空間から算出した出現頻度を$n$次属性内出現頻度と呼ぶ． 
本稿では2次属性空間を用いる．3次属性空間までを用いると，概念「個人情報」に対して2次属性「力」の属性「動物，筋肉，物体，…」が含まれる．このため，概念に関係のない語が多くなってしまうためである．

\ref{tf_idf}節で説明したtf・idf重みの考え方をもとに，未定義語の属性$A$の2次属性内出現頻度を$freq(A)$，未定義語の一次属性の総数を$R$とし，未定義語の属性$A$の概念ベース空間のidf値を$cidf(A)$とすると，重み$wc(A)$は以下の式で表される．
\begin{equation}
 wc(A)=\frac{\log(freq(A))}{\log(R)}{\cdot}cidf(A)
\end{equation}


\subsection{相互追加}
\label{addict}

新規概念に属性を追加した場合，その概念自身は属性として他の概念を持つが，その概念を属性として持つ概念は存在しない．したがって，他の概念に新しく追加をした概念を属性として追加する手法が必要となる．新概念と属性の例を表\ref{table:attribute_and_weight_of_new_concept}に示す．
新概念とその獲得した属性にはWeb上のホームページにともに出現しており，共起という関係があり，属性から見ても新概念とのなんらかの関連性があると考えられる．このため，新概念を属性の追加候補とする．例であると，概念「放送」，「テレビ」，「車両」に「ワンセグ」という語を属性追加候補とする．
しかし，全ての属性追加候補を属性として追加すると雑音が非常に多くなってしまう．概念「車両」に対して「ワンセグ」という属性は不必要であるため，選別して追加を行う．このように，新概念から取得した語に対して新概念を属性として選別して追加することを相互追加と呼ぶこととする．選別方法としては，2次属性内出現頻度を属性数で割った値（以下2次属性内出現頻度割合と記述）が0.149以上かつ関連度0.068以上の場合に追加を行う．選別のための閾値の設定は実験により定めた．実験方法としては，概念とその概念と関係がある概念の組を1,780組集め，その全ての組における2次属性内出現頻度割合と関連度を求め，その平均値を閾値に設定した．実験に使用した2対の関係がある概念の組を表\ref{table:example_of_association_word}に示す．

\begin{table}[t]
\caption{新概念の属性と重み}
\label{table:attribute_and_weight_of_new_concept}
\input{02table09.txt}
\end{table}
\begin{table}[t]
\caption{2対の関係がある概念の組の例}
\label{table:example_of_association_word}
\input{02table10.txt}
\end{table}

追加する概念$A$と追加される概念$B$との間の関連度$DoA(A,B)$と追加する属性の概念ベース空間のidf値を$cidf(A)$とすると追加した属性の重み$wc(A)$は以下の式で表される．
\begin{equation}
 wc(A)=DoA(A,B){\cdot}cidf(A)
\end{equation}


\section{EMDを用いた文書検索}
\label{retrieval_documents_using_EMD}

検索要求と検索対象の間の類似度を求める際，いくら単語間の関連性を正確に定義できたとしても，その値をもとにうまく計算できないと文書間の正確な類似度を求めることはできない．計算の仕方としては，様々な方法が考えられ，例えば単語間の関連性が高い順に単語の対応をとり計算する方法などが挙げられる．1対1で対応を取る方法では，検索要求と検索対象の語の少ない方の語の数しか対応がとれない．例えば，検索要求の語が3語，検索対象の語が100語であった場合，検索対象の97語は計算の対象外となる．さらに，実際の検索において，ユーザは検索要求にあまり多くの語を入力しないと考えられ，検索要求と検索対象との語数の差は非常に大きいと想定され，文書内の単語の重要性と単語間の関連性を考慮しM対Nで柔軟に対応を取る必要がある．

そこで本研究では類似画像検索の分野で注目されているEMD~\cite{Rubner:00}を用いて文書間の類似度を算出する方法を用いる．EMDは輸送問題における輸送コストの最適解を求めるアルゴリズムであり，需要地（供給地）の重みと需要地と供給地間の距離を定義できればどのような問題にも適用できる．このEMDを用いることで単語の重みと単語間の関連性を考慮して柔軟に対応を取り，文書間の類似度を求めることができる．


\subsection{EMDとは}

EMDは線形計画問題の一つであるヒッチコック型輸送問題において計算される距離尺度であり，2つの離散分布において，一方の分布を他方の分布に変換するための最小コストとして定義される．輸送問題とは，需要地の需要を満たすように供給地から需要地へ輸送を行う場合の最小輸送コストを解く問題である．

EMDを求める際，二つの分布は要素の重み付き集合として表現される．一方の分布$P$を集合として表現すると，$P=\{(p_1,w_{p_1}),\ldots,(p_m,w_{p_m})\}$となる．今，分布$P$は$m$個の特徴量で表現されており，$p_i$は特徴量，$w_{pi}$はその特徴量に対する重みである．同様に，一方の分布$Q$も集合として表すと，$Q=\{(q_1,w_{q_1}),\ldots,(q_n,w_{q_n})\}$となる．EMDの計算は，2つの分布において特徴量の数が異なっている場合でも計算が可能であるという性質を持っている．今，$p_i$と$q_j$の距離を$d_{ij}$とし，全特徴間の距離を$D=[d_{ij}]$とする．ここで，$p_i$から $q_j$への輸送量を$f_{ij}$とすると，全輸送量は$F=[f_{ij}]$となる．ここで，式\ref{eq:work}に示すコスト関数を最小とする輸送量$F$を求め，EMDを計算する．
\begin{equation}
 WORK(P,Q,F)=\sum_{i=1}^m\sum_{j=1}^n d_{ij} f_{ij}\label{eq:work}
\end{equation}
ただし，上記のコスト関数を最小化する際，以下の制約条件を満たす必要がある．
{\allowdisplaybreaks
\begin{gather}
 f_{ij}\geq0，\quad 1\leq i \leq m，\quad 1\leq j \leq n
	\label{eq:st1}\\
 \sum_{j=1}^n f_{ij} \leq w_{p_i}，\quad 1 \leq i \leq m
	\label{eq:st2} \\
 \sum_{i=1}^m f_{ij} \leq w_{q_j}，\quad 1 \leq j \leq n
	\label{eq:st3} \\
 \sum_{i=1}^m\sum_{j=1}^n f_{ij} = \min\left(\sum_{i=1}^m w_{p_i}，\sum_{j=1}^n w_{q_j}\right)
	\label{eq:st4}
\end{gather}
}

ここで，式\ref{eq:st1}は輸送量が正であることを表し，$p_i$から$q_j$に送られる一方通行であることを表している．式\ref{eq:st2}は輸送元である$p_i$の重み以上に輸送できないことを表す．式\ref{eq:st3}は輸送先である$q_j$の重み以上に受け入れることができないことを表す．最後に式\ref{eq:st4}は総輸送量の上限を表し，それは輸送先または輸送元の総和の小さい方に制限されることを表す．
以上の制約条件の下で求められた最適な全輸送量$F$を用いて分布$P$，$Q$間のEMD を以下のように求める．
\begin{equation}
 EMD(P,Q)=\frac{\sum_{i=1}^m \sum_{j=1}^n d_{ij} f_{ij}}{\sum_{i=1}^m \sum_{j=1}^n f_{ij}}
\end{equation}
ここで，最適なコスト関数$WORK(P,Q,F)$をEMD としてそのまま用いないのは，コスト関数は輸送元もしくは輸送先の重みの総和に依存するので，正規化することによってその影響を取り除くためである． 


\subsection{EMDの文書検索への適用}

\begin{figure}[b]
\begin{center}
\includegraphics{16-3ia2f5.eps}
\caption{EMDを文書検索に適用した例}
\label{fig:EMD_to_document}
\end{center}
\end{figure}

図\ref{fig:EMD_to_document}にEMDを文書検索に適用した例を示す．
EMDを文書検索に適用するには需要地と供給地，需要量と供給量，各需要地と供給地間の距離を定義する必要がある．需要地としては，検索課題の索引語を，供給地としては検索対象の索引語を割り当てる．需要量と供給量はそれぞれ索引語の\ref{tf_idf}節で説明したtf・idf重みを用いる．そして，需要地と供給地間の距離は索引語間の関連性と見立てることができ，提案手法においては概念ベースを用いた一致度計算により求めることができる．一致度は関連性が高いと値も大きくなるため，1から一致度の値を引いた値に変換する．EMDの計算は図\ref{fig:EMD_to_document}の下方で求まる．「梅」と「祭り」間の輸送量が1となっているのは，「梅」から「うめ」に重み2を輸送し，「梅」の余った重み1を「祭り」に輸送したためである．このように，関連性が高い語に優先して重みを輸送し，供給量がなくなるか需要量が満たされるまで輸送を行う．このように索引語間の関連性と重みを考慮したM対Nでの柔軟な対応が可能である．EMDの特徴として．索引語間の距離の値が0から1であるなら，EMDも0から1の値になる．そして，EMDは文書間が似ていると値が低くなり，似ていないと値が高くなる．よって値が低い文書から順にユーザに提示することで文書検索が実現できる．


\section{実験と評価}

単語の関連性に着目した提案手法の有効性を検証するため情報検索システムテストコレクションNTCIR3-WEBを用いて，表記を用いる他の手法との比較実験を行った．比較手法としては，ベクトル空間モデル，Okapi BM25と，同じ索引語間の距離を0，それ以外を1とした素朴なEMDを用いた．


\subsection{評価方法}
\label{evaluation method}

今回の評価では，検索課題41件と正解文書とランダムに選択した文書を合わせた10,000件の文書を使用し，評価実験を行った．また，正解文書リストが存在し，各検索課題に対して，各文書がH（高適合），A（適合），B（部分的適合），C（不適合）の4段階の適合度が設定されており，以下の基準で評価する．

LEVEL1: H判定とA判定を適合

LEVEL2: H判定とA判定とB判定を適合

各検索課題に対して，10,000件の検索対象全てのスコアを求め，スコア順に並べ変える．そして，正解文書リストを参照し正解文書の順位を調べ評価する．


\subsection{評価指標}

評価指標には，各検索課題毎の平均精度 (Avelage Precision，AP)，平均精度の平均 (Mean Average Precision，MAP) と再現率—精度グラフを使用した．検索課題に対する平均精度APは式\ref{eq:average_precision}のように定義される．まず順位$i$位の文書が適合しているならば1，そうでなければ0となる変数を$z_i$とする．$S$を適合文書の総数，$n$は出力文書数である．
\begin{equation}\label{eq:average_precision}
 AP=\frac{1}{S}\sum_{i=1}^{n} \frac{z_i}{i}\left( 1+\sum_{k=1}^{i-1} z_k \right)
\end{equation}
平均精度の平均(MAP)は，全ての検索課題に対して平均精度を平均したものであり，式\ref{eq:mean_average_precision}によって求められる．具体的には，検索課題が$K$件ありそれぞれの課題に対するあるシステムの平均精度を$AP_h$と表記すれば$(h=1,...,K)$，
\pagebreak
その平均がMAPに相当し，以下の式に示す．
\begin{equation}\label{eq:mean_average_precision}
 MAP=\frac{1}{K}\sum_{h=1}^{K}AP_h
\end{equation}
再現率—精度グラフとは再現率の11個の点ごとに，41個の検索課題の精度を平均してグラフを描いたものである．


\subsection{比較手法}

本節では，提案手法との比較に用いているベクトル空間モデル\cite{Salton:75}とOkapiBM25 \cite{Robertson:95}について述べる．


\subsubsection{ベクトル空間モデル}

ベクトル空間モデルは，情報検索の分野で幅広く利用されている検索モデルである．各語の重みから構成されるベクトルとして検索課題と文書をそれぞれ表現し，二つのベクトルの成す角度の余弦によって類似度を計算する点に特徴がある．重みの種類にはいくつかの種類があるが，本実験では\ref{tf_idf}節で説明したtf・idf重みを用いる．検索課題$q$と文書$d_i$の索引語の語の総数（異なり）を$M$とすれば，文書と検索課題はそれぞれ以下のような$M$次元ベクトルで表現できる．
\begin{gather}
 d_i=(w_{i1},w_{i2},…,w_{iM}) \\
 q=(w_{q1},w_{q2},…,w_{qM})
\end{gather}
検索課題$q$に対する文書$d_i$の得点$s_q(d_i)$は2つのベクトルの角度の余弦により求まる．式を以下に示す．
\begin{equation}
 s_q(d_i)=\frac{\sum_{j=1}^M w_{ij}w_{qj}}{ \sqrt{\sum_{j=1}^M w_{ij}^{2}} \sqrt{\sum_{j=1}^M w_{qj}^{2}} }
\end{equation}


\subsubsection{Okapi BM25}

S. Robertsonを中心に開発されたOkapiと呼ばれる次世代検索システムにおいて使用されている確率型の検索モデルBM25は，ベクトル空間モデルと同等，あるいはそれ以上の性能を示すことでよく知られている．原理的には，検索課題$q$と文書ベクトル$d_i$が与えられた時に，その文書が検索課題に適合している確率を推計するものである．検索課題$q$と文書$d_i$の索引語の語の総数（異なり）を$M$とすれば，検索課題$q$に対する文書$d_i$の得点$s_q(d_i)$は以下の式で表される．
\begin{equation}
 s_q(d_i)=\sum_{j=1}^M(w_{ij}\times x_{qj} \times \tau_j)
\end{equation}
ただし，$x_{qj}$は検索課題$q$での語$t_j$の出現回数である．ここで
\begin{gather}
 w_{ij}=\frac{3.0 x_{ij}}{(0.5+1.5l_i / \bar{l})+x_{ij}} \\
 \tau_j=\log\frac{N-n_j+0.5}{n_j+0.5}
\end{gather}
である．$x_{ij}$は文書$d_i$での$t_j$の出現回数であり，$N$は文書総数で，$n_j$は語$t_j$が出現する文書数である．なお，
\begin{equation}
 l_i=\sum_{j=1}^M{x_{ij}}
\end{equation}
は文書$d_i$の長さであり，
\begin{equation}
 \bar{l}=\frac{1}{N}\sum_{i=1}^N{l_i}
\end{equation}
はデータベース全体での文書長の平均を意味する．


\subsection{評価結果}

平均精度の平均 (MAP) を表\ref{table:MAP_with_other_method}に示す．再現率—精度グラフを図\ref{fig:RPC_with_other_method(level1)}，\ref{fig:RPC_with_other_method(level2)}に示す．

\begin{table}[b]
\caption{MAP（表記のみ活用する他の手法との比較）}
\label{table:MAP_with_other_method}
\input{02table11.txt}
\end{table}

表\ref{table:MAP_with_other_method}より，LEVEL1では提案手法の精度はベクトル空間モデルより20.30\%，Okapi BM25より17.91\%，EMDより9.22\%の精度向上を達成し，LEVEL2では提案手法の精度はベクトル空間モデルより10.71\%，Okapi BM25より9.45\%，EMDより4.03\%の精度向上を達成した．

図\ref{fig:RPC_with_other_method(level1)}，\ref{fig:RPC_with_other_method(level2)}より全ての再現率レベルで精度が改善している．この結果より単語間の関連性にもとづき文書間の類似性を求める本手法が有効であることがわかる．また，ベクトル空間モデルより素朴なEMDがよりよい結果となっている．これは，EMDが輸送問題として距離を計算していることに起因する．

例えば，検索課題を$(\sqrt{3}/2,1/2)$としたとき，ベクトル空間モデルでは$(1,0)$と$(1/2,\sqrt{3}/2)$では同じ類似度の0.866となるが，EMDでは0.134と0.268と異なり，(1,0)の方が検索要求に近くなる．これは$(1,0)$では一番目の方が大きいが，$(1/2,\sqrt{3}/2)$では反転していることが理由である．本実験ではこの効果が良い方に働いていると考えられる．

\begin{figure}[t]
\begin{center}
\includegraphics{16-3ia2f6.eps}
\caption{LEVEL1での再現率—精度グラフ（他手法との比較）}
\label{fig:RPC_with_other_method(level1)}
\end{center}
\end{figure}
\begin{figure}[t]
\begin{center}
\includegraphics{16-3ia2f7.eps}
\caption{LEVEL2での再現率—精度グラフ（他手法との比較）}
\label{fig:RPC_with_other_method(level2)}
\end{center}
\end{figure}

次に，比較手法と提案手法が統計的に有意な差があるか検定を行った．検定方法は参考文献\cite{kishida:01}を参考にした．以下にその検定方法を述べる．

比較する2つの手法を$a$と$b$，検索課題数を$K$，検索課題$h$の平均精度を$v_h$とし，MAPを$\bar{v}=K^{-1}\sum_{h=1}^K v_h$とする．同一の検索課題に対する2つの平均精度を比較することになるので，対標本と捉えることができ，検索課題ごとの手法間の差自体を標本と考える．すると，帰無仮説「2つの手法の平均精度の母平均の差は0である」の下に正規母集団を仮定すれば，
\begin{equation}
 t=\frac{\bar{v_a}-\bar{v_b}}{\sqrt{\frac{s_a^2+s_b^2-2Cov_{ab}}{K}}}
\end{equation}
が自由度$K-1$の$t$分布に従うことになる．この時，$s_a^2$と$s_b^2$は平均精度の標本分散であり，
$Cov_{ab}$は$v_{ha}$と$v_{hb}$との共分散で，$Cov_{ab}=(K-1)^{-1}\sum_{h=1}^{K}(v_{ha}-\bar{v_a})(v_{hb}-\bar{v_b})$となる．

この検定方法を利用し，今回の実験でのLEVEL1における提案手法と比較手法の中で一番評価が高かった素朴なEMDとの評価に有意な差があるかどうか片側検定を行う．$a$を提案手法，$b$を素朴なEMDとすると，$s_a^2=0.071701$，$s_b^2=0.067863$，$\bar{v_a}-\bar{v_b}=0.044005$，$Cov_{ab}=0.056419$となるので，$t=1.723548$となる．自由度40の$t$分布に従うので，$1.723548>1.684$より，帰無仮説は棄却され，5\%水準で提案手法と素朴なEMDとの差は有意であることがわかる．また，同様の検定をベクトル空間モデルと行うと$t=3.01916$，OkapiBM25では$t=3.563336$となり1\%水準で有意な差があることを確認できた．提案手法が表記に頼る比較手法に比べ統計的に有意な差があることがわかった．


\section{概念ベースの自動拡張手法の評価}

概念ベースの自動拡張手法の効果を検証するために，自動拡張手法を用いない手法（概念ベースに存在しない索引語で同じ索引語間の距離を0それ以外の距離を1）と提案手法との比較評価を行った．評価方法は\ref{evaluation method}節と同様の方法で行った．
以下，自動拡張手法を用いないEMDと概念ベースを組み合わせた手法をEMD＋CBと記す．平均精度の平均 (MAP) を表\ref{table:MAP(level1)}に示す．再現率—精度グラフを図\ref{figure:RPC(level1)}，\ref{figure:RPC(level2)}に示す．
表\ref{table:MAP(level1)}から分かるように，LEVEL1において1.58\%の精度向上を達成し，LEVEL2において3.04\%向上している．LEVEL1においては全ての再現率レベルで提案手法がEMD+CBを上回っているが，LEVEL2においては再現率レベル0.3の点で0.0001だけEMD＋CBを下回っている．
より詳細に検証するために，検索課題ごとの平均精度の差をプロットしたものを図\ref{figure:difference_from_average_precision}に示す．横軸は本実験における検索課題の番号（1〜41）で，縦軸は検索課題毎のグラフが上に伸びているほど自動拡張手法によって精度が向上したことを示し，逆に下にのびているほど精度は低下していることを示している．この結果から検索課題14と検索課題29の時に精度の低下が著しいことがわかる．



\begin{table}[b]
\caption{MAP（自動拡張手法使用と未使用での比較）}
\label{table:MAP(level1)}
\input{02table12.txt}
\end{table}

課題14は「宮部みゆきの執筆した小説に対する書評・レビューが読みたい」という課題で，課題29は「シフォンケーキの作り方が書かれている文書を探したい．」といった課題である．これらに形態素解析を施し，検索課題idfによる不要語の削除を行うと，検索課題14は「宮部／みゆき／執筆／小説／書評／レビュー」となり，検索課題29は「シフォンケーキ／作り方」となる．このうち未定義語は「宮部／シフォンケーキ」であった．

\begin{figure}[t]
\begin{center}
\includegraphics{16-3ia2f8.eps}
\caption{LEVEL1での再現率—精度グラフ（自動拡張手法使用と未使用での比較）}
\label{figure:RPC(level1)}
\end{center}
\end{figure}
\begin{figure}[t]
\begin{center}
\includegraphics{16-3ia2f9.eps}
\caption{LEVEL2での再現率—精度グラフ（自動拡張手法使用と未使用での比較）}
\label{figure:RPC(level2)}
\end{center}
\end{figure}

検索課題14と検索課題29における上位25件の順位の変動を図\ref{figure:change_of_order}に示す．図\ref{figure:change_of_order}において色が付いてる文書がLEVEL1における正解文書である．一番精度低下が著しかった課題29で，EMD+CBにおいて47位であった不適合文書が提案手法では3位に，5位であった不適合文書が4位になり，適合文書が3位から5位に下がった．提案手法において5位であった高適合文書と3位であった不適合文書を図\ref{figure:high_conformity_document}，\ref{figure:non_conformity_document}に示す．
3位の文書はケーキ屋の紹介が書かれている文書で，4位の文書もケーキに関する文書であった．5位の高適合文書はシフォンケーキの作り方の手順が書いてあった．提案手法では，シフォンケーキがケーキに関する様々な語と高関連であると判定し，ケーキに関する文書が検索の上位にきてしまった．ユーザは「ケーキの作り方」と入力せず「シフォンケーキの作り方」と入力したのはケーキ全般の情報よりシフォンケーキに絞った情報が欲しいといったことを暗に示していると考えられ，シフォンケーキを定義してしまったことが悪影響を及ぼしてしまった．

\begin{figure}[t]
\begin{center}
\includegraphics{16-3ia2f10.eps}
\caption{課題別の平均精度の差}
\label{figure:difference_from_average_precision}
\end{center}
\end{figure}
\begin{figure}[t]
\begin{center}
\includegraphics{16-3ia2f11.eps}
\caption{課題14と課題29における上位25件のランク付け}
\label{figure:change_of_order}
\end{center}
\end{figure}

シフォンケーキのようなケーキといった一般的な語に比べ，具体的な語はユーザが検索結果を絞るために用いる語であり，具体的な語に対しては，表記に頼る方が有効に働くと考えられる．具体的な語を判別し，なんらかの対応を行うことでこの問題は解決できると考えられる．

\begin{figure}[t]
\begin{center}
\includegraphics{16-3ia2f12.eps}
\caption{高適合文書}
\label{figure:high_conformity_document}
\end{center}
\vspace{-0.5\baselineskip}
\end{figure}
\begin{figure}[t]
\begin{center}
\includegraphics{16-3ia2f13.eps}
\caption{不適合文書}
\label{figure:non_conformity_document}
\end{center}
\vspace{-1\baselineskip}
\end{figure}


\section{おわりに}

本論文では，索引語の関連性を概念ベースにより定義し，
それをもとにEMDによって文書間の類似性を求める手法を提案した．
\pagebreak
さらに概念ベースに存在しない語においてはWebをもとに語の意味を定義し概念ベースを自動的に拡張することで対応し，全ての索引語間の距離を概念ベースにより求めることを可能とする手法を提案した．そして，その有効性をWeb検索評価用テストコレクションNTCIR3-WEBを用いて検証した．

結果として，表記に頼る他の手法に比べ良好な結果を得て，単語の関連性に着目した本手法の有効性を確認できた．さらに，統計的検定を用いることで，提案手法と比較手法の性能の差に有意性があることを確認した．またWebにより概念ベースを自動的に拡張する手法を使用と未使用で比較評価を行い，Webにより未定義語を定義することの有効性を示し，あらゆる新語に対応できる索引語の網羅性を実現した．

今後の研究課題としては，検索要求内に具体的な語が含まれている場合の対応である．ユーザは検索結果を絞るために具体的な語を用いていると考えられ，これらの語をシソーラスなどをもとに判別し対応を行えば更なる精度向上が期待できる．また，今後は情報検索に限らず文書分類など様々な分野へ応用していきたい．





\bibliographystyle{jnlpbbl_1.3}
\begin{thebibliography}{}

\bibitem[\protect\BCAY{笠原\JBA 松澤\JBA 石川}{笠原\Jetal }{1997}]{kasahara:97}
笠原要\JBA 松澤和光\JBA 石川勉 \BBOP 1997\BBCP.
\newblock \JBOQ 国語辞書を利用した日常語の類似性判別\JBCQ\
\newblock \Jem{情報処理学会論文誌}, {\Bbf 38}  (7), \mbox{\BPGS\ 1272--1283}.

\bibitem[\protect\BCAY{岸田}{岸田}{2001}]{kishida:01}
岸田和明 \BBOP 2001\BBCP.
\newblock \JBOQ 検索実験における評価指標としてのMean Average
  Precisionの性質\JBCQ\
\newblock \Jem{情報処理学会研究報告}, {\Bbf 2001}  (74), \mbox{\BPGS\ 97--104}.

\bibitem[\protect\BCAY{小島\JBA 渡部\JBA 河岡}{小島\Jetal }{2002}]{kojima:02}
小島一秀\JBA 渡部広一\JBA 河岡司 \BBOP 2002\BBCP.
\newblock \JBOQ
  連想システムのための概念ベース構成法—属性信頼度の考え方に基づく属性重みの決
定\JBCQ\
\newblock \Jem{自然言語処理}, {\Bbf 9}  (5), \mbox{\BPGS\ 93--110}.

\bibitem[\protect\BCAY{Miller}{Miller}{1995}]{Miller:95}
Miller, G. \BBOP 1995\BBCP.
\newblock \BBOQ WordNet: A lexical database for English\BBCQ\
\newblock {\Bem Commun. ACM}, {\Bbf 38}  (11), \mbox{\BPGS\ 39--41}.

\bibitem[\protect\BCAY{奥村\JBA 土屋\JBA 渡部\JBA 河岡}{奥村\Jetal
  }{2007}]{okumura:07}
奥村紀之\JBA 土屋誠司\JBA 渡部広一\JBA 河岡司 \BBOP 2007\BBCP.
\newblock \JBOQ 概念間の関連度計算のための大規模概念ベースの構築\JBCQ\
\newblock \Jem{自然言語処理}, {\Bbf 14}  (5), \mbox{\BPGS\ 41--64}.

\bibitem[\protect\BCAY{Robertson, Walker, Jones, Beaulieu, \BBA\
  Gatford}{Robertson et~al.}{1995}]{Robertson:95}
Robertson, S., Walker, S., Jones, S., Beaulieu, M., \BBA\ Gatford, M. \BBOP
  1995\BBCP.
\newblock \BBOQ Okapi at TREC-3\BBCQ\
\newblock In {\Bem In proceeding of the 3rd Text Retrieval Conference},
  \mbox{\BPGS\ 109--126}.

\bibitem[\protect\BCAY{Rubner, Tomasi, \BBA\ Guibas}{Rubner
  et~al.}{2000}]{Rubner:00}
Rubner, Y., Tomasi, C., \BBA\ Guibas, L. \BBOP 2000\BBCP.
\newblock \BBOQ The earth mover's distance as a metric for image
  retrieval\BBCQ\
\newblock {\Bem Int. J. Comput. Vision}, {\Bbf 40}, \mbox{\BPGS\ 99--121}.

\bibitem[\protect\BCAY{Salton \BBA\ Buckley}{Salton \BBA\
  Buckley}{1988}]{Salton:88}
Salton, G.\BBACOMMA\ \BBA\ Buckley, C. \BBOP 1988\BBCP.
\newblock \BBOQ Term-weighting approaches in automatic text retrieval\BBCQ\
\newblock {\Bem Information Processing and Management}, {\Bbf 41}  (4),
  \mbox{\BPGS\ 513--523}.

\bibitem[\protect\BCAY{Salton, Wong, \BBA\ Yang}{Salton
  et~al.}{1975}]{Salton:75}
Salton, G., Wong, A., \BBA\ Yang, C. \BBOP 1975\BBCP.
\newblock \BBOQ A Vector space model for automatic indexing\BBCQ\
\newblock {\Bem Communications of the ACM}, {\Bbf 18}  (3), \mbox{\BPGS\
  613--6201}.

\bibitem[\protect\BCAY{辻\JBA 渡部\JBA 河岡}{辻\Jetal }{2004}]{tuji:04}
辻泰希\JBA 渡部広一\JBA 河岡司 \BBOP 2004\BBCP.
\newblock \JBOQ wwwを用いた概念ベースにない新概念およびその属性獲得手法\JBCQ\
\newblock \Jem{第18回人工知能学会全国大会論文集，2D1-02}.

\bibitem[\protect\BCAY{Wan \BBA\ Peng}{Wan \BBA\ Peng}{2006}]{Wan:06}
Wan, X.\BBACOMMA\ \BBA\ Peng, Y. \BBOP 2006\BBCP.
\newblock \BBOQ The Earth Mover's Distace as a Semantic Measure for Document
  Similality\BBCQ\
\newblock In {\Bem Proceeding of the 14th ACM international conference on
  Information and knowledge management}, \mbox{\BPGS\ 301--302}.

\bibitem[\protect\BCAY{渡部\JBA 奥村\JBA 河岡}{渡部\Jetal }{2006}]{watabe:06}
渡部広一\JBA 奥村紀之\JBA 河岡司 \BBOP 2006\BBCP.
\newblock \JBOQ 概念の意味属性と共起情報を用いた関連度計算方式\JBCQ\
\newblock \Jem{自然言語処理}, {\Bbf 13}  (1), \mbox{\BPGS\ 53--74}.

\bibitem[\protect\BCAY{柳本\JBA 大松}{柳本\JBA 大松}{2007}]{yanagimoto:07}
柳本豪一\JBA 大松繁 \BBOP 2007\BBCP.
\newblock \JBOQ Earth Mover's Distanceを類似度として用いた情報検索\JBCQ\
\newblock \Jem{平成19年度 電気学会全国大会，3-065}.

\end{thebibliography}

\begin{biography}
\bioauthor{藤江　悠五}{
2007年同志社大学工学部知識工学科卒業．
2009年同大学院工学研究科知識工学専攻博士前期課程修了．
同年，パナソニック株式会社入社．
知識情報処理の研究に従事．
}
\bioauthor{渡部　広一}{
1983年北海道大学工学部精密工学科卒業．
1985年同大学院工学研究科情報工学専攻修士課程修了．
1987年同精密工学専攻博士後期課程中途退学．
同年，京都大学工学部助手．
1994年同志社大学工学部専任講師．
1998年同助教授．工学博士．
現在同教授．
主に，進化的計算法，コンピュータビジョン，概念処理などの研究に従事．
言語処理学会，人工知能学会，情報処理学会，電子情報通信学会，システム制御情報学会，精密工学会各会員．
}

\bioauthor{河岡　　司}{
1966年大阪大学工学部通信工学科卒業．
1968年同大学院修士課程修了．
同年，日本電信電話公社入社，情報通信網研究所知識処理研究部長，
NTTコミュニケーション科学研究所所長を経て，同志社大学工学部教授．
2009年定年退職．
工学博士．
主にコンピュータネットワーク，知識情報処理の研究に従事．
言語処理学会，人工知能学会，情報処理学会，電子情報通信学会，IEEE(CS)各会員．
}

\end{biography}


\biodate



\end{document}
