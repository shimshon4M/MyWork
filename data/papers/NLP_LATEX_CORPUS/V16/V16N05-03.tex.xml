<?xml version="1.0" ?>
<root>
  <section title="Introduction">Wordsensedisambiguation(WSD)isoneofthefundamentalproblemsincomputationallinguistics.ThetaskofWSDistoresolvetheinherentpolysemyofwordsbydeterminingtheappropriatesense(s)foreachpolysemouswordinagiventext.Itisconsideredtobeanintermediate,butnecessarystepformanyNLPapplications,includingmachinetranslationandinformationextraction,whichrequiretheknowledgeofwordsensestoperformbetter.Onemajorobstacleforlarge-scaleandpreciseWSDisthedatasparsenessproblemcausedbythefine-grainednatureofthesensedistinction.Inrecentyears,inordertoresolvethisproblem,severalsemi-supervisedapproacheshavebeenexplored.Whilesomeresearchershaveaddressedthescarcityofthetrainingdatadirectly,byexploringthemethodstoobtainmoretaggedinstancesfromunannotatedcorpora(e.g.),otherresearchershaveusedunannotatedcorporatoextractusefulglobalinformation,suchasthedomaininformation,andincorporatedthisinformationintosupervisedWSDframeworks.TheuseofglobalinformationextractedfromunannotatedcorporahassucceededindramaticallyincreasingtheperformanceofWSD;however,ontheotherhand,theeffectivenessoflocalorsyntacticinformationhasnotbeenfullyexamined.Onesuchinformationyettobeexploredistheinterdependencyofwordsenses.AlthoughtheuseoflocalandsyntacticinformationhasbeencommoninWSD,traditionalapproachestosupervisedWSDaretypicallybasedontheindividualclassificationframeworkforeachword,inwhicheachword'ssenseistreatedindependently,regardlessofanyinterdependenciesorcooccurrencesofwordsenses.Accordingly,theresultingsenseassignmentmaybesemanticallyinconsistentoverthesentence.Tosolvethisproblemisofgreatinterestfrombothapracticalandtheoreticalviewpoint.Inthispaper,wepresentaWSDmodelthatnaturallyhandlesallcontentwordsinasentence.Wefocusonusingtheinterdependencyofwordsenses,sothatourmodelcanoutputasemanticallyconsistentassignmentofsensestothewholesentence.Specifically,weassumethattherearestrongsensedependenciesbetweenasyntacticheadanditsdependentsinthedependencytree.Furthermore,wecombinethesesensedependencieswithvariouscoarse-grainedsensetagsets.Thesecombinedfeaturesareexpectedtoalleviatethedatasparsenessproblem,andalsoenableourmodeltoworkevenforwordsthatdonotappearinthetrainingdata,whichtraditionalindividualclassifierscannothandle.Asamachinelearningmethod,weadoptthetree-structuredconditionalrandomfields(T-CRFs).WesolveWSDasalabelingproblemtoasentencedescribedasadependencytree,wheretheverticescorrespondtothewords,andtheedgescorrespondtothesensedependencies.T-CRFsalsoenableustoincorporatevarioussensetagsetsalltogetherintoasimpleframework.Inourexperiments,threeinterestingresultsarefound:theinterdependencyofwordsensescontributetotheimprovementofWSDmodels,thecombinedfeatureswithcoarse-grainedsensetagsworkeffectively,andthetree-structuredmodeloutperformsthelinear-chainmodel.Theseresultsareconfirmedonthreedatasets(theSemCorcorpusandtheSenseval-2and-3Englishall-wordstasktestsets)andontwosenseinventories(WordNetsynsetsandsupersenses).Ourfinalmodelisshowntoperformcomparablytostate-of-the-artWSDsystems.Therestofthepaperisorganizedasfollows:InSection,wedescribecurrentproblemsofWSDandrelatedworks.InSection,wedescribebackgroundtopicsrelatedtoWordNet.InSection,wedescribeourmodelandthemachinelearningmethodthatweuse.InSection,,and,wepresentourexperimentalsetup,theresults,andanin-depthanalysisonthecontributionofthesensedependencies.Finally,inSection,wepresentourconcludingremarks.</section>
  <section title="Problems and related works"/>
  <subsection title="Word sense dependencies">FortheunsupervisedWSD,whichaimstodisambiguatepolysemouswordswithoutusinganytaggedcorpora,theuseofsensedependencieshasbeenacommonmethod.introducedanunsupervisedgraph-basedalgorithm,andshowsthesignificantsuperiorityofthesequencelabelingmodelovertheindividuallabelassignment.builtamodelbasedonvariouswordsemanticsimilaritymeasuresandgraphcentralityalgorithms,whichalsousedthegraphstructurethatincorporatesthesensedependencies.Thus,forunsupervisedWSD,theeffectivenessofsensedependencieshasbeenshownbyseveralresearches,althoughthedependenciesthattheyhaveconsideredarebasedonlyoninformationinWordNet,andarefixedinadvancewithoutanyoptimizationforrealtexts.Onthecontrary,mostapproachestosupervisedWSDaretosolveanindependentclassificationproblemforeachword(e.g.).TheseapproacheshavebeendevelopedalongwithresearchbasedonthelexicalsampletaskintheSensevals(EvaluationExercisesfortheSemanticAnalysisofText).However,asdescribedinSection,theycannothandletheinterdependenciesamongwordsenses,andmayoutputasemanticallyincoherentassignmentofsenses.Recently,withthegrowinginterestintheall-wordstask,afewsupervisedWSDsystemshaveincorporatedthedependenciesbetweenwordsenses.SenseLearnerandSuperSenseLearnerincorporatedsequentialsensedependenciesintothesupervisedWSDframeworks.alsotookasequentialtaggingapproachforthedisambiguationofWordNetsupersenses.Theseapproachesassumethesensedependenciesbetweenadjacentwords,andoptimizethembasedontaggedcorpora.However,theycannothandlelongerdependenciesthatareconsideredtobesemanticallydependentoneachother(e.g.averbanditsobject).Noteadditionally,thatthedependenciesthattheyhaveconsideredarebetweeneitherWordNetsynsetsorsupersenses,andhencearenotcombinedwithfiner-orcoarser-grainedtagsets.Inthiscontext,itisofinteresttonotewhetherthesensedependenciesonasyntacticstructure,ratherthanonalinearchain,workeffectivelyornot.Totheextentofourknowledge,thereexistsnoWSDmodelthatconsiderstheinterdependenciesofwordsensesonasyntacticstructure.Furthermore,despitetheapproachesdescribedabove,thecontributionofsensedependenciesforthesupervisedWSDhasnotbeenexplicitlyexaminedthusfar.Thesequestionsareclarifiedbyourresearch.</subsection>
  <subsection title="The use of coarse-grained tag sets">InSection,wepresentedoneofthemostsignificantissuesinWSD---thedatasparsenessproblem.Thisproblemmayevenbemagnifiedwhenwetakeintoconsiderationtheinterdependenciesofwordsenses,whicharedescribedascombinationsoftwowordsenses.Inordertorelievethisproblem,weusethehierarchicalinformationintheWordNet,includingthesuperordinatewordsandsupersenses,asdescribedinSection.Althoughsuchinformationhasneverbeencombinedwiththesensedependencies,theuseofthehierarchicalinformationhasbeenmotivatedbyseveraldifferentresearches.Forexample,aWSDsystemby,rankedsecondintheSenseval-3,consistsoftwomodels:thefirstmodelappliedtowordsseeninthetrainingdata,andthesecondmodelthatperformsageneralizeddisambiguationprocessforwordsunseeninthedata,byusingthehierarchicalinformationintheWordNet.ThefinegranularityoftheWordNetsynsetsisnotjustamajorobstacleinachievingahigh-performanceWSD,butissometimestoofine-grainedevenforahumantodistinguish.Thisisreflectedinthelowinter-annotatoragreementofsensetagging,whichimpliesthatWSDmodelswouldbeunlikelytoperformbetterthanthisaccuracy.Ontheotherhand,reportedthatcoarse-grainedsensedistinctionsaresufficientforseveralNLPapplications.Inparticular,theuseofthesupersensesasthesenseinventoryhasrecentlybeeninvestigatedby,andhasreceivedmuchattentionintheWSDfield.Inthiscase,theinter-annotatoragreementsareturnedouttoreachnearly90%.Forthisreason,weusetheWordNetsupersenses,aswellasthesynsets,asthesenseinventoryforourexperiments.</subsection>
  <section title="WordNet">TheWordNetisabroad-coveragemachine-readabledictionary(MRD)forEnglish,containingabout150,000words.WordNetalsoservesasanontology,inwhichrelationsamongwordsandwordsenses,andwell-organizedhierarchiesofsensesaredefined.Inthispaper,wealwaysrefertotheWordNetversion2.0andSensevaldatasetsforthisareavailable.unlessotherwisenoted.ThestatisticsoftheWordNet2.0isshowninTableand.InWordNet,nounsandverbsareorganizedintohierarchicalstructureswithIS-A(hypernym--hyponym)relationshipsamongwords.Allnounsandverbs,withtheexceptionofsometop-levelconcepts,areclassifiedintoprimitivegroupscalledsupersenses,whichwedescribelater.FigureshowstheWordNethierarchicalstructureforthefirstsense(financialbank)ofthenounbank,whereeachlineindicatesasynsetwiththelistofsynonymouswordsheadedbyitssupersenselabel;anarrowdenotesthatthetwosynsetsareinanIS-Arelation.Thesynsetgroup#1,grouping#1isatop-levelbroadsemanticcategorythatcorrespondstohierarchy,therearesomecasesinwhichasynsetbelongstoadifferentsupersensethanthatofitsparent's.thesupersensegroupnoun.group.Thelowersynsetssocialgroup#1,organization#1,organisation#3,andinstitution#1,establishment#2aremorespecificsynsets,whichinthispaperwecallthefirst,second,andthirdgeneralsynsets.Notethatweusethishierarchicalinformationforonlynounsandverbs,becauseadjectivesandadverbsdonothavesuchhierarchicalstructuresastheyhave.</section>
  <subsection title="Supersense">Asupersenseisacoarse-grainedsemanticcategory,withwhicheachnounorverbsynsetinWordNetisuniquelyassociated.Nounandverbsynsetsareassociatedwith26and15categories,respectively.Thecoarse-grainedsetsofsenselabelsareeasilydistinguishable,andenableustobuildahigh-performanceandrobustmodelwithsmalltrainingdata.WecanalsoexpectthemtoactasagoodsmoothingfeatureforWSD,whichwouldmakeupforthesparsenessoffeaturesassociatedwithfiner-grainedsenses.TheeffectivenessofusingsupersensesforWSDhasrecentlybeenshownbyseveralresearchers.Thecompletelistsofsupersensesareshownbelow.</subsection>
  <subsection title="Sense frequency information">Typically,sensesofawordarenotuniformlydistributed.SincedatasparsityhasbeenasignificantissueinWSD,sensefrequencyinformationishelpfulinachievingagoodperformance.Thisinformationactsasausefulfeaturethatoffersapreferenceforfrequentsenses,andalsoasaback-offfeature,whichenablesourmodeltooutputthefirstsense(ormostfrequentsense)whennootherfeaturesareactiveforthatword.WeusethesensefrequencyinformationavailableintheWordNet,whichisextractedfromastandard,balancedcorpus,theSemCor.Duetothelimitationofthecomputationaltimeandmemory,weincorporatethesensefrequencyinformationinaratherindirectmanner,sothateveryfeaturecanbedescribedasabinaryfeature.Forthesupersense-basedmodel,weuseonlythefirstsense(mostfrequentsense)ofawordasafeature.Sincethefirstsensebaselineishighlycompetitive-2Englishall-wordstasktestset,and63.4%fortheSenseval-3Englishall-wordstasktestset.SincethesensefrequencyinformationinWordNetisbasedontheSemCor,thisbaselineperformsfarbetterontheSemCor:75.9%forthebrown1sectionand74.3%forthebrown2section.intheall-wordsWSD,thisfeatureisexpectedtoaccountforasubstantialproportionofthesensefrequencyinformation.Forthesynset-basedmodel,wealternativelyusethesenserankingofasenseamongitscandidatesenses.Thisisbecausethefirstsensefeatureisinappropriatewhensufficienttraininginstancesarenotavailableforeverysense.SincesensesofawordinWordNetareorderedaccordingtofrequency,itcanrepresentthefrequencyofasenseinasimpleway,whilethesensedistributionofeverywordistreatedequally.</subsection>
  <section title="WSD model with tree-structured CRFs"/>
  <subsection title="Approach">InSection,wedescribedtwoproblemsintheWSDfield:theindependentclassificationofeachword'ssense,andthescarcityofthetrainingdata.Weaddresstheseproblemsbycombiningtwomethods.Thefirstmethodistheuseofthesyntacticdependenciesofwordsensesonadependencytree.Inparticular,weassumethattherearestrongdependenciesofwordsensesbetweenasyntacticheadanditsdependentsinthedependencytree,ratherthanbetweenneighboringwordsinthesentence.Totheextentofourknowledge,ourmodelisthefirstWSDmodelthatincorporatesthesensedependenciesbasedonasyntacticstructure.Thesecondmethodisthecombinationofvariouscoarse-grainedsensetagsetswiththeWordNetsynsets.Inourexperiments,thesetagsetsareusedintwoways.Onewaydirectlyusesthemasthesenseinventory,insteadofthefinersenseinventory.Inoursupersense-basedmodel,weusethesupersensesasthesenseinventory,andeachwordsenseisdisambiguatedatthegranularitylevelofsupersenses.Thismethodservesusmuchmoretraininginstancesforeachcoarsersense,whilewecannolongerdistinguishthefinersensesinsideit.Theotherwayusesthecoarse-grainedtagsetsincombinationwithfinersensetagsets.Inoursynset-basedmodel,threecoarse-grainedlabelsetsareincorporatedincombinationwiththefine-grainedWordNetsynsets.Althoughthesensedisambiguationisstillbasedonthefinersenses,thecoarsersensetagswillhelpthediscriminationofthefinersenses,byservinggeneralizedinformationforeachfinersense.Finally,theprocessofWSDissummarizedbelow.Atthebeginning,weparsetargetsentenceswithadependencyparser,andcompacttheoutputtreessothattheycandescribeinformativedependenciesamongwords,asdescribedinSection.Then,theWSDtaskisregardedasalabelingtaskonthetreestructures.ByusingT-CRFs,wecanmodelthisasthemaximizationoftheprobabilityofwordsensetrees,giventhescoresforverticesandedges.Inthetrainingphase,allvertexfeaturesandedgefeaturesareextractedusingthegold-standardsenses,andtheweightvectorsareoptimizedoverthetrainingdata.Inthetestingphase,allpossiblecombinationsofsensesareevaluatedforeachsentence,andthemostprobablesenseassignmentisselected.</subsection>
  <subsection title="Tree-structured conditional random fields">ConditionalRandomFields(CRFs)aregraph-basedprobabilisticdiscriminativemodelsproposedby.CRFsarestate-of-the-artmethodsforsequencelabelingproblemsinmanyNLPtasks.CRFsconstructaconditionalmodelp(y|x)fromasetofpairedobservationsandlabelsequences.Theconditionalprobabilityofalabelsequenceyconditionedonadatasequencexisgivenbywheref_jandg_karethefeaturevectorsforanedgeandavertex,_jand_karetheweightvectors,y_eandy_varethesetofcomponentsofyassociatedwithanedgeeandavertexv,andZ(x)isthepartitionfunctionwhichconstrainsthesumofalltheprobabilitiestobe1.Tree-structuredCRFs(T-CRFs)aredifferentfromwidelyusedlinear-chainCRFs,inthattherandomvariablesareorganizedinatreestructure.Hence,theyareappropriateformodelingthesyntacticdependenciesofwordsenses,whichcannotberepresentedbylinearstructures.Inthismodel,theoptimallabelassignmentyforanobservationsequencexiscalculatedbyy&amp;=_yp(y|x)&amp;=_y1Z(x)_vV_j_jf_j(v,x,y(v))+_k_kg_k(v,v',x,y(v),y(v'))&amp;=_y_vV_j_jf_j(v,x,y(v))+_k_kg_k(v,v',x,y(v),y(v'))alignwherevdenotesavertexcorrespondingtoawordwhilev'denotesthevertexcorrespondingtoitsparentinthedependencytree.Ifweinsteadinterpretv'asthevertexassociatedwiththeprecedingwordinasentence,T-CRFsarereducedtolinear-chainCRFs.AlthoughT-CRFsarerelativelynewmodels,theyhavealreadybeenappliedtoseveralNLPtasks,suchassemanticrolelabelingandsemanticannotation,provingtobeusefulinmodelingthesemanticstructureofatext.OurmodelisthefirstapplicationofT-CRFstoWSD.</subsection>
  <subsection title="Graph construction">Inthissection,weintroducethemethodofbuildinggraphstructuresonwhichCRFsareconstructed.First,wedescribehowtoconstructatreeusedinthetree-structuredmodel.Letusconsiderthesynset-leveldisambiguationofthefollowingsentence.Inthebeginning,weparsethissentencewithSagaeandTsujii'sdependencyparser,whichoutputsparsedtreesintheCoNLL-Xdependencyformat.Theleft-handsideofFigureshowstheparsedtreeforSentence(i),whereeachchild--parentedgedenotesadirecteddependencyofwords,andthelabelsontheedgesdenotethedependencytypes.Whilethisdependencytreedescribesdependenciesamongallwords,includingcontentwordsandfunctionwords,someofthesedependenciesarenotinformativeforourWSDtask,becauseourtaskdoesnotfocusonthedisambiguationoffunctionwords.Forexample,ontheleft-handsideofFigure,thedependenciesamongconfidence,in,andbankaresplitintothetwodependenciesconfidence--inandin--bank;henceourmodelcannotcapturethedirectdependencybetweenconfidenceandbank,whichareconsideredtobesemanticallycorrelated.Onewaytosolvethisproblemistousehigher-order(semi-Markov)dependencies,butthismaydrasticallyincreasethecomputationalcost.Thus,forthesynset-basedmodel,weconverttheoutputdependencytreeintoatreeofcontentwords,asexemplifiedontheright-handsideofFigure.Inthisprocess,thefunctionwordsareremovedfromthetree,andtheirparentandchildverticesaredirectlyconnected.Theremovedwordsareincludedasafeatureforthenewedge.Now,thedependencybetweenconfidenceandbankinFigureisdescribedasadirectedge.Thus,bythecompactionofthetrees,ourmodelcancapturemoreusefuldependenciesamongwordsenses.Forthesupersense-basedmodel,wefurtherconvertthetreeintoatreeofnounsandverbs,becausesupersensesaredefinedforonlythesetwopartsofspeech.Theinclusionofremovedwordsanddependencyrelationlabelsareperformedinexactlythesamemannerasinthesynset-basedmodel.Consequently,thetreeontherighthandsideofFigureremainsunchanged,becauseinthiscasethesentencedoesnotcontainanyadjectivesnoradverbs.Forthelinear-chainmodels,parsingisunnecessary.Atfirst,weconnecteveryadjacentwordswithanedge,andbuildalinearchain.Next,forthesamereasonasforthetree-structuredcase,weremovethosewordsthatwedonotneedtodisambiguatefromthegraph,inordertocapturethedirectdependenciesbetweencontentwords(ornounsandverbsinthesupersense-basedmodel).TheprocessofcompactingthetreeisdescribedinFigure.</subsection>
  <subsection title="Example">Inthissection,letuspresentanintuitiveillustrationofhowourmodelworks.Here,wefocusonthreewordsdestroy,confidence,andbankinSentence(i).Forsimplicity,weconsideronlytwomajorsensesforeachwordasdescribedinTable,sothatthenumberofpossiblesenseassignmentsis2^3=8.Afteranappropriatecompactionofthedependencytree,dependenciesamongdestroy,confidence,andbank,arerepresentedasdirectconnections.Now,ourobjectiveistodeterminethecorrectassignmentofsensestothesewords,giventhetrainedweightvectorforfeatures.Weconductthisbyevaluatingthescoresforallpossibleassignmentofsenses.Letusstartfromthedependencybetweenconfidenceandbank.Thefirstintuitionwouldbethatconfidence(n)#2isstronglyrelatedtoagrouporaninstitution(financialbank),butisunrelatedtoanaturallandscape(riverbank),whileconfidence(n)#1dependsmostlyonpersonsandnotonotherentities.Becausebankdoesnothavea``person''meaning,theweightofconfidence(n)#2--bank(n)#1isexpectedtobehigherthanthoseofotherpossiblesensebigrams.Asimilarargumentcanbemadeforthedependencybetweendestroyandconfidence.Wecanassumethatdestroy(v)#1isusuallyassociatedwithrealobjects,whereasdestroy(v)#2cantakeeitherarealentityoranabstractthingasitsdirectobject.Givenconfidencedoesnothavean``object''meaning,theweightsofdestroy(v)#2--confidence(n)#1anddestroy(v)#2--confidence(n)#2wouldbelargestamongothers.Finally,givenallscoresforthesesensedependencies,wecanevaluatetheoverallscoreforthesentence,andseedestroy(v)#2,confidence(n)#2,bank(n)#1isthemostprobableassignmentofsenses.Inpractice,specificbigramsofsynsetssuchasconfidence(n)#2--bank(n)#1anddestroy(v)#2--confidence(n)#2maynotappearinthetrainingdata.Inthiscase,sensebigramscombinedwithcoarsersenselabelsworkeffectively.Forexample,iftherearesynsetbigramssuchasdestroy(v)#2--affection(n)#1inthetrainingdata,themodelcanstillperformthedisambiguationprocessproperlybyconsideringageneralizedsynset--supersensebigramdestroy(v)#2--noun.feeling.ThedetaileddescriptionofsensebigramsareprovidedinSection.</subsection>
  <subsection title="Sense labels">UsinginformationintheWordNet,wemakeuseoffoursenselabelsforeachword:asynsetS_WS,twogeneralsynsetsS_G1andS_G2,andasupersenseS_SS,whichweintroducedinSection.Theselabelsrepresentwordsensesatdifferentgranularitylevels,andaretobecombinedwiththevertexandedgefeatures.WehereafterdistinguisheachsenselabelbyputtingoneoftheprefixesWS,G1,G2,andSS,asinWS:bank#1andSS:noun.group.Theexampleofthesenselabelsfordestroy(v)#1isshowninTable.Forthewordsotherthannounsandverbs,thesupersenseN/Aisassigned.Inourmodel,wecombinethesynsetandsupersenselabelswiththevertexfeatures,andcombineallfoursenselabelswiththeedgefeatures.WedenotethesetofsenselabelsforvertexfeaturesbyS_VT(=S_WS,S_SS),andtheoneforedgefeaturesbyS_ED(=S_WS,S_G2,S_G1,S_SS).</subsection>
  <subsection title="Vertex features"/>
  <subsubsection title="Synset-based model">WeimplementasvertexfeaturesasetoftypicalcontextualfeatureswidelyusedinmanysupervisedWSDmodels.Mostofthesefeaturesarethoseusedby,withtheexceptionofthesyntacticfeatures.Inordertoseetheeffectivenessofsensedependencyfeatures,weincludeasvertexfeaturesthewordforms,lemmas,andpartsofspeechofboththeparentandthechildwordsinthedependencytree.Thesefeaturesprovidethesyntacticinformationoftheparentandchildwordsthatarenotsemanticallydisambiguated.Therefore,ifthesensebigramfeaturesworkeffectivelyoverthesefeatures,itclearlyshowsthatthereexistinstancesthatcannotbedisambiguatedwithoutconsideringtheinterdependencyofwordsenses.Thelistofvertexfeaturesalsoincludestheinformationofboththeprecedingandfollowingwords,whichinthelinear-chainmodelplaysthesameroleastheparentandchildinformationinthetree-structuredmodel.Belowisthelistofcontextualinformationusedforthevertexfeaturesinthesynset-basedmodel.WerefertothesefeaturesasF_VT(v).form(WF):wordformasitappearsinatext.context(GC):bag-of-wordswithina60-wordwindow.PoS(LP):(-3),(-2),(-1),(0),(1),(2),and(3),whereiin(i)denotestherelativepositiontothetargetword.context(LC):(-2),(-1),(0),(1),(2),(-2,-1),(-1,1),(1,2),(-3,-1),(-2,1),(-1,2),and(1,3),where(i)denotesthewordattherelativepositioni,and(i,j)then-gramfromtherelativepositionitoj.context(SC):wordforms,lemmas,andpartsofspeechoftheparentandchildwords.UsingthiscontextualinformationF_VT(v)andthesetofvertexlabelsS_VT,weconstructasetoffeaturesonavertexvbyS_VT(v)F_VT(v).Additionally,weincludethesenserankingfeature(seeSectionfordetail),whichisnotcombinedwithanysenselabelnorwithanycontextualinformation.</subsubsection>
  <subsubsection title="Supersense-based model">Forthesupersense-basedmodel,weusevertexfeaturesbasedon,whichincludessomefeaturesfromthenamedentityrecognitionliterature,includingthewordshapefeatures,alongwiththestandardfeaturesetforWSD.Asthesensefrequencyinformation,weusethefirstsensefeature.Unlikeinthesynset-basedmodel,wedonotincorporatethesyntacticinformationoftheparentandchildwords,sinceithasbeenreportednottoimprovetheperformanceby.</subsubsection>
  <subsection title="Edge features">Wedesignasetofedgefeaturesthatrepresentstheinter-wordsensedependencies.Foreachedge,wedefinethesensebigramfeaturesS_ED(v)S_ED(v').Moreover,inadditiontothesesimplebigrams,wedefinetwokindsofcombinedbigrams:thesensebigramswiththedependencyrelationlabels(e.g.WS:confidence#2--(NMOD)--WS:bank#1),andthesensebigramswithremovedwordsinbetween(e.g.WS:confidence#2--in--WS:bank#1).Consequently,thenumberoffeaturesforeachedgeis4^23=48.</subsection>
  <section title="Experimental setup"/>
  <subsection title="Data sets">Inthissection,weintroducecorporathatwehaveusedfortheevaluation.SemCorisacorpus,inwhichallcontentwordsareannotatedwiththeWordNetsynsets,andconsistsofbalanced352filesfromtheBrownCorpus.Itisdividedintothreeparts:brown1,brown2,andbrownvsections.Inbrown1andbrown2,allcontentwords(nouns,verbs,adjectives,andadverbs)aresemanticallyannotated,whileinbrownvonlyverbsareannotated.Also,weusetwodatasetsfromtheSensevalexercises:theSenseval-2Englishall-wordstasktestset,consistingofthreearticlesfromtheWallStreetJournal,andtheSenseval-3Englishall-wordstasktestset,consistingoftwoarticlesfromtheWallStreetJournalandafictionexcerptfromtheBrowncorpus.Asthedatasetsforevaluation,weusethebrown1andbrown2sections(denotedasSEM)ofSemCor,andtheSenseval-2and-3all-wordstasktestsets(denotedasSE2andSE3,respectively).Weusetheconvertedversionsrada/downloads.htmlannotatedwithWordNet2.0synsets.Inthesedatasets,multi-wordexpressionsarealreadysegmented,whiletheyarenotintheoriginalcorpora.However,ontheotherhand,ourmodelcannotoutputanyanswerstomulti-wordexpressionsthathavenodirectlycorrespondingWordNetsynsets,becausewetreateachexpressionasoneunitintheprocessofWSD.Forexample,themulti-wordexpressiontear-filledistreatedasoneinstance.ButitisnottaggedwithanyWordNetsynsetsintheconvertedcorpus,whileintheoriginalcorpusitistaggedwithtwoWordNetsynsetsfortearandfilled.Forthisreason,weexcludesuchinstancesbeforehand,andevaluateourmodelsonlyonexpressionsthathavecorrespondingsynsetsintheWordNet.Theresultingstatistics,34instances),andthemulti-wordexpressionsthathavenodirectlycorrespondingWordNetsynset(29instances).ofthedatasetsareshowninTable.Theevaluationofourmodelisperformedbysplittingthesecorporaintotraining,development,andtestsets.Atfirst,allfilesinSEMaresortedaccordingtotheirfilenamesanddistributedintofivedatasetsinorder(denotedasSEM-A,SEM-B,SEM-C,SEM-D,andSEM-E),sothateachsethasalmostthesamedistributionofdomainsisnamedaccordingtothedomainthatitbelongsto..Next,eachofthesefivedatasetsisagaindividedintotwohalves:SEM-A1,SEM-A2,,SEM-E1,andSEM-E2,alsoaccordingtotheorderoffilenames.Ourevaluationisbasedona5-foldcrossvalidationscheme.Inthetrainingphase,foursets(e.g.SEM-A,SEM-B,SEM-C,andSEM-D)intheSEMareusedfortraining.Next,fortheevaluationonSemCor,onehalfoftherest(e.g.SEM-E1)isusedfordevelopmentandtheotherhalf(e.g.SEM-E2)isusedforevaluation.FortheevaluationontheSensevaldatasets,allinstancesoftherest(e.g.SEM-E)areusedfordevelopmentandoneoftheSensevaldatasets(SE2orSE3)isusedforevaluation.Finally,forthecomparisonwithstate-of-the-artmodels,ourmodelistrainedonthewholesetofSEM,andSE2andSE3areusedfordevelopmentandevaluationrespectivelyOurT-CRFmodelistrainedbyusingAmis.Duringthedevelopmentphase,wetunetheGaussianparameterfortheL_2regularizationterm.</subsection>
  <subsection title="Evaluation and models">Astheevaluationmeasure,weusethestandardrecallmeasure,whichisequivalenttotheprecisionasweoutputanswerstoallinstances.Weevaluateourmodelsbasedontherecallsaveragedoverthefivetrialsofthecrossvalidation.Thesynset-basedevaluationisperformedbasedontheWordNetsynsets.Weevaluatetheoutputsofoursystemforallinstancesthataresemanticallytaggedinthedatasets.Eachtargetwordiseitheranoun,verb,adjective,oradverb.Forthesupersense-basedevaluation,wefollowmostoftheexperimentalsetupin.Astheynoted,intheWordNet,thereissemanticallyinconsistentlabelingofsupersensessuchthattoplevelsynsetsaretaggedasthesupersensenoun.Topsratherthanthespecificsupersensetheygovern.Forexample,nounssuchaspeachandplumaretaggedasnoun.plantbuttheirhypernymplantbelongstonoun.Tops.Forthisreason,weadoptedthemodificationofnounsupersensesinthesamewayas,substitutingnoun.Topslabelswithmorespecificsupersenselabelswhenpossible,andleftsomegeneralnounswithnoun.Tops.Theevaluationisbasedonthesemodifiedlabels.Weignoretheadjectiveandadverbinstancesinthesupersense-basedevaluation.Tableisthelistofmodelsthatweusefortheevaluation,whereFSandSRcorrespondtothefirstsenseandsenserankingfeaturesrespectively,andnon-dependencydenotesmodelsthatdonotincorporatethesensedependencyfeatures.</subsection>
  <section title="Result"/>
  <subsection title="Contribution of sense dependencies">Inthissection,wefocusonthecontributionofthesensedependencies.Tableshowsthecomparisonsbetweenthetree-structuredmodelswithsensedependencies(dependencymodels)andthemodelswithoutsensedependencies(non-dependencymodels).Eachfiguredisplaysthemeanrecall(equivalenttotheprecisions)averagedoverthefivetrials,the``Diff.''rowsshowthedifferencesbetweenthedependencymodelsandthenon-dependencymodels,andanddenotethestatisticalsignificanceofp&lt;0.05andp&lt;0.01respectively.FromTable,itisseenthatwiththesensefrequencyinformation,thetree-structuredmodels(statistically)significantlyoutperformedthenon-dependencymodelsonallthedatasets.Theseimprovementsseeminconsiderableinfigures;however,consideringthatforinstancetheNo-Dep-SS-FSmodeloutperformstheBaseline-SSmodelbyonly0.37%onSEM,thefurtherimprovementof0.21%issubstantial,becauseitindicatesthatourdependencymodelcouldhandle57%moreinstancesoverthefirstsensebaseline.Notethat,withoutthesensefrequencyinformation,thesynset-basedtree-structuredmodel(Tree-WS)performedworsethanthenon-dependencymodel(NoDep-WS)onallthedatasets,whereasthesupersense-basedmodel(Tree-SS)exhibitedtherobustnessregardlessoftheexistenceofthesensefrequencyinformation.Theseresultssuggestthatforthesynset-basedmodel,inwhichmostsynsetsdonothaveenoughinstancesinthetrainingdata,thecombinationwithsensefrequencyinformationisnecessaryinordertoavoidthedatasparsenessproblem.Similarly,Tableshowsthecomparisonsbetweenthelinear-chaindependencymodelsandthenon-dependencymodels.Inthesupersense-basedevaluation,althoughthedifferencesareslightlysmallerthaninthetree-structuredmodels,weconfirmedthatthesensedependencieswiththesensefrequencyinformationworkeffectively,withtheoverallimprovementsof0.20--0.30%forthethreedatasets.However,withoutthefrequencyinformation,nostatisticallysignificantimprovementnordeteriorationisobserved.Inthesynset-basedevaluation,theoveralltrendisalmostsameasinthetree-structuredcase.Nonetheless,bytheincorporationofthesensedependencies,theimprovementswiththesensefrequencyinformationwereevenless,andthedeteriorationswithoutthefrequencyinformationwereevenmorethaninthetree-structuredcase.Theseresultsareconsistentwiththeresultsinthefollowingsection,wherethetree-structuredmodelsareshowntooutperformthelinear-chainmodels.</subsection>
  <subsection title="Tree-structured CRFs vs linear-chain CRFs">Inthissection,letusfocusonthedifferencebetweenthetree-structuredmodelsandthelinear-chainmodels.InTable,althoughsomeofthedifferencesaremarginal,wecanseethatthetree-structuredmodelsoutperformedthelinear-chainmodels,byfocusingonthestatisticallysignificantdifferences.Theseresultssuggestthatthedependenciesonthetreestructurescapturemoreimportantcharacteristicsthanthoseonthelinearchainsdo.</subsection>
  <subsection title="Contribution of coarse-grained sense labels">Tableshowsthecontributionsofthecoarse-grainedsenselabels.WhereasTree-WS-SRandTree-WSuseallfoursenselabelsfortheedgefeatures(S_ED=S_WS,S_G2,S_G1,S_SS),Tree-WS-SR'andTree-WS'onlyusethesynsetlabels(S_ED=S_WS)sothatwecanseethecontributionofthecoarse-grainedlabels.Althoughtheimprovementsarenotstatisticallysignificant,wecanseethatthecoarse-grainedlabelsconsistentlydidimprovetheperformanceonallthedatasets.</subsection>
  <subsection title="Comparison with state-of-the-art models">Tableshowsthecomparisonofourmodelwiththestate-of-the-artWSDsystems.SincetheevaluationhereisperformedwiththeSensevalofficialscorer,thefiguresareslightlydifferentthanonourevaluationschemeusedintheothersections.OurbestmodelTree-WS-SRoutperformedthetwobestsystemsintheSenseval-3(GamblandSenseLearner),butlaggedbehindPNNLby1.6%.However,ourmodelcannothandlemulti-wordexpressionsthatdonotexistintheWordNetasnotedinSection,andallsystemsinTableexceptforSimil-Primeutilizeothersense-annotatedcorpora,suchastheSensevallexicalsampletaskdatasetsorexamplesentencesintheWordNet,inadditiontoSemCor.Takingintoconsiderationthesefactors,wecanconcludethattheperformanceofourT-CRFmodeliscomparabletothatofstate-of-the-artWSDsystems.</subsection>
  <section title="Discussion and analysis"/>
  <subsection title="Edge feature overview">Tableshowsthelistofthe15largest-weightedsensedependencyfeaturesinthetree-structured,synset-basedmodel(Tree-WS).Thelistincludesmanyfeaturesassociatedwithadjective--nounrelations(e.g.SS:noun.person--WS:distinguished(a)#1)andverb--nounrelations(e.g.WS:have(v)#2--SS:noun.attribute).Hereinafter,denotesinEquation,anddenotestheexponentialof.Wecallafeatureeitherwithapositivelambdaorwithanalphalargerthan1asanexcitatoryfeature,andthatfeatureeitherwithanegativelambdaoranalphasmallerthan1asaninhibitoryfeature.Also,Tableshowsthe15largest-weightedsensedependencyfeaturesinthelinear-chain,synset-basedmodel.Whencomparedtotheoutputsofthetree-structuredmodel,wecanseethatthelinear-chainmodelcapturesmoresuccessivenoun--noundependencies,whilethetree-structuredmodelcapturesmoreadjective--nounandverb--objectdependencies.Thus,althoughthedifferenceoftherecallsissmall,wecanassumethatthesensedependencyfeaturesinthetree-structuredmodelandthoseinthelinear-chainmodelhavedifferentcontributionstotheresults.Thesimultaneoususeofbothisofinterest;however,sinceitmakesourmodelnolongeratree,theimplementationisnotstraightforward.Hence,thisisleftasoneofourfutureworks.</subsection>
  <subsection title="Instance-based analysis"/>
  <subsubsection title="Overview">Inthissection,wepresentinstance-basedanalysesbasedonthefirst100instancesforwhichtheanswerofthedependencymodelTree-WS-SRdiffersfromthatofthenon-dependencymodelNoDep-WS-SRinthefirsttrialonSemCor.Weextractedonlythelargest-weightededgefeatureforeachinstance,assumingthatthisfeaturehadthelargestcontributiontotheresult.Theseinstancesconsistof54positiveinstances,forwhichTree-WS-SRoutputthecorrectanswerwhileNoDep-WS-SRdidnot,and46negativeinstances,forwhichTree-WS-SRdidnotoutputthecorrectanswerwhileNoDep-WS-SRdid.Tableandshowthecountofeachedgetypefortheseinstances.Forbothpositiveandnegativeinstances,theverb--noundependenciesarethedominantdependencies,whichaccountfor48%ofalltheinstances.Onenoteworthypointisthatmorenoun--noundependenciesarefoundinthepositiveinstancesthaninthenegativeinstances,whichmightsuggestthatnoun--noundependenciesareparticularlylikelytocaptureusefuldependenciesandcontributetopositiveinstances.</subsubsection>
  <subsubsection title="Verb--noun dependencies">Letuspresenttwoinstancesinwhichtheverb--noundependenciesworkedeffectively.Thefirstsentenceis:Theverbtakehasasmanyas42sensesintheWordNet.Butfortunatelly,thefirstsixsensesbelongtodifferentsupersenses,andourdependencymodelsucceededinoutputtingthecorrectsensetake#4(SS:verb.contact,takephysically)bymakinguseofthestrongdependencySS:verb.contact--SS:noun.substance(=1.209,=0.1898),givendust#1belongstonoun.substance.ThesecondinstanceisalsoapositiveinstancefromtheSEM-Adataset.Here,hasisanambiguousverbthathas19sensesintheWordNet.Thecorrectsensehereishave(v)#2(SS:verb.stative,haveasafeature).Givensense-of-humor#1belongstothesupersensenoun.attribute,thecorrectsensewasoutputbythestrongverb--objectdependencyG1:have(v)#2--(OBJ)--SS:noun.attribute(=1.331,=0.2860).Whilethisverb--objectdependencyhadalargeexcitatoryweight,thecorrespondingverb--subjectdependencyhadaninhibitoryweight(G1:have(v)#2--(SBJ)--SS:noun.attribute(=0.919,=-0.0845)),whichindicatesthatthedependencyrelationlabelalsocontributedtotheresult.Notealsothatthislong-distancedependencycannotbedescribedbylinear-chainmodels.Next,letusshowatypicalnegativeexample,whereaverb--subjectdependencyworkedinappropriately.Thecorrectsenseforshowhereisshow#2(verb.cognition,establishthevalidity),butthemodeloutputshow#3(verb.communication,proveevidencefor)affectedbythelongdependencyWS:testify(v)#2--(SBJ)--SS:noun.act(=1.186,=0.1706)betweeneffortsandshow.Thissubject-verbinformationseemstobeinadequateforthedisambiguationofshow.</subsubsection>
  <subsubsection title="Noun--noun dependencies">Next,wefocusonthenoun--noundependencies.Thefirstexampleisanegativeinstance.Thenouncareerhastwomeanings:theparticularoccupationforwhichyouaretrained(career#1)andthegeneralprogressionofyourworkingorprofessionallife(career#2).Fromthephrasecareerasaplayer,wemightfirstassumethatthecorrectsenseofcareercanbeeitheroftwosenses,withthepossibilitythatthereisapreferenceforcareer#2,justasexplainedbythelargest-weighteddependencyWS:career(n)#2--(NMOD)--SS:noun.person(=1.071,=0.06859)betweencareerandplayer.However,infact,thecorrectanswerhereiscareer#1,andthedeterminingclueforthisinstanceseemstobetheverb--objectdependencybetweenendandcareer,whichwasnotcapturedbyourmodel.Amongthetenpositiveinstancesofthenoun--noundependencies,fourinstanceswerecontributedbythenoun--of--noundependencies.Sincedependenciesofthistypewerenotobservedinthenegativeinstances,theyseemtoparticularlycontributetothepositiveinstances.Letusconsiderthefollowingexample.Althoughthecorrectsensetime#5(noun.Tops,thecontinuumofexperienceinwhicheventspassfromthefuturethroughthepresenttothepast)isnotafrequentsense,ourmodelcorrectlyoutputthissensebyusingthedependencySS:noun.object--of--WS:time(n)#5(=1.054,=0.05259),givennaturalorder#1belongstothesupersensenoun.object.</subsubsection>
  <subsubsection title="Coordination dependencies">Throughouranalysis,weobservedthatthenoun--noundependenciesincoordinationrelationsworkremarkablywell.Inthefollowingsentence,threewordsnails,levels,andTsquaresareinacoordinationrelation.Here,thecorrectsensefornailisnail#2(noun.artifact,athinpointedpieceofmetal)andthatforlevelislevel#5(noun.artifact,indicatorofthehorizontal).Therelativelylowfrequencyofthesesensespreventsourmodelfromoutputtingthecorrectsensesinanordinalway.However,thedependencymodelcouldcapturethefactthattwowordsinacoordinationrelationarequitelikelytobelongtothesamesemanticgroup(SS:noun.artifact--(COORD)--SS:noun.artifact(=1.367,=0.3126)),andhencesucceededinthecorrectdisambiguationofallthesethreewords.Moregenerally,wehaveobservedthatthecoordinationfeaturesforanedgethatconnectsthesamesupersenseallhavepositiveweights.</subsubsection>
  <section title="Conclusion">Inthispaper,weproposedanovelapproachfortheall-wordsWSD,focusingontheuseofsyntacticdependenciesofwordsenses,andinvestigatedthecontributionofthesedependenciestoWSD.Ourproposalsweretwofold:toconsiderthesensedependenciesondependencytrees,andtousethecombinedbigramsoffine-andcoarse-grainedsensesasedgefeatures.Inourexperiments,thesensedependencyfeatureswereshowntoworkeffectivelyforWSD,witha0.29%,0.64%,and0.30%improvementofrecallsforSemCor,Senseval-2,andSenseval-3datasetsrespectively.Despitethesmallimprovementsinoverallfigures,theseimprovementsindeedcorrespondto11--26%improvementsoverthefirstsensebaseline.Thedependencytreestructureswereshowntobeappropriateinmodelingthedependenciesofwordsenses,bytheresultsthatthetree-structuredmodelsoutperformedthelinear-chainmodels.Intheanalysissection,wepresentedanin-depthanalysisoftheoutputs,andobservedthatthenoun--noundependenciesparticularlycontributetothepositiveinstances.Inaddition,thecombinationofcoarse-grainedtagsetswiththesensedependencyfeaturesconsistentlyimprovedtheperformanceofWSDoneverydataset,althoughtheimprovementswerenotstatisticallysignificant.However,ourexperimentsalsoshowedthatevenwhencombinedwiththecoarse-grainedtagsets,thesensedependencyfeaturesdonotimprovetheperformanceofWSDunlesscombinedwithpropersensefrequencyinformationrelievingthedatasparsenessproblem.Thesupersense-basedWSDmodels,onthecontrary,exhibitedtherobustnessregardlessoftheexistenceofthesensefrequencyinformation.Theperformanceofourtree-structuredmodelwascomparabletothatofthestate-of-the-artWSDsystems.AlthoughourmodelwasbasedonasimpleframeworkandwastrainedonlyontheSemCorcorpus,theresultsthatwegainedwerepromising,suggestingthatourmodelstillhasagreatpotentialforimprovement.Ournextinterestistocombineourframeworkwiththerecently-developedsemi-supervisedframeworks.ThecombinationofthelocalandsyntacticdependencieswiththeglobalinformationisexpectedtofurthertheWSDresearch.workwaspartiallysupportedbyGrant-in-AidforSpeciallyPromotedResearch(MEXT,Japan).document</section>
</root>
