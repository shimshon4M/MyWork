<?xml version="1.0" ?>
<root>
  <section title="Introduction">ClauseSplitting(CS)isthetaskofsplittingacomplexsentenceintoseveralclauses.Thistaskisimportantforvarioustaskssuchasmachinetranslation,aligningparalleltexts,texttospeechsystemsandtransformationbynaturallanguagesentencesintologicalformsanditbecametheshared-taskprobleminCoNLL2001.CSisadeeperlevelofpartialparsing,whichisthetaskofrecoveringonlyalimitedamountofsyntacticinformation.Machinelearningtechniquesinthelastdecadehavepermeatedmostareasofnaturallanguageprocessing.Thereasonisthatavastnumberofmachinelearningalgorithmshaveprovedtobeabletolearnfromnaturallanguagedatagivenarelativelysmallcorrectlyannotatedcorpus.Therefore,machinelearningalgorithmsmakeitpossibletowithinashortperiodoftimedeveloplanguageresources(dataanalyzedonvariouslinguisticlevels)thatarenecessaryfornumerousapplicationsinnaturallanguageprocessing.ForpartialparsingsuchasCS,thereisalotofinterestinthedesignoflearningsystemswhichperformonlyapartialanalysisofasentence.Recently,manymachinelearningmethodshavebeensuccessfullyappliedtopartialparsingtasks.Mostlearningmodelsusedforpartialparsingtasksarediscriminativemodels.Theseapproacheshavereachedthe-state-of-the-artofpartialparsing.proposesthediscriminativemodel,so-calledFR-Perceptronforrecognizingstructuresofclauses.TheyusedaglobalonlinelearningalgorithmtotrainadiscriminativemodelforCS.However,FR-Perceptronupdatesthelearningfunctions,dependingonthepredictionsonthatexampleandcomputationtimeforthetrainingandcodingprocessaretime-consuminginlargetrainingsetsasdescribedbelow.ThiscanbeinefficientwhenweapplyCStootherapplications.Thispaperbuildsonthepreviousworksdescribedby(Nguyenetal.2007)withanovelandefficientmethodforCSasanalternativetrainingmethodtoFR-Perceptron.TheCRFsmodeldefinesaconditionaldistributionoverlabelinggivenanobservationanditallowsfortheuseofverylargesetsofarbitrary,overlappingandnon-independentfeaturesandhasefficienttraininganddecodingprocesseswhichbothfindgloballyoptimalsolution.claimedthat``theCRFsmethoddoesabetterjobofparameterestimationforthesamefeatureset,andisparallelizable,sothateachpassoverthetrainingsetcanrequirejustafractionofthecomputationtimeoftheperceptronmethod''.Recently,therearemanysuccessfulapplicationsofCRFsincludingShallowParsingandNamedEntityRecognition.However,usingCRFsforCSisnotcompletelysimplebecausetherearethreeweaknessesforapplyingCRFs:Thelongdistancedependencebetweenastartposition(S)andanendposition(E)ofaclause.ItisdifficultfortheconventionalCRFstodealwiththeproblemoflongdistancedependencebetweenSandE.WeseetheexampleinSection,whichshowsthatthedistancefromthestartpositiontotheendpositionoftheclauseislong.Balancingbetweenanumberofstartwordpositionsandthoseofendwordpositionsofclausesinasentence.Theclausescanbeembeddedintheouterclauses.Toovercomethedrawbacksmentionedabove,weuseN-bestlistbyadaptingtheJoint-CRFs,andsimultaneouslyweuserichlinguisticinformationandproposeanewbottom-updynamicalgorithmfordecoding.Theexperimentsshowthatourresultsarecompetitivewiththepreviousresults.Especially,theprecisionofourresultsperformsbetterthanthatofthepreviousmethods.Additionally,withdecodingprocess,oursystemisalsomorethanapproximately50timesfasterthanthatofwritteninPerl.Therestofthispaperisstructuredasfollows.Section2reviewsrelatedwork.Section3formulatestheClausesplittingproblem.Section4brieflyintroduceslinearchainCRFsandJoint-CRFsandhowtoapplythemtoClauseSplitting.Section5describesanddiscussestheexperimentalresults.Finally,conclusionsaregiveninSection6.</section>
  <section title="Related Work">ManysupervisedmethodshavebeendevelopedforClauseSplitting.usedadiscriminativemodelapproachforit.Theyappliedagloballearningalgorithm,FR-Perceptrontorecognizestructureofclauses.Theydividedtheproblemintotwolayersoflocalsubproblems:afilteringlayer,whichreducesthesearchspacebyidentifyingplausibleclausecandidates;andarankinglayer,whichbuildstheoptimalclausestructurebydiscriminatingamongcompetingclauses.Arecognition-basedfeedbackruleispresented,whichreflectstoeachlocalfunctionitscommittederrorfromaglobalpointofview,andfollowtotrainthemtogetheronlineasperceptrons.Asaresult,thelearnedfunctionautomaticallybehavesasafilterandranker,ratherthanasabinaryclassifier.TheFR-PerceptronmethodshowsthebestresultforClauseSplittingnow.appliedtheAdaboostalgorithm.TheyimprovedClauseIdentificationbyusingglobalinferenceonthetopoftheoutcomeclauseshierarchicallylearnedbylocalclassifiers.OtherapproachessuchasMaximumEntropy,andWinnowareappliedforCStoo.AnumberofdifferentmethodsforthesupervisedlearningapproachwereusedfortheCoNLL-2001sharedtask.Thesemethodsincludeboostingdecisiontreesanddecisiongraphs,neuralnetworks,memory-basedlearning,statistical,andsymboliclearning.</section>
  <section title="Clause Splitting Problem">Atadeeperlevelofpartialparsingisclausesplitting.Aclauseisasequenceofwordsinasentenceandisagrammaticalunitthatincludes,atminimum,apredicateandanexplicitorimpliedsubject,andexpressesaproposition.Forexample:givenaninputsentence:CoachtheminhandlingcomplaintssothattheycanresolveproblemsimmediatelyTheproblemistosplitasentenceintoclausesasfollows:(Coachthemin(handlingcomplaints)(sothat(theycanresolveproblems)))Theproblemismoredifficultthansimplydetectingnon-recursivephrasesinsentences.ClauseSplittingisdividedintothreeparts:identifyingclausestarts,identifyingclauseends,andfindingcompleteclauses.FormulationLetXbeasentencespace,andYbeaclausespace.WecanconsideramodelforfindingclausesasafunctionR:XYwhich,givenasentencex,identifiesthesetofclausesyYofxX.First,weassumeafilterfunctionFwhich,givenasentencexconsistingofasequenceofnwords(x_1,x_2,,x_n),identifiesasetofcandidateclauses,F(x)PwherePisthesetofallpossibleclauses.Acandidateclauseisrepresentedas(s,e)forthesentencexwhere(s,e)isthesequenceofconsecutivewordsfromwordx_stowordx_e,Second,weassumeascorefunctionwhich,givenaclause,producesareal-valuepredictionoftheclause.Weidentifyasetofclausesforasentenceaccordingtothefollowingoptimalitycriterion:inwhichC(x)isasetofclausesforasentencex,and(s,e)_kisak-thclauseiny.Wewillidentifytheclausestarts(Task1)andtheclauseends(Task2)topredictasetofcandidateclausesforfindingcompleteclauses(Task3).</section>
  <section title="Applying CRFs to Clause Splitting">Inthissection,weshowhowtoovercomethedrawbacksofapplyingCRFsandJoint-CRFstoCSasmentionedintheintroduction.First,wepresentanoverviewoftheCRFsandJoint-CRFsmodels,wethenproposeadecodingalgorithmaswellasexploitingrichlinguisticinformationtodealwiththeproblemwhenapplyingCRFsandJoint-CRFstoCS.</section>
  <subsection title="Conditional Random Fields">ConditionalRandomFields(CRFs)areundirectedgraphicalmodelsusedtocalculatetheconditionalprobabilityofvaluesondesignatedoutputnodes,givenvaluesassignedtootherdesignatedinputnodesfordatasequences.CRFsmakeafirst-orderMarkovindependenceassumptionamongoutputnodes,andthuscorrespondtofinitestatemachine(FSMs).Leto=(o_1,o_2,,o_T)besomeobservedinputdatasequence,suchasasequenceofwordsinatext(valuesonTinputnodesofthegraphicalmodel).LetSbeafinitesetofFSMstates,eachisassociatedwithalabellsuchasaclausestartposition.Lets=(s_1,s_2,,s_T)besomesequencesofstates(valuesonToutputnodes).CRFsdefinetheconditionalprobabilityofastatesequencegivenaninputtobewhereZ_o=_s(_t=1^TF(s,o,t))isanormalizationfactoroverallstatesequences.WedenotetobetheKronecker-.LetF(s,o,t)bethesumofCRFsfeaturesattimepositiont:wheref_i(s_t-1,s_t,t)=(s_t-1,l^')(s_t,l)isatransitionfeaturefunctionwhichrepresentssequentialdependenciesbycombiningthelabell^'ofthepreviousstates_t-1andthelabellofthecurrentstates_t,suchasthepreviouslabell^'=AV(adverb)andthecurrentlabell=JJ(adjective).g_j(o,s_t,t)=(s_t,l)x_k(o,t)isaper-statefeaturefunctionwhichcombinesthelabellofcurrentstates_tandacontextpredicate,i.e.,thebinaryfunctionx_k(o,t)thatcapturesaparticularpropertyoftheobservationsequenceoattimepositiont.Forinstance,thecurrentlabelisJJandthecurrentwordis``conditional``.TrainingCRFsLet=_i,_jbethesetofweightsinaCRFsmodel.issettomaximizetheconditionallog-likelihoodofstatesequencesinsometrainingset,D=o,s^(1),,o,s^(N):wherethesecondsumisaGaussianprioroverparameters(withvariance^2)whichprovidessmoothingtoavoidoverfittinginthetrainingdata.Whenthetraininglabelsmakethestatesequenceunambiguous,thelikelihoodfunctioninexponentialmodelssuchasCRFsisconvex,andfindingtheglobaloptimumisguaranteed.ParameterestimationofaCRFsmodelrequiresaniterativeprocedure.Currently,variousmethodscanbeusedtooptimizeL_,includingIterativeScalingalgorithmssuchasGISandIIS,andquasi-NewtonmethodssuchasL-BFGS.Amongthesemethods,L-BFGSisthemostefficient.L-BFGSrequiresonlythatoneprovidesthefirst-derivativeofthefunctiontobeoptimized.Lets^(j)denotethestatepathoftrainingsequencej,andthenthefirst-derivativeofthelog-likelihoodisinwhichC_k(s,o)isthecountoffeaturef_k,givensando.Thefirsttwotermscorrespondtothedifferencebetweentheempiricalandthemodelexpectedvaluesoffeaturef_k.Thelasttermisthefirst-derivativeoftheGaussianprior.InferenceinCRFsGiventheconditionalprobabilityofthestatesequencedefinedin(2)andsetoftheparameters=,,inferenceinCRFsistofindthemostlikelystatesequences^*subjectto:[s^*=argmax_sp_(s|o)=argmax_sexp(_t=1^TF(s,o,t))]Wecanefficientlycalculates^*withtheViterbialgorithm.ForViterbialgorithm,weusethetableforstoringtheprobabilityofthemostlikelypathuptotimet,whichaccountsforthefirsttobservationsandendsinstates_i.Wedefinethisprobabilitytobe_t(s_i)(0tT-1),(s_iS),where_0(s_i)istheprobabilityofstartingineachstates_i.Wearegivenarecursiveformulationasfollows:wheres_jS.Theformula(6)terminatesinthemostlikelystates_i^*wheres_i^*=argmax_s_i[_T(s_i)].Froms_i^*,wecanbacktrackthroughthedynamicprogrammingtabletorecovers^*.</subsection>
  <subsection title="Joint Conditional Random Fields">ThereisthelimitationofapplyingCRFstothreesubproblems:IfweprocessTask1,Task2,andTask3separatelythenerrorsinprocessingnearlyalwayscascadethroughchain,causingerrorsinthefinaloutput.Totacklethislimitation,weintroducetheuseofJoint-CRFsofTask1,Task2,andTask3.OurJoint-CRFsmodelsisbasedontheDual-layerConditionalRandomFieldsdevelopedbyforsegmentationandtagger.Wecombinethreesubproblems:Task1,Task2,andTask3usingthejointprobabilitymodelwithJoint-CRFs.LetW=W_1,W_2,,W_ndenotetheobservedsentencewhereW_iisthei-thwordinthesentence,S=S_1,S_2,,S_kdenotesalabelofTask1whereS_iastartpositionword(S),orawordwhichisnotastartpositionword(*),E=E_1,E_2,,E_mdenotesalabelofTask2whereE_ianendpositionword(E),orawordwhichisnotanendpositionword(*),C=C_1,C_2,,C_mdenotealabelofaclausewhereC_ithenamedclauselabelsforTask3(wecanseetheexampleinSection5inmoredetail).Ourgoalistoidentifyastartwordofclause,anendwordofclauseandaboundarylabelofaclausethatmaximizethejointprobabilityP(S,E,C|W).Wecanformulatethejointproblemas:(S,E)^*,C^*&amp;=_S,E,CP(S,E,C|W)&amp;=_S,E,CP(C|(S,E),W)P(S,E|W)&amp;_S,E,CP(C|S_1,S_2,,S_n,E_1,E_2,,E_m)P(S,E|W)&amp;=_S,E,CP(C|Identify(S,E,W))P(S,E|W)&amp;_S,E,CP(C|Identify(S,E,W))P(S|W)P(E|W)alignwhere(S,E)^*andC^*isthemostlikely(boundarylabelatthestartword,boundarylabelattheendword)andaboundarylabelofaclause,respectively,Identify(S,E,W)=S_1,S_2,,S_n,_1,E_2,,E_misasetoftheresultofTask1andTask2.ApplyingBayes'stheorem,theabovejointprobabilityP(S,E,C|W)isfactorizedintotwoterms,P(C|(S,E),W)andP(S,E|W).ThefirsttermrepresentstheconditionalprobabilityofTask3,giventheresultofTask1andTask2(Identify(S,E,W)),thesecondtermrepresentstheconditionalprobabilityofTask1andTask2givenW.NoteP(S,E|W)P(S|W)P(E|W),assumingthatidentifyingastartwordofclause(S)andidentifyinganendofword(E)ofclauseisindependenttogether,inwhichP(S|W)andP(E|W)aretheconditionalprobabilityofTask1givenWandtheconditionalprobabilityofTask2givenW,respectively.Intraining,theprobabilityP(S,E,C|W)canberewritten(accordingtoformula2)as:whereF_1,F_2andF_3arethesumofCRFsfeaturesofTask1,Task2andTask3,respectivelyandZ_o(C),Z_o(S)andZ_o(E)arethenormalizingtermoftheprobabilityP(C|Identify(S,E,W)),P(S|W)andP(E|W),respectively.TheirpropertiesandfunctionsarethesameascommonCRFsdescribedin4.1.Wecanconsiderthelearningprocessintotwosteps:oneforlearningthefirstlayerofTask1(S)andTask2(E),andoneforlearningthesecondlayerofTask3.N-bestListApproximationforDecodingAdopting,wealsouseaN-bestlistapproximationmethod.WelimitourrerankingtargetstotheN-bestlist=S_1,E_1,S_2,E_2,,S_N,E_N,inwhich=S_1,E_1,S_2,E_2,,S_N,E_NisrankedbytheprobabilityP(S|W)andP(E|W).Therefore,maximumofthejointprobabilityP(S,E,C|W)canbedefinedapproximately:(S,E)^*,C^*&amp;=_S,E,CP(S,E,C|W)&amp;_(S,E),CP(S,E,C|W)&amp;_(S,E),CP(C|Identify(S,E,W))P(S|W)P(E|W)alignWeobtaintheN-bestlistofTask1(S)andTask2(E)andtheircorrespondingprobabilitiesP(S|W)andP(E|W)(S,E)byusingacombinationofforwardViterbiandbackwardA*search.GivenaparticularSandE,themostclauseboundariesanditsprobabilityP(C|identify(S,E,W))canbecalculatedbytheViterbialgorithminsection4.1.</subsection>
  <subsection title="Features">Thesetoffeaturesweuseisthesameasthatoffeaturesreportedin.Thesetoffeaturesincludesfeaturesatwordlevelandfeaturesatsentencelevel.FeaturesatwordlevelThefeaturesareusedwithawindowrepresentationofsize2.Forawindowcenteredatthethewordx_t,weusethefollowingfeaturesextractedfrom(x_t-2,x_t-1,x_t,x_t+1,x_t+2).Wherexcanbe:Wordform(w)andPOStag(p).Chunkingtag(c).Count:thenumberofaparticularlinguisticelementwhichappearinasentencefragment.Weconsidertwofragmentsofasentence,withseparatefeaturesforeach:fromthebeginningofthesentencetow_i(CountBegin),andfromw_itotheend(CountEnd).Thelinguisticelementsareenumeratedasfollows:Relativepronouns(e.g``that'',``where'',``who'',``which'',``whom'',``whose'')Punctuationmarks(.,;:)QuotesVerbphrasechunksRelativephrasechunksThefeaturetemplatesatwordlevelisdescribedinTable1.\noindentFeaturesatsentencelevelThesefeaturesareusedforcapturinglong-distancedependenciesandidentifyingtheclauseboundariesofaclausecandidate(s,e):Top-moststructure:Apatternrepresentingtherelevantelementsofthetop-moststructureformingthecandidatefromstoe.Thefollowingelementsareusedtoformthepattern:PunctuationmarksCoordinateconjunctions(e.g.,``and'',``or'')Theword``that''Relativepronouns(e.g.,``that'',``which'',``who'',``whom'',``whose'')VerbphrasechunksThetopclausewithinthe[x_s,,x_e]wherethepatternonlyconsidersthetop-moststructure.Wewillignoreaclausewhichappearsinthepattern.Forexample,thepatternfortheclause``((toraise)VPratesoncontainers(carryingU.S.exportstoAsia)Sabout10%)``isVP-%-S-%.Thenumberofclausesfoundinsidethecandidate[x_s,,x_e].</subsection>
  <subsection title="Decoder for Clause Splitting">Asmentioninsection1withthreeweakness,wedonotapplyCRFstoTask3directly.Inthissection,wewilldescribeanalgorithmfordecodingClauseSplittinginasegmentofasentencefromltor.ItisadynamicalgorithmpresentedinFigure1asarecursivefunction.WeuseresultsofTask1andTask2asinputofTask3.Arraymstart[]=[s_1,s_2,,s_h]andmend[]=[e_1,e_2,,e_m]storeresultsofTask1(s_iwherei1,,hisi-thstartwordpositionofaclause)andTask2(e_jwherej1,,misj-thendwordpositionofclause)respectivelyinasegment(0,n);mstart.size()=handmend.size()=marethenumberofmstartarrayelementsandmendarrayelementsrespectively.Set=s_1,s_2,,s_h,e_1,e_2,,e_m-1.BidimensionalarrayBestClause[l,r]storesclauseswithanoptimalsplitfoundin(l,r).Bidimensionalarrayscore[l,r]storesthescoreoftheclausecandidate(l,r).RecursivefunctionFindClauseinFigure1includes4parametersl,r,i,j,inwhichiandjareindexesofmstart[]andmend[]respectively.FindClause(l,r,i,j)findsanoptimalclausesplitforthesegment(l,r)andstoresitinBestClause[l,r].ThecalltothefunctionFindClause(0,n,0,mend.size())scansthewholesentenceandtheoptimalclausesplitforthesentenceisstoredinBestClause[0,n],inwhichnisthelengthofthesentence.Whatthealgorithmdoesisroughlyinterpretedasfollows:Letlbeastartpositionofacandidateclauseandrbeanendpositionofit.SuchpositionsarebasedonTask1andTask2.Thealgorithmpicksupallcandidateclausesandfindtheoptimalsplitpositionkbetweenlandr.Theoptimalsplitpositioniscalculatedusingthescorefunctiondescribedbelow.Allcandidateclauses(l,r)arecheckedusingtherecursivefunctionFindClause(l,r,i,j),whereirepresentsinformationaboutpossiblerightpositionstolandjpossibleleftpositiontor.Thealgorithmstartsfromthelongestsegment(aninputsentence)(0,n),andnarrowdownthesegmentusingtheparameteriandj.IntheFigure1,beginningfromline4toline9ofthefunctionusestworecursivecallsonthesentencesegmenttoenumerateallclausecandidates(s_i,e_j)(s_imstart[],e_jmend[])ofsegment(l,r).Line10ofthefunctionfindstheoptimalsplitk^*forthecurrentsentencesegment.Theline11willassigntheunionoftwodisjointsplitsBestClause[l,k^*]andBestClause[k^*+1,r]whichcoversthesegment(l,r)toBestClause[l,r]of(l,r)segment.Theline12and13treatthecasethataclause(l,r)isaddedtoBestClause[l,r].Asentencerequiresafunctioncallforeachclausecandidateandthereisaquadraticnumberofclausecandidatesoveranumberofstartwordsandendwordsinthesentence.Thefunctionconsumesalineartimeforselectingtheoptimalsplitplusthecostofthescoringfunction.Consequently,computationtimeofidentifyingaclausesplitinasentenceisO(n^2(n+cost(score)))wherenisanumberofstartwordsandendwordsinasentence.Becausenissosmall,computationtimeofCSisconsumedessentiallybycomputationtimeofViterbialgorithmcalculatingcost(score).\noindentScoringItisessentialthatweidentifythescoreofacandidateclause.WeusetheViterbialgorithminthedecodingprocessforTask3.DenoteasasetofboundarylabelsofclausesintheoutputswhichViterbialgorithmproducestopredictlabelsofclausesin(l,r)segment.Thescoreofacandidateclause(l,r)isdefinedasfollows:inwhich_T(s_k)isthatof(6).Wecansmooththescore(l,r)ofacandidateclause(l,r)usingsomelinguisticelementsofclausecandidate(l,r):verbphrasechunks:n_1Punctuationmarks:n_2Coordinateconjunctions(e.g``or'',``and''):n_3pronouns(e.g``that'',``which'',``whose'',``who'',``whom''):n_4Finally,wedefinescore(l,r)below:inwhichcount(n_i)isthenumberofn_iinclausecandidate(l,r).</subsection>
  <section title="Experiments">WeconductedtheexperimentsandevaluatedtheresultswithourCRFsframework.WeusedthePennTreebankwhichisusedintheCoNLL2001sharedtaskasdatafortrainingandtestingtheclausesplitting.WSJsectionsfrom15to18wereusedastrainingdata(8,936sentences),section20asdevelopmentdata(2,012sentences),andsection21astestdata(1,671sentences).ThedataoftheCoNLL2001sharedtaskincludessentenceswithwords,theclausesplitsolutions,POSlabelsandchunkslabels.Thedatafilescontainfourcolumnseparatedbyablankspace.Eachtoken(awordorapunctuationmark)isputonaseparatelineandthereisanemptylineaftereachsentence.Thefirstitemoneachlineisatoken,thesecondisthepart-of-speechtagofthetoken,thethirdisachunkingtagofthetoken,andthefourthisthenamedclauselabel.ForTask1,thelabelofeachtokendefineswhetherthetokenisnotastartpositionwordofaclause(*),orastartpositionwordofaclause(S).ForTask2,thelabelofeachtokendefineswhetherthetokenisnotanendpositionwordofaclause(*),oranendpositionwordofaclause(E).ForTask3,thelabelofeachtokendefineswhetherthetokenisnotaboundarylabelofaclause(*),oraboundarylabelofaclause(S*,*S),*S)S)S),.Theclauselabelsoftheexampleinsection3isdescribedinTable2.Inoursystem,weusedCRF++(V0.44)toimplementtheCRFsframework.WeevaluatedclausesplittingbasedonthestandardmeasureswhicharewidelyusedinInformationRetrieve:precision(p)-theproportionofcorrectlyrecognizedclausesinoutput,recall(r)-theproportionofcorrectlyrecognizedclausesincorrectclausesandtheirharmonicmeanF_1.Let.bethenumberofelementsinaset.Thecomputationoftheevaluationinatestsetincludingkelements(x^i,y^i)_1^kcanbeformulatedbelow:[p=_i=1^k|y^iR(x^i)|_i=1^k|R(x^i)|r=_i=1^k|y^iR(x^i)|_i=1^k|y^i|F_1=2prp+r]whereR(x^i)isasetofclausesthatareidentifiedforasentencex^i.ForTask1andTask2,weusedtheframeworkCRFswiththesetoffeaturesinsectionasunigramfeaturetemplates.WealsousedsomeconstraintsforViterbialgorithmintheformula(6)asfollows:Startpositionofaclausemustbetheboundaryofachunk.Endpositionofaclausemustbetheboundaryofachunk.WecombinedoutputsofTask1andTask2withchunkingtagrespectivelytoenrichthedependenceofitslinguisticinformation.ThiscombiningisdescribedinTable3.TheresultsofTask1andTask2areshowninTable4.WeexperimentedonTask1andTask2withasetoffeaturesasbigramfeaturetemplates.TheresultsofTask1andTask2areshowninTable5.TheyshowthatF1valueofTask1forthetestsetimproves0.61%andF1valueofTask2forthetestsetimproves0.94%.WealsoexperimentedTask1,Task2andTask3usingJoint-CRFswithasetoffeaturesasbigramfeaturetemplates.WechoseN=10forusingintheN-bestlist.OurresultsareshownonTable5.TheJoint-CRFsmethodshows0.22%and0.21%improvementinF1ofTask1andTask2,respectively.</section>
  <subsection title="Using CRFs and Joint-CRFs to predict score">WeusedCRFsandJoint-CRFs(jointwithTask1,andTask2)withthebigramfeaturetemplatesforTask3presentedinSection.Thenweusedtheformula(6)forViterbialgorithmtocountscore(l,r)ofclausecandidate(l,r)segmentusingthescorefunction(13).TheresultofTask3(identifyingtheclauses)isshowninTable6.TheF1performanceofTask3usingJoint-CRFsimprovesby0.31%comparedwiththatofTask3usingCRFs.</subsection>
  <subsection title="Combining linguistic information">WeimprovedF1valueofTask3byusinglinguisticinformationforsmoothingthescorefunctionpresentedinSectionandthescorefunctionisdefinedastheformula(14).TheresultisshowedinTable7.TheF1performancesare84.09%and84.66%,whichareimprovedby1.25%and1.51%comparedwiththecaseofusingtheformula(13),respectively.TheTable7alsoshowsthattheF1performanceusingJoint-CRFsoutperforms0.57%higherthanusingCRFs.Table8showsacomparisonofourmethodswiththepreviousworksonthesametrainingandtestingdata.Theresultsalsoshowthatourmethodiscomparabletothatof,whichisthestate-of-theartresult,andoutperformedothermethods.WeseethattheresultofoutperformsslightlyourresultbecausetheycombinethethreetasksofCStogetherwithendtoendwhilewecombinethethreetaskwithanintermediaryrole.Withtheerror-drivenmethod,theyfeedbackerrorsontrainingprocess.However,ourmethodshowsprecisionimprovesthatofothermethods.ThisisveryusefulwhenweapplyCSforotherapplicationssuchasmachinetranslationbecausetheclausesneedtobeidentifiedcorrectly.Wecarriedoutstatisticalsignificancetestsusingthet-test.Pairwiset-testshowedthattheprecisionofourresultsissignificantlybetterthanthatoftheresultofunderthesignificantlevel4.9810^-6(p-value).</subsection>
  <subsection title="Evaluating decoding speed">Table9showstheaveragedecodingtimeofatestsetandthatperasentencebytwomethodsourmethodandCarrerasetal.05onIntel(R)Xeon(TM)CPU3.06GHz,4GRAMmachinewithFedoraCore5.TheresultshowsthatthecomputationtimeofoursystemimprovesthatofCarrerasetal.05,bytheapproximatefactorof50,thoughtheexperimentofourmethodwasimplementedinC++,andthatofCarrerasetal.05inPerl.</subsection>
  <subsection title="Relation between performance of Task 3 and results of Task 1 and Task 2">InordertotesthowtheresultsofTask1andTask2effectontheperformanceofTask3,weconductedanexperimentbyperformingTask3usingthegoldstandarddataofTask1andTask2.Table10showsthatF_1valuesoftheTask3are84.28%and85.99%withaddinglinguisticinformation,respectively.WeseethattheresultsofusingthegoldstandarddataofTask1andTask2(85.99%)couldimproveourresults(84.09%).ThisexplainsthattheperformanceofTask1andTask2areimportanttotheresultofsplittingclause.However,themainerrorsoftheTask3arethemiss-correspondingofthestartingpoint(theresultofTask1)andtheendingpoint(theresultofTask2).Theseerrorsarecausedbytheinappropriatescoresindecodingalgorithm.Ourfutureworkisfocusedonhowtofindabetterscoringmethodforthedecodingalgorithm.</subsection>
  <section title="Conclusion">Inthispaper,wehavepresentedtheCRFs-basedframeworkapproachforclausesplitting.Wehaveproposedanewbottom-updynamicalgorithmfordecodingandsomeeffectivelinguisticinformationforclausesplitting.WecomparedtheresultsofexploitingourframeworktothepreviousworksintheCONLL2001sharedtask.Theexperimentsshowthatourresultiscompetitivewiththestate-of-the-artresultsofclausesplitting.studywassupportedbyJapanAdvancedInstituteofScienceandTechnology,the21^stCenturyCOEProgram:``VerifiableandEvolvablee-Society''.document</section>
</root>
