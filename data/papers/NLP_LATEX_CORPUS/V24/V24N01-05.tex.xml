<?xml version="1.0" ?>
<root>
  <jtitle>自動生成した学習データを用いた文書分類器に基づくFAQ検索システム</jtitle>
  <jauthor>牧野拓哉野呂智哉岩倉友哉</jauthor>
  <jabstract>本論文では，ユーザからの自然文による問い合わせを対応するFrequentlyAskedQuestion(FAQ)に分類する文書分類器を用いたFAQ検索手法を提案する．本文書分類器は，問い合わせ中の単語を手掛かりに，対応するFAQを判別する．しかし，FAQの多くは冗長性がないため，FAQを学習データとして文書分類器を作成する方法では，ユーザからの多様な問い合わせに対応するのが難しい．そこで，この問題に対処するために，蓄積されたユーザからの問い合わせ履歴から学習データを自動生成し，文書分類器を作成する．さらに，FAQおよび文書分類用に自動生成した学習データを用いて，通常使われる表層的な手がかりに加えて，本文書分類器の出力を考慮するランキングモデルを学習する．ある企業のコールセンターの4,738件のFAQおよび問い合わせ履歴54万件を用いて本手法を評価した．その結果，提案手法が，pseudo-relevancefeedbackおよび，統計的機械翻訳のアライメント手法を用いて得られる語彙知識によるクエリ拡張手法と比較し，高いランキング性能を示した．</jabstract>
  <jkeywords>FAQ検索，文書分類，ランキング学習</jkeywords>
  <section title="はじめに">製品やサービスを提供する多くの企業は顧客の問い合わせに対応するために，コールセンターを運営している．コールセンターでは，オペレータが電話やメールによる顧客問い合わせに対応する際や，顧客自身が答えを探す際の支援のために，FrequentlyAskedQuestion(FAQ)の整備および，FAQ検索システムを導入していることが多い．FAQ検索の利用者は，自然文や単語の集合を検索クエリとして，検索を実施するのが一般的である．しかし，FAQは過去の問い合わせ履歴の中から，同様の質問をまとめ，それらを代表するような抽象的な表現で作成されることが多いため，類義語や同義語，表記の揺れといった問題により，正しく検索できない場合がある．たとえば，以下の例のように入力の問い合わせと対応するFAQで語彙が一致しないことがある．	問い合わせ：○○カードの再度発行をしたい．今から出張だが、カードが見当たらない．どうしたらよいか．	正解のFAQの質問部分：○○カードを紛失・盗難・破損した場合の手続き方法	不正解のFAQの質問部分：○○カードを新規発行する方法この例では，正解のFAQへの語彙の一致は「○○カード」のみである．一方，不正解のFAQには，「○○カード」に加え，「発行」も一致するため，不正解のFAQが上位にランクされてしまう．このような問題に対して，たとえば，Yahoo!知恵袋などのコミュニティ型質問応答サイトにおける類似質問検索では，統計的機械翻訳で用いられるアライメントモデルを適用する方法が提案されている．また，Web検索においては，ユーザのクエリに対して得られた検索結果の上位の文書集合を適合文書とみなしてクエリを拡張するpseudo-relevancefeedbackといった手法も用いられている．しかし，アライメントモデルが学習しているのは，単語と単語の対応確率であり，FAQを特定するために有効な語を学習しているとは言えない．また，Webやコミュニティ型質問応答サイトなど複数の適合文書が得られる可能性がある場合に用いられるpseudo-relevancefeedbackは，適合するFAQが複数存在することがWeb検索ほど期待できないFAQ検索では十分な効果が得られない可能性がある．本論文では，問い合わせを対応するFAQに分類する文書分類器を利用したFAQ検索システムを提案する．本システムでは，機械学習を基に各FAQに関連のある単語を学習することで，問い合わせ中の単語が検索対象のFAQに一致していなくてもFAQを精度良く検索することを目指す．しかし，FAQだけを文書分類器のための学習データとして用いる場合は，FAQに出現する単語だけの判別しかできないという問題が残る．そこで，文書分類器を学習するために，コールセンターにて蓄積されている顧客からの問い合わせとオペレータの対応内容である問い合わせ履歴から自動生成した学習データを用いる．問い合わせ履歴には，問い合わせに対するオペレータの対応内容は記入されているものの，明示的にどのFAQが対応するという情報は付与されていない場合がある．そのため，本論文では，Jeonらの「似た意味の質問には似た回答がされる」という仮定に基づき，FAQの回答部分と問い合わせ履歴の対応内容の表層的類似度を計算し，閾値以上となった対応内容と対になっている問い合わせをそのFAQに対応するものとみなして学習データとする方法を用いる．さらに，本論文では，文書分類器の判別結果に加え，問い合わせと検索対象のコサイン類似度といった多くの手法で用いられている特徴を考慮するために，教師有り学習に基づくランキングモデルの適用を提案する．素性には，問い合わせとFAQの単語ベクトル間のコサイン類似度などに加えて，文書分類器が出力するスコアを用いる．ある企業のコールセンターのFAQおよび問い合わせ履歴を用いて提案手法を評価をした．提案手法は，pseudo-relevancefeedbackおよび統計的機械翻訳のアライメント手法を用いて得られる語彙知識によるクエリ拡張手法と比較して，高いランキング性能を示した．</section>
  <section title="関連研究">類似質問を検索する方法として，機械翻訳で用いられる単語単位のアライメントモデルであるIBMModelを用いた手法が提案されている．IBMModelは単語の対応確率をEMアルゴリズムで推定する手法である．統計的機械翻訳では，アライメントモデルは，原言語と，目的言語の文の対からなる対訳コーパスを用いて，単語間の対応確率を推定するために用いられる．類似質問検索においては，質問とその回答の対を対訳コーパスとみなしたり，あるいは類似する回答を持つこの方法では，FAQと問い合わせ間の単語の対応確率を学習する．しかしながら，単語間の対応確率は，対応するFAQを検索するために有効な語彙知識であるとは言えない．例えば，入力の「方法」とFAQの「方法」が対訳コーパス中で良く共起して出現し，学習の結果，対応確率が高くなったとする．この対応確率を利用してFAQをスコアリングすると，「方法」が出現する誤ったFAQが上位になりうる．CaoらはYahoo!Answersのカテゴリ情報を考慮して，回答済みの質問を検索する手法を提案した．Yahoo!Answersの質問にはユーザによってカテゴリが付与されているため，この手法はカテゴリが付与された質問を学習データとして，事前に入力の質問をカテゴリに分類するための分類器を作成する．実際に検索する際には，まず入力の質問が検索対象の質問に付与されているカテゴリに所属する確率を分類器を使って計算する．入力の質問と検索対象の質問との間の単語の一致や，単語の対応確率に対して，カテゴリの確率を重みとして与え，検索対象の質問に対するスコアを計算する．文書分類器を用いて検索するという観点で本論文と類似する研究であるが，本論文で扱う問い合わせ履歴の問い合わせには事前にカテゴリが付与されていないこと，本論文ではFAQを直接カテゴリとみなしていることが異なる．Singh，ZhouらはWikipediaを外部知識として利用して，コミュニティ質問応答サイトの類似質問検索性能を上げる手法を提案した．たとえばFAQ検索においては，業務ルールなどのドメイン固有の知識を含むため，一般的な知識源だけでは十分ではない．</section>
  <section title="提案手法">提案手法の学習時の処理を図に示す．提案手法の学習は大きく3つの処理からなる．まず，既存の方法を用いてFAQと問い合わせ履歴を用いて学習データを自動生成する（節）．続いて，問い合わせを対応するFAQに分類するための文書分類器を学習する（節）．最後に，学習データと，分類器の出力を素性に加えて，問い合わせに対して，正解のFAQが不正解のFAQよりもスコアが高くなるようにランキングモデルを学習する（節）．文書分類器の出力するスコアは問い合わせとFAQの単語の厳密一致や単語の関連度に依存せずに出力することができる．ランキング学習を適用することで，文書分類器から得られるスコアを，問い合わせとFAQの単語ベクトルのコサイン類似度などの素性とともに用いてFAQ検索結果のランキングをおこなうことが可能となる．</section>
  <subsection title="学習データの自動生成">先行研究に従い，FAQと問い合わせのペアについてお互いの回答部分の類似度をもとに自動で学習データを生成する．この手法は，似た意味の質問には似た回答がされるという仮説に基づき，回答間の表層的な類似度が閾値以上の回答済み質問文の対を収集した．この仮説はコールセンターではより有効であると考えられる．なぜならば，オペレータは問い合わせに対して対応する際に，対応するFAQを検索し，その回答部分を引用することが少なくないためである．学習データの生成には，FAQの質問Qおよび回答A，問い合わせ履歴中の問い合わせIおよびオペレータの対応内容Rを用いる．図の例ではQ，A，I，Rはそれぞれ「○○カードを紛失」，「ヘルプデスクへご連絡ください」，「○○カードを失くしたかも」，「ヘルプデスクへご連絡ください」を形態素解析して得た名詞，動詞，形容詞の集合である．学習データの自動生成には，質問Qと回答Aの対からなるFAQの集合=(Q_1,A_1),,(Q_||,A_||)および，問い合わせIとオペレータの対応内容Rの対からなる問い合わせ履歴の集合=(I_1,R_1),...,(I_||,R_||)を用いる．具体的には全文検索を使って，オペレータの対応内容，FAQの回答の内容語でお互いにOR検索し，式()によってスコアhrank(A_i,R_j)を計算する．rank_A_iは問い合わせ履歴の回答R_jを入力としてFAQの回答A_1,...,A_||を検索した場合のA_iの順位，rank_R_jはFAQの回答A_iを入力として問い合わせ履歴の回答R_1,...,R_||を検索した場合のR_jの順位である．hrank(A_i,R_j)があらかじめ人手で設定した閾値を超えたFAQと問い合わせのペアの集合D=(Q_i,A_i),I_j|1i||,1j||を生成する．例えば，問い合わせ履歴の回答R_jでFAQの回答を検索して，FAQの回答A_iの順位が2位でFAQの回答A_iで問い合わせ履歴の回答を検索して，問い合わせ履歴の回答R_jが1位だった場合，hrankは0.75となる．学習データの自動生成手順をAlgorithmに示す．hrankを計算するために，事前に問い合わせ履歴の回答を入力としたときのFAQの回答の順位の逆数をM_1N^|F||H|に，FAQの回答を入力としたときの問い合わせ履歴の回答の順位の逆数をM_2N^|H||F|に格納する．順位のリストranksを得るために，GetRanksの第一引数を入力（AもしくはR），第二引数を順位を付与する対象の文書集合（もしくは）として実行する．順位を付与するために，全文検索エンジンを使う．もし検索時に該当の文書が得られない場合，その文書の順位は，入力が問い合わせ履歴の回答であれば||，FAQの回答であれば||とする．[t]algorithm</subsection>
  <subsection title="文書分類器の学習">文書分類器の学習には，節で生成した学習データDを用いて，FAQごとに正例と負例を作成して二値分類器を学習する．対象のFAQと対応する問い合わせの集合を正例，その他のFAQと対応する問い合わせの集合をすべて負例として学習データとする．対応する問い合わせを持たないFAQも存在するため，対象のFAQそのものも正例に追加している．例えば，「○○カードを紛失・盗難・破損した場合の手続き方法」というFAQの分類器を学習するときには，正例に「○○カードの再発行をしたい．今から出張だが、カードが見当たらない．どうしたらよいか．」という問い合わせがあった場合，「○○カード」，「再発行」，「見当らない」といった素性の重みを正の方向に大きく更新する．学習にはAdaptiveRegularizationofWeightsLearningを用いた．素性には，内容語（名詞，動詞，形容詞），係り受け関係にある名詞と動詞の対を用いる．名詞句は同一の文節中に連続して出現する接頭詞と名詞とした．また，少なくとも片方が内容語であるような単語bigramの出現も素性として用いる．</subsection>
  <subsection title="ペアワイズランキング学習">ペアワイズランキング学習では，節で生成した学習データDを用いて，問い合わせに対して，正解のFAQが，不正解のFAQよりもスコアが高くなるように重みベクトルを更新する．ランキングの重みの学習アルゴリズムにはSOLAR-IIを用いた．SOLAR-IIはペアワイズランキングのオンライン学習手法であり，AdaptiveRegularizationofWeightsLearningのように，素性の重みwR^dに対して共分散行列R^ddを保持する．重みの更新時に，分散の値が小さい素性ほど，学習の信頼度が高いとみなして，重みの更新幅を小さくする．[b]algorithmランキングの重みベクトルの更新手順をAlgorithmに示す．最初に重みwを0，共分散行列を単位行列Eとして初期化する．問い合わせに対する正解のFAQおよびランダムに選択した不正解のFAQから抽出した素性ベクトルx_pR^dおよびx_nR^dをExtractFeatureVectorによって取得し，2つのベクトルの差xをもとに重みを更新する．はハイパーパラメータであり，値が大きいほど重みの更新幅を小さくする．本論文では1.0とした．はヒンジ損失であり，正解のFAQのスコアが不正解のFAQのスコアよりも低い値となったときに0以上の値を取る．は事例に含まれる素性の分散が小さい，つまり信頼度が高いほど大きな値を取る．そのため，信頼度が高い素性を多く含む事例に対して順位の予測を誤った場合には重みの更新幅や信頼度の更新幅を減らし，学習が過敏になり過ぎないようにする役割を持つ．学習を高速化するために，負例の生成にランダムサンプリングを適用した．ランダムサンプリングによるランキング学習でも，ペアワイズランキング学習で良い性能を出しているRankingSVMと同等の性能であることが示されている．負例の数Kは300とした．ExtractFeatureVectorでは基本的な素性のグループBasefeaturesおよび自動生成した学習データを用いた素性tfidf_FAQ+queryおよびfaq-scorerを抽出する．Basefeaturescos-q,cos-a:cos-qは，問い合わせとFAQの質問に対する内容語（名詞，動詞，形容詞）のコサイン類似度．cos-aは，問い合わせとFAQの回答に対する内容語のコサイン類似度．これらの値は，問い合わせに出現する単語をより含み，出現する単語の異なり数が少ないFAQほど1に近い値を取り，そうでないほど0に近い値を取る．dep:係り受け関係にある文節に出現する名詞，名詞句，動詞の対の一致回数．np:FAQの質問と問い合わせに対して，出現する名詞句が一致する割合．tfidf\_FAQ+query:FAQの質問Q，回答AおよびD中のそのFAQに対して生成されたL個の学習データI'_l_l=1^Lを用いて計算するtfidfに基づくスコアscore(Q,A,I'_l_l=1^L,I)=_Q,A(tfidf_sim(Q,I),tfidf_sim(A,I))+_I'_ltfidf_sim(I,I'_l)．付録の式()を用いて計算した．入力の問い合わせに対して質問もしくは回答と一致している単語が多いほど高く，さらに学習データの問い合わせ集合の中で一致している単語が多いものが存在するFAQに対して高いスコアとなる．	faq-scorer:問い合わせに対して，該当するFAQの二値分類器のマージンを計算し，sigmoid関数によって[0,1]へ変換した値を素性に用いる．		この分類器は過去の問い合わせ履歴を使って，どのような表現が出現する問い合わせならばこのFAQが正解らしいかどうかを学習したものである．		そのため，この素性は問い合わせに対してこのFAQが正解らしいほどスコアが1に近く，そうでないほど0に近い値を取る．学習した重みベクトルwを使って未知の問い合わせIに対してFAQをランキングするときには，各FAQから抽出した素性ベクトルxとwの内積を計算して，その値をもとにFAQをソートする．</subsection>
  <section title="実験">本実験では，文書分類器の出力を用いたランキングの有効性を確認するために，ある企業のFAQおよび問い合わせ履歴を用いて，既存手法との比較をおこなう．また，自動生成した学習データを分析し，ランキングの評価値への影響を調べる．</section>
  <subsection title="実験設定">実験にはある企業のFAQおよび問い合わせ履歴を用いた．問い合わせ履歴は個人情報を含むため，人名や個人を特定しうる数字列や地名，所属などの情報をパターンマッチによって秘匿化している．そのため，本来は個人情報ではない文字列も秘匿化されていることがある．今回の実験で用いたFAQの数は4,738件で，問い合わせ履歴はおよそ54万件である．評価のために，問い合わせ履歴中の286件に対し，3人のアノテータで正解のFAQを付与したデータを作成した．アノテータには，問い合わせに対してもっとも対応するFAQを1つ付与するよう依頼した．評価データ中に付与されたFAQの異なり数は186件となった．正解を付与した問い合わせ履歴の286件のうち，86件は開発用のデータ，残りは評価データに用いた．開発用データは，学習データ自動生成の閾値を決定するために用いた．本実験では，閾値を0から1まで0.1刻みで変えて，MRRが最も高くなる0.4とした．評価データは，各手法の精度評価に用いる．回答が短いFAQは，誤った問い合わせが多くペアになりうるため，文字数が10文字以下のFAQに対しては学習データの自動生成候補から除外した．また，学習データの生成後，学習データの中から評価データに含まれる問い合わせを削除した．形態素解析器，係り受け解析器にはそれぞれ，MeCab，CaboChaを用いた．システム辞書にはmecab-ipadic-NEologdを用いた．ユーザ辞書には秘匿化で用いたタグを追加し，秘匿化した際に用いたタグが分割されないようにしている．評価尺度にはランキングの評価で用いられるMRR(MeanReciprocalRank)，Precision@N(P@N)を用いた．MRRは式()で表され，正解の順位r_iの逆数に対して平均を取った値であり，正解のFAQを1位に出力できるほど1に近い値を取り，そうでないほど0に近い値を取る．P@Nは式()で表され，正解がN位以上になる割合である．正解がN位以上に出力している問い合わせが多いほど1に近い値を取り，そうでないほど0に近い値を取る．MRR=1N_i=1^N1r_i[1ex]P@N=正解がN位以上の評価データの数評価データの数gather</subsection>
  <subsection title="比較手法">tf-idf法に基づく全文検索2種類，pseudo-relevancefeedbackおよび翻訳モデルを用いる手法と比較する．全文検索にはElasticsearchを用いた．索引語は形態素解析器のkuromojiによって形態素解析をおこない，品詞で指定された条件と一致しない形態素の原形とした．また，全角半角は統一し，アルファベットはすべて小文字化した．</subsection>
  <subsection title="実験結果"/>
  <subsubsection title="自動生成した学習データの分析">Algorithmの自動生成手法により，学習データDのサイズは38,420件となった．3,185件のFAQに対して学習データを生成し，学習データが生成されていないFAQも含めて平均すると1件のFAQにつき8.03件の問い合わせを生成した．問い合わせと対応するFAQの対は自動生成するため，学習データDの問い合わせとFAQが正しく対応しているとは限らない．自動生成した学習データDの中からランダムに50件の事例を抽出し，問い合わせとFAQの対応が正しいかそうでないかを人手で評価した結果を表に示す．おおよそ半分のデータは正解のFAQと正しい対応になっており，残りの半分は不正解のFAQと対応する．FAQの回答が短い場合には，類似する回答がされる問い合わせが多くなることがあるのと，回答の内容は同じであるが，FAQの質問と対応する問い合わせの内容が意味的に一致しないような事例がみられた．提案手法の文書分類器の性能はFAQに対して学習データとして生成できた問い合わせの数が影響すると想定される．FAQごとに正例として生成できた問い合わせ数を調べた．図は評価データおよび開発データに含まれるFAQに対して生成された問い合わせ数を表すヒストグラムである．区間の幅を10とした．評価データに含まれるFAQのうち，1件も正例となる学習データが生成できなかったFAQは7件となった．</subsubsection>
  <subsubsection title="ランキングの評価">比較手法と提案手法の実験結果を表に示す．faq-scorerは，節で作成した文書分類器の出力に応じてFAQをランキングした場合の結果である．Basefeatures&amp;tfidf_FAQ+queryおよびBasefeatures&amp;tfidf_FAQ+query&amp;faq-scorerは節で挙げた素性を用いて学習したランキングモデルである．faq-scorerは比較手法よりも高いP@1となった．一方で他の評価値はtfidf_FAQ+queryを下回った．Basefeaturesおよびtfidf_FAQ+queryに加えてfaq-scorerを素性としてランキングモデルを学習することでどの評価値も他の比較手法より高くなったことから，文書分類器を素性として加えることで，精度改善に貢献することがわかる．</subsubsection>
  <subsubsection title="結果分析">正例として生成された問い合わせ数が文書分類器に及ぼす影響を調べるため，評価データに含まれるFAQを正例として生成された問い合わせ数でまとめてfaq-scorerおよびtfidf_FAQ+queryのMRRを算出した結果を図に示す．プロットする学習データの数は25までとした．評価データの数が多くないためばらつきがみられるが，生成される学習データの数が多いFAQほど，文書分類器は正しく分類できる傾向にあることを確認できる．一方で学習データが少ないFAQに対してはtfidf_FAQ+queryよりも誤りが多い．提案手法に対する学習データの影響を調べるため，学習データの数を変えて実験した．Basefeatures+tfidf_FAQ+queryのMRRの学習曲線を図に示す．MRRの学習曲線をプロットするために，学習データとしてFAQと問い合わせの対応を1,000件ずつ増やして文書分類器およびランキングモデルを学習している．提案手法は学習データの量に応じてMRRが向上している．表が示すようなある程度ノイズを含むような学習データであっても，量を増やすことでランキングの性能向上に貢献していることがわかる．文書分類器は問い合わせに出現する単語などを素性として分類するため，問い合わせのトピックが複数存在するような場合に誤りやすいと考えられる．ただし，トピックを明示的に与えることが難しい．そこで問い合わせの単語数が長いほど複数のトピックが出現しやすいという仮定のもと，問い合わせの単語数に応じてMRRの評価値を算出する．評価データを単語数で10刻みで分割し，単語数が1から100までの問い合わせ集合に対して，Basefeatures&amp;faq-scorerとtfidf_FAQについてMRRを評価した．結果を表に示す．単語数が1から10個の問い合わせは，単語数が10個以上の問い合わせに比べてMRRが高くなる傾向にあるが分かる．このことから，単語数が多い問い合わせに対しては質問のトピックを認識するような技術が必要であるが，これは今後の課題とする．最後に，学習結果の内容の詳細を確認するため，「○○カードを紛失・盗難・破損した場合の手続き」というFAQについて，文書分類器の学習結果の内容を調べた．大きい重みのついた素性から順に眺め，人手で選択した素性を表に示す．自動生成した学習データを用いることで，「磁気不良」「おとした」等のこのFAQの質問や回答には出現しない表現であるが，判別に寄与する語彙を学習していることがわかる．</subsubsection>
  <section title="おわりに">本論文では，FAQおよび問い合わせ履歴が持つ特徴を利用して自動生成した学習データを用いて，問い合わせを対応するFAQへ分類する文書分類器を学習し，その文書分類器の出力をランキング学習の素性として用いる手法を提案した．ある企業のFAQを用いた評価実験から，提案手法がFAQ検索の性能向上に貢献することを確認した．今後は，学習データの自動生成方法をより改善すること，さらに，より検索対象が多い場合でも同様の結果が得られるか検証したい．</section>
  <section title="Elasticsearchで利用したtfidfスコアの計算式">本論文でElasticsearchを利用したFAQのスコア計算は2種類ある．1つはFAQの質問と回答を利用する場合で，もう1つはFAQの質問と回答およびD中のそのFAQに対して生成された学習データを利用する場合である．FAQが質問Q，回答Aとフィールドを分けて索引付けしている場合，問い合わせIを入力としたときのFAQのスコアは次のように計算する．この計算では入力の問い合わせに対して，質問もしくは回答と一致している単語が多いほど高いスコアとなる．FAQが質問Q，回答Aに加えて，D中でそのFAQに対して生成された学習データI'_l_l=1^Lと3つのフィールドで索引付けしている場合，問い合わせIに対するFAQのスコアを次のように計算する．この計算では入力の問い合わせに対して，質問もしくは回答と一致している単語が多いほど高く，さらに学習データの問い合わせ集合の中で一致している単語が多いものが存在するFAQに対して高いスコアとなる．質問，回答，学習データなどの検索対象をT，Tの索引語の数を|T|，Tの索引語と一致する問い合わせ中の名詞，動詞，形容詞からなる単語集合をi_s_s=1^Sとすると，以下のようにtfidfに基づく類似度を次のように設定した．ただし，coord&amp;=S|I|,fieldNorm&amp;=1|T|,align*とする．coordはTがより多くの単語を含んでいる検索クエリを含んでいるほど高い値を取る．fieldNormはTの索引語の数|T|が大きいほど小さい値を取る．document</section>
</root>
