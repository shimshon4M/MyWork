    \documentclass[japanese]{jnlp_1.4}
\usepackage{jnlpbbl_1.3}
\usepackage[dvipdfm]{graphicx}
\usepackage{amsmath}
\usepackage{amssymb} 
\usepackage{hangcaption_jnlp}

\usepackage{algorithm}
\usepackage{algpseudocode}
\newcommand{\FAQSet}{}
\newcommand{\HistorySet}{}



\Volume{24}
\Number{1}
\Month{February}
\Year{2017}

\received{2016}{5}{20}
\revised{2016}{8}{5}
\accepted{2016}{9}{30}

\setcounter{page}{117}

\jtitle{自動生成した学習データを用いた文書分類器に基づく \\ FAQ 検索システム}
\jauthor{牧野　拓哉\affiref{Author_1} \and 野呂　智哉\affiref{Author_1} \and 岩倉　友哉\affiref{Author_1}}

\jabstract{
本論文では，ユーザからの自然文による問い合わせを対応するFrequently Asked Question (FAQ) に分類する文書分類器を用いたFAQ検索手法を提案する．
本文書分類器は，問い合わせ中の単語を手掛かりに，対応するFAQを判別する．しかし，FAQの多くは冗長性がないため，FAQを学習データとして文書分類器を作成する方法では，ユーザからの多様な問い合わせに対応するのが難しい．そこで，この問題に対処するために，蓄積されたユーザからの問い合わせ履歴から学習データを自動生成し，文書分類器を作成する．さらに，FAQおよび文書分類用に自動生成した学習データを用いて，通常使われる表層的な手がかりに加えて，
本文書分類器の出力を考慮するランキングモデルを学習する．ある企業のコールセンターの4,738件のFAQおよび問い合わせ履歴54万件を用いて本手法を評価した．
その結果，提案手法が，pseudo-relevance feedbackおよび，統計的機械翻訳のアライメント手法を用いて得られる
語彙知識によるクエリ拡張手法と比較し，高いランキング性能を示した．
}
\jkeywords{FAQ検索，文書分類，ランキング学習}

\etitle{An FAQ Search Method Using a Document Classifier Trained With Automatically Generated Training Data}

\eauthor{Takuya Makino\affiref{Author_1} \and Tomoya Noro\affiref{Author_1} \and Tomoya Iwakura\affiref{Author_1}}

\eabstract{
  We propose an Frequently Asked Question (FAQ) search method that uses a document classifier for classifying a natural language query to a corresponding FAQ. The document classifier classifies a query with words that occur in the query. However, since FAQs have little redundancy, using FAQs as training data for the document classifier is not sufficient for classifying queries that have the similar meaning but different surface expressions. To tackle this problem, our method generates training data automatically from FAQs and corresponding histories and trains the document classifier with them. Furthermore, with the automatically generated training data, our method learns a ranking model that uses classification results of the document classifier. Experimental results on a company FAQs and corresponding histories showed that our method outperformed pseudo-relevance feedback and query expansion model that uses word alignment model in statistical machine translation.
}
\ekeywords{FAQ Retrieval, Document Classification, Learning to Rank}

\headauthor{牧野，野呂，岩倉}
\headtitle{自動生成した学習データを用いた文書分類器に基づく FAQ 検索システム}

\affilabel{Author_1}{株式会社富士通研究所}{FUJITSU LABORATORIES LTD.}



\begin{document}
\maketitle


\section{はじめに}

製品やサービスを提供する多くの企業は顧客の問い合わせに対応するために，
コールセンターを運営している．

コールセンターでは，オペレータが電話やメールによる顧客問い合わせに対応する際や，
顧客自身が答えを探す際の支援のために，Frequently Asked Question (FAQ) の整備および，FAQ検索システムを導入していることが多い．

FAQ検索の利用者は，自然文や単語の集合を検索クエリとして，検索を実施するのが一般的である．
しかし，FAQは過去の問い合わせ履歴の中から，同様の質問をまとめ，
それらを代表するような抽象的な表現で作成されることが多いため，
類義語や同義語，表記の揺れといった問題により，正しく検索できない場合がある．
たとえば，以下の例のように入力の問い合わせと対応するFAQで語彙が一致しないことがある．
\begin{itemize}
	\item 問い合わせ：○○カードの再度発行をしたい．今から出張だが、カードが見当たらない．どうしたらよいか．
	\item 正解のFAQの質問部分：○○カードを紛失・盗難・破損した場合の手続き方法
	\item 不正解のFAQの質問部分：○○カードを新規発行する方法
\end{itemize}
\noindent
この例では，正解のFAQへの語彙の一致は「○○カード」のみである．
一方，不正解のFAQには，「○○カード」に加え，「発行」も一致するため，不正解のFAQが上位にランクされてしまう．

このような問題に対して，たとえば，Yahoo!知恵袋などのコミュニティ型質問応答サイトにおける類似質問検索では，
統計的機械翻訳で用いられるアライメントモデルを適用する方法が提案されている \cite{riezler:07,soricut:04,xue:08}．
また，Web検索においては，ユーザのクエリに対して得られた検索結果の上位の文書集合を適合文書とみなしてクエリを拡張する
pseudo-relevance feedbackといった手法も用いられている．

  しかし，アライメントモデルが学習しているのは，単語と単語の対応確率であり，FAQを特定するために有効な語を学習しているとは言えない．
また，Webやコミュニティ型質問応答サイトなど複数の適合文書が得られる可能性がある場合に用いられる
pseudo-relevance feedbackは，適合するFAQが複数存在することがWeb検索ほど期待できないFAQ検索では
十分な効果が得られない可能性がある．

本論文では，問い合わせを対応するFAQに分類する文書分類器を利用したFAQ検索システムを提案する．
本システムでは，
機械学習を基に各FAQに関連のある単語を学習することで，
問い合わせ中の単語が検索対象のFAQに一致していなくても
FAQを精度良く検索することを目指す．

しかし，FAQだけを文書分類器のための学習データとして用いる場合は
，FAQに出現する単語だけの判別しかできないという問題が残る．そこで，文書分類器を学習するために，
コールセンターにて蓄積されている顧客からの問い合わせとオペレータの対応内容である
問い合わせ履歴から自動生成した学習データを用いる．
問い合わせ履歴には，
問い合わせに対するオペレータの対応内容は記入されているものの，明示的にどのFAQが対応するという情報は付与されていない場合がある．
そのため，
本論文では，
Jeonらの\cite{jeon:05}
「似た意味の質問には似た回答がされる」という仮定に基づき，
FAQの回答部分と問い合わせ履歴の対応内容の表層的類似度を計算し，
閾値以上となった対応内容と対になっている問い合わせをそのFAQに対応するものとみなして学習データとする方法を用いる．

さらに，本論文では，  文書分類器の判別結果に加え，問い合わせと検索対象のコサイン類似度といった
多くの手法で用いられている特徴を考慮するために，教師有り学習に基づくランキングモデルの適用を提案する．
素性には，問い合わせとFAQの単語ベクトル間のコサイン類似度などに加えて，文書分類器が出力するスコアを用いる．

ある企業のコールセンターのFAQおよび問い合わせ履歴を用いて提案手法を評価をした．
提案手法は，pseudo-relevance feedbackおよび統計的機械翻訳のアライメント手法を用いて得られる語彙知識によるクエリ拡張手法と比較して，高いランキング性能を示した．


\section{関連研究}

類似質問を検索する方法として，機械翻訳で用いられる単語単位のアライメントモデルであるIBM Model \cite{brown:93} を用いた
手法が提案されている\cite{jeon:05,riezler:07,soricut:04,xue:08}．
IBM Modelは単語の対応確率をEMアルゴリズムで推定する手法である．
統計的機械翻訳では，アライメントモデルは，原言語と，目的言語の文の対からなる対訳コーパスを用いて，
単語間の対応確率を推定するために用いられる．
類似質問検索においては，質問とその回答の対を対訳コーパスとみなしたり，あるいは類似する回答を持つ
この方法では，FAQと問い合わせ間の単語の対応確率を学習する．
しかしながら，単語間の対応確率は，対応するFAQを検索するために有効な語彙知識であるとは言えない．
例えば，入力の「方法」とFAQの「方法」が対訳コーパス中で良く共起して出現し，学習の結果，対応確率が高くなったとする．
この対応確率を利用してFAQをスコアリングすると，「方法」が出現する誤ったFAQが上位になりうる．

Caoら \cite{cao:10,cao:09}はYahoo! Answersのカテゴリ情報を考慮して，回答済みの質問を検索する手法を提案した．
Yahoo! Answersの質問にはユーザによってカテゴリが付与されているため，
この手法はカテゴリが付与された質問を学習データとして，事前に入力の質問をカテゴリに分類するための分類器を作成する．
実際に検索する際には，まず入力の質問が検索対象の質問に付与されているカテゴリに所属する確率を分類器を使って計算する．
入力の質問と検索対象の質問との間の単語の一致や，単語の対応確率に対して，カテゴリの確率を重みとして与え，検索対象の質問に対するスコアを計算する．
文書分類器を用いて検索するという観点で本論文と類似する研究であるが，
本論文で扱う問い合わせ履歴の問い合わせには事前にカテゴリが付与されていないこと，
本論文ではFAQを直接カテゴリとみなしていることが異なる．


Singh\cite{singh:12}，Zhouら\cite{zhou:13}はWikipediaを外部知識として利用して，コミュニティ質問応答サイトの類似質問検索性能を上げる手法を提案した．たとえばFAQ検索においては，業務ルールなどのドメイン固有の知識を含むため，一般的な知識源だけでは十分ではない．

\begin{figure}[b]
\begin{center}
\includegraphics{24-1ia6f1.eps}
\end{center}
\caption{提案手法のモデルを学習する処理の概要と例}
\label{fig:flow}
\end{figure}


\section{提案手法}

提案手法の学習時の処理を図\ref{fig:flow}に示す．
提案手法の学習は大きく3つの処理からなる．
まず，既存の方法を用いてFAQと問い合わせ履歴を用いて学習データを自動生成する（\ref{sec:train-data}節）．
続いて，問い合わせを対応するFAQに分類するための文書分類器を学習する（\ref{sec:classifier}節）．
最後に，学習データと，分類器の出力を素性に加えて，問い合わせに対して，正解のFAQが不正解のFAQよりもスコアが高くなるようにランキングモデルを学習する（\ref{sec:learn2rank}節）．
文書分類器の出力するスコアは問い合わせとFAQの単語の厳密一致や単語の関連度に依存せずに出力することができる．
ランキング学習を適用することで，文書分類器から得られるスコアを，問い合わせとFAQの単語ベクトルのコサイン類似度などの素性とともに用いてFAQ検索結果のランキングをおこなうことが可能となる．


\subsection{学習データの自動生成}
\label{sec:train-data}

先行研究\cite{jeon:05}に従い，FAQと問い合わせのペアについてお互いの回答部分の類似度をもとに自動で学習データを生成する．

この手法は，似た意味の質問には似た回答がされるという仮説に基づき，
回答間の表層的な類似度が閾値以上の回答済み質問文の対を収集した．
この仮説はコールセンターではより有効であると考えられる．
なぜならば，オペレータは問い合わせに対して対応する際に，対応するFAQを
検索し，その回答部分を引用することが少なくないためである．

学習データの生成には，FAQの質問$Q$および回答$A$，問い合わせ履歴中の問い合わせ$I$およびオペレータの対応内容$R$を用いる．
  図\ref{fig:flow}
  の例では$Q$，$A$，$I$，$R$はそれぞれ「○○カードを紛失」，
  「ヘルプデスクへご連絡ください」，
  「○○カードを失くしたかも」，
  「ヘルプデスクへご連絡ください」を形態素解析して得た名詞，動詞，形容詞の集合である．

学習データの自動生成には，質問$Q$と回答$A$の対からなるFAQの集合$\FAQSet = \{(Q_1, A_1), \ldots, \linebreak
(Q_{|\FAQSet|}, A_{|\FAQSet|})\}$および，問い合わせ$I$とオペレータの対応内容$R$の対からなる問い合わせ履歴の集合$\HistorySet = \{(I_1, R_1), ..., (I_{|\HistorySet|}, R_{|\HistorySet|})\}$を用いる．

具体的には全文検索を使って，オペレータの対応内容，FAQの回答の内容語でお互いにOR検索し，式(\ref{eq:hrank})によってスコア$\textrm{hrank}(A_{i}, R_{j})$を計算する．
\begin{equation}
  \textrm{hrank}(A_{i}, R_{j}) = \frac{1}{2} \left(\frac{1}{rank_{A_{i}}} + \frac{1}{rank_{R_{j}}}\right)
	\label{eq:hrank}
\end{equation}
$\textrm{rank}_{A_{i}}$は問い合わせ履歴の回答$R_{j}$を入力としてFAQの回答$A_1, ..., A_{|\FAQSet|}$を検索した場合の$A_{i}$の順位，
$\textrm{rank}_{R_{j}}$はFAQの回答$A_{i}$を入力として問い合わせ履歴の回答$R_1, ..., R_{|\HistorySet|}$を検索した場合の$R_{j}$の順位である．
$\textrm{hrank}(A_{i}, R_{j})$があらかじめ人手で設定した閾値を超えたFAQと問い合わせのペアの集合
$D=\{\langle (Q_i, A_i), I_j\rangle|1 \leq i \leq |\FAQSet|, 1 \leq j \leq |\HistorySet|\}$を生成する．
例えば，問い合わせ履歴の回答$R_j$でFAQの回答を検索して，FAQの回答$A_i$の順位が2位で
FAQの回答$A_i$で問い合わせ履歴の回答を検索して，問い合わせ履歴の回答$R_j$が1位だった場合，
hrankは0.75となる．

学習データの自動生成手順をAlgorithm \ref{alg:gendata}に示す．
hrankを計算するために，事前に問い合わせ履歴の回答を入力としたときのFAQの回答の順位の逆数を$\mathbf{M}_1 \in \mathbb{N}^{|F|\times|H|}$に，FAQの回答を入力としたときの問い合わせ履歴の回答の順位の逆数を
$\mathbf{M}_2 \in \mathbb{N}^{|H|\times|F|}$に格納する．順位のリスト$\textrm{ranks}$を得るために，$\textrm{GetRanks}$の第一引数を入力（$A$もしくは$R$），第二引数を順位を付与する対象の文書集合（$\FAQSet$もしくは$\HistorySet$）として実行する．
順位を付与するために，全文検索エンジンを使う．もし検索時に該当の文書が得られない場合，その文書の順位は，入力が問い合わせ履歴の回答であれば$|\FAQSet|$，FAQの回答であれば$|\HistorySet|$とする．

\begin{algorithm}[t]
\caption{学習データの自動生成の疑似コード}
\label{alg:gendata}
\input{06algo01.txt}
\end{algorithm}


\subsection{文書分類器の学習}
\label{sec:classifier}

文書分類器の学習には，\ref{sec:train-data}節で生成した学習データ$D$を用いて，FAQごとに正例と負例を作成して二値分類器を学習する．
対象のFAQと対応する問い合わせの集合を正例，その他のFAQと対応する問い合わせの集合をすべて負例として
学習データとする．対応する問い合わせを持たないFAQも存在するため，対象のFAQそのものも正例に追加している．

例えば，「○○カードを紛失・盗難・破損した場合の手続き方法」というFAQの分類器を学習するときには，
正例に「○○カードの再発行をしたい．今から出張だが、カードが見当たらない．どうしたらよいか．」という
問い合わせがあった場合，「○○カード」，「再発行」，「見当らない」といった素性の重みを正の方向に大きく更新する．
学習にはAdaptive Regularization of Weights Learning \cite{koby:09}を用いた．

素性には，内容語（名詞，動詞，形容詞），係り受け関係にある名詞と動詞の対を用いる．
名詞句は同一の文節中に連続して出現する接頭詞と名詞とした．
また，少なくとも片方が内容語であるような単語bigramの出現も素性として用いる．


\subsection{ペアワイズランキング学習}
\label{sec:learn2rank}

ペアワイズランキング学習では，\ref{sec:train-data}節で生成した学習データ$D$を用いて，問い合わせに対して，
正解のFAQが，不正解のFAQよりもスコアが高くなるように重みベクトルを更新する．

ランキングの重みの学習アルゴリズムにはSOLAR-IIを用いた\cite{wang:15}．
SOLAR-IIはペアワイズランキングのオンライン学習手法であり，Adaptive Regularization of Weights Learning \cite{koby:09}のように，素性の重み$\mathbf{w} \in \mathbb{R}^d$に対して共分散行列$\Sigma \in \mathbb{R}^{d \times d}$を保持する．
重みの更新時に，分散の値が小さい素性ほど，学習の信頼度が高いとみなして，重みの更新幅を小さくする．

\begin{algorithm}[b]
\caption{ペアワイズランキング学習}
\label{alg:pairrank}
\input{06algo02.txt}
\end{algorithm}

ランキングの重みベクトルの更新手順をAlgorithm \ref{alg:pairrank}に示す．
最初に重み$\mathbf{w}$を$\mathbf{0}$，共分散行列$\Sigma$を単位行列$\mathbf{E}$として初期化する．
問い合わせに対する正解のFAQおよびランダムに選択した不正解のFAQから抽出した素性ベクトル$\textbf{x}_p \in \mathbb{R}^d$
および$\textbf{x}_n \in \mathbb{R}^d$を$\textrm{ExtractFeatureVector}$によって取得し，
2つのベクトルの差$\textbf{x}$をもとに重みを更新する．
$\gamma$はハイパーパラメータであり，値が大きいほど重みの更新幅を小さくする．本論文では$1.0$とした．
$\alpha$はヒンジ損失であり，正解のFAQのスコアが不正解のFAQのスコアよりも低い値となったときに$0$以上の値を取る．
$\beta$は事例に含まれる素性の分散が小さい，つまり信頼度が高いほど大きな値を取る．そのため，信頼度が高い素性を多く含む事例に対して順位の予測を
誤った場合には重みの更新幅や信頼度の更新幅を減らし，学習が過敏になり過ぎないようにする役割を持つ．

学習を高速化するために，負例の生成にランダムサンプリングを適用した．
ランダムサンプリングによるランキング学習でも，ペアワイズランキング学習で良い性能を出しているRanking SVM\cite{joachims:02}と同等の性能であることが示されている\cite{sculley:09}．
負例の数$K$は$300$とした．

$\textrm{ExtractFeatureVector}$では基本的な素性のグループ\textbf{Base features}および自動生成した学習データを用いた素性\textbf{tfidf\_FAQ+query}および\textbf{faq-scorer}を抽出する．

\begin{itemize}
  \item {\bf Base features}
  \begin{itemize}
    \item {\bf cos-q, cos-a}: cos-q は，問い合わせとFAQの質問に対する内容語
      （名詞，動詞，形容詞）のコサイン類似度．
      cos-aは，問い合わせとFAQの回答に対する内容語のコサイン類似度．
      これらの値は，問い合わせに出現する単語をより含み，
      出現する単語の異なり数が少ないFAQほど1に近い値を取り，
      そうでないほど0に近い値を取る．
    \item {\bf dep}: 係り受け関係にある文節に出現する名詞，名詞句，動詞の対の一致回数．
    \item {\bf np}: FAQの質問と問い合わせに対して，出現する名詞句が一致する割合．
  \end{itemize}
\item {\bf tfidf\_FAQ+query}: FAQの質問$Q$，回答$A$および$D$中のそのFAQに対して
  生成された$L$個の学習データ$\{I'_l\}_{l=1}^L$を用いて計算するtfidfに基づくスコア\\
  $\textrm{score}(Q, A, \{I'_l\}_{l=1}^L, I) = 
  \max_{Q,A}(\textrm{tfidf\_sim}(Q, I), \textrm{tfidf\_sim}(A, I)) +
  \max_{{I'}_{l}} \textrm{tfidf\_sim}(I, I'_l)$．
\\
  付録\ref{ap:es}の式(\ref{eq:es})を用いて計算した．
  入力の問い合わせに対して質問もしくは回答と一致している単語が多いほど高く，
  さらに学習データの問い合わせ集合の中で一致している単語が多いものが存在するFAQに
  対して高いスコアとなる．
	\item {\bf faq-scorer}: 問い合わせに対して，該当するFAQの二値分類器のマージンを
    計算し，sigmoid関数によって$[0, 1]$へ変換した値を素性に用いる．
		この分類器は過去の問い合わせ履歴を使って，どのような表現が出現する問い合わせならば
    このFAQが正解らしいかどうかを学習したものである．
		そのため，この素性は問い合わせに対してこのFAQが正解らしいほどスコアが1に近く，
    そうでないほど0に近い値を取る．
\end{itemize}

学習した重みベクトル$\mathbf{w}$を使って未知の問い合わせ$I$に対してFAQをランキングするときには，
各FAQから抽出した素性ベクトル$\mathbf{x}$と$\mathbf{w}$の内積を計算して，その値をもとにFAQをソートする．


\section{実験}

  本実験では，文書分類器の出力を用いたランキングの有効性を確認するために，
ある企業のFAQおよび問い合わせ履歴を用いて，既存手法との比較をおこなう．
また，自動生成した学習データを分析し，ランキングの評価値への影響を調べる．


\subsection{実験設定}

実験にはある企業のFAQおよび問い合わせ履歴を用いた．問い合わせ履歴は個人情報を含むため，
人名や個人を特定しうる数字列や地名，所属などの情報をパターンマッチによって秘匿化している．
そのため，本来は個人情報ではない文字列も秘匿化されていることがある．

今回の実験で用いたFAQの数は4,738件で，問い合わせ履歴はおよそ54万件である．
評価のために，問い合わせ履歴中の286件に対し，3人のアノテータで正解のFAQを付与したデータを作成した．
アノテータには，問い合わせに対してもっとも対応するFAQを1つ付与するよう依頼した．
評価データ中に付与されたFAQの異なり数は186件となった．

正解を付与した問い合わせ履歴の286件のうち，
86件は開発用のデータ，残りは評価データに用いた．
開発用データは，学習データ自動生成の閾値を決定するために用いた．
本実験では，閾値を0から1まで0.1刻みで変えて，MRRが最も高くなる$0.4$とした．
評価データは，各手法の精度評価に用いる．
回答が短いFAQは，誤った問い合わせが多くペアになりうるため，文字数が10文字以下のFAQに対しては学習データの自動生成候補から除外した． 
また，学習データの生成後，学習データの中から評価データに含まれる問い合わせを削除した．

形態素解析器，係り受け解析器にはそれぞれ，MeCab\footnote{MeCab: Yet Another Part-of-Speech and Morphological Analyzer $\langle$https://taku910.github.io/mecab/$\rangle$ （2016年5月20日アクセス）}，CaboCha\footnote{CaoboCha: Yet Another Japanese Dependency Structure Analyzer $\langle$https://taku910.github.io/cabocha/$\rangle$ （2016年5月20日アクセス）}を用いた．
システム辞書にはmecab-ipadic-NEologd \cite{sato:15} を用いた．
ユーザ辞書には秘匿化で用いたタグを追加し，秘匿化した際に用いたタグが分割されないようにしている．


評価尺度にはランキングの評価で用いられるMRR (Mean Reciprocal Rank) ，Precision@N (P@N) を用いた．
MRRは式(\ref{eq:mrr})で表され，正解の順位$r_i$の逆数に対して平均を取った値であり，正解のFAQを1位に出力できるほど1に近い値を取り，そうでないほど0に近い値を取る．
P@Nは式(\ref{eq:patn})で表され，正解が$N$位以上になる割合である．正解が$N$位以上に出力している問い合わせが多いほど1に近い値を取り，そうでないほど0に近い値を取る．
{\allowdisplaybreaks
\begin{gather}
  \textrm{MRR} = \frac{1}{N} \sum_{i=1}^{N} \frac{1}{r_i}
  \label{eq:mrr} \\[1ex]
  \textrm{P@N} = \frac{\textrm{正解がN位以上の評価データの数}}{\textrm{評価データの数}}
  \label{eq:patn}
\end{gather}
}


\subsection{比較手法}

tf-idf法に基づく全文検索2種類，pseudo-relevance feedbackおよび翻訳モデルを用いる手法と比較する．

全文検索にはElasticsearch\footnote{Elastic Revealing Insights from Data (Formerly Elasticsearch) $\langle$https://www.elastic.co/jp/$\rangle$ （2016年5月20日アクセス）}を用いた．
索引語は形態素解析器のkuromoji\footnote{elastic/elasticsearch-analysis-kuromoji: Japanese (kuromoji) Analysis Plugin $\langle$https://github.com/\linebreak[2]elastic/\linebreak[2]elasticsearch-analysis-kuromoji$\rangle$ （2016年5月20日アクセス）}によって形態素解析をおこない，品詞で指定された条件\footnote{$\langle$https://svn.apache.org/repos/asf/lucene/dev/branches/lucene3767/solr/example/solr/conf/lang/stoptags\_\linebreak[2]ja.txt$\rangle$ （2016年5月20日アクセス）}と一致しない形態素の原形とした．
また，全角半角は統一し，アルファベットはすべて小文字化した．


\begin{description}
  \item[tfidf\_FAQ] 検索対象はFAQの質問および回答としてtfidfにもとづく類似度を計算する．類似度は付録\ref{ap:es}の式(\ref{eq:es-base})を用いた．
  \item[tfidf\_FAQ+query] \ref{sec:learn2rank}節と同じ計算式を用いた．
  \item[pseudo-relevance feedback]
pseudo-relevance feedbackにはoffer weight \cite{robertson:94}を用いた．
offer weightは式(\ref{eq:ow})のようにして，単語$w$に対するスコアを計算する．
\begin{equation}
  score(w) = r \log\left(\frac{(r+0.5)(N-n-R+r+0.5)}{(n-r+0.5)(R-r+0.5)}\right),
	\label{eq:ow}
\end{equation}
  \item[翻訳モデル]
翻訳モデルに基づく検索には，
Jeonら \cite{jeon:05}の手法を用いる．この手法では入力の問い合わせ$I$を受け付け，式(\ref{eq:jeon-score})によって検索対象のFAQを，質問部分$Q$を用いてスコアリングする．
\begin{equation}
	P(Q|I) = \prod_{w \in Q} P(w|I)
	\label{eq:jeon-score}
\end{equation}
ただし$P(w|I)$は式(\ref{eq:jeon-pwd})のように計算する．
\begin{equation}
	P(w|I) = (1-\lambda) \sum_{t \in Q} (P_{tr}(w|t) P_{ml}(t|I)) + \lambda P_{ml}(w|C)
	\label{eq:jeon-pwd}
\end{equation}
式(\ref{eq:jeon-pwd})の$P_{tr}(w|t)$は，\ref{sec:train-data}節で生成した$D$におけるFAQの質問と問い合わせを対訳部分とみなして
GIZA++\footnote{GIZA++ $\langle$http://www.statmt.org/moses/giza/GIZA++.html$\rangle$ （2016年5月20日アクセス）}を使って学習した単語$w$と$t$の対応確率である．
$P_{ml}(t|I)$
は問い合わせ$I$における単語$t$の相対的な重要度である．本論文では文書頻度の逆数に対して対数を掛けた値を求め，問い合わせに出現する単語の値の総和で割った値とした．
$C$は$D$の問い合わせの集合とした．そのため$P_{ml}(w|C)$は単語$w$の一般的な重要度を表す．
Jeonらの設定に従い，$P_{tr}(w|w)=1$というヒューリスティクスを加えている．
$\lambda$は$0$から$1$まで$0.1$刻みで変えて実験をおこない，開発データのMRRが最も良くなる値を用いた．
\end{description}



\subsection{実験結果}

\subsubsection{自動生成した学習データの分析}

Algorithm \ref{alg:gendata}の自動生成手法により，学習データ$D$のサイズは38,420件となった．
3,185件のFAQに対して学習データを生成し，
学習データが生成されていないFAQも含めて平均すると1件のFAQにつき8.03件の問い合わせを
生成した．

問い合わせと対応するFAQの対は自動生成するため，学習データ$D$の問い合わせとFAQが
正しく対応しているとは限らない．
自動生成した学習データ$D$の中からランダムに50件の事例を抽出し，問い合わせとFAQの対応が正しいかそうでないかを人手で評価した結果を表
\ref{tab:human-eval-pair}
に示す．
おおよそ半分のデータは正解のFAQと正しい対応になっており，残りの半分は不正解のFAQと対応する．
FAQの回答が短い場合には，類似する回答がされる問い合わせが多くなることがあるのと，
回答の内容は同じであるが，FAQの質問と対応する問い合わせの内容が意味的に一致しないような事例がみられた．

\begin{table}[b]
\caption{人手によるFAQと問い合わせの対応の評価}
\label{tab:human-eval-pair}
\input{06table01.txt}
\end{table}

提案手法の文書分類器の性能はFAQに対して学習データとして生成できた問い合わせの数
が影響すると想定される．FAQごとに正例として生成できた問い合わせ数を調べた．
図\ref{fig:histogram-train}
は評価データおよび開発データに含まれるFAQに対して生成された問い合わせ数を表すヒストグラムである．
区間の幅を10とした．
評価データに含まれるFAQのうち，1件も正例となる学習データが生成できなかったFAQは
7件となった．

\begin{figure}[t]
\begin{center}
\includegraphics{24-1ia6f2.eps}
\end{center}
\hangcaption{正例として生成された問い合わせ数ごとのFAQの度数分布．x軸はひとつのFAQに対して正例として生成された問い合わせの数で，y軸はFAQの数を表す．}
\label{fig:histogram-train}
\end{figure}


\subsubsection{ランキングの評価}

比較手法と提案手法の実験結果を表\ref{tab:compare}に示す．
faq-scorerは，\ref{sec:classifier}節で作成した文書分類器の出力に応じてFAQをランキングした場合の
結果である．
  Base features\&tfidf\_FAQ+queryおよびBase features\&tfidf\_FAQ+query\&faq-scorerは
\ref{sec:learn2rank}節で挙げた素性を用いて学習したランキングモデルである．

\begin{table}[t]
\caption{評価結果}
\label{tab:compare}
\input{06table02.txt}
\vspace{4pt}\small
paired t-testをおこない，有意水準0.05でBase features\&tfidf\_FAQ+query\&faq-scorerと有意差がある比較手法の結果に$\dagger$を付与した．
\end{table}

faq-scorerは比較手法よりも高いP@1となった．一方で他の評価値はtfidf\_FAQ+queryを下回った．
Base featuresおよびtfidf\_FAQ+queryに加えてfaq-scorerを素性としてランキングモデルを学習することでどの評価値も他の比較手法より高くなったことから，文書分類器を素性として加えることで，精度改善に貢献することがわかる．


\subsubsection{結果分析}

正例として生成された問い合わせ数が文書分類器に及ぼす影響を調べるため，
評価データに含まれるFAQを正例として生成された問い合わせ数でまとめてfaq-scorerおよびtfidf\_FAQ+queryのMRRを算出した結果を
図\ref{fig:mrr-numtrain}に示す．
  プロットする学習データの数は25までとした．
  評価データの数が多くないためばらつきがみられるが，生成される学習データの数が多い
  FAQほど，文書分類器は正しく分類できる傾向にあることを確認できる．
  一方で学習データが少ないFAQに対してはtfidf\_FAQ+queryよりも誤りが多い．

\begin{figure}[b]
\begin{center}
\includegraphics{24-1ia6f3.eps}
\end{center}
\caption{評価データ中のFAQの集合を正例として生成された問い合わせ数でまとめて算出したMRR}
\label{fig:mrr-numtrain}
\end{figure}
\begin{figure}[b]
\begin{center}
\includegraphics{24-1ia6f4.eps}
\end{center}
\hangcaption{Base features+tfidf\_FAQ+queryのMRRの学習曲線．横軸は利用した学習データの件数を表し，縦軸はMRRを表す．}
\label{fig:mrr-lr}
\end{figure}


提案手法に対する学習データの影響を調べるため，学習データの数を変えて実験した．

Base features+tfidf\_FAQ+queryのMRRの学習曲線を図\ref{fig:mrr-lr}に示す．MRRの学習曲線をプロットするために，
学習データとしてFAQと問い合わせの対応を1,000件ずつ増やして文書分類器およびランキングモデルを学習している．
提案手法は学習データの量に応じてMRRが向上している．
表\ref{tab:human-eval-pair}
が示すような
ある程度ノイズを含むような学習データであっても，
量を増やすことでランキングの性能向上に貢献していることがわかる．

文書分類器は問い合わせに出現する単語などを素性として分類するため，
問い合わせのトピックが複数存在するような場合に誤りやすいと考えられる．
ただし，トピックを明示的に与えることが難しい．
そこで問い合わせの単語数が長いほど複数のトピックが出現しやすいという
仮定のもと，問い合わせの単語数に応じてMRRの評価値を算出する．

評価データを単語数で10刻みで分割し，単語数が1から100までの問い合わせ集合に対して，
Base features\&faq-scorerとtfidf\_FAQについてMRRを評価した．
結果を表\ref{tab:mrr_each_num_token}に示す．
単語数が1から10個の問い合わせは，単語数が10個以上の問い合わせに比べてMRRが高くなる
傾向にあるが分かる．
このことから，単語数が多い問い合わせに対しては質問のトピックを認識するような
技術が必要であるが，これは今後の課題とする．

\begin{table}[t]
\caption{全文検索とBase features+tfidf\_FAQ+queryの単語数ごとのMRR}
\label{tab:mrr_each_num_token}
\input{06table03.txt}
\end{table}

最後に，学習結果の内容の詳細を確認するため，
「○○カードを紛失・盗難・破損した場合の手続き」というFAQについて，
文書分類器の学習結果の内容を調べた．
大きい重みのついた素性から順に眺め，人手で選択した素性を表\ref{tab:feature}に示す．
自動生成した学習データを用いることで，
「磁気不良」「おとした」等のこのFAQの質問や回答には出現しない表現であるが，
判別に寄与する語彙を学習していることがわかる．

\begin{table}[t]
\caption{FAQ「○○カードを紛失・盗難・破損した場合の手続き」の学習結果の中で重みが大きい素性}
\label{tab:feature}
\input{06table04.txt}
\end{table}


\section{おわりに}

本論文では，FAQおよび問い合わせ履歴が持つ特徴を利用して自動生成した学習データを用いて，
問い合わせを対応するFAQへ分類する文書分類器を学習し，
その文書分類器の出力をランキング学習の素性として用いる手法を提案した．
ある企業のFAQを用いた評価実験から，提案手法がFAQ検索の性能向上に貢献することを確認した．
今後は，学習データの自動生成方法をより改善すること，
さらに，より検索対象が多い場合でも同様の結果が得られるか検証したい．




\bibliographystyle{jnlpbbl_1.5}
\begin{thebibliography}{}

\bibitem[\protect\BCAY{Brown, Pietra, Pietra, \BBA\ Mercer}{Brown
  et~al.}{1993}]{brown:93}
Brown, P.~F., Pietra, V. J.~D., Pietra, S. A.~D., \BBA\ Mercer, R.~L. \BBOP
  1993\BBCP.
\newblock \BBOQ The Mathematics of Statistical Machine Translation: Parameter
  Estimation.\BBCQ\
\newblock {\Bem Computational Linguistics}, {\Bbf 19}  (2), \mbox{\BPGS\
  263--311}.

\bibitem[\protect\BCAY{Cao, Cong, Cui, \BBA\ Jensen}{Cao et~al.}{2010}]{cao:10}
Cao, X., Cong, G., Cui, B., \BBA\ Jensen, C.~S. \BBOP 2010\BBCP.
\newblock \BBOQ A Generalized Framework of Exploring Category Information for
  Question Retrieval in Community Question Answer Archives.\BBCQ\
\newblock In {\Bem Proceedings of the 19th International Conference on World
  Wide Web}, WWW '10, \mbox{\BPGS\ 201--210}. ACM.

\bibitem[\protect\BCAY{Cao, Cong, Cui, Jensen, \BBA\ Zhang}{Cao
  et~al.}{2009}]{cao:09}
Cao, X., Cong, G., Cui, B., Jensen, C.~S., \BBA\ Zhang, C. \BBOP 2009\BBCP.
\newblock \BBOQ The Use of Categorization Information in Language Models for
  Question Retrieval.\BBCQ\
\newblock In {\Bem Proceedings of the 18th ACM Conference on Information and
  Knowledge Management}, CIKM '09, \mbox{\BPGS\ 265--274}. ACM.

\bibitem[\protect\BCAY{Crammer, Kulesza, \BBA\ Dredze}{Crammer
  et~al.}{2009}]{koby:09}
Crammer, K., Kulesza, A., \BBA\ Dredze, M. \BBOP 2009\BBCP.
\newblock \BBOQ Adaptive Regularization of Weight Vectors.\BBCQ\
\newblock In Bengio, Y., Schuurmans, D., Lafferty, J.~D., Williams, C. K.~I.,
  \BBA\ Culotta, A.\BEDS, {\Bem Advances in Neural Information Processing
  Systems 22}, \mbox{\BPGS\ 414--422}. Curran Associates, Inc.

\bibitem[\protect\BCAY{Jeon, Croft, \BBA\ Lee}{Jeon et~al.}{2005}]{jeon:05}
Jeon, J., Croft, W.~B., \BBA\ Lee, J.~H. \BBOP 2005\BBCP.
\newblock \BBOQ Finding Similar Questions in Large Question and Answer
  Archives.\BBCQ\
\newblock In {\Bem Proceedings of the 14th ACM International Conference on
  Information and Knowledge Management}, CIKM '05, \mbox{\BPGS\ 84--90}. ACM.

\bibitem[\protect\BCAY{Joachims}{Joachims}{2002}]{joachims:02}
Joachims, T. \BBOP 2002\BBCP.
\newblock \BBOQ Optimizing Search Engines Using Clickthrough Data.\BBCQ\
\newblock In {\Bem Proceedings of the 8th ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}, KDD '02, \mbox{\BPGS\ 133--142}.
  ACM.

\bibitem[\protect\BCAY{Riezler, Vasserman, Tsochantaridis, Mittal, \BBA\
  Liu}{Riezler et~al.}{2007}]{riezler:07}
Riezler, S., Vasserman, A., Tsochantaridis, I., Mittal, V., \BBA\ Liu, Y. \BBOP
  2007\BBCP.
\newblock \BBOQ Statistical Machine Translation for Query Expansion in Answer
  Retrieval.\BBCQ\
\newblock In {\Bem Proceedings of the 45th Annual Meeting of the Association of
  Computational Linguistics}, \mbox{\BPGS\ 464--471}. Association for
  Computational Linguistics.

\bibitem[\protect\BCAY{Robertson \BBA\ Jones}{Robertson \BBA\
  Jones}{1994}]{robertson:94}
Robertson, S.~E.\BBACOMMA\ \BBA\ Jones, K.~S. \BBOP 1994\BBCP.
\newblock \BBOQ Simple, Proven Approaches to Text Retrieval.\BBCQ\
\newblock \BTR\ Technical Report No. 356.

\bibitem[\protect\BCAY{Sato}{Sato}{2015}]{sato:15}
Sato, T. \BBOP 2015\BBCP.
\newblock \BBOQ Neologism Dictionary based on the Language Resources on the Web
  for MeCab.\BBCQ\ \texttt{https://github.com/neologd/mecab-ipadic-neologd}.

\bibitem[\protect\BCAY{Sculley}{Sculley}{2009}]{sculley:09}
Sculley, D. \BBOP 2009\BBCP.
\newblock \BBOQ Large Scale Learning to Rank.\BBCQ\
\newblock In {\Bem NIPS Workshop on Advances in Ranking}.

\bibitem[\protect\BCAY{Singh}{Singh}{2012}]{singh:12}
Singh, A. \BBOP 2012\BBCP.
\newblock \BBOQ Entity based Q\&A Retrieval.\BBCQ\
\newblock In {\Bem Proceedings of the 2012 Joint Conference on Empirical
  Methods in Natural Language Processing and Computational Natural Language
  Learning}, \mbox{\BPGS\ 1266--1277}. Association for Computational
  Linguistics.

\bibitem[\protect\BCAY{Soricut \BBA\ Brill}{Soricut \BBA\
  Brill}{2004}]{soricut:04}
Soricut, R.\BBACOMMA\ \BBA\ Brill, E. \BBOP 2004\BBCP.
\newblock \BBOQ Automatic Question Answering Using the Web: Beyond the
  Factoid.\BBCQ\
\newblock In Susan~Dumais, D.~M.\BBACOMMA\ \BBA\ Roukos, S.\BEDS, {\Bem
  HLT-NAACL 2004: Main Proceedings}, \mbox{\BPGS\ 57--64}. Association for
  Computational Linguistics.

\bibitem[\protect\BCAY{Wang, Wan, Zhang, \BBA\ Hoi}{Wang
  et~al.}{2015}]{wang:15}
Wang, J., Wan, J., Zhang, Y., \BBA\ Hoi, S. \BBOP 2015\BBCP.
\newblock \BBOQ SOLAR: Scalable Online Learning Algorithms for Ranking.\BBCQ\
\newblock In {\Bem Proceedings of the 53rd Annual Meeting of the Association
  for Computational Linguistics and the 7th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, \mbox{\BPGS\
  1692--1701}. Association for Computational Linguistics.

\bibitem[\protect\BCAY{Xue, Jeon, \BBA\ Croft}{Xue et~al.}{2008}]{xue:08}
Xue, X., Jeon, J., \BBA\ Croft, W.~B. \BBOP 2008\BBCP.
\newblock \BBOQ Retrieval Models for Question and Answer Archives.\BBCQ\
\newblock In {\Bem Proceedings of the 31st Annual International ACM SIGIR
  Conference on Research and Development in Information Retrieval}, SIGIR '08,
  \mbox{\BPGS\ 475--482}. ACM.

\bibitem[\protect\BCAY{Zhou, Liu, Liu, Zeng, \BBA\ Zhao}{Zhou
  et~al.}{2013}]{zhou:13}
Zhou, G., Liu, Y., Liu, F., Zeng, D., \BBA\ Zhao, J. \BBOP 2013\BBCP.
\newblock \BBOQ Improving Question Retrieval in Community Question Answering
  Using World Knowledge.\BBCQ\
\newblock In {\Bem Proceedings of the 23rd International Joint Conference on
  Artificial Intelligence}, IJCAI '13, \mbox{\BPGS\ 2239--2245}. AAAI Press.

\end{thebibliography}




\appendix

\section{Elasticsearchで利用したtfidfスコアの計算式}\label{ap:es}

本論文でElasticsearchを利用したFAQのスコア計算は2種類ある．1つはFAQの質問と回答を利用する場合で，もう1つはFAQの質問と回答および$D$中のそのFAQに対して生成された学習データを利用する場合である．

FAQが質問$Q$，回答$A$とフィールドを分けて索引付けしている場合，問い合わせ$I$を
入力としたときのFAQのスコアは次のように計算する．
\pagebreak
\begin{equation}
  \textrm{score}(Q, A, I) = \max_{Q,A}(\textrm{tfidf\_sim}(Q, I), \textrm{tfidf\_sim}(A, I)),
  \label{eq:es-base}
\end{equation}
この計算では入力の問い合わせに対して，質問もしくは回答と一致している単語が多いほど高い
スコアとなる．

FAQが質問$Q$，回答$A$に加えて，
$D$中でそのFAQに対して生成された学習データ$\{I'_l\}_{l=1}^L$
と3つのフィールドで索引付けしている場合，問い合わせ$I$に対するFAQのスコアを次のように計算する．
\begin{equation}
  \textrm{score}(Q, A, \{I'_l\}_{l=1}^L, I) = \max_{Q,A}(\textrm{tfidf\_sim}(Q, I), \textrm{tfidf\_sim}(A, I)) + \max_{{I'}_{l}} \textrm{tfidf\_sim}(I, I'_l),
  \label{eq:es}
\end{equation}
この計算では入力の問い合わせに対して，質問もしくは回答と一致している単語が多いほど高く，
さらに学習データの問い合わせ集合の中で一致している単語が多いものが存在するFAQに対して高いスコアとなる．

質問，回答，学習データなどの検索対象を$T$，$T$の索引語の数を$|T|$，
$T$の索引語と一致する問い合わせ中の名詞，動詞，形容詞からなる単語集合を
$\{i_s\}_{s=1}^{S}$とすると，以下のようにtfidfに基づく類似度を次のように設定した．
\begin{equation}
  \textrm{tfidf\_sim}(T, I) = \textrm{coord} \sum_{s=1}^S(\textrm{tf}(i_s) \cdot \textrm{idf}(i_s)^2 \cdot \textrm{fieldNorm}), \nonumber
\end{equation}
ただし，
\begin{align*}
  \textrm{coord} &= \frac{S}{|I|}, \\
  \textrm{fieldNorm} &= \frac{1}{|T|},
\end{align*}

とする．coordは$T$がより多くの単語を含んでいる検索クエリを含んでいるほど高い値を取る．
fieldNormは$T$の索引語の数$|T|$が大きいほど小さい値を取る．


\begin{biography}
\bioauthor{牧野　拓哉}{
  2012年東京工業大学総合理工学研究科物理情報システム専攻修士課程修了．同年，（株）富士通研究所入社．
  自然言語処理の研究開発に従事．
  言語処理学会会員．
}
\bioauthor{野呂　智哉}{
2002年東京工業大学大学院情報理工学研究科計算工学専攻修士課程修了．
2005年同専攻博士課程修了．
同専攻助手，助教を経て，2015年（株）富士通研究所入社．
現在に至る．博士（工学）．自然言語処理の研究開発に従事．
言語処理学会会員．
}
\bioauthor{岩倉　友哉}{
  2003年（株）富士通研究所入社．
  2011年東京工業大学大学院総合理工学研究科物理情報システム専攻博士課程修了．
  博士（工学）．現在，（株）富士通研究所主任研究員．自然言語処理の研究開発に従事．情報処理学会，言語処理学会会員．
}

\end{biography}


\biodate




\end{document}
