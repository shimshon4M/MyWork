<?xml version="1.0" ?>
<root>
  <subsection title="Classic n-grams with Linear History">TheclassicgenerativestoryforlanguagemodelsisasaMarkovprocess.Generationofasequenceofwordsisbasedonthenotionof`history'or`context',i.e.,theorderedlistofwordsalreadygenerated.Itwouldbedesirabletoconsidercompletehistories,howeverinpracticethisisnottractable.Tocountersparsityandcomputationalissues,aselectivehistorymustbeused.Theseideasinspirethedesignoftheclassicn-gramlanguagemodel.Weassumean(n-1)thorderMarkovpropertyandusealinearcontext,i.e.,modeltheprobabilityofanygivenwordasbeingconditionalonthepreviousn-1words.Theprobabilityofasentencew_1,...,w_mcanbewrittenas:P(w_1,...,w_m)&amp;=_i^mP(w_iw_1,...,w_i-1)&amp;_i^mP(w_iw_i-n+1,...,w_i-1).align*Inmanycases,thislinearhistorycanbehelpfulindeterminingthenextword.Forexample,theword`Francisco'ismorelikelytoappearafter`San'than`Los'.Butwhenitcomestomodelingotherissuesaffectingfluency,suchaswordagreement,wemustalsousenon-localcontext,ideallyatthesametimewithoutincreasingmodelsparsity.Thisistheprimarymotivationforourtree-basedlanguagemodel</subsection>
  <section title="Introduction">Sincetheearlydaysofword-basedtranslation,theresearchcommunityhasbeenmovingtowardsmoreandmoresyntacticapproachestotranslation.Classicn-gramlanguagemodelsareeffectiveatcapturingtranslationfluencyatthewordlevel,howeversuchapproachesoftenfailatthesyntacticandsemanticlevel.Inthisstudyweabstractthetraditionaldefinitionofaclassicn-gramtodependencytreesandshowhowourapproachisabletoimprovemorechallengingissuessuchaslong-distancewordagreement.Theprimarymotivationforusingstructuredlanguagemodelsisthatwecanreducethe`distance'betweenwordsthatareinterdependentonasyntactic(andoftensemantic)level.Whiletree-basedmodelshavemorecomplicatedstructurethantheirstring-basedcounterparts,thesparsityofthemostimportantinformationisreduced,givingamorecompactandrelevantrepresentationofcontext.Despiterecentadvancesinneuralandstructuredlanguagemodelingtechnology,themostwidespreadlanguagemodelingparadigminmajortranslationsystemsisstillclassicn-grammodeling.Classicn-grammodelsarecombinedwithavarietyofsmoothingmethods,themostpopularbeingmodifiedKneser-NeyandStupidBackoff,toformsimpleandrobustmodelsoflinearwordcontext.Thereexisthighlyoptimizedimplementations,suchasKenLM,makingn-grammodelspopularinmodernsystems.Therehavebeenanumberofissuesthathavepreventedthewidespreadadoptionoftree-basedlanguagemodels.Webelievethatthetwomainproblemsarethelackofanagreedstandardonthemosteffectivedefinitionoftree-basedcontext,andtherequirementforasyntax-baseddecoderforfullintegrationintoamachinetranslationsystem.Theuseofparsersaslanguagemodelshasshownlittleimprovementintranslationexperimentsandtherehavebeenpreviousattemptstousesyntax-basedlanguagemodelingthathavefailedtoshowanystatisticallysignificantincreaseinBLEU(seeerrataccb/publications/incremental-syntactic-language-models-for-phrase-based-translation-errata.pdf).Inthispaperweshowthatwecanstillachieveasignificantimprovementintranslationqualityjudgedbyhumanswithoutrequiringasyntax-baseddecoder.Inthispaperwefrequentlyrefertotheproblemoflong-distancewordagreement.Thisisatrickyissueforstring-basedmachinetranslation,whichdoesnotconsiderlong-distancedependenciesandthereforeissusceptibletoerrorssuchasincorrectnoun/verbagreements.Weshowthatourmodelismosteffectiveforlanguages,suchasthoseintheSlavicfamily,thataremorphologicallyrichandallowforfreewordorder,becausetheycontainmoreexamplesofnon-localdependenciesthateffectfluency.SeeFigure~foranexampleofsuchalong-distancewordagreementerror</section>
  <section title="Related Work">Theideaofcapturingstructureinalanguagemodelhasbeenaroundsincethelate90's,particularlyinthespeechrecognitioncommunity.Suchtree-based,or`structured'languagemodelshavemainlyconsideredonlylimitedwordhistories,suchasancestorsorspecificparent/siblingrelationships,howevermorerecentattemptshavestartedtodefinemoregeneralsyntacticwordhistories.Thebeginningsofsuchgeneralizedapproachescanbetracedbackto`arbori-context'trees,whicharedesignedtoselectoptimalpartialhistories.Othereffectivesyntacticapproachesinrecentyearshaveincludedabilinguallanguagemodelenhancedwithsomedependencyinformation,specificallythePOStagsofparent/grandparentandclosestleft/rightsiblings,andmodelingagenerativedependencystructureontopofaclassicn-gramlanguagemodel.Whilenotdirectlydesignedas`syntax-based'languagemodels,approachesbasedonneuralnetworkshavealsobeenshowntobeeffectiveatcapturingamoregeneralwordhistorythanclassicn-grammodels.Suchapproachesincludefeed-forwardandrecurrentneuralnetworklanguagemodelsandmorerecentlyLSTM-basedlanguagemodels.Themostrecentapproachatthetimeofwritingconsidersahybridapproachofsyntacticandneuralnetworkcomponents.Twomajordrawbacksofneuralnetworkbasedapproachesarethatitcanbedifficultto`reverseengineer'thesyntacticknowledgelearnedandthatmodeltrainingcanstrugglecomputationallyonlargedata.Ourapproachexpandsonexistingstudiesbyproposingamoregeneralizedframeworkforsyntacticcontextandanalyzingitseffectivenessforalargerangeoflanguagepairs.Weshowthatitisapplicableeventosystemswithouttarget-sidesyntax</section>
  <section title="Model Details"/>
  <subsubsection title="Task-specific shape selection">Previousdefinitionsoftree-basedhistorieshaveconsideredsubsetsofthepossiblet-treeletshapesthatwehavedefinedabove,suchasancestorchainsw_i,T(w_i),...,T^l-1(w_i)andrestrictedparent/siblingrelations.Ourdefinitionnotonlyexpandsuponthese,butalsoaddsflexibilityasweareabletoturneachshapetype`on'and`off'dependingontherequirementsofthetask.Anadditionalbenefitisthatitbecomespossibletocomparedirectlywithpreviousworkbysimplyselectingt-treeletshapes.Inparticular,wefoundthattherearetypesofdependencyrelationsthatmayormaynotaffectwordagreementdependingonlanguagesandparsingstandards.ThenaturallanguageexamplesshowninFigure~giveclassiccaseswherecertaint-treeletshapesareimportantfordeterminingwordmorphology.Thereequallyaretypesthatarenever(orveryrarely)usedincertainlanguages,andwefoundthatconsideringthesetypesattimescausedunnecessarynoise.Thisissimilartousingunnecessarilylongn-gramsizesinclassiclanguagemodels,wherethereisoftennotenoughgaininexpressivenesstowarranttheadditionalerrorscausedbysparsityandirregularsmoothing.SeeSection~forexperimentalresultsandamoredetailedanalysisoftherelativeperformanceofvariousspecialcasesofourmodel,includingcomparisonwithpreviouswork</subsubsection>
  <subsection title="Smoothing">Sinceweconductedourtranslationexperimentsonweb-scaledata,wedesignedourmodeltobeusedwithStupidBackoffsmoothing,whichhasbeenshowntoperformwellonthiskindofdata.ThemathematicalformulationofStupidBackoffsmoothingisshownbelow.Theformulabelowisappliedrecursivelyuntilaknownn-gramisfound,andunigramscoresaredefinedasSB(w_i)=c(w_i)/_wc(w),wherec(w)istheobservedfrequencyofwordwinthetrainingcorpus.Theparametercontrolsthedegreetowhichwepenalizebackingofftoshortern-grams.Theprimaryadvantageofthissmoothingmethodisthatitdoesnotrequirethecalculationorlookupofmodifiedt-treeletcounts.Thisallowsforfastandsimplecalculationofbackoffprobabilities,andworkswellwhenusingeacht-treelettypescoreasafeature.NotethatStupidBackoffsmoothingwasalsousedinsimilarworkthatweuseforcomparison.Whenbackingofftoshortert-treelets,notethatwecalculateprobabilitiesbasedontheshortershapetype,andthatthistypeisalwaysuniquebecauseofthe(in-orderdepth-first)orderingconstraint.Forexample(inFigure~),`ilestgrand'(type4)isbackedoffto`estgrand'(type2)non-ambiguously</subsection>
  <subsection title="Application to SMT: Filtering and Reranking">Inourexperiments(seeSection~)wemeasurethetranslationimprovementgainedbyrerankingk-bestmachinetranslationoutputusingourlanguagemodel.Rerankingisaveryflexibleandsimpleapproach.InparticularwedonotmakeanyassumptionsaboutthedecodingalgorithmoftheunderlyingMTsystemandweareabletouseastandardstring-to-stringsystembysimplyparsingtheoutput.Asmentionedintheintroduction,webelievethatamajorstumblingblockforsyntax-basedlanguagemodelinghasbeenthelackofapplicabilitytostring-basedMTsystems(whicharestillthemostcommon),andweshowthatforourmodelthisisnotanissue.Theobviousproblemofusingstringoutputisthatwecannotguaranteereliableparsing,particularlyof(poorlyformed)machinetranslationoutput.Weproposethesimpleapproachofusingafilteringheuristicbasedondependencytreeconsistencytoreducethisproblem.Weparseallk-bestcandidatesandextractthedependencytreeletsofsizelcenteredoneachwordthatdiffersbetweeneachcandidateandthe1-best(baseline)translation.Wethendiscardanyk-bestcandidatesthatcontainanysuchtreeletswithadifferentdependencystructuretothecorrespondingtreeletinthe1-besttranslation.Thissimpleheuristicwasveryeffectiveatreducingerrorscausedbybadtranslations/parses,assimplewordchanges(e.g.,changingthegenderofadefinitearticle)shouldnotaffecttheparsetree.Naturallythisfilteringleadstoasmallreductioninrecall</subsection>
  <section title="Experimental Setup">WeperformedaseriesofexperimentstomeasuretheimprovementintranslationqualityobtainablebyrerankingMToutputusingourproposedtree-basedlanguagemodel.Inparticularwewereinterestedinimprovingmorphologicalerrorssuchaswordagreement</section>
  <subsection title="Language Choice">Inourexperimentswebuiltandevaluatedmodelsforninemajorlanguages.Thisallowedustoanalyzeclearlythetypesofmorphologicalerrorthattheproposedmodelwasabletoimprove.Thelanguageswereselectedfromavarietyoflanguagefamiliesandalldisplaywordagreementtovariousdegrees.Thelanguageschosenwere:CzechandRussian[Slavic];Hungarian[Uralic];DutchandGerman[Germanic];French,PortugueseandSpanish[Romance];andHebrew[Semitic].ForconsistencyweusedEnglishasthesourcelanguageforalltranslationexperiments.SeeTable~foranoverviewofthecharacteristicsoftheselanguagesaffecting(long-distance)wordagreements.Alllanguagemodelsweretrainedonmixeddomainmonolingualwebcorporaof5--10billionuniquesentencesperlanguage</subsection>
  <subsection title="Automatic Evaluation Metrics">TranslationqualitywasmeasuredwithBLEUandthelanguagemodelwasintrinsicallyevaluatedusingamethodofevaluationwecall`win-rate'(seebelow).TheBLEUmetrichasthefollowingformulation:whereBPisabrevitypenalty,w_nareweightsandp_naren-gramprecisions.Ascanbeseenfromthedefinition,BLEUconsidersonlytheprecisionoflocaln-grams.BLEUhasbeenshowninthepasttobeineffectiveinevaluatingsyntax-basedapproaches,withimprovementsbeing`invisibleto[such]ann-grammetric'.WealsofoundthatBLEUwasunreliableatreflectingchangesintranslationqualityforlong-distancedependencies,andthatthesensitivitywaslowbecauseonlyasmallfractionofwordswerechangedbyusingtheproposedmodel(forexamplemanysentencesdonotcontainwordagreementerrors).Nonethelessitwaspracticaltousesuchanautomaticmeasurementforparametertuning.Asanotherpointofreference,wealsousedamethodofintrinsiclanguagemodelevaluationwecall`win-rate'.Foreachsentencewecalculatedthelanguagemodelscore(usingtheproposedmodel)ofthebaselineMTsystemoutputandthereferencetranslation.Thewinratewasthencalculatedasfollows,givingtheratioofnumberoftimesourmodelgivesahigherscoretothereferencetranslationthantothebaselineoutput.ThemodelcanbeconsideredusefulifitcansuccessfullygiveahigherscoretothereferencetranslationthanthebaselineMToutput.Whilewedonotclaimthatthismetricisstronglycorrelatedwithhumanjudgment,webelieveitgivesusefulinformationandisverysimpletoimplement.Theclassicmethodofintrinsiclanguagemodelevaluationisperplexity,howeverwechosenottousethismeasurebecauseitassumesnormalizedprobabilities,whichwecannotstrictlyguaranteewhenusingourmodelapproximationsandStupidBackoffsmoothing</subsection>
  <subsection title="Training and Lookup">Priortomodeltraining,wetokenizedtheentiretrainingcorpusandcollectedwordfrequencies.Tokenswithfrequencylessthanorequaltoacertainthreshold(inourcase1)werereplacedwithan`unknown'tokeninordertomodelt-treeletcountsduringlookupthatincludeout-of-vocabularytokens.Trainingwasconductedbyparsingtrainingsentencesandcountingallt-treeletsofsize3.Itwouldbepossibletouselongert-treeletshoweverwefoundthattherewerenotmanycaseswherelongercontextwasnecessaryfordeterminingcorrectwordagreement.Tosavememoryt-treeletscouldbeprunedbasedonfrequency,howeverwefoundthatthisnegativelyimpactedperformance(seeTable~).Parsingwasconductedwiththeshift-reducedependencyparserdescribedin</subsection>
  <subsection title="Reranking SMT Output">Inordertoevaluatetheeffectivenessoftheproposedmodelwetestedtheabilityofourlanguagemodeltorerankthe1,000-besttranslationoutputofastring-basedSMTsystem.Thebaselinetranslationsystemwasastate-of-the-artin-housephrase-basedtranslationsystemtrainedonlargewebdata.Thebaselineusedastandard5-gramlanguagemodeltrainedonthesamedataastheproposedtree-basedmodelandforcomparisonalsousedStupidBackoffsmoothing.The1,000-besttranslationcandidateswerefilteredusingthedependencytreeconsistencyheuristicdescribedinSection~.Wealsoremovednoisysentencesconsistingover50%non-alphanumericcharacters,andevaluatedonsentenceswithlengthbetween10and30words</subsection>
  <section title="Optimization of Model Parameters">Wefirstexploredtheeffectsofvaryingourmodelparameters,inparticulartheselectionoft-treeletshapes,comparingwithpreviouswork.Theexperimentswereconductedonadevelopmentdatasetconsistingofapproximately10,000sentencesperlanguagethatwereheldoutfromourbaselineandlanguagemodeltrainingdata.Forcomparisonwithpreviouswork,wefirstexperimentedwithsettingsenablingvarioussetsoft-treeletshapes(forsizel3).Thesetups`Ancestors'and`Siblings'weredesignedtocorrespondtothemodelsofandrespectively.Notethattherearesomeslightdifferences,inparticularthesmoothingalgorithmfor`Siblings'(Shenetal.didnotmentionanysmoothing)andthefactthatourmodelsaremoregeneralthanpreviouswork,differentiatingbetweenleftandrightchildren.Thefourmodelvariantstestedwereasfollows:Ancestors:ancestors,nosiblings(types:1--2,6--9)Siblings:siblings,noancestors(types:1--5)AllTypes:allt-treelettypes(types:1--9)Trigrams:allpure3-grams,siblingsandancestors(types:3--9</section>
  <subsection title="Results">Table~showstheresultsforthefoursystemvariants.Wecanseethatthemosteffectivesettingsweretouseallt-treeletsoralltrigrams,andthesemoregeneralsetupsperformedbetterthanthemorerestrictivesettingsbasedonpreviouswork.Wefoundthatusingonlytrigramsgavebetterresultsthanforallt-treeletsbecausethelowerordert-treeletsoftengavelessreliableinformation(i.e.,weneedlongercontext).Additionaltuningexperimentsshowedthatimprovementsweremadebyincreasingmodelsize(reducingt-treeletfilteringthresholdfrequencyffortraining,seeTable~).Anincreaseofonaverage0.1BLEUperlanguagewasobservedbyvaryingthebeamwidthfrom1to100,andweusedabeamwidthof100forallourevaluationresults.Wenotethattheparsingqualitywasroughlythesameforalllanguages.Fordetailedparserevaluation,see.Wealsofoundempiricallythatitwaseffectivetopenalizeunseent-treeletsmoreheavilythaninpreviousworkbychangingthebackoffparameterfromthestandard0.4to0.004(seeSection~formoredetails).Wedidnotconductafull-scaleexperimenttofindtheoptimumvalue</subsection>
  <section title="Final Evaluation and Error Analysis"/>
  <subsection title="Experimental Settings">Weconductedafullevaluationofourproposedapproachonninelanguagepairs.Forthefinalevaluationweusedthe`Trigrams'settingsthatwereshowninSection~tobethemosteffectiveoverall.Wedecidedtousethissettingforalllanguagepairs,sincewedidnotbelievethattheBLEUandwin-ratescoresgaveaclearenoughwinnerforeachindividuallanguagepair.Theexperimentswereconductedbytranslatingmixed-domainEnglishwebsentencesthatwereheldoutfromthebaselineSMTsystemandlanguagemodeltrainingdata.Aswewereinterestedinevaluatingthedifferencesbetweenthebaselineandproposedmodels,wetranslatedalargetestsetthenforevaluationrandomlyselected(onaverage)400sentencesperlanguagethathaddifferentoutputbetweenthebaselineandproposedsystems.ThechangeratesinTable~showthepercentagesofsentencesthatweretranslateddifferently</subsection>
  <subsection title="Human Evaluation">Translationqualitywasmeasuredbyskilledhumanratersinordertomaximizethereliabilityoftheevaluation.Theraterswerebilingualspeakersofeachlanguagepairbutnotprofessionaltranslators.Foreachsentencetheraterswereinstructedtogiveascorebetween0and6(inclusive),giventhesourcesentenceandtranslation,oneraterpersentence.TheratingguidelinesareshowninTable~.ThenumberofratersperlanguagepairareshowninTable~.Wecalculatedthefollowingscoresforeachlanguagepair:`mean-diff-score':Themeandifferencebetweenthesentence-levelhumanratingsoftheproposedandbaselinesystems.`mean-diff-sign':Themeandifferencebetweenthenumberofsentence-levelwinsandlosses(intermsofhumanratings)oftheproposedandbaselinesystems.`change-rate':Percentageofsentencesthatweredifferentbetweenthebaselineandproposedsystems.`baseline':Meansentence-levelhumanevaluationscoreforthebaselinesystem.`proposed':Meansentence-levelhumanevaluationscorefortheproposedsystem</subsection>
  <subsection title="Discussion">Theresultsofthehumanevaluationshowedanoticeabledifferencebetweentheeffectivenessofthetree-basedlanguagemodelfordifferentlanguages.Inparticular,themorphologicalcharacteristicsofeachlanguage(tosomeextentcapturedbythelanguagefamily)appeartoeffectgreatlytheutilityofastructuredlanguagemodelincomparisontothebaseline,whichusesastandardn-grammodel.Allninelanguagesrequirecorrectwordagreementforhighfluency,howeverthetypeandnatureoftheseagreementsvariesbetweenlanguage.Forexample,adjective-nounandnoun-verbagreementinRomancelanguagescanbeexpressedrelativelysimplywithann-grammodelasthewordsinquestionnormallyappeartogether,whichcouldexplainwhywesawlessimprovementforthislanguagefamily.Incontrast,casechoiceinSlaviclanguagesrequiresconsiderationofcomplicatedandlong-distancedependencies,whichisconsistentwiththelargeimprovementshownbyourproposedsystem.Similarobservationsthatsuchapproachesaremoreeffectiveforlanguageswithrelativelyfreewordorderhavebeenmadeinpreviouswork.Themagnitudeofimprovementperlanguagealsoshowedsomecorrelationwiththebaselinetranslationquality.Analysisofthescoresgivenbyhumanratersshowedatendencyforloweredsensitivitytowordendingswhenthetranslationqualitywashigh.Wefoundmanyexamplesofgenerallywell-translatedsentences(particularlyinPortugueseandSpanish)thatweregivenascoreof6(outof6)irrespectiveofwordendingerrors.Thismeansthattherewereanumberofimprovementsnotreflectedinourresults.Conversely,Russiansentenceswithaloweraveragescoretendedtogainorloseawholepointwhenwordendingswerechanged.Thiscouldalsobeduetothetypeoferrorsthemselves,asforexampleadjectiveagreementmistakesareunlikelytoimpedeunderstandingasmuchascaseerrors,andshowstheimportanceofaccuratelong-distanceagreement</subsection>
  <subsection title="Error Categorization">Table~givesanerrorcategorizationforarandomsampleoftenincorrectsentencesforeachoffivelanguages(Dutch,French,German,PortugueseandRussian).Thecategoriesaredefinedasfollows:OOV(26%):t-treeletnotseenintrainingdataMeaning(22%):Meaningoforiginalsentencechanged(e.g.,tense)Tricky(16%):Varioustrickycases(e.g.,incorrectchoiceofindicative/subjunctive,caseselectionrequiringsemanticinference)Parse(14%):Parseerrorcausedincorrectt-treeletlookupNoise(12%):Brokeninputsentence,incorrecthumanratingContext(10%):Modelusedinappropriatecontext(e.g.,tooshort</subsection>
</root>
