<?xml version="1.0" ?>
<root>
  <jtitle>リカレントニューラルネットワークによる単語アラインメント</jtitle>
  <jauthor>田村晃裕渡辺太郎隅田英一郎</jauthor>
  <jabstract>本論文では，隠れ層の再帰的な構造により，過去のアラインメント履歴全体を活用するリカレントニューラルネットワーク(RNN)による単語アラインメントモデルを提案する．ニューラルネットワークに基づくモデルでは，従来，教師あり学習が行われてきたが，本論文では，本モデルの学習法として，Dyerらの教師なし単語アラインメントを拡張して人工的に作成した負例を利用する教師なし学習法を提案する．提案モデルは，IBMモデルなどの多くの従来手法と同様に，各方向で独立にアラインメントを学習するため，両方向を考慮した大域的な学習を行うことができない．そこで，各方向のモデルの合意を取るように同時に学習することで，アラインメントの精度向上を目指す．具体的には，各方向のモデルのwordembeddingの差を表すペナルティ項を目的関数に導入し，両方向でwordembeddingを一致させるようにモデルを学習する．日英及び仏英単語アラインメント実験を通じて，RNNに基づくモデルは，フィードフォワードニューラルネットワークによるモデルやIBMモデル4よりも単語アラインメント精度が高いことを示す．さらに，日英及び中英翻訳実験を通じて，これらのベースラインと同等かそれ以上の翻訳精度を達成できることを示す．</jabstract>
  <jkeywords>単語アラインメント，リカレントニューラルネットワーク，教師なし学習，合意制約</jkeywords>
  <section title="はじめに">対訳文中の単語の対応関係を解析する単語アラインメントは，統計的機械翻訳に欠かせない重要な処理の一つであり，研究が盛んに行われている．その中で，生成モデルであるIBMモデル1-5やHMMに基づくモデルは最も有名な手法であり，それらを拡張した手法が数多く提案されている．近年では，Yangらが，フィードフォワードニューラルネットワーク(FFNN)の一種である「Context-DependentDeepNeuralNetworkforHMM(CD-DNN-HMM)」をHMMに基づくモデルに適用した手法を提案し，中英アラインメントタスクにおいてIBMモデル4やHMMに基づくモデルよりも高い精度を達成している．このFFNN-HMMアラインメントモデルは，単語アラインメントに単純マルコフ性を仮定したモデルであり，アラインメント履歴として，一つ前の単語アラインメント結果を考慮する．一方で，ニューラルネットワーク(NN)の一種にフィードバック結合を持つリカレントニューラルネットワーク(RNN)がある．RNNの隠れ層は再帰的な構造を持ち，自身の信号を次のステップの隠れ層へと伝達する．この再帰的な構造により，過去の入力データの情報を隠れ層で保持できるため，入力データに内在する長距離の依存関係を捉えることができる．このような特長を持つRNNに基づくモデルは，近年，多くのタスクで成果をあげており，FFNNに基づくモデルの性能を凌駕している．例えば，言語モデルや翻訳モデルの構築で効果を発揮している．一方で，単語アラインメントタスクにおいてRNNを活用したモデルは提案されていない．本論文では，単語アラインメントにおいて，過去のアラインメントの情報を保持して活用することは有効であると考え，RNNに基づく単語アラインメントモデルを提案する．前述の通り，従来のFFNNに基づくモデルは，直前のアラインメント履歴しか考慮しない．一方で，RNNに基づくモデルは，隠れ層の再帰的な構造としてアラインメントの情報を埋め込むことで，FFNNに基づくモデルよりも長い，文頭から直前の単語アラインメントの情報，つまり過去のアラインメント履歴全体を考慮できる．NNに基づくモデルの学習には，通常，教師データが必要である．しかし，単語単位の対応関係が付与された対訳文を大量に用意することは容易ではない．この状況に対して，Yangらは，従来の教師なし単語アラインメントモデル（IBMモデル，HMMに基づくモデル）により生成した単語アラインメントを疑似の正解データとして使い，モデルを学習した．しかし，この方法では，疑似正解データの作成段階で生み出された，誤った単語アラインメントが正しいアラインメントとして学習されてしまう可能性がある．これらの状況を踏まえて，本論文では，正解の単語アラインメントや疑似の正解データを用意せずにRNNに基づくモデルを学習する教師なし学習法を提案する．本学習法では，Dyerらの教師なし単語アラインメントを拡張し，正しい対訳文における単語対と語彙空間全体における単語対を識別するようにモデルを学習する．具体的には，まず，語彙空間全体からのサンプリングにより偽の対訳文を人工的に生成する．その後，正しい対訳文におけるアラインメントスコアの期待値が，偽の対訳文におけるアラインメントスコアの期待値より高くなるようにモデルを学習する．RNNに基づくモデルは，多くのアラインメントモデルと同様に，方向性（「原言語f目的言語e」又は「ef」）を持ち，各方向のモデルは独立に学習，使用される．ここで，学習される特徴は方向毎に異なり，それらは相補的であるとの考えに基づき，各方向の合意を取るようにモデルを学習することによりアラインメント精度が向上することが示されている(Matusov,Zens,andNey2004;Liang,Taskar,andKlein2006;Graca,Ganchev,andTaskar2008;Ganchev,Graca,andTaskar2008)．matusov04,liang06,graca08,gancev08そこで，提案手法においても，「fe」と「ef」の2つのRNNに基づくモデルの合意を取るようにそれらのモデルを同時に学習する．両方向の合意は，各方向のモデルのwordembeddingが一致するようにモデルを学習することで実現する．具体的には，各方向のwordembeddingの差を表すペナルティ項を目的関数に導入し，その目的関数にしたがってモデルを学習する．この制約により，それぞれのモデルの特定方向への過学習を防ぎ，双方で大域的な最適化が可能となる．提案手法の評価は，日英及び仏英単語アラインメント実験と日英及び中英翻訳実験で行う．評価実験を通じて，前記提案全てを含む「合意制約付き教師なし学習法で学習したRNNに基づくモデル」は，FFNNに基づくモデルやIBMモデル4よりも単語アラインメント精度が高いことを示す．また，機械翻訳実験を通じて，学習データ量が同じ場合には，FFNNに基づくモデルやIBMモデル4を用いた場合よりも高い翻訳精度を実現できることを示す．具体的には，アラインメント精度はFFNNに基づくモデルより最大0.0792（F1値），IBMモデル4より最大0.0703（F1値），翻訳精度はFFNNに基づくモデルより最大0.74%(BLEU)，IBMモデル4より最大0.58%(BLEU)上回った．また，各提案（RNNの利用，教師なし学習法，合意制約）個別の有効性も検証し，機械翻訳においては一部の設定における精度改善にとどまるが，単語アラインメントにおいては各提案により精度が改善できることを示す．以降，節で従来の単語アラインメントモデルを説明し，節でRNNに基づく単語アラインメントモデルを提案する．そして，節でRNNに基づくモデルの学習法を提案する．節では提案手法の評価実験を行い，節で提案手法の効果や性質についての考察を行う．最後に，節で本論文のまとめを行う．</section>
  <section title="従来の単語アラインメントモデル">今まで数多くの単語アラインメント手法が提案されてきており，それらは，生成モデル（例えば）と識別モデル（例えば）に大別できる．節では生成モデルを概観し，節では識別モデルの一例として，提案手法のベースラインとなるFFNNに基づくモデルを説明する．</section>
  <subsection title="生成モデル">生成モデルでは，J単語から構成される原言語の文をf_1^J=f_1,,f_J，それに対応するI単語で構成される目的言語の文をe_1^I=e_1,,e_Iとすると，f_1^Jはe_1^Iからアラインメントa_1^J=a_1,,a_Jを通じて生成されると考える．ここで，各a_jは，原言語の単語f_jが目的言語の単語e_a_jに対応する事を示す隠れ変数である．通常，目的言語の文には単語「null」(e_0)が加えられ，f_jが目的言語のどの単語にも対応しない場合，a_j=0となる．そして，f_1^Jがe_1^Iから生成される生成確率は，次の通り，e_1^Iが生成する全アラインメントとの生成確率の総和で定義される：IBMモデル1，2やHMMに基づくモデルでは，式()中の特定アラインメントa_1^Jとの生成確率p(f_1^J,a_1^J|e_1^I)をアラインメント確率p_aと語彙翻訳確率p_tで定義するにおいて，a_0=0である．：この3つのモデルでは，アラインメント確率の定義が異なる．例えば，HMMに基づくモデルでは単純マルコフ性を持つアラインメント確率を用いる：p_a(a_j|a_j-a_j-1)．また，目的言語の各単語に対する稔性(fertility)や歪み(distortion)を考慮するIBMモデル3-5も提案されている．これらのモデルは，EMアルゴリズムにより，単語単位のアラインメントが付与されていない対訳文の集合（ラベルなし学習データ）から学習される．また，ある対訳文(f_1^J,e_1^I)の単語アラインメントを解析する際は，学習したモデルを用いて，次式()を満たすアラインメント（ビタビアラインメント）a_1^Jを求める：例えば，HMMに基づくモデルは，ビタビアルゴリズムによりビタビアラインメントを求めることができる．</subsection>
  <subsection title="FFNNに基づく単語アラインメントモデル">FFNNは，非線形関数を持つ隠れ層を備えることにより，入力データから多層的に非線形な素性を自動的に学習することができ，入力データの複雑な特徴を捉えることができる．近年，その特長を活かし，音声認識，統計的機械翻訳やその他の自然言語処理等，多くの分野で成果をあげている．Yangらは，FFNNの一種であるCD-DNN-HMMをHMMに基づくアラインメントモデルに適用したモデルを提案した．本節では，提案手法のベースラインとなる，このFFNNに基づく単語アラインメントモデルを説明する．FFNNに基づくモデルは，式()のアラインメント確率p_a及び語彙翻訳確率p_tをFFNNにより計算する：ただし，全単語にわたる正規化は計算量が膨大となるため，確率の代わりにスコアを用いる．t_aとt_tは，それぞれ，アラインメントスコアと語彙翻訳スコアであり，p_aとp_tに対応する．また，s_NNはアラインメントa_1^Jのスコアであり，「c(単語w)」は単語wの文脈を表す．ビタビアラインメントは，典型的なHMMに基づくアラインメントモデル同様，ビタビアルゴリズムにより求める．アラインメントスコアは直前のアラインメントa_j-1に依存しているため，FFNNに基づくモデルも単純マルコフ過程に従う．図に，語彙翻訳スコアt_t(f_j,e_a_j|c(f_j),c(e_a_j))を計算するネットワーク構造（語彙翻訳モデル）を示す．このネットワークは，lookup層（入力層），1層の隠れ層，出力層から構成され，各層は，それぞれ，重み行列L，H,B_H，O,B_Oを持つ．Lはwordembedding行列であり，各単語を特徴付ける低次元の実ベクトルとして，単語の統語的，意味的特性を表す．原言語の単語集合をV_f，目的言語の単語集合をV_e，wordembeddingの長さをMとすると，LはM(|V_f|+|V_e|)行列である．ただし，V_fとV_eには，それぞれ，未知語を表すunkと単語「null」を表すnullを追加する．この語彙翻訳モデルは，入力として，計算対象である原言語の単語f_jと目的言語の単語e_a_jと共に，それらの文脈単語を受け付ける．文脈単語とは，予め定めたサイズの窓内に存在する単語であり，図は窓幅が3の場合を示している．まず，lookup層が，入力の各単語に対して行列Lから対応する列を見つけ，wordembeddingを割り当てる．そして，それらを結合させた実ベクトルz_0を隠れ層に送る．次に，隠れ層がlookup層の出力z_0を受け取り，z_0の非線形な特徴を捉える．最後に，出力層が隠れ層の出力z_1を受け取り，語彙翻訳スコアを計算して出力する．隠れ層，出力層が行う具体的な計算は次の通りである=f(H_lz_l-1+B_H_l)．複数の隠れ層を用いた実験は今後の課題とする．：z_1&amp;=f(Hz_0+B_H),_t&amp;=Oz_1+B_O.alignここで，H，B_H，O，B_Oは，それぞれ，|z_1||z_0|，|z_1|1，1|z_1|，11行列である．また，f(x)は非線形活性化関数であり，本論文の実験では，に倣い，htanh(x)(x)=-1，x&gt;1の時はhtanh(x)=1，それ以外の時はhtanh(x)=xである．を用いた．アラインメントスコアt_a(a_j-a_j-1|c(e_a_j-1))を計算するアラインメントモデルも，語彙翻訳モデルと同様に構成できる．語彙翻訳モデル及びアラインメントモデルの学習では，次式()のランキング損失を最小化するように，各層の重み行列を最適化する．最適化は，サンプル毎に勾配を計算してパラメータを更新する確率的勾配降下法(SGD)で行い，各重みの勾配は，誤差逆伝播法で計算する．ここで，は最適化するパラメータ（重み行列の重み），Tは学習データ，s_はパラメータのモデルによるa_1^Jのスコア（式()参照），a^+は正解アラインメント，a^-はパラメータのモデルでスコアが最も高い不正解アラインメントである．</subsection>
  <section title="RNNに基づく単語アラインメントモデル">本節では，アラインメントa_1^JのスコアをRNNにより計算する単語アラインメントモデルを提案する：ここで，t_RNNはアラインメントa_jのスコアであり，FFNNに基づくモデルと異なり，直前のアラインメントa_j-1だけでなく，j-1個の全てのアラインメントの履歴a_1^j-1に依存している．また，本モデルにおいても，FFNNに基づくモデルと同様，確率ではなくスコアを用いる．図にRNNに基づくモデルのネットワーク構造を示す．このネットワークは，lookup層（入力層），隠れ層，出力層から構成され，各層は，それぞれ，重み行列L，H^d,R^d,B_H^d，O,B_Oを持つ．隠れ層の重み行列H^d，R^d，B_H^dは，直前のアラインメントa_j-1からの距離d(d=a_j-a_j-1)毎に定義される．本論文の実験では，8より大きい距離及び-8より小さい距離は，それぞれ，「8」と「-8」にまとめた．つまり，隠れ層は，直前のアラインメントからの距離dに対応した重み行列H^-8,H^-7,,H^7,H^8,R^-8,R^-7,,R^7,R^8,B_H^-8,B_H^-7,,B_H^7,B_H^8を用いてy_jを算出する．ビタビアラインメントは，FFNNに基づくモデルと同様に，図のモデルをf_1からf_Jに順番に適用して求める．ただし，アラインメントa_jのスコアは，y_iを通じてa_1からa_j-1の全てに依存しているため，動的計画法に基づくビタビアルゴリズムは適用できない．そこで，実験では，ビームサーチにより近似的にビタビアラインメントを求める．図のモデルによりf_jとe_a_jのアラインメントのスコアを計算する流れを説明する．まず，f_jとe_a_jの2単語がlookup層へ入力される．そして，lookup層が2単語それぞれをwordembeddingに変換し，そのwordembeddingを結合させた実ベクトルx_jを隠れ層に送る．このlookup層が行う処理は，FFNNに基づくモデルのlookup層と同じである．次に，隠れ層は，lookup層の出力x_jと直前のステップj-1の隠れ層の出力y_j-1を受け取り，それらの間の非線形な特徴を捉える．この時に用いる重み行列H^d，R^d，B_H^dは，直前のアラインメントa_j-1との距離dにより区別されている．隠れ層の出力y_jは，出力層と次のステップj+1の隠れ層に送られる．そして最後に，出力層が，隠れ層の出力y_jに基づいてf_jとe_a_jのアラインメントのスコアt_RNN(a_j|a_1^j-1,f_j,e_a_j)を計算して出力する．隠れ層，出力層が行う具体的な計算は次の通りであるは0とし，直前ステップの隠れ層からの出力y_0は考慮しない：y_1=f(H^dx_1+B^d_H).：&amp;y_j=f(H^dx_j+R^dy_j-1+B^d_H),&amp;t_RNN=Oy_j+B_O.alignここで，H^d，R^d，B^d_H，O，B_Oは，それぞれ，|y_j||x_j|，|y_j||y_j-1|，|y_j|1，1|y_j|，11行列である．ただし，|y_j-1|=|y_j|である．また，f(x)は非線形活性化関数であり，と同様に，本論文ではhtanh(x)を用いる．前述の通り，FFNNに基づくモデルは，語彙翻訳スコア用とアラインメントスコア用の2つのモデルから構成される．一方で，RNNに基づくモデルは，直前のアラインメントとの距離dに依存した重み行列を隠れ層で使うことで，アラインメントと語彙翻訳の両者を考慮する1つのモデルで単語アラインメントをモデル化する．また，RNNに基づくモデルは再帰的な構造をした隠れ層を持つ．このため，過去のアラインメント履歴全体をこの隠れ層の入出力y_iにコンパクトに埋め込むことで，直前のアラインメント履歴のみに依存する従来のFFNNに基づくモデルよりも長いアラインメント履歴を活用して単語アラインメントを行うことができる．</section>
  <section title="モデルの学習">提案モデルの学習では，特定の目的関数に従い，各層の重み行列（つまり，L，H^d，R^d，B^d_H，O，B_O）を最適化する．最適化は，単純なSGD（バッチサイズD=1）よりも収束が早いミニバッチSGDにより行う．また，各重みの勾配は，通時的誤差逆伝播法で計算する．通時的誤差逆伝播法は，時系列（提案モデルにおけるj）でネットワークを展開し，時間ステップ上で誤差逆伝播法により勾配を計算する手法である．提案モデルは，FFNNに基づくモデル同様，式()で定義されるランキング損失に基づいて教師あり学習することができる（節参照）．しかし，この学習法は正解の単語アラインメントが必要であるという問題がある．この問題を解決するため，次の節で，ラベルなし学習データから提案モデルを学習する教師なし学習法を提案する．</section>
  <subsection title="教師なし学習">本節で提案する教師なし学習は，Dyerらにより提案されたcontrastiveestimation(CE)に基づく教師なし単語アラインメントモデルを拡張した手法である．CEとは，観測データの近傍データを疑似負例と捉え，観測データとその近傍データを識別するモデルを学習する手法である．Dyerらは，ラベルなし学習データ中の対訳文Tにおいて考えられる全ての単語アラインメントを観測データ，目的言語側を単語空間V_e全体とした単語アラインメント，つまり，対訳文T中の原言語の各単語とV_e中の各単語との全単語対を近傍データとしてCEを適用した．提案する学習法は，この考え方を目的関数のランキング損失に導入する：ここで，は対訳文(f,e)に対する全ての単語アラインメントの集合，E_[s_]はにおけるスコアs_の期待値を表す．は対訳文T中の目的言語の各単語をV_e全体とした対訳対集合である．したがって，e^+は学習データT中の目的言語の文であり，e^-は|e^+|個の目的言語の単語で構成される疑似の文である(e^-V_e^|e^+|)．一つ目の期待値が観測データ，二つ目の期待値が近傍データに関する項である．しかしながら，式()中のに対する期待値の計算量は膨大となる．そこで，計算量を削減するため，NoiseContrastiveEstimationに基づくNegativeSamplingのように，近傍データ空間からサンプリングした空間を用いる．つまり，各原言語の文f^+に対するe^-として，|e^+|個の目的言語の単語で構成される全ての文ではなく，サンプリングしたN文を使う．さらに，ビーム幅Wのビームサーチにより期待値を計算することで，スコアが低いアラインメントを切り捨て計算量を削減する：式()において，e^+は学習データ内でf^+の対訳となっている目的言語の文((f^+,e^+)T)であり，e^-は無作為に抽出された長さ|e^+|の疑似の目的言語の文である．つまり，|e^+|=|e^-|である．そして，Nは，各原言語の文f^+に対して抽出する疑似の目的言語の文の数である．GENは，ビームサーチにより探索される単語アラインメント空間であり，全ての単語アラインメント空間の部分集合である．各e^-は，無作為に抽出した|e^+|個の目的言語の単語を順番に並べることで生成する．学習に効果的な負例を生成するために，e^-の各単語は，V_eから抽出する代わりに，l_0正則化付きIBMモデル1によって対訳文中でf_if^+との共起確率がC以上と判定された目的言語の単語集合から抽出する．l_0正則化付きIBMモデル1は，単純なIBMモデル1と比較して，より疎なアラインメントを生成するため，疑似翻訳e^-の候補の範囲を制限することが可能となる．</subsection>
  <subsection title="両方向の合意制約">FFNNに基づくモデルとRNNに基づくモデルは，共に方向性を持つモデルである．すなわち，fに対するeのアラインメントモデルにより，単語f_jに対してeとの1対多アラインメントを表す．通常，方向性を持つモデルは方向毎に独立に学習され，両方向のアラインメント結果をヒューリスティックに結合し決定される．Yangらの研究においても，FFNNに基づくモデルは独立に学習されている．一方で，各方向のモデルの合意を取るように同時に学習することで，アラインメント精度を改善できることが示されている．例えば，MatusovらやLiangらは，目的関数を「fe」と「ef」の2つのモデルのパラメータで定義し，2つのモデルを同時に学習している．また，GanchevらやGracaらは，EMアルゴリズムのEステップ内で，各方向のモデルが合意するような制約をモデルパラメータの事後分布に課している．そこで，提案モデルの学習においても両方向の合意制約を導入し，それぞれのモデルの特定方向への過学習を防ぎ，双方で大域的な最適化を可能とする．具体的には，各方向のwordembeddingが一致するようにモデルを学習する．これを実現するために，各方向のwordembeddingの差を表すペナルティ項を目的関数に導入し，その目的関数に基づいて各方向のモデルを同時に学習する：ここで，_FEと_EFは，それぞれ，feとefのアラインメントモデルのパラメータ，_Lはlookup層のパラメータ（Lの重みでありwordembeddingを表す），は合意制約の強さを制御するパラメータ，はのノルムである．実験では2-ノルムを用いた．この合意制約は，教師あり学習と教師なし学習の両方に導入可能である．教師あり学習の場合は，式()のloss()として式()を用い，教師なし学習の場合は式()を用いる．両方向の合意制約を導入した教師なし学習の手順をアルゴリズム1にまとめる．ステップ2では，学習データTからバッチサイズ分のD個の対訳文(f^+,e^+)^Dを無作為に抽出する．ステップ3-1と3-2では，それぞれ，各f^+とe^+に対して，l_0正則化付きIBMモデル1(IBM1)が特定した翻訳候補の単語集合から無作為に単語をサンプリングすることにより，負例となる対訳文をN個(e^-^Nとf^-^N)生成する（節参照）．ステップ4-1と4-2では，特定の目的関数に従い，SGDにより各層の重み行列を更新する（節と参照）．このステップでは，_FEと_EFの更新は同時に行われ，各方向のwordembeddingを一致させるために，_EFは_FEの更新に，_FEは_EFの更新に制約を課している^t，_FE^tの更新の際には，それぞれ，t-1回目に更新された_FE^t-1と_EF^t-1が制約として使われ，更新中の_EF^t，_FE^tはお互いに依存しないことに注意されたい．_EF^tと_FE^tをお互いに依存させて同時に最適化する学習もあり得るが，今後の課題としたい．．</subsection>
  <section title="評価実験"/>
  <subsection title="実験データ">提案手法の有効性を検証するため，単語アラインメントの精度及び翻訳精度の評価実験を行った．単語アラインメントの評価実験は，NAACL2003のsharedtaskで使われたHansardsデータにおける仏英のタスク(Hansards)と，BasicTravelExpressionCorpus(BTEC)における日英のタスク(IWSLT_a)で実施した．翻訳精度の評価実験は，IWSLT2007における日英翻訳タスク(IWSLT)，新聞データから作成されたFBISコーパスにおける中英翻訳タスク(FBIS)，NTCIR-9及びNTCIR-10における日英特許翻訳タスク(NTCIR-9,NTCIR-10)で行った．表に各タスクで使用する対訳文の数を示す．「Train」は学習データ，「Dev」はディベロップメントデータ，「Test」はテストデータを表す．IWSLT_a及びIWSLTの実験データは共にBTECのデータであり，IWSLT_aの実験データは，IWSLTの学習データのうち，単語アラインメントが人手で付与された9,960対訳文である．9,960の対訳文の最初の9,000を学習データ，残りの960をテストデータとした．IWSLT_aの学習データは単語アラインメントが付与されているラベルあり学習データであるのに対し，Hansardsの学習データは単語アラインメントが付与されてないラベルなし学習データである．Hansards及びIWSLT_aのアラインメントタスクでは，各アラインメントモデルのハイパーパラメータは学習データの一部を用いた2分割交差検証により予め決定し，ディベロップメントデータは使わなかった_aの学習データの最初の2,000文を用いた2分割交差検証で最適なパラメータを用いた．IWSLT_a以外のデータに対してもこの検証により得られたパラメータを使った．ディベロップメントデータを使った各タスクでのパラメータ調整は今後の課題としたい．．また，NAACL2003のsharedtaskオリジナルの学習データの総数は約110万文対あるが，今回のHansardsの実験では，学習時の計算量を削減するため，無作為にサンプリングした10万文対を学習データとして用いた．大規模データの実験は今後の課題とする．FBISでは，NIST02の評価データをディベロップメントデータとして使い，NIST03とNIST04の各評価データでテストした．</subsection>
  <subsection title="実験対象">評価実験では，提案手法であるRNNに基づくモデルに加え，ベースラインとして，IBMモデル4とFFNNに基づくモデルを評価した．また，単語アラインメントタスクにおける合意制約の有効性を考察するため，ベースラインとして，典型的なHMMに基づくアラインメントモデルであるVogelらのモデル(HMM_indep)とこのVogelらのモデルにLiangらの両方向の合意制約を導入したモデル(HMM_joint)も評価した．IBMモデル4は，IBMモデル1-4とHMMに基づくモデルを順番に適用して学習した．具体的には，IBMモデル1，HMMに基づくモデル，IBMモデル2，3，4をこの順で5回ずつ繰り返した(1^5H^53^54^5)．これは，GIZA++のデフォルトの設定である(IBM4)．HMM_indep及びHMM_jointはBerkleyAlignerを用いた．Liangらの通り，IBMモデル1，HMMに基づくモデルを順番に5回ずつ繰り返し，各モデルを学習した．FFNNに基づくモデルでは，wordembeddingの長さMを30，文脈の窓幅を5とした．したがって，|z_0|は300=3052である．また，隠れ層として，ユニット数|z_1|が100の層を1層使用した．このFFNNに基づくモデルは，に倣って節の教師あり手法により学習したモデルFFNN_sに加えて，節と節で提案した教師なし学習や合意制約の効果を確かめるため，FFNN_s+c，FFNN_u，FFNN_u+cのモデルを評価した．「s」は教師ありモデル，「u」は教師なしモデル，「+c」は学習時に合意制約を使うことを意味する．RNNに基づくモデルでは，wordembeddingの長さMを30，再帰的に連結している隠れ層のユニット数|y_j|を100とした．したがって，|x_j|は60=302である．また，提案の学習法の効果を検証するため，FFNNに基づくモデル同様，RNN_s，RNN_s+c，RNN_u，RNN_u+cの4種類を評価した．FFNNに基づくモデル及びRNNに基づくモデルの各層のユニット数やMなどのパラメータは，学習データの一部を用いた2分割交差検証により予め設定した．NNに基づくモデルの学習について説明する．まず，各層の重み行列を初期化する．具体的には，lookup層の重み行列Lは，局所解への収束を避けるため，学習データの原言語側と目的言語側からそれぞれ予め学習したwordembeddingに初期化する．その他の重みは，[-0.1,0.1]のランダムな値に初期化する．wordembeddingの学習には，Mikolovらの手法を基にしたRNNLMツールキット（デフォルトの設定）を用いる．その際，コーパスでの出現数が5回以下の単語はunkに置き換える．各重みの初期化後は，ミニバッチSGDにより特定の目的関数に従って各重みを最適化する．本実験では，バッチサイズDを100，学習率を0.01とし，50エポックで学習を終えた．また，学習データへの過学習を避けるため，目的関数にはl2正則化項（正則化の比率は0.1）を加えた．教師なし学習におけるパラメータW，N，Cは，それぞれ，100，50，0.001とし，合意制約に関するパラメータは0.1とした．翻訳タスクでは，フレーズベース機械翻訳(SMT)システムMosesを用いた．日本語の各文はChaSen，中国語の各文はStanfordChinesesegmenterにより単語へ分割した．その後，40単語以上の文は学習データから除いた．言語モデルは，SRILMツールキットにより，modifiedKneser-Neyスムージングを行い学習した．IWSLT，NTCIR-9及びNTCIR-10では，学習データの英語側コーパスから構築した5グラム言語モデル，FBISでは，EnglishGigawordのXinhua部分のデータから構築した5グラム言語モデルを使用した．翻訳モデルは，各単語アラインメント手法により特定されたアラインメント結果に基づいて学習した．SMTシステムの各パラメータは，ディベロップメントデータを用いてMERTによりチューニングした．</subsection>
  <subsection title="実験結果（単語アラインメント）">表に各手法の単語アラインメントの精度をF1値で示す．NNに基づく教師ありモデルに対しては，学習データに付与されている正しい単語アラインメントを学習したモデル(REF)と，IBM4で特定した単語アラインメントを学習したモデル(IBM4)の2種類の精度を示す．Hansardsの学習データには正しい単語アラインメントが付与されていないため，REFに対する実験は実施していない．評価手順は，まず，各アラインメントモデルにより，feとefのアラインメントをそれぞれ生成する．その後，「grow-diag-final-and」ヒューリスティックスにより，両方向のアラインメントを結合する．そして，その結合したアラインメント結果をF1値で評価する．有意差検定は，有意差水準5%の符号検定で行った．具体的には，テストデータの各単語に対して，他方の手法では不正解だが正しく判定したものを+，他方の手法では正解だが誤って判定したものを-として，2手法の評価に有意な差があるかどうかを片側検定の符号検定で判定した．表中の「+」は，ベースラインとなるFFNNに基づくモデルFFNN_s(REF/IBM4)との精度差が有意であることを示し，「++」は，ベースラインのFFNNに基づくモデルFFNN_s(REF/IBM4)に加えてIBM4との精度差も有意であることを示す．また，正しい教師ラベルを使用するモデル(REF)と使用しないモデル（REF以外）のそれぞれで最高の精度を太字で示す．表より，IWSLT_aとHansardsの両タスクにおいて，本論文の提案手法（RNNに基づくモデル，教師なし学習，合意制約）RNN_u+cが最もアラインメント精度が高いことが分かる．特に，ベースラインとの精度差は有意であることから，本論文の提案を組み合わせることにより，従来手法より高いアラインメント精度を達成できることが実験的に確認できる．次に，本論文の各提案の個別の有効性について確認する．表より，IWSLT_aとHansardsの両タスクにおいて，RNN_s/s+c/u/u+c(IBM4)，RNN_s/s+c(REF)は，それぞれ，，FFNN_s/s+c/u/u+c(IBM4)，FFNN_s/s+c(REF)よりも精度が良い．特に，IWSLT_aでは，RNN_s(REF)，RNN_s(IBM4)とFFNN_s(REF)，FFNN_s(IBM4)とのそれぞれの性能差は有意であることが分かる．これは，RNNに基づくモデルにより長いアラインメント履歴を捉えることで，アラインメント精度が向上することを示しており，RNNを利用したモデルの有効性を確認できる．ただし，Hansardsにおいては，RNNの効果が少ない．この言語対による効果の違いについては節で考察する．IWSLT_aとHansardsの両タスクにおいて，RNN_s+c(REF/IBM4)，RNN_u+cのアラインメント精度は，それぞれ，RNN_s(REF/IBM4)，RNN_uを上回っており，これらの精度差は有意であった．さらに，FFNN_s+c(REF/IBM4)，FFNN_u+cは，それぞれ，FFNN_s(REF/IBM4)，FFNN_uより有意にアラインメント精度が良い．この結果より，教師ありと教師なしの両方の学習において，両方向の合意制約を導入することでFFNNに基づくモデル及びRNNに基づくモデルのアラインメント精度を改善できることが分かる．一方で，HMM_jointの方がHMM_indepよりも精度が良いことから，提案の合意制約に限らず，両方向の合意をとるようにモデルを学習することは有効であることが確認できる．HMMに基づくモデルに導入したLiangらの両方向の合意制約と提案の合意制約の傾向の違いは，節で考察する．IWSLT_aでは，RNN_uとRNN_u+cは，それぞれ，RNN_s(IBM4)とRNN_s+c(IBM4)より有意にアラインメント精度が良い．一方で，Hansardsでは，これらの精度は同等である．この傾向はFFNNに基づくモデルでも同様である．これは，学習データの質（IBM4の精度）が悪い場合，教師あり学習はIBM4による疑似学習データに含まれる誤りの悪影響を受けるのに対し，提案の教師なし学習は学習データの質に依らずに精度の良いFFNNやRNNに基づくモデルを学習できることを示している．</subsection>
  <subsection title="実験結果（機械翻訳）">表に各手法により付与されたアラインメントを用いたSMTシステムの翻訳精度を示す．評価尺度は，大文字と小文字を区別したBLEU4を用いた．MERTの不安定な振る舞いの影響を緩和するため，MERTによるチューニングは3回行い，その平均値を表に示す．IWSLTでは，アラインメントモデル及び翻訳モデルの学習には学習データ全てを用いた．一方で，NTCIR-9，NTCIR-10とFBISでは，アラインメントモデルの学習における計算量を削減するため，学習データから無作為にサンプリングした10万文対からアラインメントモデルを学習した．その後，学習したアラインメントモデルにより学習データ全ての単語アラインメントを自動的に付与し，翻訳モデルを学習した．また，詳細な比較を行うため，全学習データから学習したIBMモデル4に基づくSMTシステムの精度をIBM4_allとして示す．翻訳精度の有意差検定は，有意差水準5%でブートストラップによる検定手法により行った．表の「*」は，ベースライン（IBM4及びFFNN_s(IBM4)）との精度差が有意であることを示す．また，各タスクで最高精度（IBM4_allを除く）を太字で示す．表と表より，単語アラインメント精度を改善しても，必ずしも翻訳精度が向上するとは限らないことが分かる．この事は従来より知られており，例えば，においても同様の現象が確認されている．しかしながら，表より，全ての翻訳タスクで，RNN_uとRNN_u+cはFFNN_s(IBM4)とIBM4よりも有意に翻訳精度がよいことが分かる．この結果から，提案手法は翻訳精度の改善にも寄与することが実験的に確認できる．また，NTCIR-9とFBISでは，提案モデルは学習データの一部から学習したが，学習データ全てから学習したIBM4_allと同等の精度を達成している．学習データ量の影響は節で考察する．</subsection>
  <section title="考察"/>
  <subsection title="RNNに基づくモデルの効果">図にFFNN_s及びRNN_sで解析した単語アラインメントの具体例を示す．三角がFFNN_sの解析結果，丸がRNN_sの解析結果，四角が正しい単語アラインメントを表す．図より，RNN_sはFFNN_sと比較して，複雑なアラインメント（例えば，図(a)中の「haveyoubeen」に対するギザギザのアラインメント）を特定できていることが分かる．これは，FFNN_sは直前のアラインメント履歴しか利用しないが，RNN_sは長いアラインメント履歴に基づいてアラインメントのパス（例えば，フレーズ単位のアラインメント）を捉えられることを示唆している．節で述べた通り，RNNに基づくモデルの効果は，日英アラインメント(IWSLT_a)と比べて仏英アラインメント(Hansards)に対して少ない．これは，英語とフランス語は語順が似ていて，日英に比べて1対1アラインメントが多く（図参照），仏英単語アラインメントは局所的な手がかりで捉えられる場合が多いためであると考えられる．図(b)は，このような単純な単語アラインメントは，FFNN_sとRNN_sの両モデルで正しく解析できることを示している．</subsection>
  <subsection title="学習データ量の影響">BTECにおける日英アラインメントタスクにおいて様々なサイズの学習データを使った時のアラインメント精度を表に示す．「40~K」，「9~K」，「1~K」は，それぞれ，IWSLTの全学習データ，IWSLT_aの全学習データ，IWSLT_aの全学習データから無作為に抽出した1,000文対を学習データとした時の，IWSLT_aのテストデータに対するアラインメント精度である．「9~K」及び「1~K」はラベルあり学習データ，「40~K」はラベルなし学習データである．そのため，教師ありモデル(REF)の「40~K」に対する実験は実施していない．表より，「1~K」のRNN_s+c(REF)と「9~K」のRNN_u+cは「40~K」のIBM4より性能がよいことが分かる．すなわち，RNNに基づくモデルは，IBM4の学習データの22.5%以下(9,000/40,000)のデータから同等の精度を持つモデルを学習できたことが分かる．その結果，表が示す通り，学習データの一部を使ったRNN_u+cに基づくSMTシステムが，全学習データを用いたIBM4_allに基づくSMTシステムと同等の精度を達成できる場合がある．表より，HMMに基づくモデルに導入したLiangらの両方向の合意制約は学習データが小規模なほど効果があることが分かる．一方で，提案の合意制約は，Liangらの合意制約と比較すると精度の改善幅は小さいが，どのテータサイズにおいても同等の効果を発揮することが確認できる．また，各データサイズで節と同様の手法の比較を行うと，教師ラベルを使わない場合はRNN_u+c，使う場合はRNN_s+c(REF)が最も性能が良い．そして，本論文で提案した，RNNの利用，教師なし学習，合意制約の個別の有効性も確認できることから，データサイズに依らず提案手法が有効であることが分かる．</subsection>
  <section title="まとめ">本論文では，RNNに基づく単語アラインメントモデルを提案した．提案モデルは，隠れ層の再帰的な構造を利用し，長いアラインメント履歴に基づいてアラインメントのパス（例えば，フレーズ単位のアラインメント）を捉えることができる．また，RNNに基づくモデルの学習法として，Dyerらの教師なし単語アラインメントを拡張して人工的に作成した負例を利用する教師なし学習法を提案した．そして，更なる精度向上のために，学習過程に各方向のwordembeddingを一致させる合意制約を導入した．複数の単語アラインメントタスクと翻訳タスクの実験を通じて，RNNに基づくモデルは従来のFFNNに基づくモデルよりアラインメント精度及び翻訳精度が良いことを示した．また，提案した教師なし学習や合意制約により，アラインメント精度を更に改善できることを確認した．提案モデルでは，アラインメント対象の文脈をアラインメント履歴(y_i)に暗示的に埋め込み利用しているが，今後は，FFNNに基づくモデルのように周辺単語の入力（c(f_j)やc(e_a_j)）として明示的に利用することも検討したい．また，Yangらは複数の隠れ層を用いることでFFNNに基づくモデルの精度を改善している．これに倣って提案モデルでも各隠れ層を複数にするなど，提案モデルの改良を行う予定である．さらに，本論文では提案モデルにより特定したアラインメントに基づいて翻訳モデルを学習したが，翻訳モデル学習時の素性としてアラインメントモデルが算出するスコアを使用したり，Watanabeらのように翻訳候補のリランキングの中で使ったりするなど，提案モデルのSMTシステムへの効果的な組み込み方に関しても検討したい．</section>
</root>
