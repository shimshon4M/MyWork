<?xml version="1.0" ?>
<root>
  <title/>
  <author/>
  <jkeywords/>
  <section title="Introduction">Theconstructionofclassesofwords,orcalculationofdistancesbetweenwords,hasfrequentlydrawntheinterestofresearchersinnaturallanguageprocessing.Manyofthesestudiesaimedatfindingclassesbasedonco-occurrences,oftencombinedwiththeaimofestablishingsemanticsimilaritybetweenwords.Wesuggestamethodforclusteringwordspurelyonthebasisofsyntacticbehavior.Weshowhowthenecessarydataforsuchclusteringcaneasilybedrawnfromapubliclyavailabletreebank,andhowdistincttypesofbehaviorcanbediscovered.Theresultingclustersdonotreflectsemanticsimilaritynoratendencytoco-occur.Theydoreflectthat,forexample,aparticularnounisoftentheobjectofaprepositionalphrase,orthataparticularverbisoftenusedtransitively.Althoughapartofspeechtagsetcanbethoughtofasaclassificationbasedonsyntacticbehavior,wecanconstructanarbitrarynumberofclusters,orabinarytreeofwordsthatsharetheirpartofspeech.Inthispapertwoevaluationsoftheresultingclustersarepresented.First,wepresentacasestudyofprepositions.Wediscussindetailabinarywordtreeforprepositionsthatwascreatedbysyntactic-behaviorbasedclustering,toshowwhatsortofpropertiesarerevealedbytheclusteringandwhatonecanlearnfromthisaboutlanguage.Manyoftheseparationsmadebythealgorithmarenaturaldivisions,whichcanoftenbefeltbyintuition,butwhichcouldnotbequantifiedbefore.Wealsopresentanexperiment,inordertoobtainmorequantitativeevaluationofclustersandtocompareclustersmadeinadifferentway.Forthisgoalexperimentswerecarriedoutinwhichtheclusterswereusedforestimationofaprobabilisticdistribution.Theideabehindthisexperimentisthatwhenthewordswithinclustersaresimilarwithrespecttotheirsyntacticbehavior,theseclustersshouldalsogivemoreinformationabouttheirbehaviorinasomewhatdifferentcontext.Finally,wedescribeanumberofwaysinwhichthesemethodscanbeappliedinNaturalLanguageProcessingapplications.Inparticular,abinarytreeofwordscanbeusedtosupplyefficientquestionstoadecisiontreebasedsystemthatpredicts,forexample,syntacticalstructure.OtherapplicationsaretobefoundinInformationRetrievalandLexicography.Themaincontributionofthismethodoverpreviousmethodsisthepossibilitytoquantifyapropertythateverywordhas,butwhichisnotrevealedbytraditionalclusteringmethods.</section>
  <section title="Headwords and Dependencies">Thedataweextractarebasedontheconceptofheadwords.Suchheadwordsarechosenforeveryconstituentintheparsetreebymeansofasimplesetofrules.Thesehavebeenusedinvariousstudiesinthisfield,see.Everyheadwordispropagatedupthroughthetreesuchthateveryparentreceivesaheadwordfromthehead-child.Figure~givesanexampleofaparsetreewithheadwords.Followingthetechniquessuggestedby,aparsetreecansubsequentlybedescribedasasetofdependencies.Everywordexcepttheheadwordofthesentencedependsonthelowestheadworditiscoveredby.Thesyntacticrelationisthengivenbythetripleofnonterminals:themodifyingnonterminal,themodifiednonterminal,andthenonterminalthatcoversthejointphrase.Table~givesanexampleofsuchadescription.Theschemeforchoosingheadwordsnecessarilyintroducessomearbitrarychoices,forexampleinthecaseofcoordinatestructuressuchas``JohnandMary,''butinmostcasesthechoiceisnotdifficultandwebelievethatthesearbitrarychoicesdonotposeagreatproblemtofrequentlyoccurringwords.OnonepointourmethodisdifferentfromthemethodsuggestedbyCollins.Collinsusesareducedsentenceinwhicheverybasicnounphrase(i.e.,anounphrasethathasnonounphraseasachild)isreducedtoitsheadword.Forexample,thephrase``acar''isreducedto``car'',and``hisresignation''isreducedto``resignation.''Thereasonforthisisthatitimprovesco-occurrencecountsandadjacencystatistics.Wehoweverdonotreducethesentencesincewedonotneedtoconsideradjacencystatisticsorunresolvedambiguities,andthereforeneverfacetheproblemthatawordinabasicnounphrase,thatisnottheheadword,isadjacenttoormodifiessomethingoutsideofthebasicnounphrase.Table~givestherelationsforonesentence,butinsteadofconsideringonesentencewecollectsuchpatternsforthewholecorpusandstudystatisticsforindividualwords.Inthiswayitcanbediscoveredthat,forexample,aparticularverbisoftenusedtransitively,orthataparticularprepositionismostlyusedtoproducelocativeprepositionalphrases.Wordscanbedistinctorsimilarinthisrespect,butnotethatthisisnotrelatedtosemanticsimilarity.Wordssuchaseatanddrinkhaveasemanticsimilarity,butmaybecompletelydifferentintheirsyntacticbehavior,whereastendandappeardonothaveanobvioussemanticrelation,buttheydohaveasimilaritysincetheycanbothbeusedasraisingverbs,aswillbeexemplifiedlater.Throughoutthispaperwewillusetheterm``word''torefertowordsforwhichthepartofspeechhasalreadybeendisambiguated.Intablesandfiguresweemphasizethisbyindicatingthepartofspeechtogetherwiththeword.</section>
  <section title="Collecting Statistics for Individual Words">Thenextstepwetake,iseliminatingoneofthetwowordsinthistableofdependencies.Considertables~and~.Theseshowwecantakethree``observations''fromthesentencebyeliminatingeithertheheadwordorthedependentword.Ifheadwordsareeliminatedweobtainthreeobservations,forthewordsJohn,Smithandfast.Ifdependentwordsareeliminatedwealsoobtainthreeobservations,twoforworksandoneforSmith.Bycollectingtheobservationsovertheentirecorpuswecanseeto/bywhatsortofwordsandwithwhatkindofrelationsawordmodifiesorismodified.Weconsiderthefollowingdistributions:p(R,t_h|w_dt_d)(R,t_d|w_ht_h)eqnarraywhereRindicatesthetriplerepresentingthesyntacticrelation,w_dadependentwordthatmodifiesheadwordw_h,andt_dandt_htheirrespectivepartofspeechtags.Forexample,inthesecondlineoftable~,whichcorrespondstodistribution~,Ris(NP,S,VP),t_his``verb'',w_dis``Smith''andt_dis``propernoun''.Statisticsofthedistributions()and()caneasilybetakenfromatreebank.WetooksuchdatafromtheWallStreetJournalTreebank,calculatingtheprobabilitieswiththeMaximumLikelihoodEstimator:[p(R,t_h|w_dt_d)=f(R,t_h,w_dt_d)_R',t'f(R',t',w_dt_d)]wherefstandsforfrequency.ThisequationshowshowtheMLestimationiscalculatedfordistribution().TheMLestimationfordistribution()iscalculatedlikewise.Notethatweonlyextractthedependencyrelations,andignorethestructureofthesentencebeyondtheserelations.ComparethedependencybehaviorofthepropernounsNipponandRep.intable~.ThewordNipponisJapaneseforJapan,andmainlyoccursinnamesofcompanies.ThewordRep.istheabbreviationofRepresentative,andobviouslyoccursmainlywithnamesofpoliticians.Ascanbeseen,thewordRep.occursfarmorefrequently,butthedistributionsarehighlysimilar.Bothalwaysmodifyanotherpropernoun,about33%ofthetimeforminganNP-SBJand67%ofthetimeanNP.Bothareaparticularkindofpropernounthatalmostalwaysmodifiesotherpropernounsandalmostneverappearsbyitself.Italsobecameclearthatthenouncompanyisverydifferentfromanounsuchashostage,sincecompanyoftenisthesubjectofaverb,whilehostageisrarelyinthesubjectposition.Botharealsoverydifferentfromthenounyear,whichisfrequentlyusedastheobjectofapreposition.Thepresentparticipleincludinghasanextremelystrongtendencytoproduceprepositionalphrases,asin``Safetyadvocates,includingsomemembersofCongress,...'',makingitdifferentfrommostotherpresentparticiples.Apasttensesuchasfellhasanunusualhighfrequencyastheheadofasentenceratherthanaverbphrase,whichisprobablyapeculiarityoftheWallStreetJournal(``Stockpricesfell...'').Ourobservationisthatamongwordswhichhavethesamepartofspeech,somewordgroupsexhibitbehaviorthatisextremelysimilar,whileothersdisplaylargedifferences.Themethodwesuggestaimsatmakingaclusteringbasedonsuchbehavior.Byusingthistechniqueanynumberofclusterscanbeobtained,sometimesfarbeyondwhathumanscanbeexpectedtorecognizeasdistinctcategories.</section>
  <section title="Comparison with Co-Occurrence Based Clustering">Clusteringofwordsbasedonsyntacticbehaviorhastoourknowledgenotbeencarriedoutbefore,butclusteringhasbeenappliedwiththegoalofobtainingclassesbasedonco-occurrences.Suchclusterswereusedinparticularforinterpolatedn-gramlanguagemodels.Bylookingatco-occurrencesitispossibletofindgroupsofwordssuchas[director,chief,professor,commissioner,commander,superintendent].Themostprominentmethodfordiscoveringtheirsimilarityisbyfindingwordsthattendtoco-occurwiththesewords.Inthiscasetheymayforexampleco-occurwithwordssuchasdecideandlecture.Thegroupofverbs[tend,plan,continue,want,need,seem,appear]alsoshareasimilarity,butonehastolookatstructuresratherthanmeaningorco-occurrencestoseewhy.Alltheseverbstendtooccurinthesamekindofstructures,ascanbeseeninthefollowingexamplesfromtheWallStreetJournal.*4mmThefunds'sharepricestendtoswingmorethanthebroadermarket.*4mmInvestorscontinuetopourcashintomoneyfunds.*4mmCrayResearchdidnotwanttofundaprojectthatdidnotinclude*4mmSeymour.*4mmNoonehasworkedouttheplayers'averageage,butmostappeartobe*4mmintheirlate30s.Whattheseverbsshareisthepropertythattheyoftenmodifyanentireclause(markedas'S'intheWallStreetJournalTreebank)ratherthannounphrasesorprepositionalphrases,usuallyformingasubjectraisingconstruction.Thisisonlyatendency,sinceallofthemcanbeusedinadifferentwayaswell,butthetendencyisstrongenoughtomaketheirusagequitesimilar.Co-occurrencebasedclusteringignoresthestructureinwhichthewordoccurs,andwouldthereforenotbetherightmethodtofindrelatedsimilarities.Asmentioned,co-occurrencebasedclusteringmethodsoftenalsoaimatproducingsemanticallymeaningfulclusters.VariousmethodsarebasedonMutualInformationbetweenclasses,see.Thismeasurecannotbeappliedinourcasesincewelookatstructureandignoreotherwords,andconsequentlyalgorithmsusingthatmeasurecannotbeappliedtotheproblemwedealwith.Thementionedstudiesuseword-clustersforinterpolatedn-gramlanguagemodels.Anotherapplicationofhardclusteringmethods(inparticularbottom-upvariants)isthattheycanalsoproduceabinarytree,whichcanbeusedfordecision-treebasedsystemssuchastheSPATTERparserortheATRDecision-TreePart-Of-SpeechTagger.Inthiscaseadecisiontreecontainsbinaryquestionstodecidethepropertiesofaword.Wepresentahardclusteringalgorithm,inthesensethateverywordbelongstoexactlyonecluster(orisoneleafinthebinaryword-treeofaparticularpartofspeech).Besideshardalgorithmstherehavealsobeenstudiestosoftclusteringwherethedistributionofeverywordissmoothedwiththenearestkwordsratherthanplacedinaclasswhichsupposedlyhasauniformbehavior.Infact,initwasarguedthatreductiontoarelativelysmallnumberofpredeterminedwordclassesorclustersmayleadtosubstantiallossofinformation.Ontheotherhand,whenusingsoftclusteringitisnotpossibletogiveayes/noansweraboutclassmembership,andbinarywordtreescannotbeconstructed.</section>
  <section title="Similarity Measure and Algorithm">Thechoiceoftheclusteringalgorithmistosomeextentindependentfromthewaydataiscollected,butasmentionedclusteringiscarriedoutonthebasisofdistributionalsimilarity,andmethodsusingMutualInformationarenotapplicable.Thealgorithmwepresenthereismeanttodemonstratehowsyntacticbehaviorcanbeusedforclustering.However,wefeeltheoptimalchoicefortheclusteringmethoddependsontheapplicationitwillbeusedfor.StudiesindistributionbasedclusteringoftenusetheKullback-Leibler(KL)distance,seeforexample.However,thisdistanceisnotsymmetrical,andsinceweare(forthetimebeing)interestedinhardclusteringitisdesirabletohaveasymmetricalmeasure.WecouldpossiblyuseJeffery'sInformation,i.e.thesumoftheKL-distances:J(p,q)&amp;=&amp;KL(p|q)+KL(q|p)*2mm&amp;=&amp;_xp(x)(p(x)q(x))+q(x)(q(x)p(x)).eqnarrayWehavetriedthisdistancemeasure,butinmanycaseswehavefoundittohaveundesirableeffects,primarilybecausethegoalofouralgorithmisjoiningwords(andtheirstatistics)togethertomakeonecluster,andadistortedimageresultsfromthismeasurewhenwordshavedifferenttotalfrequencies.Furthermore,Jeffery'sInformationisundefinedifeitherdistributionhasavalueof0andtheothernot.Forthisreasontheywouldhavetobesmoothedwith,forexample,apartofspeechbaseddistribution,suchasp(R,t_h|w_dt_d)&amp;=&amp;p(R,t_h|w_dt_d)+(1-)p(R,t_h|t_d),eqnarraybutwewantedtoavoidusinganunlexicaldistributionsincewebelievelexicalinformationismorevaluable.Insteadwesuggestadifferentmeasure.WhatwewouldliketouseisthesumoftheKL-distancesbetweenahypotheticalwordthatwouldbecreatediftheobservationswerejoinedandtherespectivewords,i.e.[M(p,q)def=KL(p|pq)+KL(q|pq)]whichreflectstheresultoftheusedoperationmoreclosely.Wewillexplainhowthesymbolshouldbeunderstood.Assumethereareanumberofpatternsi=1...n,andobservedfrequenciesa_1...a_nforwordw_at_a,andb_1...b_nforwordw_bt_b.Also,letA=_ia_iandB=_ib_i.TheMaximum-Likelihoodestimatesforw_aarethuscalculatedasp_a(x)=a_x/Aandlikewiseforw_b.Usingthenotationp_a+b(i)=a_i+b_iA+B,wedefinethedistancebetweenwordsasM(w_at_a,w_bt_b)&amp;=&amp;KL(w_at_a|w_at_aw_bt_b)+KL(w_bt_b|w_at_aw_bt_b)*2cm&amp;=&amp;_ip_a(i)(p_a(i)p_a+b(i))+p_b(i)(p_b(i)p_a+b(i))eqnarray*whichcanbeinterpretedasthesumofKL-distancesbetweenahypotheticalwordthatwouldbecreatediftheobservationsofthewordsw_at_aandw_bt_bwouldbejoinedtogether,andw_at_aandw_bt_brespectively.LikeJeffery'sInformation,thismeasureissymmetrical,althoughnotatruedistancesinceitdoesnotobeythetriangleinequality.Thismeasureismoreappropriatefortworeasons.First,thisdistributionisbettertailoredtowardmakingclusterswhereobservationswillbejoinedtogether.Second,wetakethissumtobezeroforvaluesofiwhena_i=b_i=0(noobservationsforeitherword),thereforepre-smoothingisnotnecessary.TheequationcaneasilybetransformedintotheformM(w_at_a,w_bt_b)&amp;=&amp;(A+BA)+(A+BB)+&amp;&amp;_ia_iA(a_ia_i+b_i)+b_iB(b_ia_i+b_i)eqnarraywhichmakescalculationsignificantlyfastersincepatternsforwhichonlyonewordhasanon-zerofrequencydonotneedtobecalculatedwithinthesummation,astheyalwaysbecomeszero.TheAlgorithmThealgorithminitiallyregardseverywordasa1-elementcluster,andworksbottomuptowardsasetofclusters.Clustersaremerged,whichmeansthattheyareconsideredtobeonenewcluster.Theobservationsarecombined;i.e.(usingthenotationfromthepreviousparagraph)[p_wa+wb(i)=a_i+b_iA+B.]Thestrategyofagreedyalgorithmisfollowed,everytimefindingthetwoclustersthathavetheleastdistancebetweenthemandmergingthemuntilthedesirednumberofclustersisreached.However,onlywordswiththesamepartofspeechmaybemerged,sodistancesbetweenwordsthathavedifferentpartsofspeecharenevercalculated.Wordscanthereforereceivea'combinedtag'consistingoftheirpartofspeechtag,andasyntacticbehaviortag.Thisissimilartowhatrefertoasastructuraltag.Thealgorithmisactuallyappliedtwice,oncetoclusteringfordependent-context()andoncetoclusteringforhead-context().Anobviousproblemwiththissortofclusteringislowfrequencywords.Formanywordsonlyoneorafewobservationsareavailable,whichmaygivesomeinformationaboutwhatsortofworditis,butwhichdoesnotgiveareliableestimateofthedistributions.Wewillmentionasolutiontothisproblemlater.Intheexamplewepresentonlywordsforwhichatleast25observationsareavailable.Oneproblemwithco-occurrencebasedclusteringthathasbeenpointedoutinthepastisthatofalmost-lineardendrograms,i.e.suchthatitformsaveryunbalancedtree,causedbythepropertiesofMutualInformation.Inthiscasethesetofwordscannotbedividedinsubsetsofroughlyequalsize.</section>
  <section title="A Case Study of Prepositions">Wepresentabinarywordtreethatwasproducedbythealgorithmdescribedintheprevioussection.Themaingoalofthisistoshowwhatsortofpropertiesarerevealedbythisclustering,andwhatkindofwordsareproblematic.Eveninsituationswherewordsareclusteredbysyntacticbehaviorwithoutmakingabinarytree,itcanbeusefultostudythetypeofpropertiesthatdecidesyntacticbehavior.ThedataforthisclusteringweretakenfromtheWallStreetJournalCorpus,usingatotalofaround800,000patternsasdata.Pleaserefertofigures~and~foranexampleoftheresultsobtainedwithclustering.Thisisadendrogramthatreflectstheclusteringprocessfromloosewordsuntilthepointweretheyareallmergedintoonecluster.Thedendrogramshowstheresultforprepositions,althoughonlythoseprepositionswereconsideredforwhichatleast25observationswereavailable.InthedivisionofwordsoverthepartsofspeechwefollowthetaggingschemeoftheWallStreetJournalTreebank,andforexamplesubordinatorssuchaswhile,ifandbecauseareincludedintheprepositions.Ofcourseitispossibletouseamorefinegrainedtagset,whenavailable.Ontheotherhand,aswillbeshownlater,thealgorithmdoesdecidetoclassifymostsubordinatorsintoonecluster.Wewilldiscussthemajordistinctionsmadebythealgorithm.Atfirstitmaynotbeclearwhywordsshouldbedividedinthisway,butinspectionofthedatafromthecorpusshowsthatmanyofthesechoicesareverynatural.Wealsodiscussinwhichcasesthedendrogramdoesnotformnaturalcategories.Thefirstpartition,markedA,isaquitenaturaldivision.Theupperbranch(fromoffthroughAbout)areprepositionsthatusuallycoversomephrasethemselves,whereastheprepositionsinthelowerbranchusuallydonotcoveranyphrase.Theprepositionwhetheroccurs,forexample,instructuressuchaswhereinourheadword-selectionschemewhetherdependsontheheadwordare.(Evenifthisischanged,theystillbecomeoneclusterbecauseofthetypicalpatternswithSandSBAR.)Forcomparison,theprepositionbelowusuallyoccursinstructuressuchas*4mmMagnarecentlycutitsquarterlydividendinhalfandthecompany's*4mmClassAsharesare(VPwallowing(PP-LOCfarbelowtheir52-weekhigh*4mmof16.125Canadiandollars(US13.73))).itistheheadwordofaprepositionalphrasebeforeitmodifiestheverb.ThepartitionmarkedwithBisnotanaturaldivision;itratherseparatesasetofprepositionsthatdonotfitinelsewhere.TheprepositionsfromperthroughAboutarenotsimilartoeachotherortootherprepositionsintheirbehavior.PartitionCagainresemblestogroupsthatcanbecharacterizedeasily.TheprepositionsbythroughAfter,thelowerbranchofC,dependedalmostexclusivelyonverbs.Theprepositionsfromoffthroughabout,theupperbranchofC,dependonmorevariedheadwords.Mostofthesefrequentlydependonbothnounsandverbs.Thefollowingexampleshowsarounddependingonanoun,althougharoundalsotendstodependoncardinalnumbers.*4mmYounowmaydropbytheVoiceofAmericaofficesinWashingtonandread*4mmthetextofwhattheVoiceisbroadcastingtothose130millionpeople*4mm(PP-LOCaroundtheworld)whotuneintoiteachweek.AnexampleforthelowerbranchofCis*4mmAplantobringthestocktomarketbeforeyearendapparently*4mm(VPwasupset(PPbytherecentweaknessofFrankfurtshareprices)).TheprepositionsattheupperbranchofpartitionDtendtoformahigheramountofPP-TMPtypephrases,asinalthough,whilethisisstronglythecasefortheprepositionswithinandthroughout,itisnotthecaseforbehind.AtpartitionEprepositionswithapreferenceforverbsareattheupperbranch.PrepositionsthatalmostexclusivelydealwithverbswereseparatedatC,butherethedistinctionislessabsolute.TheprepositionsattheupperbranchofEhaveachanceofabouttwothirdstodependonaverb,whereasthisisonlyonethirdatthelowerbranch.PartitionFisonceagainaveryclear,naturaldivision.Theprepositionsin,onandathaveastrongtendencytoformphrasesofthetypePP-LOCasin*4mmMr.Nixonistraveling(PP-LOCinChina)asaprivatecitizen,but*4mmhehasmadeclearthatheisanunofficialenvoyfortheBush*4mmadministration.theprepositionsatthelowerbranch,ofthroughabouthavemuchlowerfrequenciesfortheselocativephrases.ThedivisionatGisalsoveryclearwhenthedataareinspected.Theupperbranchreflectsprepositionsforwhichthecoveringphrase(themiddlepartofthetriplerepresentingthegrammaticalrelation)ismostlyVPorNP.TheprepositionsForthroughAfteratthelowerbranchofGaremainlycoveredbyphrasesoftypeS.Aprepositionsuchasduringisfoundinstructuressuchas*4mmFujitsusaidit(VPbidtheequivalentoflessthanaU.S.pennyon*4mmthreeseparatemunicipalcontracts(PP-TMPduringthepasttwoyears)).aprepositionsuchaswithoutisusuallyfoundinthePP-S-VPpattern:*4mm(SInfact,(PPwithoutWallStreetfirmstradingfortheirown*4mmaccounts),thestock-indexarbitragetradingopportunitiesforthebig*4mmfunds(VPmaybeallthemoreabundant).)AtHthisisfurtherdividedinwordsthattendmoretodependonloosewords,PPtypephrases(suchaswithoutinthelastexample)orStypephrases,atthelowerbranch,andthosethatusuallydependonheadsofaVP.AsforthedivisionatpointI,theprepositionsnextthroughAlthoughsharethepropertythattheircoveringphrase(themiddlepartofthetriplerepresentingthegrammaticalrelation)isoftenofthetypeSBAR-ADVorSBAR-PRP.Theprepositionsattheupperbranch,whetherthroughdown,mainlysharenothavingthisproperty.WhilethestatusoftheupperbranchofJissomewhatunclear,thelowerbranchofJisaperfectlyclearandintuitivegroup.AllofthewordsfromthoughthroughAlthoughappearalmostexclusivelyinthepatterns(--,SBAR-ADV,S),(--,S,S),(--,SBAR-PRP,S)and(--,SBAR-PRP,--).Anexampleis*4mmThegroupsaysstandardizedachievementtestscoresaregreatlyinflated*4mmbecauseteachersoften``teachthetest''asMrs.Yeargindid,*4mm(SBAR-ADValthough(Smostarenevercaught)).inourheadwordschemearebecomestheheadwordoftheSBAR-ADVtypephrase.Concluding,manyofthedivisionsmadebythealgorithmarequitenatural.Therearesomepartsofspeech(suchasnounsandverbs)wereamuchlargernumberofwordsisincludedinthehierarchy,whilesomeotherpartsofspeech,forexamplepersonalpronouns,produceverysmallhierarchies.Ingeneralthehierarchyismoreinterestingforpartsofspeechthatareusedinavariedway,andlessinterestingfor,forexample,symbolssuchasthepercentagesign,thatareusedinamonotoneway.Itisinterestingtoseethatcapitalizationturnsouttobeameaningfulpredictoraboutthewayawordwillbeusedforsomewords,butnotforothers.ThewordpairsoandSo,andthepairbecauseandBecauseareclusterednexttoeachother,whichindicatesthattheymodifythesamekindofstructures,independentofwhethertheyareatthebeginningofthesentence.ThewordpairunderandUnder,andthepairafterandAfterontheotherhandareratherfarapart,indicatingthattheirusagechangessubstantiallywhentheybecomethefirstwordofthesentence.Co-occurrence-basedclusteringalgorithmstendtomakedifferentchoices.Althoughtheexactchoicesdependonthealgorithmchosen,wementionsomegeneraltendencies.Thepreposition&quot;since&quot;tendstobeclusteredcloseto&quot;of&quot;and&quot;from&quot;asallofthemoftenoccurnexttoornearyear-indications(&quot;1986&quot;),month-names(&quot;January&quot;)andweekdays(&quot;Friday&quot;).Also,&quot;next&quot;,&quot;on&quot;,&quot;before&quot;and&quot;by&quot;tendtoformagroupbecauseoftheirhighco-occurrencewithweekdays.Also,&quot;as&quot;tendstocomeclosetowordssuchas&quot;so&quot;and&quot;if&quot;becauseoftheirhighco-occurrencewithpersonalpronouns.Anotherdifferenceisthatwefoundlittleinfluenceofcapitalization.Beingatthestartofasentenceappearedtohavelittleeffectonthesurroundingwords.Thewordsthatarehardertoclassifyalsotendtodiffer;thereareforexamplefixedpatternssuchas&quot;alongwith&quot;,&quot;amongothers&quot;,&quot;atleast&quot;and&quot;underpressure&quot;thatarenotsharedbyotherprepositions.</section>
  <section title="Quantitative Evaluation: Dependency Relations">Thedistributionoftypesofdependencyrelationsbetweenwordsisofimportancetonaturallanguageprocessing,partlybecauseofthedirectpracticalapplicationinparsingsystems,butevenmoresobecauseitcanshowfundamentalpropertiesoflanguage.Atthesametimeitisclearthatgoodestimationtechniqueswillbeimportantduetotheenormousnumberofparameters.Considerthedistributionofthedependencyrelationbetweentwowords,giventhatthereissomedependencyrelationbetweenthem,andgiventheirrespectivepartsofspeech:p(R|w_dt_d,w_ht_h).eqnarrayTheusedsymbolscarrythesamemeaningasinsection~.Ourconjectureisthattheclassesweobtainedwillalsoindicatesimilaritywithinthedistribution().AdistributionsimilartothisisusedinforEnglishandinforJapanese,inbothcaseswiththegoalofcarryingoutsyntacticalanalysis.Weusethisforamorequantitativeevaluationoftheclustersthatcanbeproducedinthewaymentionedearlier.Thedistributionshowedhereisclearlydifferentfromthatusedforclustering,butitisalsorelatedtothesyntacticstructuresinwhichwordsoccur.Ifwordstendtooccurinthesamesortofstructures,weexpectatendencyoftheirmarginalswithrespecttothisdistributiontosharesimilaritiesaswell.Thebaselinemodelweuseistheregularmodelsmoothedwithpartofspeechbasedprobabilitiesusingamethodsuggestedby.Tobeexact,p_lexical(R|w_dt_d,w_ht_h)&amp;=&amp;_1(w_dt_d,w_ht_h)p(R|w_dt_d,w_ht_h)+&amp;&amp;_2(w_dt_d,w_ht_h)p(R|t_d,w_ht_h)+&amp;&amp;_3(w_dt_d,w_ht_h)p(R|w_dt_d,t_h)+&amp;&amp;_4(w_dt_d,w_ht_h)p(R|t_d,t_h)eqnarray*whichwerefertointhetableaslexical.ThebasicprobabilitiesarefoundwiththeMost-Likelihoodestimator,andtheinterpolationparametersareestimatedfromdataheldoutforthisgoal,butnotseparatelyforeverywordpair.Assuggestedbytheyaredividedintobucketsbasedontheircount,sothatallwordpairswiththesamefrequencyhavethesameinterpolationparameters.Thefrequencieswerealsojoinedintoonebucketwhentherewastoolittledatatoestimatethem.Weusedthesametechniqueforallothermodels.Theclass-basedmodelwecomparethiswithisp_classes(R|w_dt_d,w_ht_h)&amp;=&amp;_1(c_d,c_h)p(R|c_d,c_h)+&amp;&amp;_2(c_d,c_h)p(R|t_d,c_h)+&amp;&amp;_3(c_d,c_h)p(R|c_d,t_h)+&amp;&amp;_4(c_d,c_h)p(R|t_d,t_h)eqnarray*wherec_distheclassofw_dt_d(usingthedependent-wordclustering),andc_htheclassofw_ht_h(usingtheheadwordclustering).Theseclassesweremadewiththenewmeasure().Inthetablethismodelisidentifiedasclasses.Sincetestdatawasnotusedforclustering,thetestdatacontainedacertainpercentageofwordsthatwereneverclusteredbythealgorithm.Forthisreasonanimaginaryextraclusterisassumed,containingallsuch``unseen''words.Probabilitiesconditionedonthisclassarealwayszero.Exactlythesamemodelhasalsobeenappliedtothemodelidentifiedasdivergence.Thismodeliscompletelyidenticaltoclasses,butwiththeclustersproducedusingdivergence()asthedistancemeasure(equation()wasusedforsmoothingworddistributions),insteadofourmeasure().Wealsopresentresultsofacombinedmodelusingbothlexicalandclass-basedprobabilities.Thisiscalculatedwiththeequationp_comb(R|w_dt_d,w_ht_h)&amp;=&amp;_1(w_dt_d,w_ht_h)p(R|w_dt_d,w_ht_h)+&amp;&amp;_2(w_dt_d,w_ht_h)p(R|c_d,c_h)+&amp;&amp;_3(w_dt_d,w_ht_h)p(R|t_d,c_h)+&amp;&amp;_4(w_dt_d,w_ht_h)p(R|c_d,t_h)+&amp;&amp;_5(w_dt_d,w_ht_h)p(R|t_d,t_h)eqnarray*andidentifiedascombined.</section>
  <section title="Experimental Results">TheexperimentswerecarriedoutwithdatafromtheWallStreetJournalCorpus.Asetofabout740,000patternswasusedastrainingdata,forclusteringwords,andtooptimizesmoothingparameters.Aseparategroupof60,000patternswasusedfortesting.Inordertoassessstatisticalsignificanceofresultsthisexperimentwascarriedout5times,eachtimeusingdifferenttestdata.Inallclusteringmethodsdifferentsetsofclassesweremadeforthedependentpositionandtheheadwordposition.Thenumberofclasseswefoundtobeoptimalwasabout3000forbothdependentpositionandheadwordposition.Table~showstheresultsforthegivenmodels.Thecrossentropyofpredictingtherelationofallpatternsisgiveninthefirstcolumn.Thesecondcolumngivestheentropyonpatternsthatoccurredatleastonceinthetrainingdata(f(R,w_dt_d,w_ht_h)&gt;0).Thethirdcolumnindicatesentropyonpatternsthatdidnotoccurinthetrainingdata(f(R,w_dt_d,w_ht_h)=0).Alldifferencesbetweenmodelswerefoundtobesignificantwithaso-calledpairedt-test(p&lt;0.001),exceptthedifferencebetweenthepairmarkedwiththesymbol``.''Apairedt-testisamethodforanalyzingthedifferencesbetweenpairsofscores,onanumberofdatasets.Asmentioned,thevaluesinthetableareaveragedresultsoffivesamples.Apairedt-testtakesintoconsiderationtheseparatesamplescoresratherthantheaverage.Theclassesproducedwiththedistancemeasure()displayacrossentropyofonlyabout2%morethanthelexicalmodel.Thesizeofthetablesneededfortheclass-basedprobabilitiesisaboutonethirdofthatoflexicaltables.Asexpected,theperformanceisnotasgoodasthelexicalmodelforthemorecommonpatterns.Theclassesthatwereproducedwithdivergence()didnotperformaswell,illustratingtheproblemswementionedearlier.Sincethelexicalprobabilitiesperformbetteronmorefrequenteventsandclass-basedprobabilitiesbetteroninfrequentevents,alogicalnextstepiscombiningthem.Thescoreofcombinedillustratesthatthiscreatesagaininentropyofabout2.7%.</section>
  <section title="Applications">Afirstapplicationofthiswork,ofwhichwecarriedoutafirststepinthisarticle,isthelexicographicaloneofstudyingwordbehavior.Somepropertiesofwords,suchasthepeculiarbehaviorofthepresentparticipleincludingorthesimilaritiesbetweenprepositionssuchasthoughandwhileonlybecomesclearoncethecorpusdataisanalyzedinthewaywedescribed.Wheninspectingmanually,thebinarywordtreerepresentationappearstobethemosteasytounderstand.Asecondapplicationofthebinarywordtreecanbefoundindecision-treebasedsystemssuchastheSPATTERparserortheATRDecision-TreePart-Of-SpeechTagger,asdescribedbyUshioda.Inthiscaseitisnecessarytouseahard-clusteringmethod,suchthatabinarywordtreecanbeconstructedbytheclusteringprocess,aswedidintheexampleintheprevioussections.Adecisiontreeclassifiesdataaccordingtoitspropertiesbyaskingsuccessive(oftenbinary)questions.Inthecaseofapartofspeechtaggeroraparsingsystem,itisparticularlyimportantforthesystemtoasklexicalizingquestions.However,questionsaboutindividualwordssuchas``Isthistheworddisplay?''arenotefficientonalargescalesinceitwouldeasilyrequirethousandsofquestions.Abinarytreeallowsonetoseparatethevocabularyintotwopartsateveryquestion,whichisefficientwhenthesetwopartsaremaximallydifferent.Inthatcaseitispossibletoobtainasmuchinformationaspossiblewithasmallnumberofquestions.Aconditionforthisapplicationisthattreesmaynotbeveryunbalanced,astheextremecaseofalineartreebecomesequaltoaskingword-by-word.Asmentioned,themethodwesuggestdidnotproduceaveryunbalancedtreeforthepartsofspeechintheWallStreetJournalTreebank.AthirdapplicationcanbefoundinInformationRetrieval.Thiscanbeseenfromtheexampleofincluding:wordswithsuchbehaviorhavelittlecontentbecausetheyhavearatherfunctionalroleinthesentence.Thiscanbeseeninthesentence``Safetyadvocates,includingsomemembersofCongress,...''wheretermssuchasSafetyadvocatesormembersofCongressindicatemuchmoreaboutthetopicofthesentencethantherelativelyemptywordincluding.Itispossibletoclusterwordsanddecidewhichclustersarelikelytoindicatethetopic,andwhicharenotlikelytodoso.Forthisapplicationawidervarietyofalgorithmscanbeapplied;wordscanforexamplebeexchangedorshuffledbetweenclassestoimprovetheentiremodel.Afourthapplicationisclass-basedsmoothingofinterpolatedn-grammodels.Theco-occurrencebasedclassesdescribedintheliteratureare,ofcourse,createdwiththisasobjectivefunction,butontheotherhandtheclasseswesuggestclearlycontaininformationthatisinaccessibletoco-occurrencebasedclasses.Itispossiblethatacombinationofco-occurrencebasedclassesandclassesofsyntacticbehaviorwouldgivebetterresults,butthiswouldhavetobedemonstratedexperimentally.Insomeoftheseapplicationswordswithalowfrequencycannotbeignoredbecauseoftheirquantity,butatthesametimethealgorithmcannotrelytooheavilyontheirobservations.Apossiblesolutionistocarryoutclusteringwithoutthesewords,anddistributethelow-frequencywordsovertheleavesofthetreeafterwards.Asolutionalongthislinewaschosenforco-occurrencebasedclusteringin,whereafirstalgorithmhandlesmorefrequentwords,andasecondalgorithmaddsthelow-frequencywordsafterwards.Itmustbeaddedthattheclassesarebasedonwordswithdisambiguatedpartofspeech.Thecorrectclasscanonlybedecidedunambiguouslyafterthepartofspeechtaghasbeendecided.Otherwise,thepossibleclasseswillhavetobecombinedinsomeway.</section>
  <section title="Conclusion">Wehavepresentedamethodwhichconstructsclassesofwordswithsimilarsyntacticbehavior,orbinarytreesthatreflectwordsimilarity,byclusteringwordsusingtreebankdata.Inthiswayitispossibletodiscoverparticulartypesofbehavior,suchasthepeculiarbehaviorofthepresentparticipleincluding,verbsthatmodifyanentireclause(raisingverbs),nounsthatprefereithersubjectpositionorobjectposition,orprepositionsthatpreferlocativephrases.Mostoftheclassesfoundinthiswaywouldnotbefoundifclusteringwereperformedonthebasisofco-occurrences,ashasbeendescribedintheliterature.Forexample,theverbs[tend,plan,continue,want,need,seem,appear]shareaparticularsentencestructureratherthan,say,thesortofnounthatbecomestheobject.Ontheotherhand,semanticalsimilaritiessuchastheoneinthegroup[director,chief,professor,commissioner,commander,superintendent],areignored.Asbecameclearfromthecasestudyofprepositions,theclusteringprocessrevealssimilaritiesinthesyntacticstructureinwhichwordsappearwhichinsomecasescanbeclearlyfeltbyintuition.Forexample,thewordsin,onandatoftenaretheheadoflocativeprepositionalphrases,andaprepositionsuchaswithinusuallyistheheadofatemporalprepositionalphrase.Usingthismethodtheseintuitionscanbequantified.Theexperimentbasedonparameterestimationalsoindicatedthattheclassificationofwordsintogroupsbasedonsyntacticalbehaviorresultsinclassesthatreflecttheirbehavior,andcantosomeextentaddtotheinformationavailableaboutthebigramdistributionwedescribed,inthefaceofsparsedata.Theapplicationsofthisworkaretobefoundinthepossibilitytocreategroupsofwordsthatshareadesiredproperty.Themosttypicaloneisthatofdecision-trees,whichneedtohaveapoolofquestionsaboutthepropertiesofthewordsthatarebeingconsidered.Forexample,ifthewordincludingisencountered,thechancethatthisisthestartofaprepositionalphraseincreasesdramatically.Thecontributionofthisworkisingivingawaytoquantifytheseproperties.wouldliketoexpressourappreciationtothereviewers.Thisarticlehasbenefitedgreatlyfromtheirvaluablecommentsandcriticisms.document</section>
</root>
