<?xml version="1.0" ?>
<root>
  <title>統計的構文解析における構文的統計情報と語彙的統計情報の統合について</title>
  <author>白井清昭乾健太郎徳永健伸田中穂積</author>
  <jabstract>本論文では，構文解析の曖昧性解消を行うために，構文的な統計情報と語彙的な統計情報を統合する手法を提案する．我々が提案する統合的確率言語モデルは，構文的優先度などの構文的な統計情報を反映する構文モデルと，単語の出現頻度や単語の共起関係などの語彙的な統計情報を反映する語彙モデルの2つの下位モデルから成る．この統合的確率言語モデルは，構文的な統計情報と語彙的な統計情報を同時に学習する過去の多くのモデルと異なり，両者を個別に学習する点に特徴がある．構文的な統計情報と語彙的な統計情報を独立に取り扱うことにより，それぞれの統計情報を異なる言語資源から独立に学習することができるだけでなく，それぞれの統計情報が曖昧性解消においてどのような効果を果たすのかを容易に分析することができる．この統合的確率言語モデルを評価するために，日本語文の文節の係り受け解析を行った．構文モデルを用いたときの文節の正解率は73.38%となり，ベースラインに比べて11.70%向上した．また，構文モデルと語彙モデルを組み合わせることにより，文節の正解率はさらに10.96%向上し84.34%となった．この結果，本研究で提案する枠組において，語彙的な統計情報は構文的な統計情報と同程度に曖昧性解消に貢献することを確認した．</jabstract>
  <jkeywords>統計的構文解析，構文的統計情報，語彙的統計情報，統合的確率言語モデル</jkeywords>
  <subsection title="構文モデルP(R)">構文モデルとしては，構文的な統計情報を反映し，かつ構文構造Rの生成確率を高い精度で推定するものであれば，どのような確率モデルを利用してもよい．構文モデルに利用できる確率モデルとしては，PCFGや確率一般化LR法(ProbabilisticGeneralizedLRMethod，以下PGLR)などが挙げられる．我々は，PGLRを構文モデルの有力な候補として考えている．PGLRとは，構文解析手法のひとつである一般化LR法を拡張したものである．PGLRは，LR表に記述された各状態遷移の遷移確率を推定し，その遷移確率の積によって1つの状態遷移列，すなわちそれに対応する構文構造の生成確率を与えるモデルである．このPGLRはPCFGに比べて，次のような特長を持つ~．文脈依存性を取り扱うことができる．隣接する品詞間の共起関係を取り扱うことができる．距離に関する優先度を取り扱うことができる．ここで，隣接する品詞間の共起関係とは，品詞bi-gramのような品詞列の出現に関する統計情報であり，形態素解析の曖昧性解消に有効であると考えられる．また，距離に関する優先度とは，単語はなるべく近い単語に係りやすいといった，係り受け関係にある単語間の距離に関する統計情報である．</subsection>
  <section title="はじめに">コーパス，辞書，シソーラスなどの機械可読な言語データの整備が進んだことから，自然言語処理における様々な問題の解決に何らかの統計情報を利用した研究が盛んに行われている．特に構文解析の分野においては，構文的な統計情報だけでなく，単語の出現頻度や単語の共起関係といった語彙的な統計情報を利用して解析精度を向上させた研究例が数多く報告されている．ここで問題となるのは，このような語彙的な統計情報を構文的な統計情報とどのように組み合わせるかということである．このとき，我々は以下の2つの点が重要であると考える．解析結果の候補に与えるスコアが，構文的な統計情報のみを反映したスコアと語彙的な統計情報のみを反映したスコアから構成的に計算できることこのことによる利点を以下に挙げる．個々の統計情報を個別に学習できる構文的な統計情報を学習する際には，学習用言語資源として比較的作成コストの高い構文構造が付加されたコーパスが必要となる．しかしながら，推定パラメタの数はそれほど多くはないので，比較的少ないデータ量で学習することができる．これに対して，語彙的な統計情報は，単語の共起に関する統計情報を学習しなければならないために大量の学習用データを必要とするが，構文構造付きコーパスに比べて作成コストの低い品詞付きコーパスを用いても学習することが十分可能である．このように，統計情報の種類によって学習に要する言語資源の質・量は大きく異なる．そこで，構文的な統計情報と語彙的な統計情報を異なる言語資源を用いて個別に学習できるように，それぞれの統計情報の独立性を保持しておくことが望ましい．曖昧性解消時における個々の統計情報の働きを容易に理解することができる例えば，曖昧性解消に失敗した場合には，構文的な統計情報と語彙的な統計情報を独立に取り扱うことにより，どちらの統計情報が不適切であるかを容易に判断することができる．個々の統計情報を反映したスコアが確率的意味を持っていること構文的な統計情報を反映したスコアと語彙的な統計情報を反映したスコアを組み合わせて全体のスコアとする場合，両者のスコアの和を計算すればいいのか，積を計算すればいいのか，またどちらか片方に重みを置かなければならないのかなど，その最適な組み合わせ方は自明ではない．このとき，個々のスコアが確率的意味を持つように学習することにより，確率の積としてそれらを自然に組み合わせることができる．ところが，語彙的な統計情報を利用して構文解析の精度を向上させる過去の研究の多くは以上の条件を満たしていない．例えば田辺らは，確率文脈自由文法(ProbabilisticContextFreeGrammar,以下PCFG)における書き換え規則の非終端記号に，その非終端記号が支配する句の主辞となる単語を付加すること(以下，これをPCFGの語彙化と呼ぶ)によって語彙的従属関係をPCFGの確率モデルに反映させる方法を提案している~．一方，英語を対象にPCFGを語彙化した研究としてはHogenoutら~，Charniak~，Collins~によるものがある．しかしながら，PCFGの語彙化によって構文的な統計情報と語彙的な統計情報を組み合わせる方法は，非終端記号に単語を付加することによって規則数が組み合わせ的に増大し，推定するパラメタ数も非常に多くなるといった問題点がある．また，構文的な統計情報と語彙的な統計情報を同時に学習するモデルとなっているが，先ほど述べたように両者は独立に学習できることが望ましい．PCFGをベースとしないSPATTERパーザ~やSLTAG~にも同様の問題が存在する．これらの研究は語彙的な統計情報を利用して解析精度の向上を図ってはいるが，構文的な統計情報と独立に学習する枠組にはなっていない．構文的な統計情報と語彙的な統計情報を独立に学習する枠組としてはLiによるものが挙げられる~．Liは，解析結果の候補Iに対して，構文的な統計情報を反映させた確率モデルP_syn(I)と単語の共起関係を反映させた確率モデルP_lex(I)を別々に学習する方法を提案している．そして，語彙的な制約は構文的な制約に優先するといった心理言語学原理に基づき，まずP_lex(I)をIのスコアとして用い，一位とそれ以外の候補のスコアの差が十分に大きくなかった場合に限りP_syn(I)をスコアとして用いている．すなわち，構文的な統計情報と語彙的な統計情報をそれぞれ独立に学習してはいるが，これらを同時に利用して曖昧性解消を行っているわけではない．また，この2つのスコアの持つ確率的意味が不明確であり(I)，P_lex(I)は確率と呼ばれてはいるが，どのような事象に対する確率なのかは不明である．，その最適な組み合わせ方は自明ではない．本研究では，構文的な統計情報と語彙的な統計情報を組み合わせる一方法として，統合的確率言語モデルを提案する~．この統合的確率言語モデルの特徴は，単語の出現頻度，および単語の共起関係といった2つの語彙的な統計情報を局所化し，構文的な統計情報と独立に取り扱う点にある．また，構文的な統計情報を構文構造の生成確率として，語彙的な統計情報を単語列の生成確率としてそれぞれ学習し，これらの積を解析結果の候補に対するスコアとすることにより，曖昧性解消に両者を同時に利用することができる．この統合的確率言語モデルの詳細については節で述べる．節ではこの統合的確率言語モデルの学習，およびそれを用いた日本語文の文節の係り受け解析実験について述べる．最後に節で結論と今後の課題について述べる．</section>
  <section title="統合的確率言語モデル">まず，本論文で一貫して用いる記号について説明する．入力文字列A=a_1,,a_mAを生成する単語列W=w_1,,w_nWを生成する品詞列L=l_1,,l_nLを生成する構文構造R本研究では，形態素解析と構文解析を同時に取り扱うことを仮定する．すなわち，入力文字列Aが与えられたときに，その正しい単語列W，正しい品詞列L，正しい構文構造Rを求めることを目的とする．例えば，「彼女がパイを食べた」という入力文に対する解析結果の候補の例を図に示す．各解析結果の候補に対してその生成確率P(R,L,W,A)を計算し，これが最大の解析結果を選択することによって曖昧性解消を行う．さらに，確率モデルP(R,L,W,A)を以下のように分解する．ここで，構文構造Rは最終的に品詞列Lを生成するものと仮定すると，P(L|R)=1となる(図参照)．また，単語列Wが決まれば入力文字列Aは一意に決まるので，P(A|W)=1となる．したがって，式()は以下のように簡略化できる．本研究では，式()に示した通り，解析結果の生成確率を以下の2つの確率モデルの積として計算する．構文モデルP(R)構文構造Rの生成確率である．この確率モデルには構文的な統計情報を反映させる．語彙モデルP(W|R)構文構造Rが与えられたときに，それから単語列Wを生成する確率である．この語彙モデルには語彙的な統計情報を反映させる．</section>
  <subsubsection title="単語生成文脈">式()の各項(図の例では式()〜())のパラメタ空間は非常に大きく，これを直接学習することは一般に不可能である．ところが，各単語w_iの生成に強く影響するのは各項の確率の前件R,w_1,,w_i-1全てではなく，その一部のみであると考えられる．例えば，図の例文において，``パイ''は動詞``食べ''のヲ格の格要素となっている．このとき，``パイ''という単語を生成する際には，式()の前件``R,を,食べ,た''(図の斜線部)のうち，品詞Nと単語``を'',``食べ''(図の丸で囲まれた部分)によって十分近似できると期待できる(式())．式()において，N[食べを]は，``食べ''という動詞のヲ格の格要素となっている名詞を表わしている．すなわち，P(パイ|N[食べを])は，``食べ''という動詞のヲ格の格要素となっている名詞から``パイ''という単語が生成される確率を表わしている．したがって，式()には，``パイ''という単語そのものがどれくらい出現しやすいかといった単語の出現頻度と，``パイ''と``食べ''がどの程度共起しやすいかといった単語の共起関係が反映されている．ここで，単語生成文脈c_iを以下のように定義する．先ほどの例においては，単語``パイ''の単語生成文脈は``食べを''である．そして，各単語w_iの生成確率の前件``R,w_1,,w_i-1''を，その単語の品詞l_iと単語生成文脈c_iに縮退することにより，語彙モデルP(W|R)を以下のように近似する．P(W|R)&amp;=&amp;_w_iP(w_i|R,w_1,,w_i-1)&amp;&amp;_w_iP(w_i|l_i[c_i])eqnarray</subsubsection>
  <subsubsection title="単語生成文脈決定規則">単語生成文脈を導入する際に問題となるのは，どのような単語に対してどのような単語生成文脈を選べばよいのかということである．我々は，これを人手で作成した規則によって記述する．以下，単語w_iの単語生成文脈c_iを決定する規則を単語生成文脈決定規則と呼ぶ．単語生成文脈決定規則の例を以下に挙げる．単語の共起関係を全く考慮しない場合単語w_iについて，周囲の単語との従属関係を考慮しない場合には，その単語の生成確率はその単語の品詞l_iのみに依存するとみなす．例えば，図の例において，助動詞``た''と動詞``食べ''を生成する際に他の単語との語彙的従属関係を考えない場合には，それぞれの生成確率(),()は以下のように近似すればよい．P(た|R)&amp;&amp;P(た|AUX)P(食べ|R,た)&amp;&amp;P(食べ|V)eqnarrayこれに対応した単語生成文脈決定規則を以下に示す．この規則は単語生成文脈を決定する際のデフォルト規則でもある．lc:default単語w_iを生成する際に他の単語との従属関係を考慮しない場合には，単語w_iの単語生成文脈c_iを空とする．lcdef格要素となる名詞が助詞を介して動詞に係る際の従属関係を考慮する場合前述のように，格要素となる名詞が助詞を介して動詞に係る際には，動詞・助詞の組と名詞との間には語彙的従属関係が存在する．このような語彙的従属関係を確率モデルに反映させるためにlc:fillerを定義する．lc:filler単語w_iの品詞l_iがN(名詞)であり，かつ助詞pを介して動詞vに係っているとき，単語w_iの単語生成文脈c_iをvpとする．このとき，w_iの生成確率P(w_i|N[vp])は動詞vの格pの格要素となる名詞Nから単語w_iが生成される確率を表わす．lcdef例えば，図の例において，名詞``パイ''は動詞``食べ''のヲ格の格要素であり，名詞``彼女''は動詞``食べ''のガ格の格要素となっている．したがって，これらの単語を生成する際にはこの規則が適用され，それぞれの生成確率(),()は以下のように近似される．P(パイ|R,を,食べ,た)&amp;&amp;P(パイ|N[食べを])P(彼女|R,が,パイ,を,食べ,た)&amp;&amp;P(彼女|N[食べが])eqnarray助詞とその係り先用言の従属関係，格間の従属関係を考慮する場合図の例文においては，2つの助詞``が''と``を''が動詞``食べ''に係っている．このとき，これらの生成確率(),()を以下のように近似しても，助詞とその係り先用言との間の語彙的従属関係，および同じ用言に係る助詞同士の従属関係(以下，これを格間の従属関係と呼ぶ)を語彙モデルに反映させることができる．P(を|R,食べ,た)&amp;&amp;P(を|P[食べ2_1,_2])P(が|R,パイ,を,食べ,た)&amp;&amp;P(が|P[食べ2_1,を])eqnarray式()は，助詞Pが2つの助詞の係り先となっている動詞``食べ''に係っているときに，品詞Pから単語``を''が生成される確率を表わしている．一方式()は，助詞Pが2つの助詞の係り先となりかつそのうちの1つは``を''である動詞``食べ''に係っているときに，品詞Pから単語``が''が生成される確率を表わしている．助詞とその係り先用言の従属関係，および格間の従属関係を語彙モデルに導入するために，lc:markerを以下のように定義する．lc:marker単語w_iの品詞l_iがP(助詞)でありかつ用言hに係っているとき，単語w_iの単語生成文脈c_iをhn_1,,_j,p_j+1,,p_nとする．このとき，w_iの生成確率P(w_i|P[hn_1,,_j,p_j+1,,p_n])は，用言hがn個の助詞の係り先となりかつ用言に近いp_j+1,,p_nの助詞が既に生成されているときに，_jとしてw_iが生成される確率を表わす．lcdeflc:markerにおいて，同じ用言に係る助詞は用言に近いものから順番に生成されると仮定している．すなわち，助詞が出現する順序も考慮されている．助詞の係り先が用言か体言かを考慮する場合助詞の係り先が用言である場合と体言である場合とでは，助詞の生成確率P(w_i|P)の分布は著しく異なると考えられる．例えば，係り先が用言の場合には``が''，``を''などの助詞は出現しやすいが，助詞``の''は出現しにくい．これに対して，係り先が体言の場合，すなわちその助詞を含む文節が連体修飾節となっている場合には，助詞``の''が出現する場合が圧倒的に多いと予想される．したがって，助詞の生成確率P(w_i|P)を学習する際に，その助詞の係り先が用言もしくは体言であるかを区別しないで学習するのは望ましいことではない．これに対応するには，以下のようなlc:josiを定義すればよい．lc:josi単語w_iの品詞l_iがP(助詞)であり，かつその助詞の係り先が体言であるとき，単語w_iの単語生成文脈c_iをnd/とする．nd/はその助詞の係り先が体言であることを表わすシンボルである．このとき，w_iの生成確率P(w_i|P[nd])は，体言を係り先とする助詞から単語w_iが生成される確率を表わす．lcdef助詞の単語生成文脈を決定する際には，助詞の係り先が用言である場合にはlc:markerが，助詞の係り先が体言である場合にはlc:josiが適用される．ここに挙げたlc:default〜#が単語生成文脈を決定するための全ての規則というわけではない．本節では，特に用言の格関係に注目して語彙モデルに反映させるべき語彙的従属関係(単語の共起関係)の例を挙げたが，他の種類の語彙的従属関係を語彙モデルに反映させるように単語生成文脈決定規則を拡張・洗練することもできる．すなわち，語彙モデルにおいてどのような語彙的従属関係を考慮するかは，単語生成文脈決定規則の追加・変更によって柔軟に調整することが可能である．単語生成文脈として何を選択するかを自動的に学習することも考えられるが，我々は言語学的知見に基づくヒューリスティクス規則によって単語生成文脈を選択する方向で研究をすすめている．なぜなら，語彙モデルにどのような種類の語彙的従属関係を反映させるかを単語生成文脈決定規則によって明確に記述することにより，モデルに反映された統計情報が曖昧性解消に有効であるかどうかなど，モデルの特性の分析を容易に行うことができるからである．</subsubsection>
  <subsubsection title="従属係数">これまでは単語を生成する際に考える単語生成文脈は常に一つであると仮定していた．しかしながら，一般には，一つの単語を生成する際に複数の単語生成文脈を考慮しなければならない場合もある．例えば，図の例文において，2つの文節``食べ-て''と``出かけ-た''は並列の関係にある．したがって，この例文中の名詞``彼女''は動詞``食べ''のハ格の格要素であり，同時に動詞``出かけ''のハ格の格要素でもある．したがって，lc:fillerに従えば，``彼女''という単語を生成する際の単語生成文脈としては食べはと出かけはの2つがある．このとき，``彼女''の生成確率は次のように推定することが望ましい．同様に，この例文中の助詞``は''は動詞``食べ''と``出かけ''の両方に係っているとみなすことができる．したがって，lc:markerに従えば，``は''という単語を生成する際の単語生成文脈として食べ2_1,をと出かけ2_1,への2つがあると考えられ，``は''の生成確率も次のように推定することが望ましい．ところが，式()や()のように複数の単語生成文脈を前件に持つ確率モデルは，推定するパラメタの数が爆発的に増大する可能性がある．そこで本研究では，複数の単語生成文脈を以下のように取り扱う．まず，説明を簡略化するために，単語w_iが2つの単語生成文脈c_1とc_2を持つとする．このとき，単語w_iの生成確率P(w_i|l_i[c_1,c_2])を以下のように近似する．P(w_i|l_i[c_1,c_2])&amp;=&amp;P(l_i[c_1,c_2]|w_i)P(w_i)P(l_i[c_1,c_2])&amp;=&amp;P(l_i[c_1]|w_i)P(l_i[c_2]|l_i[c_1],w_i)P(w_i)P(l_i[c_1])P(l_i[c_2]|l_i[c_1])&amp;&amp;P(l_i[c_1]|w_i)P(l_i[c_2]|l_i,w_i)P(w_i)P(l_i[c_1])P(l_i[c_2]|l_i)&amp;=&amp;P(l_i[c_1]|w_i)P(l_i[c_1])P(l_i[c_2]|l_i,w_i)P(l_i[c_2]|l_i)P(w_i)&amp;=&amp;P(w_i|l_i[c_1])P(w_i)P(w_i,|l_i,l_i[c_2])P(w_i|l_i)P(w_i)&amp;=&amp;P(w_i|l_i)P(w_i|l_i[c_1])P(w_i|l_i)P(w_i|l_i[c_2])P(w_i|l_i)eqnarray式()から式()の変形において，2つの単語生成文脈c_1とc_2は互いに独立であると仮定している．P(l_i[c_2]|l_i[c_1])&amp;&amp;P(l_i[c_2]|l_i)P(l_i[c_2]|l_i[c_1],w_i)&amp;&amp;P(l_i[c_2]|l_i,w_i)eqnarrayここで，従属係数D(w_i|l_i[c_i])を式()のように定義する．この従属係数を用いれば，式()から式()が導かれる．P(w_i|l_i[c_1,c_2])&amp;&amp;P(w_i|l_i)D(w_i|l_i[c_1])D(w_i|l_i[c_2])eqnarray以上では単語w_iが2つの単語生成文脈を持つ場合を考えていたが，単語w_iがn個の単語生成文脈c_1,,c_nを持つ場合にも同様の近似が可能であり，最終的に以下の式が得られる．式()で定義した従属係数D(w_i|l_i[c_i])は単語w_iと単語生成文脈c_iの相関関係を評価する統計量である．例えば，w_iとc_iに相関関係がない場合，すなわちw_iとc_iが互いに独立である場合には，式()の分子P(w_i|l_i[c_i])は分母P(w_i|l_i)にほぼ等しくなり，従属係数D(w_i|l_i[c_i])は1に近い値を取る．これに対し，w_iとc_iに正の相関関係がある場合には，単語生成文脈c_iを前件に加えた確率P(w_i|l_i[c_i])は単語生成文脈c_iを無視した確率P(w_i|l_i)よりも大きくなるので，その従属係数は1より大きい値を取る．同様に，w_iとc_iに負の相関関係がある場合には従属係数は1より小さい値を取る．複数の単語生成文脈c_1,,c_nの下での単語w_iの生成確率は，単語生成文脈を無視した単語の生成確率P(w_i|l_i)と，w_iとc_iの相関関係を他の単語生成文脈とは独立に評価した従属係数D(w_i|l_i[c_i])の積によって計算できることを式()は示している．従属係数D(w_i|l_i[c_i])を他の単語生成文脈と独立に推定・学習することにより，確率モデルのパラメタ空間を推定可能な大きさに抑制することができる．例えば，図の例において，``彼女''の生成確率()と``は''の生成確率()はそれぞれ以下のように推定される．&amp;&amp;P(彼女|N[食べは,出かけは])&amp;&amp;P(彼女|N)D(彼女|N[食べは)D(彼女|N[出かけは])[3mm]&amp;&amp;P(は|P[食べ2_1,を,出かけ2_1,へ])&amp;&amp;P(は|P)D(は|P[食べ2_1,を])D(は|P[出かけ2_1,へ])eqnarray従属係数を導入する利点として，単語生成文脈を複数取り扱うことができるという点の他に，式()に示すように，語彙モデルP(W|R)を単語の出現頻度のみを反映したP_cf(W|L)と単語の共起関係のみを反映したD(W|R)との積に分解できるという点が挙げられる．P(W|R)&amp;&amp;_iP(w_i|l_i[C_w_i])&amp;&amp;_w_iP(w_i|l_i)_c_ijC_w_iD(w_i|l_i[c_ij])&amp;=&amp;P_cf(W|L)D(W|R)[3mm]P_cf(W|L)&amp;=&amp;_w_iP(w_i|l_i)[3mm]D(W|R)&amp;=&amp;_w_i_c_ijC_w_iD(w_i|l_i[c_ij])eqnarray上式において，C_w_iは単語w_iの単語生成文脈の集合を表わしている．式()の統計量P_cf(W|L)は，単語生成文脈を無視したときに品詞l_iから単語w_iが生成される確率の積であり，単語の出現頻度に関する優先度が反映される．これに対し，式()の統計量D(W|R)は各単語w_iとその単語生成文脈c_ijの従属係数の積を表わしており，w_iとc_ijの相関関係に関する優先度(すなわち単語の共起関係)が反映される．このように，語彙モデルを単語の出現頻度，および単語の共起関係のみを反映させた2つの統計量の積として分解することにより，節で述べたように，曖昧性解消時におけるそれぞれの統計情報の働きを容易に理解することができる．</subsubsection>
  <section title="評価実験">本節では，前節で提案した統合的確率言語モデルの評価実験について述べる．統合的確率言語モデルは本来形態素解析，構文解析を同時に行うことを前提としているが，そのような大規模な実験を行う前の予備実験として，まずは文節列を入力とする文節間の係り受け解析のみを行った．</section>
  <subsection title="構文モデルの学習">本節の実験では，入力として単語列，品詞列，文節区切りが与えられたときに，それぞれの文節の係り先となる文節を決定する．このような文節の係り受け解析をCFG(文脈自由文法)を用いて行った．まず，CFG規則の終端記号として，文節の統語的特性を反映した文節ラベルを用いる．この文節ラベルの定義を()に示す．ここで，``受け属性''，``係り属性''はそれぞれ文節の受け属性と係り属性であり，``連用''，``連体''，``格関係''の組によって表わされる．例えば，``パイ-を''や``彼女-の''など，「名詞助詞」といった品詞並びによって構成される文節は，他の文節から連体修飾を受ける可能性があるので受け属性は``(連体)''となり，他の文節を連体修飾したり用言を修飾してその格要素および表層格を表わす可能性があるので係り属性は``(連用,格関係)''となる．また``読点の有無''は，その文節の末尾が読点であれば``1''，そうでなければ``0''といった値を取る．これは，読点を末尾に持つ文節は直後の文節には係りにくく，読点を末尾に持たない文節よりも遠くに係る傾向があるので，この違いを構文モデルに反映させるためである．一方``用言種別''は，``格関係''を受け属性に含む文節タイプを細分化するための属性であり，文節の主辞が自動詞，他動詞，形容詞，名詞述語のときにはそれぞれ``自動詞''，``他動詞''，``形容詞''，``名詞述語''といった値を取る．また，``格関係''を受け属性に持たない文節のときにはその値は常に``φ''である．節で例示した単語生成文脈決定規則は，単語の共起関係の中でも特に用言の格関係に注目している．用言を主辞とする文節の文節ラベルを細分化したのはこのためである．この文節ラベルは，文節を構成する単語列の品詞情報をもとに一意に決定されるものとする．また，これらの文節ラベルの整合性をチェックする規則を作成し，その集合を文節の係り受け解析に用いるCFGとした．このCFGの概要を表に示す．本実験では，構文モデルP(R)としてPGLRを利用した．また，この構文モデルの学習には京大コーパスを使用した．京大コーパスの各例文には，単語区切り，単語の品詞，文節区切りと文節の係り受け解析の結果(構文構造)が付加されている．京大コーパスの9,944例文に対して，コーパスの各例文とそれに付加された構文構造を作り出すようなLR表における状態遷移列を求め，また状態遷移が行われた回数を数え上げる．このようにして得られた状態遷移回数を状態遷移確率に変換することにより，PGLRのパラメタ推定を行った．</subsection>
  <subsection title="語彙モデルの学習">本実験では，式()に示した語彙モデルP(W|R)=P_cf(W|L)D(W|R)のうち，P_cf(W|L)の計算を省略できる．なぜなら，単語列及び品詞列はすでに入力として与えられているため，全ての解析結果の候補について品詞から単語への生成確率の積P_cf(W|L)は等しいからである．したがって，語彙モデルとして学習するのは従属係数の積D(W|R)のみでよい．今回の実験では，lc:filler〜#によって定められる従属係数(),(),()をD(W|R)の要素とし，これらの学習を行った．まず，格要素の従属係数()の学習について説明する．RWCコーパス~とEDR共起辞書から，名詞nが助詞pを介して動詞vに係る事例(n,p,v)をそれぞれのべ6,888,849組，975,510組収集した．式()の分子および分母の確率モデルはこれらの訓練事例から最尤推定した．さらに，分子の確率モデルP(n|N[vp])を推定する際に以下のような近似を行った．名詞nの意味クラスによる抽象化名詞nの意味クラスの集合をC_n=c_n_1,,c_n_mとして，P(n|N[vp])を以下のように推定した．今回の実験では，名詞意味クラスc_nとして，日本語語彙体系の名詞シソーラスのルートから深さ3に位置する151個の意味クラスの集合を用いた．これらの意味クラスは互いに排他的である．D(n|N[vp])を推定する場合，名詞nが日本語語彙体系に登録されておらず，その名詞意味クラスが不明な場合には，その従属係数は学習不可能であるとしてD(n|N[vp])1とした．これは，nとvpとの間の従属関係を無視することに相当する．バックオフ方式によるスムージング確率モデルP(c_n|N[vp])を推定する際，この確率の分母となる事例(*,p,v)(*は任意の名詞意味クラスを表わす)の出現回数がある閾値よりも小さい場合には，vを動詞意味クラスc_vを用いて抽象化した確率モデルP(c_n|N[c_vp])によって近似した．また，事例(*,p,c_v)の出現回数がを越えない場合には，動詞意味クラスc_vの抽象度を段階的に上げていき，必ず個以上の訓練事例から確率モデルを推定するようにした．本実験においては，動詞意味クラスc_vとして分類語彙表~の5桁および2桁の分類コードを動詞意味クラスとして利用した．動詞を分類語彙表の2桁の分類コードに抽象化しても学習事例数がを越えなかったとき，もしくは(*,p,v)の事例数が以下でありかつ動詞vが分類語彙表に登録されていなかった場合には，十分信頼度の高い確率モデルが学習できなかったとして，従属係数D(n|N[vp])1とした．なお，今回は=100として実験を行った．次に，用言に係る助詞に関する従属係数()の学習について説明する．n個の助詞p_1,,p_nが同じ用言hに係っている場合には，それぞれのp_iに対応する従属係数()の積を計算すれば良い．この従属係数の積は式()のように変形できる．&amp;&amp;_iD(p_i|P[hn_1,,_i-1,p_i,,p_n])&amp;=&amp;_iP(p_i|P[hn_1,,_i,p_i+1,,p_n])P(p_i|P)&amp;=&amp;P(p_1,,p_n|P_1,,P_n[hn_1,,_n])_iP(p_i|P)&amp;def=&amp;D(p_1,,p_n|P_1,,P_n[hn_1,,_n])eqnarrayしたがって，学習しなければならないのは，ある用言hがP_1,,P_nのn個の助詞の係り先となっているときに単語p_1,,p_nを同時に生成する確率モデルP(p_1,,p_n|P_1,,P_n[hn_1,,_n])と，品詞P(助詞)から単語p_iが生成される確率P(p_i|P)である．以降，簡単のため，前者の確率モデルを以下のように記述する．P(p~|~h,n)&amp;def=&amp;P(p_1,,p_n|P_1,,P_n[hn_1,,_n])&amp;&amp;但し，~p=(p_1,,p_n)eqnarray確率モデルP(p|h,n)を学習するために，n個の助詞pが同じ用言hに係るという事例(p,h)をEDRコーパスから収集した．今回の実験では，用言hとして動詞，形容詞，名詞述語の3つを考えた．用言hが動詞，形容詞，名詞述語であるときの，またhに係る助詞の数nが1，2，3，4以上であるときの事例(p,h)ののべ数を表にまとめる．nが4以上のときには学習に十分な事例を収集することができなかった．そこで，nが4以上のときには，従属係数を1，すなわち助詞とその係り先用言との語彙的従属関係や格間の従属関係を無視することにした．n=1のときの式()の分子の確率モデルP(p|h,n)は表に示した事例から最尤推定した．また，n=2,3のときの確率モデルP(p|h,n)は最大エントロピー法を用いて推定した．最後に，体言に係る助詞に関する従属係数()の学習について説明する．この従属係数を学習するために，EDRコーパスから体言に係る助詞pをのべ273,062個収集した．式()の分子はこの訓練データから最尤推定した．また，式()の分母P(p|P)は，ここで収集した体言に係る助詞の事例と，表に示した用言に係る助詞の事例から，同様に最尤推定した．尚，式()の分母の各項P(p_i|P)も式()の分母の確率モデルと同じものを使用した．</subsection>
  <subsection title="実験結果">節にて学習した構文モデルP(R)，および節にて学習した語彙モデルP(W|R)を用いて，文節の係り受け解析を行った．まず，テスト文として，京大コーパスの中から文節数7〜9の文をランダムに500文選び，これをテスト文とした．構文モデルP(R)を学習する際に用いた訓練用例文にはこれらのテスト文は含まれていない．文節数7〜9という比較的文長の短い例文をテスト文として選んだのは，本実験で用いたPGLRパーザがまだ開発の途中であり，長い文長の例文の解析に非常に多くの時間を要するためである．テスト文の係り受け解析結果の評価尺度として，文節の正解率を以下のように定義する．この文節の正解率は生成確率が一位である解析結果の候補について計算する．また，文の最後に位置する2つの文節は評価の対象から除外する．これは，文の一番最後にある文節は係り先がなく，また文の最後から2番目にある文節は常に文の一番最後の文節に係るからである．節に述べたように，語彙モデルにおいてはいくつかの種類の統計情報を取り扱う．ここでは，構文的な統計情報，および語彙モデルにおいて考慮された語彙的な統計情報のそれぞれの曖昧性解消における効果を調べるために，以下に述べる6種類のモデルを用意し，それらを比較した．結果を表に示す．表から，語彙モデルにおいて考慮した語彙的な統計情報のうち，体言に係る助詞に関する従属係数(モデルP)が正解率の向上に一番大きく貢献することがわかる．すなわち，助詞が用言に係っているか否かの違いがその生成確率に大きく影響し，その違いを考慮することによって曖昧性解消の精度を大きく向上させることができた．また，表における``後置詞節''とは，``彼女-が''，``パイ-を''など，用言の格要素および表層格を表わす可能性のある文節を指す．テスト文全体における2,975個の文節のうち，1,788個がこの後置詞節に相当する．この後置詞節のみで評価した場合，全ての文節で評価した場合に比べて，語彙的な統計情報を考慮したモデル(F,M,P,all)と構文的な統計情報のみを考慮したモデル(Syn)との文節の正解率の差が大きくなっている．これは，今回の実験で用いた語彙モデルにおいては，語彙的な統計情報の中でも用言の格関係に注目しているため，語彙モデルが``後置詞節''の係り先の曖昧性解消に特に有効に働いているためと考えられる．構文モデルと全ての語彙的従属関係を考慮した語彙モデルを組み合わせて曖昧性解消に用いた場合(all)，構文モデルのみを用いた場合(Syn)と比べて文節の正解率が10.96%向上し，また構文モデルのみを曖昧性解消に用いたときのベースラインとの文節の正解率の差が11.70%であることから，文節の係り受け解析の精度向上において，語彙モデルは構文モデルと同程度の貢献をしていると考えられる．本研究で提案した統合的確率言語モデルにおいては，語彙的な統計情報を局所化し構文的な統計情報とは独立に学習しているが，このようなアプローチにおいても，語彙的な統計情報は曖昧性解消の精度向上に十分大きく貢献すると期待できる．最後に，本研究で提案する統合的確率言語モデルを用いた解析結果とKNPパーザによる解析結果との比較を行った．KNPパーザは形態素解析システムJUMANの形態素解析結果を入力とし，文節の区切りを認定してから文節の係り受け解析を行う．そこで，節の実験で用いた500個のテスト文のうち，JUMANの形態素解析結果による形態素区切りおよびKNPパーザによる文節区切りの結果がコーパスと一致した388文を対象に，両者の係り受け解析結果の比較を行った．結果を表に示す．本手法はKNPパーザよりも文節の正解率で1%程度劣っている．今回の実験では，統合的確率言語モデルに組み込む語彙的従属関係として，格要素と動詞との従属関係，助詞と係り先用言との従属関係，格間の従属関係などを考慮した．しかしながら，これ以外にも，曖昧性解消に有効であると考えられる語彙的従属関係が数多く存在する．特に，今回の実験では連体修飾に関しては語彙的従属関係を何も考慮していないので，そのことによる解析誤りが多かった．例えば，「彼女の紫色の帽子が風に飛ばされた」という文においては，文節``彼女-の''が(a)``紫色-の''に係る，(b)``帽子-が''に係るという2つの解釈がある．ところが，連体修飾する``彼女''については語彙的従属関係を考慮していないので，より近い文節に係る解釈(a)に高い確率が与えられてしまう．これを回避するためには，以下のような従属係数を学習し語彙モデルに加えればよい．式()の分子P(n_1|N[n_2])は，ある名詞Nがn_2を連体修飾しているとき，その名詞として単語n_1が生成される確率を表わしている．このような従属係数を考慮することにより，``彼女''は``紫色''よりも``帽子''を連体修飾することが多い，すなわちD(彼女|N[紫色])&lt;&lt;D(彼女|N[帽子])であると考えられるので，正しい解釈(b)に高い確率を与えると期待できる．このように，統合的確率言語モデルに新たな種類の語彙的従属関係を反映させるときには，それに対応した従属係数を新たに語彙モデルに加えるという形で容易に対処できる．これは，語彙的従属関係を局所化して構文的優先度などの他の統計情報と独立に学習するように，また異なる種類の語彙的従属関係は異なる従属係数として独立に学習するようにモデルを設計したことに依る．一方，後置詞節のみで評価した場合には，本手法とKNPパーザの文節の正解率はほぼ等しい．とはいえ，後置詞節の係り先の特定に失敗する場合も少なくない．我々は現在その原因を調査中であり，その一部については既に報告している．今後，曖昧性解消に有効な統計情報を新たに組み込んだり，また解析誤りの原因を調査しそれらに対処することにより，係り受け解析の精度向上を図っていきたい．</subsection>
  <section title="おわりに">本研究では，形態素解析・構文解析を同時に行う際に，構文的な統計情報と語彙的な統計情報を組み合わせて曖昧性を解消するひとつの手法を提案した．我々の手法の特徴は，構文的優先度，隣接する品詞間の共起関係，距離に関する優先度といった構文的な統計情報を構文モデルP(R)として，単語の出現頻度および単語の共起関係を語彙モデルP(W|R)として，それぞれ独立に学習する点にある．このことは，個々の統計情報を異なる言語資源から学習できるだけでなく，曖昧性解消時における個々の統計情報の働きを容易に分析することができる．実際に，京大コーパスを用いて構文モデルを，RWCコーパスやEDRコーパスを用いて語彙モデルを学習した．また，これらの確率モデルを用いた日本語文の文節の係り受け解析実験の結果，構文的な統計情報と語彙的な統計情報のそれぞれが曖昧性解消に大きく貢献することを確認した．最後に今後の課題について述べる．まず，統合的確率言語モデルが本来想定している形態素解析と構文解析を同時に行い，その有効性を実験的に確認することが挙げられる．また，今回の実験では文長の比較的短い文を対象にしたが，文長の長い文の係り受け解析を行うことにより，統合的確率言語モデルの特性をさらに調査する必要がある．文長の長い文においては，二重格を取りにくいなどの格間の従属関係がさらに有効に働くのではないかと予想される．最後に，統合的確率言語モデルと他の統計的構文解析に関する研究とを実験的に比較することが挙げられる．特に今回の実験は日本語を対象にしたが，構文的な統計情報と語彙的な統計情報を独立に学習するアプローチが英語などの他の言語においても本当に有効であるのかどうかは今後調査していく必要があると思われる．</section>
</root>
