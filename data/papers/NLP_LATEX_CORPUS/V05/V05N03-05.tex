



\documentstyle[jnlpbbl]{jnlp_j}

\setcounter{page}{1}
\setcounter{巻数}{5}
\setcounter{号数}{3}
\setcounter{年}{1998}
\setcounter{月}{7}
\受付{}{}{}
\再受付{}{}{}
\採録{}{}{}


\title{統計的構文解析における構文的統計情報と\\
  語彙的統計情報の統合について}
\author{白井 清昭\affiref{TITECH} \and
  乾 健太郎\affiref{KYUTECH} \and
  徳永 健伸\affiref{TITECH} \and
  田中 穂積\affiref{TITECH}}

\headauthor{白井，乾，徳永，田中}
\headtitle{統計的構文解析における構文的統計情報と語彙的統計情報の統合について}

\affilabel{TITECH}{東京工業大学大学院 情報理工学研究科 計算工学専攻}
{Department of Computer Science,
  Graduate School of Information Science and Engineering,
  Tokyo Institute of Technology}
\affilabel{KYUTECH}{九州工業大学 情報工学部 知識情報工学科}
{Department of Artificial Intelligence, Kyushu Institute of Technology}

\jabstract{
\quad 
本論文では，構文解析の曖昧性解消を行うために，
構文的な統計情報と語彙的な統計情報を統合する手法を提案する．
我々が提案する統合的確率言語モデルは，
構文的優先度などの構文的な統計情報を反映する構文モデルと，
単語の出現頻度や単語の共起関係などの語彙的な統計情報を反映する
語彙モデルの2つの下位モデルから成る．
この統合的確率言語モデルは，
構文的な統計情報と語彙的な統計情報を同時に学習する
過去の多くのモデルと異なり，
両者を個別に学習する点に特徴がある．
構文的な統計情報と語彙的な統計情報を独立に取り扱うことにより，
それぞれの統計情報を異なる言語資源から独立に学習することが
できるだけでなく，
それぞれの統計情報が曖昧性解消においてどのような効果を
果たすのかを容易に分析することができる．
この統合的確率言語モデルを評価するために，
日本語文の文節の係り受け解析を行った．
構文モデルを用いたときの文節の正解率は73.38\%となり，
ベースラインに比べて11.70\%向上した．
また，構文モデルと語彙モデルを組み合わせることにより，
文節の正解率はさらに10.96\%向上し84.34\%となった．
この結果，本研究で提案する枠組において，
語彙的な統計情報は構文的な統計情報と
同程度に曖昧性解消に貢献することを確認した．
}

\jkeywords{統計的構文解析，構文的統計情報，語彙的統計情報，統合的確率言語モデル}

\etitle{A Framework of Integrating Syntactic and\\Lexical Statistics in Statistical Parsing}
\eauthor{Kiyoaki Shirai\affiref{TITECH} \and
  Kentaro Inui\affiref{KYUTECH} \and
  Takenobu Tokunaga\affiref{TITECH} \and
  Hozumi Tanaka\affiref{TITECH}}

\eabstract{
\quad
In this paper,
we propose a new framework of statistical language modeling
integrating syntactic statistics and lexical statistics.
Our model consists of two submodels,
the syntactic model and lexical model.
The syntactic model reflects syntactic statistics,
such as structural preferences,
whereas the lexical model reflects lexical statistics,
such as the occurrence of each word and word collocations.
One of the characteristics of our model
is that it learns both types of statistics separately,
although many previous models learn them simultaneously.
Learning each submodel separately enables us to
use a different language source for different submodels,
and to make understanding of each submodel's behavior
much easier.
We conducted a preliminary experiment,
where our model was applied to the disambiguation
of dependency structures of Japanese sentences.
The syntactic model achieved 73.38\% in {\it Bunsetu\/} phrase accuracy,
which is 11.70 points above the baseline,
and when incorporating the lexical model with the syntactic model,
further 10.96 point gain was achieved, to 84.34\%.
Thus the contribution of lexical statistics for disambiguation
is as great as that of syntactic statistics in our framework.
}

\ekeywords{statistical parsing, syntactic statistics, lexical statistics, integrated probabilistic language model}


\newcounter{lc-rule-number}
\newcommand{\lcrule}[1]{}
\newenvironment{lcdef}[1]{}{}
\newcommand{\scl}{}
\newcommand{\slot}[2]{}
\newcommand{\head}[3]{}
\newcommand{\morph}[2]{}
\newcommand{\nont}[4]{}
\newcommand{\pret}[4]{}
\begin{document}
\thispagestyle{myheadings}
\maketitle

\section{はじめに}
\label{sec:intro}

コーパス，辞書，シソーラスなどの機械可読な言語データの
整備が進んだことから，
自然言語処理における様々な問題の解決に
何らかの統計情報を利用した研究が盛んに行われている．
特に構文解析の分野においては，構文的な統計情報だけでなく，
単語の出現頻度や単語の共起関係といった語彙的な統計情報を利用して
解析精度を向上させた研究例が数多く報告されている
\cite{schabes:92:a,magerman:95:a,hogenout:96:a,li:96:a,charniak:97:a,collins:97:a}．
ここで問題となるのは，このような語彙的な統計情報を
構文的な統計情報とどのように組み合わせるかということである．
このとき，我々は以下の2つの点が重要であると考える．

\begin{itemize}

\item
  解析結果の候補に与えるスコアが，
  構文的な統計情報のみを反映したスコアと
  語彙的な統計情報のみを反映したスコアから
  構成的に計算できること

  このことによる利点を以下に挙げる．
  \begin{itemize}
  \item 個々の統計情報を個別に学習できる

    構文的な統計情報を学習する際には，
    学習用言語資源として比較的作成コストの高い
    構文構造が付加されたコーパスが必要となる\footnote{
      Inside-Outsideアルゴリズム\cite{lari:90:a} に代表されるような
      EMアルゴリズムを用いて，
      構文構造が付加されていないコーパスから
      構文的な統計情報を学習する研究も行われている．
      しかしながら，
      このような教師なしの学習は一般に精度が悪く，
      現時点では構文構造が付加されたコーパスを利用した方が
      品質の良い統計情報を学習できると考えられる．
      }．
    しかしながら，
    推定パラメタの数はそれほど多くはないので，
    比較的少ないデータ量で学習することができる．
    これに対して，語彙的な統計情報は，
    単語の共起に関する統計情報を学習しなければならないために
    大量の学習用データを必要とするが，
    構文構造付きコーパスに比べて作成コストの低い
    品詞付きコーパスを用いても学習することが十分可能である．
    このように，
    統計情報の種類によって学習に要する言語資源の質・量は大きく異なる．
    そこで，構文的な統計情報と語彙的な統計情報を
    異なる言語資源を用いて個別に学習できるように，
    それぞれの統計情報の独立性を保持しておくことが望ましい．

  \item
    曖昧性解消時における個々の統計情報の働きを
    容易に理解することができる

    例えば，曖昧性解消に失敗した場合には，
    構文的な統計情報と語彙的な統計情報を独立に取り扱うことにより，
    どちらの統計情報が不適切であるかを容易に判断することができる．
  \end{itemize}

\item 
  個々の統計情報を反映したスコアが確率的意味を持っていること

  構文的な統計情報を反映したスコアと
  語彙的な統計情報を反映したスコアを組み合わせて
  全体のスコアとする場合，
  両者のスコアの和を計算すればいいのか，
  積を計算すればいいのか，
  またどちらか片方に重みを置かなければならないのかなど，
  その最適な組み合わせ方は自明ではない．
  このとき，個々のスコアが確率的意味を持つように学習することにより，
  確率の積としてそれらを自然に組み合わせることができる．
\end{itemize}

ところが，語彙的な統計情報を利用して構文解析の精度を向上させる
過去の研究の多くは以上の条件を満たしていない．
例えば田辺らは，確率文脈自由文法
(Probabilistic Context Free Grammar, 以下PCFG)における
書き換え規則の非終端記号に，
その非終端記号が支配する句の主辞となる単語を付加すること
(以下，これをPCFGの語彙化と呼ぶ)によって
語彙的従属関係をPCFGの確率モデルに反映させる方法を
提案している~\cite{tanabe:95:a}．
一方，英語を対象にPCFGを語彙化した研究としては
Hogenoutら~\cite{hogenout:96:a}，
Charniak~\cite{charniak:97:a}，Collins~\cite{collins:97:a} に
よるものがある．
しかしながら，
PCFGの語彙化によって構文的な統計情報と
語彙的な統計情報を組み合わせる方法は，
非終端記号に単語を付加することによって規則数が組み合わせ的に増大し，
推定するパラメタ数も非常に多くなるといった問題点がある．
また，構文的な統計情報と語彙的な統計情報を同時に学習する
モデルとなっているが，
先ほど述べたように両者は独立に学習できることが望ましい．
PCFGをベースとしないSPATTERパーザ~\cite{magerman:95:a}や
SLTAG~\cite{schabes:92:a,resnik:92:b}にも同様の問題が存在する．
これらの研究は語彙的な統計情報を利用して解析精度の向上を
図ってはいるが，
構文的な統計情報と独立に学習する枠組にはなっていない．

構文的な統計情報と語彙的な統計情報を独立に学習する枠組としては
Liによるものが挙げられる~\cite{li:96:a,li:96:b}．
Liは，解析結果の候補$I$ に対して，
構文的な統計情報を反映させた確率モデル$P_{syn}(I)$ と
単語の共起関係を反映させた確率モデル$P_{lex}(I)$ を
別々に学習する方法を提案している．
そして，
語彙的な制約は構文的な制約に優先するといった心理言語学原理に基づき，
まず$P_{lex}(I)$ を$I$ のスコアとして用い，
一位とそれ以外の候補のスコアの差が十分に大きくなかった場合に限り
$P_{syn}(I)$ をスコアとして用いている．
すなわち，構文的な統計情報と語彙的な統計情報を
それぞれ独立に学習してはいるが，
これらを同時に利用して曖昧性解消を行っているわけではない．
また，この2つのスコアの持つ確率的意味が不明確であり\footnote{
  $P_{syn}(I)$，$P_{lex}(I)$ は確率と呼ばれてはいるが，
  どのような事象に対する確率なのかは不明である．
  }，
その最適な組み合わせ方は自明ではない．


本研究では，
構文的な統計情報と語彙的な統計情報を組み合わせる一方法として，
統合的確率言語モデルを提案する~\cite{inui:97:b,inui:97:e,sirai:96:a}．
この統合的確率言語モデルの特徴は，
単語の出現頻度，および単語の共起関係といった
2つの語彙的な統計情報を局所化し，
構文的な統計情報と独立に取り扱う点にある．
また，構文的な統計情報を構文構造の生成確率として，
語彙的な統計情報を単語列の生成確率としてそれぞれ学習し，
これらの積を解析結果の候補に対するスコアとすることにより，
曖昧性解消に両者を同時に利用することができる．
この統合的確率言語モデルの詳細については\ref{sec:model} 節で述べる．
\ref{sec:exp-stat} 節ではこの統合的確率言語モデルの学習，
およびそれを用いた日本語文の文節の係り受け解析実験について述べる．
最後に\ref{sec:conclusion} 節で結論と今後の課題について述べる．
\section{統合的確率言語モデル}
\label{sec:model}

まず，本論文で一貫して用いる記号について説明する．

\begin{itemize}
\item 入力文字列 $A=a_1,\cdots,a_m$
\item $A$ を生成する単語列 $W=w_1,\cdots,w_n$
\item $W$ を生成する品詞列 $L=l_1,\cdots,l_n$
\item $L$ を生成する構文構造 $R$
\end{itemize}
本研究では，形態素解析と構文解析を同時に取り扱うことを仮定する．
すなわち，入力文字列$A$ が与えられたときに，
その正しい単語列$W$，正しい品詞列$L$，
正しい構文構造$R$ を求めることを目的とする．
例えば，「彼女がパイを食べた」という入力文に対する
解析結果の候補の例を図\ref{fig:examsent} に示す．

\begin{figure}[htbp]
  \begin{center}
    \leavevmode
    \atari(91,37)


    \caption{例文``彼女がパイを食べた''とその解析結果}
    \label{fig:examsent}
  \end{center}
\end{figure}

各解析結果の候補に対してその生成確率$P(R,L,W,A)$ を計算し，
これが最大の解析結果を選択することによって
曖昧性解消を行う．
さらに，確率モデル$P(R,L,W,A)$ を以下のように分解する．
\begin{equation}
  \label{eq:model-org}
  P(R,L,W,A) = P(R) \cdot P(L|R) \cdot P(W|L,R) \cdot P(A|W,L,R)
\end{equation}
ここで，構文構造$R$ は最終的に品詞列$L$ を生成するものと
仮定すると，$P(L|R) = 1$ となる(図\ref{fig:examsent} 参照)．
また，単語列$W$ が決まれば入力文字列$A$ は一意に決まるので，
$P(A|W)=1$ となる．
したがって，式(\ref{eq:model-org})は以下のように簡略化できる．
\begin{equation}
  \label{eq:model}
  P(R,L,W,A) = P(R) \cdot P(W|R)
\end{equation}
本研究では，式(\ref{eq:model})に示した通り，
解析結果の生成確率を以下の2つの確率モデルの積として計算する．
\begin{enumerate}
\item 構文モデル$P(R)$

  構文構造$R$ の生成確率である．
  この確率モデルには構文的な統計情報を反映させる．

\item 語彙モデル$P(W|R)$

  構文構造$R$ が与えられたときに，
  それから単語列$W$ を生成する確率である．
  この語彙モデルには語彙的な統計情報を反映させる．
\end{enumerate}
\subsection{構文モデル$P(R)$}
\label{sec:syn-model}

構文モデルとしては，構文的な統計情報を反映し，
かつ構文構造$R$ の生成確率を高い精度で推定するものであれば，
どのような確率モデルを利用してもよい．
構文モデルに利用できる確率モデルとしては，
PCFGや確率一般化LR法(Probabilistic Generalized LR Method，
以下PGLR)
などが挙げられる．

我々は，PGLRを構文モデルの有力な候補として考えている．
PGLRとは，構文解析手法のひとつである一般化LR法を拡張したものである．
PGLRは，LR表に記述された各状態遷移の遷移確率を推定し，
その遷移確率の積によって1つの状態遷移列，すなわち
それに対応する構文構造の生成確率を与えるモデルである\footnote{
  一般化LR法に確率を組み込む試みには様々なものが
  あるが~\cite{wright:90:a,ng:91:a,briscoe:93:a}，
  本研究におけるPGLRとはInuiらによるモデル~\cite{inui:97:d,virach:97:c}
  を指す．
  }．
このPGLRはPCFGに比べて，次のような特長を持つ~\cite{inui:97:d}．

\begin{itemize}
\item 文脈依存性を取り扱うことができる．
\item 隣接する品詞間の共起関係を取り扱うことができる．
\item 距離に関する優先度を取り扱うことができる．
\end{itemize}
ここで，隣接する品詞間の共起関係とは，
品詞bi-gramのような品詞列の出現に関する統計情報であり，
形態素解析の曖昧性解消に有効であると考えられる．
また，距離に関する優先度とは，
単語はなるべく近い単語に係りやすいといった，
係り受け関係にある単語間の距離に関する統計情報である．
\subsection{語彙モデル$P(W|R)$}
\label{sec:lex-model}

語彙モデルは，品詞列$L$ を末端とする構文構造$R$ が与えられたときに，
それから単語列$W$ を生成する確率である．
この語彙モデルは，式(\ref{eq:lex-model-org})のような
各単語$w_i$ の生成確率の積として計算することができる．
\begin{equation}
  \label{eq:lex-model-org}
  P(W|R) = \prod_{w_i} P(w_i | R,w_1,\cdots,w_{i-1})
\end{equation}
例えば，図\ref{fig:examsent} の例において，
単語を文の後ろから順番に生成していくと仮定すると，
語彙モデル$P(W|R)$ は以下のような単語の生成確率の積として計算できる．
\begin{eqnarray}
  P(W|R) &=& P(彼女,が,パイ,を,食べ,た|R) \\
  \label{eq:der1-ta}
  &=& P(た|R) \cdot \\
  \label{eq:der1-tabe}
  && P(食べ|R,た) \cdot \\
  \label{eq:der1-o}
  && P(を|R,食べ,た) \cdot \\
  \label{eq:der1-pai}
  && P(パイ|R,を,食べ,た) \cdot \\
  \label{eq:der1-ga}
  && P(が|R,パイ,を,食べ,た) \cdot \\
  \label{eq:der1-kanojo}
  && P(彼女|R,が,パイ,を,食べ,た)
\end{eqnarray}
\subsubsection{単語生成文脈}
\label{eq:lexical-context}

式(\ref{eq:lex-model-org})の各項
(図\ref{fig:examsent} の例では
式(\ref{eq:der1-ta})〜(\ref{eq:der1-kanojo}))
のパラメタ空間は非常に大きく，
これを直接学習することは一般に不可能である．
ところが，各単語$w_i$ の生成に強く影響するのは
各項の確率の前件$R,w_1,\cdots,w_{i-1}$ 全てではなく，
その一部のみであると考えられる．
例えば，図\ref{fig:examsent} の例文において，
``パイ''は動詞``食べ''のヲ格の格要素となっている．
このとき，``パイ''という単語を生成する際には，
式(\ref{eq:der1-pai})の前件``$R,を,食べ,た$''
(図\ref{fig:lc-pai} の斜線部)のうち，
品詞$N$ と単語``を'',``食べ''
(図\ref{fig:lc-pai} の丸で囲まれた部分)によって
十分近似できると期待できる(式(\ref{eq:lc-pai1}))．
\begin{equation}
  \label{eq:lc-pai1}
  P(パイ | R, を, 食べ, た) \simeq P(パイ | N[\slot{食べ}{を}])
\end{equation}

\begin{figure}[htbp]
  \begin{center}
    \leavevmode
    \atari(78,29)
    
    \caption{``パイ''を生成するときの単語生成文脈}
    \label{fig:lc-pai}
  \end{center}
\end{figure}

\noindent
式(\ref{eq:lc-pai1})において，$N[\slot{食べ}{を}]$ は，
``食べ''という動詞のヲ格の格要素となっている
名詞を表わしている．
すなわち，$P(パイ | N[\slot{食べ}{を}])$ は，
``食べ''という動詞のヲ格の格要素となっている名詞から
``パイ''という単語が生成される確率を表わしている．
したがって，式(\ref{eq:lc-pai1})には，
``パイ''という単語そのものがどれくらい出現しやすいかといった
単語の出現頻度と，
``パイ''と``食べ''がどの程度共起しやすいかといった
単語の共起関係が反映されている．

ここで，単語生成文脈$c_i$ を以下のように定義する．
\begin{quote}
  単語$w_i$ の単語生成文脈$c_i$ とは，
  $w_i$ の生成確率の前件$R,w_1,\cdots,w_{i-1}$ から
  $w_i$ の生成に強く影響する部分のみを取り出したものである．
\end{quote}
先ほどの例においては，単語``パイ''の単語生成文脈は
``$\slot{食べ}{を}$''である．
そして，
各単語$w_i$ の生成確率の前件``$R,w_1,\cdots,w_{i-1}$''を，
その単語の品詞$l_i$ と単語生成文脈$c_i$ に縮退することにより，
語彙モデル$P(W|R)$ を以下のように近似する．
\begin{eqnarray}
  \nonumber
  P(W|R) &=& \prod_{w_i} P(w_i | R,w_1,\cdots,w_{i-1}) \\
  \label{eq:lex-model}
  &\simeq& \prod_{w_i} P(w_i | l_i[c_i])
\end{eqnarray}
\subsubsection{単語生成文脈決定規則}
\label{sec:lc-rule}

単語生成文脈を導入する際に問題となるのは，
どのような単語に対して
どのような単語生成文脈を選べばよいのかということである．
我々は，これを人手で作成した規則によって記述する．
以下，単語$w_i$ の単語生成文脈$c_i$ を決定する規則を
単語生成文脈決定規則と呼ぶ．

単語生成文脈決定規則の例を以下に挙げる．

\begin{itemize}
\item 単語の共起関係を全く考慮しない場合

  単語$w_i$ について，
  周囲の単語との従属関係を考慮しない場合には，
  その単語の生成確率はその単語の品詞$l_i$ のみに依存するとみなす．
  例えば，図\ref{fig:examsent} の例において，
  助動詞``た''と動詞``食べ''を生成する際に
  他の単語との語彙的従属関係を考えない場合には，
  それぞれの生成確率(\ref{eq:der1-ta}),(\ref{eq:der1-tabe})は
  以下のように近似すればよい．
  \begin{eqnarray}
    \label{eq:der2-ta}
    P(た|R) &\simeq& P(た|AUX) \\
    \label{eq:der2-tabe}
    P(食べ|R,た) &\simeq& P(食べ|V)
  \end{eqnarray}

  これに対応した単語生成文脈決定規則を以下に示す．
  この規則は単語生成文脈を決定する際のデフォルト規則でもある．

  \bigskip
  \begin{lcdef}{lc:default}
    単語$w_i$ を生成する際に他の単語との従属関係を考慮しない場合には，
    単語$w_i$ の単語生成文脈$c_i$ を空とする．
  \end{lcdef}
  \bigskip

\item 格要素となる名詞が助詞を介して動詞に係る際の従属関係を
  考慮する場合

  前述のように，格要素となる名詞が助詞を介して動詞に係る際には，
  動詞・助詞の組と名詞との間には語彙的従属関係が存在する．
  このような語彙的従属関係を確率モデルに反映させるために
  \lcrule{lc:filler} を定義する．
  
  \bigskip
  \begin{lcdef}{lc:filler}
    単語$w_i$ の品詞$l_i$が$N$(名詞)であり，
    かつ助詞$p$ を介して動詞$v$ に係っているとき，
    単語$w_i$ の単語生成文脈$c_i$ を
    $\slot{v}{p}$ とする．
    このとき，$w_i$ の生成確率$P(w_i | N[\slot{v}{p}])$ は
    動詞$v$ の格$p$ の格要素となる名詞$N$ から
    単語$w_i$ が生成される確率を表わす．
  \end{lcdef}
  \bigskip

  例えば，図\ref{fig:examsent} の例において，
  名詞``パイ''は動詞``食べ''のヲ格の格要素であり，
  名詞``彼女''は動詞``食べ''のガ格の格要素となっている．
  したがって，これらの単語を生成する際にはこの規則が適用され，
  それぞれの生成確率(\ref{eq:der1-pai}),(\ref{eq:der1-kanojo})は
  以下のように近似される．
  \begin{eqnarray}
    \label{eq:der2-pai}
    P(パイ|R,を,食べ,た) &\simeq& P(パイ|N[\slot{食べ}{を}]) \\
    \label{eq:der2-kanojo}
    P(彼女|R,が,パイ,を,食べ,た) &\simeq&
    P(彼女|N[\slot{食べ}{が}])
  \end{eqnarray}

\item
  助詞とその係り先用言の従属関係，格間の従属関係を考慮する場合

  図\ref{fig:examsent} の例文においては，
  2つの助詞``が''と``を''が動詞``食べ''に係っている．
  このとき，
  これらの生成確率(\ref{eq:der1-o}),(\ref{eq:der1-ga})を
  以下のように近似しても，
  助詞とその係り先用言との間の語彙的従属関係，および
  同じ用言に係る助詞同士の従属関係(以下，これを格間の従属関係と呼ぶ)を
  語彙モデルに反映させることができる．
  \begin{eqnarray}
    \label{eq:der2-o}
    P(を|R,食べ,た) &\simeq&
    P(を|P[\head{食べ}{2}{\phi_1,\phi_2}]) \\
    \label{eq:der2-ga}
    P(が|R,パイ,を,食べ,た) &\simeq&
    P(が|P[\head{食べ}{2}{\phi_1,を}])
  \end{eqnarray}

  式(\ref{eq:der2-o})は，助詞$P$ が
  2つの助詞の係り先となっている動詞``食べ''に係っているときに，
  品詞$P$ から単語``を''が生成される確率を表わしている．
  一方式(\ref{eq:der2-ga})は，助詞$P$ が
  2つの助詞の係り先となりかつそのうちの1つは``を''である
  動詞``食べ''に係っているときに，
  品詞$P$ から単語``が''が生成される確率を表わしている．

  助詞とその係り先用言の従属関係，および
  格間の従属関係を語彙モデルに導入するために，
  \lcrule{lc:marker} を以下のように定義する．

  \bigskip
  \begin{lcdef}{lc:marker}
    単語$w_i$ の品詞$l_i$が$P$(助詞)であり
    かつ用言$h$ に係っているとき，
    単語$w_i$ の単語生成文脈$c_i$ を
    $\head{h}{n}{\phi_1,\cdots,\phi_j,p_{j+1},\cdots,p_n}$ とする．
    このとき，$w_i$ の生成確率
    $P(w_i | P[\head{h}{n}{\phi_1,\cdots,\phi_j,p_{j+1},\cdots,p_n}])$は，
    用言$h$ が$n$ 個の助詞の係り先となりかつ
    用言に近い$p_{j+1},\cdots,p_n$ の助詞が既に生成されているときに，
    $\phi_j$ として$w_i$ が生成される確率を表わす．
  \end{lcdef}
  \bigskip

  \lcrule{lc:marker} において，同じ用言に係る助詞は
  用言に近いものから順番に生成されると仮定している．
  すなわち，助詞が出現する順序も考慮されている．


\item 助詞の係り先が用言か体言かを考慮する場合

  助詞の係り先が用言である場合と体言である場合とでは，
  助詞の生成確率$P(w_i | P)$ の分布は著しく異なると考えられる．
  例えば，係り先が用言の場合には``が''，``を''などの
  助詞は出現しやすいが，助詞``の''は出現しにくい．
  これに対して，係り先が体言の場合，
  すなわちその助詞を含む文節が連体修飾節となっている場合には，
  助詞``の''が出現する場合が圧倒的に多いと予想される．
  したがって，助詞の生成確率$P(w_i | P)$ を学習する際に，
  その助詞の係り先が用言もしくは体言であるかを区別しないで
  学習するのは望ましいことではない．
  これに対応するには，
  以下のような\lcrule{lc:josi} を定義すればよい．

  \bigskip
  \begin{lcdef}{lc:josi}
    単語$w_i$ の品詞$l_i$が$P$(助詞)であり，
    かつその助詞の係り先が体言であるとき，
    単語$w_i$ の単語生成文脈$c_i$ を{\it nd\/} とする．
    {\it nd\/} はその助詞の係り先が体言であることを表わす
    シンボルである．
    このとき，$w_i$ の生成確率$P(w_i | P[nd])$は，
    体言を係り先とする助詞から
    単語$w_i$ が生成される確率を表わす．
  \end{lcdef}
  \bigskip
  助詞の単語生成文脈を決定する際には，
  助詞の係り先が用言である場合には\lcrule{lc:marker} が，
  助詞の係り先が体言である場合には\lcrule{lc:josi} が適用される．
\end{itemize}

ここに挙げた\lcrule{lc:default}〜\#\ref{lc:josi} が
単語生成文脈を決定するための全ての規則というわけではない．
本節では，特に用言の格関係に注目して語彙モデルに反映させるべき
語彙的従属関係(単語の共起関係)の例を挙げたが，
他の種類の語彙的従属関係を語彙モデルに反映させるように
単語生成文脈決定規則を拡張・洗練することもできる．
すなわち，
語彙モデルにおいてどのような語彙的従属関係を考慮するかは，
単語生成文脈決定規則の追加・変更によって柔軟に調整することが
可能である．

単語生成文脈として何を選択するかを自動的に
学習することも考えられる\footnote{
  例えば，
  Magerman は確率の前件としてどのような素性を選択すれば
  よいのかを決定木を用いて自動学習している~\cite{magerman:95:a}．
  }が，
我々は言語学的知見に基づくヒューリスティクス規則によって
単語生成文脈を選択する方向で研究をすすめている．
なぜなら，語彙モデルにどのような種類の語彙的従属関係を反映させるかを
単語生成文脈決定規則によって明確に記述することにより，
モデルに反映された統計情報が曖昧性解消に有効であるかどうかなど，
モデルの特性の分析を容易に行うことができるからである．
\subsubsection{従属係数}
\label{sec:dp}

これまでは単語を生成する際に考える単語生成文脈は
常に一つであると仮定していた．
しかしながら，一般には，一つの単語を生成する際に
複数の単語生成文脈を考慮しなければならない場合もある．
\begin{figure}[tbp]
  \begin{center}
    \leavevmode
    \atari(106,30)

    \caption{並列構造を持つ例文}
    \label{fig:exam-coord}
  \end{center}
\end{figure}
例えば，図\ref{fig:exam-coord} の例文において，
2つの文節``食べ-て''\footnote{
  ``-''は単語の区切りを表わす．
  }と``出かけ-た''は並列の関係にある．
したがって，この例文中の名詞``彼女''は動詞``食べ''の
ハ格の格要素であり\footnote{
  本研究では，名詞が助詞を介して用言に係る場合は常に，
  その名詞を用言の表層格の格要素として取り扱う．
  }，
同時に動詞``出かけ''のハ格の格要素でもある．
したがって，\lcrule{lc:filler} に従えば，
``彼女''という単語を生成する際の単語生成文脈としては
$\slot{食べ}{は}$ と$\slot{出かけ}{は}$ の2つがある．
このとき，``彼女''の生成確率は次のように推定することが望ましい．
\begin{equation}
  \label{eq:der-cor1-kanojo}
  P(彼女 | N[\slot{食べ}{は},\slot{出かけ}{は}])
\end{equation}
同様に，この例文中の助詞``は''は
動詞``食べ''と``出かけ''の両方に係っているとみなすことができる．
したがって，\lcrule{lc:marker} に従えば，
``は''という単語を生成する際の単語生成文脈として
$\head{食べ}{2}{\phi_1,を}$ と$\head{出かけ}{2}{\phi_1,へ}$ の
2つがあると考えられ，
``は''の生成確率も次のように推定することが望ましい．
\begin{equation}
  \label{eq:der-cor1-wa}
  P(は | P[\head{食べ}{2}{\phi_1,を},\head{出かけ}{2}{\phi_1,へ}])
\end{equation}

ところが，
式(\ref{eq:der-cor1-kanojo})や(\ref{eq:der-cor1-wa})の
ように複数の単語生成文脈を前件に持つ確率モデルは，
推定するパラメタの数が爆発的に増大する可能性がある．
そこで本研究では，複数の単語生成文脈を以下のように取り扱う．
まず，説明を簡略化するために，
単語$w_i$ が2つの単語生成文脈$c_1$ と$c_2$ を持つとする．
このとき，単語$w_i$ の生成確率$P(w_i | l_i[c_1,c_2])$ を
以下のように近似する．
\begin{eqnarray}
  P(w_i|l_i[c_1,c_2])&=&
  \frac{P(l_i[c_1,c_2]|w_i) \cdot P(w_i)}{P(l_i[c_1,c_2])} \\
  \label{eq:prove-dp-1}
  &=&
  \frac{P(l_i[c_1]|w_i) \cdot P(l_i[c_2]|l_i[c_1],w_i) \cdot P(w_i)}
       {P(l_i[c_1]) \cdot P(l_i[c_2]|l_i[c_1])} \\
  \label{eq:prove-dp-2}
  &\simeq&
  \frac{P(l_i[c_1]|w_i) \cdot P(l_i[c_2]|l_i,w_i) \cdot P(w_i)}
       {P(l_i[c_1]) \cdot P(l_i[c_2]|l_i)} \\
  &=&
  \frac{P(l_i[c_1]|w_i)}{P(l_i[c_1])} \cdot
  \frac{P(l_i[c_2]|l_i,w_i)}{P(l_i[c_2]|l_i)} \cdot P(w_i) \\
  &=&
  \frac{P(w_i|l_i[c_1])}{P(w_i)} \cdot
  \frac{P(w_i,|l_i,l_i[c_2])}{P(w_i|l_i)} \cdot P(w_i) \\
  \label{eq:prove-dp}
  &=&
  P(w_i|l_i)\cdot
  \frac{P(w_i|l_i[c_1])}{P(w_i|l_i)}\cdot
  \frac{P(w_i|l_i[c_2])}{P(w_i|l_i)}
\end{eqnarray}
式(\ref{eq:prove-dp-1})から式(\ref{eq:prove-dp-2})の変形において，
2つの単語生成文脈$c_1$ と$c_2$ は互いに独立であると
仮定している．
\begin{eqnarray}
   P(l_i[c_2]|l_i[c_1]) &\simeq& P(l_i[c_2]|l_i)\\
   P(l_i[c_2]|l_i[c_1],w_i) &\simeq& P(l_i[c_2]|l_i,w_i)
\end{eqnarray} 

ここで，従属係数$D(w_i|l_i[c_i])$ を
式(\ref{eq:def-dp})のように定義する．
\begin{equation}
  \label{eq:def-dp}
  D(w_i|l_i[c_i]) = \frac{P(w_i|l_i[c_i])}{P(w_i|l_i)}
\end{equation}
この従属係数を用いれば，
式(\ref{eq:prove-dp})から式(\ref{eq:double-dp})が導かれる．
\begin{eqnarray}
  \label{eq:double-dp}
  P(w_i|l_i[c_1,c_2]) &\simeq&
  P(w_i|l_i)\cdot D(w_i|l_i[c_1])\cdot D(w_i|l_i[c_2])
\end{eqnarray}
以上では単語$w_i$ が2つの単語生成文脈を持つ場合を考えていたが，
単語$w_i$ が$n$ 個の単語生成文脈$c_1,\cdots,c_n$ を持つ場合にも
同様の近似が可能であり，最終的に以下の式が得られる．
\begin{equation}
  \label{eq:multi-dp}
  P(w_i | l_i[c_1,\cdots,c_n]) \simeq
  P(w_i | l_i) \cdot \prod_{c_i} D(w_i | l_i[c_i])
\end{equation}

式(\ref{eq:def-dp})で定義した従属係数$D(w_i|l_i[c_i])$ は
単語$w_i$ と単語生成文脈$c_i$ の相関関係を評価する統計量である．
例えば，$w_i$ と$c_i$ に相関関係がない場合，
すなわち$w_i$ と$c_i$ が互いに独立である場合には，
式(\ref{eq:def-dp})の分子$P(w_i|l_i[c_i])$ は
分母$P(w_i|l_i)$ にほぼ等しくなり，
従属係数$D(w_i|l_i[c_i])$ は1に近い値を取る．
これに対し，$w_i$ と$c_i$ に正の相関関係がある場合には，
単語生成文脈$c_i$ を前件に加えた確率$P(w_i|l_i[c_i])$ は
単語生成文脈$c_i$ を無視した確率$P(w_i|l_i)$ よりも
大きくなるので，その従属係数は1より大きい値を取る．
同様に，$w_i$ と$c_i$ に負の相関関係がある場合には
従属係数は1より小さい値を取る．

複数の単語生成文脈$c_1,\cdots,c_n$ の下での
単語$w_i$ の生成確率は，
単語生成文脈を無視した単語の生成確率$P(w_i | l_i)$ と，
$w_i$ と$c_i$ の相関関係を他の単語生成文脈とは独立に
評価した従属係数$D(w_i|l_i[c_i])$ の積によって
計算できることを式(\ref{eq:multi-dp})は示している．
従属係数$D(w_i|l_i[c_i])$ を他の単語生成文脈と
独立に推定・学習することにより，
確率モデルのパラメタ空間を推定可能な大きさに抑制することができる．
例えば，
図\ref{fig:exam-coord} の例において，
``彼女''の生成確率(\ref{eq:der-cor1-kanojo})と
``は''の生成確率(\ref{eq:der-cor1-wa})は
それぞれ以下のように推定される．
\begin{eqnarray}
  \nonumber
  && P(彼女 | N[\slot{食べ}{は},\slot{出かけ}{は}]) \\
  \label{eq:der-cor2-kanojo}
  &\simeq& P(彼女 | N) \cdot
  D(彼女 | N[\slot{食べ}{は}) \cdot 
  D(彼女 | N[\slot{出かけ}{は}]) \\[3mm]
  \nonumber
  && P(は | P[\head{食べ}{2}{\phi_1,を},\head{出かけ}{2}{\phi_1,へ}]) \\
  \label{eq:der-cor2-wa}
  &\simeq& P(は | P) \cdot 
  D(は | P[\head{食べ}{2}{\phi_1,を}]) \cdot
  D(は | P[\head{出かけ}{2}{\phi_1,へ}])
\end{eqnarray}

従属係数を導入する利点として，
単語生成文脈を複数取り扱うことができるという点の他に，
式(\ref{eq:lex-model-der-dp})に示すように，
語彙モデル$P(W|R)$ を
単語の出現頻度のみを反映した$P_{cf}(W|L)$ と
単語の共起関係のみを反映した$D(W|R)$ との積に
分解できるという点が挙げられる．
\begin{eqnarray}
  P(W|R) &\simeq& \prod_{i} P(w_i | l_i[C_{w_i}]) \\
  &\simeq& \prod_{w_i} P(w_i|l_i) \cdot 
      \prod_{c_{ij} \in C_{w_i}} D(w_i | l_i[c_{ij}]) \\
  \label{eq:lex-model-der-dp}
  &=& P_{cf}(W|L) \cdot D(W|R) \\[3mm]
  \label{eq:lex-model-der}
  P_{cf}(W|L) &=& \prod_{w_i} P(w_i|l_i) \\[3mm]
  \label{eq:lex-model-dp}
  D(W|R) &=&
  \prod_{w_i} \prod_{c_{ij} \in C_{w_i}} D(w_i | l_i[c_{ij}])
\end{eqnarray}
上式において，$C_{w_i}$ は単語$w_i$ の単語生成文脈の集合を
表わしている．

式(\ref{eq:lex-model-der})の統計量$P_{cf}(W|L)$ は，
単語生成文脈を無視したときに
品詞$l_i$ から単語$w_i$ が生成される確率の積であり，
単語の出現頻度に関する優先度が反映される．
これに対し，
式(\ref{eq:lex-model-dp})の統計量$D(W|R)$ は
各単語$w_i$ とその単語生成文脈$c_{ij}$ の従属係数の積を
表わしており，
$w_i$ と$c_{ij}$ の相関関係に関する優先度
(すなわち単語の共起関係)が反映される．
このように，
語彙モデルを単語の出現頻度，および
単語の共起関係のみを反映させた2つの統計量の積として
分解することにより，
\ref{sec:intro} 節で述べたように，
曖昧性解消時におけるそれぞれの統計情報の働きを
容易に理解することができる．
\section{評価実験}
\label{sec:exp-stat}

本節では，
前節で提案した統合的確率言語モデルの評価実験について述べる．
統合的確率言語モデルは本来形態素解析，構文解析を
同時に行うことを前提としているが，
そのような大規模な実験を行う前の予備実験として，
まずは文節列を入力とする文節間の係り受け解析のみを行った．
\subsection{構文モデルの学習}
\label{sec:learn-syn-model}

本節の実験では，入力として単語列，品詞列，文節区切りが
与えられたときに，それぞれの文節の係り先となる文節を決定する．
このような文節の係り受け解析をCFG(文脈自由文法)を用いて行った．

まず，CFG規則の終端記号として，
文節の統語的特性を反映した文節ラベルを用いる．
この文節ラベルの定義を(\ref{eq:BP-label})に示す．
\begin{equation}
  \label{eq:BP-label}
  文節ラベル ~~\stackrel{def}{=}~~
  (受け属性,係り属性,読点の有無,用言種別)
\end{equation}
ここで，``受け属性''，``係り属性''は
それぞれ文節の受け属性と係り属性であり，
``連用''，``連体''，``格関係''の組によって表わされる．
例えば，``パイ-を''や``彼女-の''など，
「名詞 助詞」といった品詞並びによって構成される文節は，
他の文節から連体修飾を受ける可能性があるので受け属性は``(連体)''となり，
他の文節を連体修飾したり用言を修飾してその格要素および表層格を表わす
可能性があるので係り属性は``(連用,格関係)''となる\footnote{
  ここでの``格関係''とは，用言を受け側とした格関係のみを指す．
  }．
また``読点の有無''は，その文節の末尾が読点であれば``1''，
そうでなければ``0''といった値を取る．
これは，読点を末尾に持つ文節は直後の文節には係りにくく，
読点を末尾に持たない文節よりも遠くに係る傾向があるので，
この違いを構文モデルに反映させるためである．
一方``用言種別''は，
``格関係''を受け属性に含む文節タイプを細分化するための属性であり，
文節の主辞が自動詞，他動詞，形容詞，名詞述語のときには
それぞれ``自動詞''，``他動詞''，``形容詞''，``名詞述語''
といった値を取る．
また，``格関係''を受け属性に持たない文節のときには
その値は常に``φ''である．
\ref{sec:lex-model} 節で例示した単語生成文脈決定規則は，
単語の共起関係の中でも特に用言の格関係に注目している．
用言を主辞とする文節の文節ラベルを細分化したのはこのためである．
この文節ラベルは，文節を構成する単語列の品詞情報をもとに
一意に決定されるものとする．
また，これらの文節ラベルの整合性\footnote{
  例えば，``連体''を係り属性に含む文節は
  ``連体''を受け属性に含まない文節には係らない．
  }をチェックする規則を作成し，
その集合を文節の係り受け解析に用いるCFGとした．
このCFGの概要を表\ref{tab:grammar} に示す．
\begin{table}[htbp]
  \begin{center}
    \caption{CFGの概要}
    \label{tab:grammar}

    \begin{tabular}{|l|r|}  \hline
      規則数       & 961 \\ \hline
      非終端記号数 &  51 \\ \hline
      終端記号数(文節ラベル数) &  42 \\ \hline
    \end{tabular}
  \end{center}
\end{table}

本実験では，構文モデル$P(R)$ としてPGLRを利用した．
また，この構文モデルの学習には
京大コーパス\cite{kurohasi:97:a} を使用した．
京大コーパスの各例文には，単語区切り，単語の品詞，
文節区切りと文節の係り受け解析の結果(構文構造)が付加されている．
京大コーパスの9,944例文に対して，
コーパスの各例文とそれに付加された構文構造を作り出すような
LR表における状態遷移列を求め，また状態遷移が行われた回数を数え上げる．
このようにして得られた状態遷移回数を状態遷移確率に
変換することにより，PGLRのパラメタ推定を行った．
\subsection{語彙モデルの学習}
\label{sec:learn-lex-model}

本実験では，式(\ref{eq:lex-model-der-dp})に示した語彙モデル
$P(W|R)=P_{cf}(W|L) \cdot D(W|R)$ のうち，
$P_{cf}(W|L)$ の計算を省略できる．
なぜなら，
単語列及び品詞列はすでに入力として与えられているため，
全ての解析結果の候補について品詞から単語への生成確率の積
$P_{cf} (W|L)$ は等しいからである．
したがって，語彙モデルとして学習するのは
従属係数の積$D(W|R)$ のみでよい．
今回の実験では，
\lcrule{lc:filler}〜\#\ref{lc:josi} によって
定められる従属係数(\ref{eq:ldp-filler}),
(\ref{eq:ldp-marker-multi2}),(\ref{eq:ldp-josi})を
$D(W|R)$ の要素とし，これらの学習を行った．

まず，格要素の従属係数(\ref{eq:ldp-filler})の学習について説明する．
\begin{equation}
  \label{eq:ldp-filler}
  D(n | N[\slot{v}{p}]) = \frac{P(n|N[\slot{v}{p}])}{P(n|N)}
\end{equation}
RWCコーパス~\cite{rwc:95:a} とEDR共起辞書\cite{edr:95:a} から，
名詞$n$ が助詞$p$ を介して動詞$v$ に係る事例
$(n,p,v)$ をそれぞれのべ6,888,849組，975,510組収集した．
式(\ref{eq:ldp-filler})の分子および分母の確率モデルは
これらの訓練事例から最尤推定した．

さらに，分子の確率モデル$P(n|N[\slot{v}{p}])$ を推定する際に
以下のような近似を行った．

\begin{itemize}
\item 名詞$n$ の意味クラスによる抽象化

  名詞$n$ の意味クラスの集合を
  $C_n = \{c_{n_1},\cdots,c_{n_m}\}$ として，
  $P(n | N[\slot{v}{p}])$ を以下のように推定した．
  \begin{equation}
    \label{eq:ldp-filler-class}
    P(n | N[\slot{v}{p}]) \simeq
    \sum_j P(n|c_{n_j}) P(c_{n_j} | N[\slot{v}{p}])
  \end{equation}

  今回の実験では，名詞意味クラス$c_n$ として，
  日本語語彙体系\cite{ikehara:93:a,ikehara:97:a} の
  名詞シソーラス
  のルートから深さ3に位置する
  151個の意味クラスの集合を用いた．
  これらの意味クラスは互いに排他的である．

  $D(n|N[\slot{v}{p}])$ を推定する場合，
  名詞$n$ が日本語語彙体系に登録されておらず，
  その名詞意味クラスが不明な場合には，
  その従属係数は学習不可能であるとして
  $D(n|N[\slot{v}{p}]) \simeq 1$ とした．
  これは，$n$ と$\slot{v}{p}$ との間の
  従属関係を無視することに相当する．


\item バックオフ方式によるスムージング

  確率モデル$P(c_n | N[\slot{v}{p}])$ を推定する際，
  この確率の分母となる事例$(*,p,v)$ 
  ($*$ は任意の名詞意味クラスを表わす)の出現回数が
  ある閾値 $\lambda$ よりも小さい場合には，
  $v$ を動詞意味クラス$c_v$ を用いて抽象化した
  確率モデル$P(c_n | N[\slot{c_v}{p}])$ によって近似した．
  \begin{equation}
    P(c_n|N[\slot{v}{p}]) \simeq P(c_n | N[\slot{c_v}{p}])
  \end{equation}
  また，事例$(*,p,c_v)$ の出現回数が $\lambda$ を越えない
  場合には，動詞意味クラス$c_v$ の抽象度を段階的に上げていき，
  必ず$\lambda$ 個以上の訓練事例から確率モデルを推定するようにした．

  本実験においては，動詞意味クラス$c_v$ として
  分類語彙表~\cite{bgh:96} の
  5桁および2桁の分類コードを動詞意味クラスとして利用した．
  動詞を分類語彙表の2桁の分類コードに抽象化しても
  学習事例数が $\lambda$ を越えなかったとき，
  もしくは$(*,p,v)$ の事例数が $\lambda$ 以下であり
  かつ動詞$v$ が分類語彙表に登録されていなかった場合には，
  十分信頼度の高い確率モデルが学習できなかったとして，
  従属係数$D(n|N[\slot{v}{p}]) \simeq 1$ とした．
  なお，今回は$\lambda=100$ として実験を行った．
\end{itemize}

\bigskip
次に，用言に係る助詞に関する従属係数(\ref{eq:ldp-marker})の
学習について説明する．
\begin{equation}
  \label{eq:ldp-marker}
  D(p_i | P[\head{h}{n}{\phi_1,\cdots,\phi_i,p_{i+1},\cdots,p_n}]) =
  \frac{P(p_i |
    P[\head{h}{n}{\phi_1,\cdots,\phi_i,p_{i+1},\cdots,p_n}])}
  {P(p_i | P)}
\end{equation}
$n$ 個の助詞$p_1,\cdots,p_n$ が同じ用言$h$ に係っている場合には，
それぞれの$p_i$ に対応する従属係数(\ref{eq:ldp-marker})の積を
計算すれば良い．
この従属係数の積は式(\ref{eq:ldp-marker-multi})のように変形できる．

\begin{eqnarray}
  &&
  \prod_i
  D(p_i | P[\head{h}{n}{\phi_1,\cdots,\phi_{i-1},p_i,\cdots,p_n}]) \\
  &=&
  \prod_i
  \frac{P(p_i |
    P[\head{h}{n}{\phi_1,\cdots,\phi_i,p_{i+1},\cdots,p_n}])}
  {P(p_i | P)} \\
  &=&
  \label{eq:ldp-marker-multi}
  \frac{P(p_1,\cdots,p_n |
    P_1,\cdots,P_n[\head{h}{n}{\phi_1,\cdots,\phi_n}])}
  {\prod_i P(p_i | P)} \\
  \label{eq:ldp-marker-multi2}
  &\stackrel{def}{=}&
  D(p_1,\cdots,p_n | P_1,\cdots,P_n[\head{h}{n}{\phi_1,\cdots,\phi_n}])
\end{eqnarray}
したがって，学習しなければならないのは，
ある用言$h$ が$P_1,\cdots,P_n$ の$n$ 個の助詞の係り先となっているときに
単語$p_1,\cdots,p_n$ を同時に生成する確率モデル
$P(p_1,\cdots,p_n | 
P_1,\cdots,P_n[\head{h}{n}{\phi_1,\cdots,\phi_n}])$ と，
品詞$P$(助詞)から単語$p_i$ が生成される確率$P(p_i|P)$ である．
以降，簡単のため，前者の確率モデルを以下のように記述する．
\begin{eqnarray}
  \label{eq:kaku-model-def}
  P(\vec{p} ~|~ h,n) 
  &\stackrel{def}{=}&
  P(p_1,\cdots,p_n | 
  P_1,\cdots,P_n[\head{h}{n}{\phi_1,\cdots,\phi_n}]) \\
  \nonumber
  && 但し，~\vec{p} = (p_1,\cdots,p_n) 
\end{eqnarray}

確率モデル$P(\vec{p} | h,n)$ を学習するために，
$n$ 個の助詞 $\vec{p}$ が同じ用言$h$ に係るという事例
$(\vec{p},h)$ をEDRコーパスから収集した．
今回の実験では，用言$h$ として動詞，形容詞，名詞述語の3つを考えた．
用言$h$ が動詞，形容詞，名詞述語であるときの，
また$h$ に係る助詞の数$n$ が1，2，3，4以上であるときの
事例$(\vec{p},h)$ ののべ数を表\ref{tab:coocr-ph} にまとめる．

\begin{table}[htbp]
  \begin{center}
    \caption{EDRコーパスから収集した事例$(\vec{p},h)$ ののべ数}
    \label{tab:coocr-ph}

    \begin{tabular}{|c||r|r|r|r|} \hline
      $h$      &
      \makebox[15mm]{$n=1$} & \makebox[15mm]{$n=2$} &
      \makebox[15mm]{$n=3$} & \makebox[15mm]{$n\ge 4$} \\ \hline\hline
      動詞     & 231,730 & 123,915 & 30,375 & 3,961 \\ \hline
      形容詞   &  19,266 &   7,686 &  1,292 &   154 \\ \hline
      名詞述語 &  28,636 &   9,327 &  1,238 &    98 \\ \hline
    \end{tabular}
  \end{center}
\end{table}

$n$ が4以上のときには学習に十分な事例を収集することができなかった．
そこで，$n$ が4以上のときには，従属係数を1，
すなわち助詞とその係り先用言との語彙的従属関係や
格間の従属関係を無視することにした．
\begin{equation}
  n \ge 4 のとき \qquad 
  D(p_1,\cdots,p_n | P_1,\cdots,P_n[\head{h}{n}{\phi_1,\cdots,\phi_n}])
  ~\simeq~1
\end{equation}
$n=1$ のときの式(\ref{eq:kaku-model-def})の分子の
確率モデル$P(\vec{p}|h,n)$ は
表\ref{tab:coocr-ph} に示した事例から最尤推定した．
また，$n=2,3$ のときの確率モデル$P(\vec{p}|h,n)$ は
最大エントロピー法を用いて推定した\footnote{
  この詳細については\cite{sirai:97:b} を参照．
  }．

最後に，体言に係る助詞に関する従属係数(\ref{eq:ldp-josi})の
学習について説明する．
\begin{equation}
  \label{eq:ldp-josi}
  D(p | P[nd]) = \frac{P(p | P[nd])}{P(p|P)}
\end{equation}
この従属係数を学習するために，EDRコーパスから
体言に係る助詞$p$ をのべ273,062個収集した．
式(\ref{eq:ldp-josi})の分子はこの訓練データから最尤推定した．
また，式(\ref{eq:ldp-josi})の分母$P(p|P)$ は，
ここで収集した体言に係る助詞の事例と，
表\ref{tab:coocr-ph} に示した用言に係る助詞の事例から，
同様に最尤推定した．
尚，式(\ref{eq:ldp-marker-multi})の分母の各項$P(p_i|P)$ も
式(\ref{eq:ldp-josi})の分母の確率モデルと同じものを使用した．


\subsection{実験結果}
\label{sec:result-stat}

\ref{sec:learn-syn-model} 節にて学習した構文モデル$P(R)$，
および\ref{sec:learn-lex-model} 節にて学習した語彙モデル$P(W|R)$ 
を用いて，文節の係り受け解析を行った．
まず，テスト文として，京大コーパスの中から文節数7〜9の文を
ランダムに500文選び，これをテスト文とした．
構文モデル$P(R)$ を学習する際に用いた訓練用例文には
これらのテスト文は含まれていない．
文節数7〜9という比較的文長の短い例文をテスト文として選んだのは，
本実験で用いたPGLRパーザがまだ開発の途中であり，
長い文長の例文の解析に非常に多くの時間を要するためである．

テスト文の係り受け解析結果の評価尺度として，
文節の正解率を以下のように定義する．
\begin{equation}
  \label{eq:def-bp-acc}
  文節の正解率 ~=~
  \frac{係り先の正しい文節の数}{テスト文に含まれる文節の数}
\end{equation}
この文節の正解率は生成確率が一位である解析結果の候補について計算する．
また，文の最後に位置する2つの文節は評価の対象から除外する．
これは，文の一番最後にある文節は係り先がなく，
また文の最後から2番目にある文節は
常に文の一番最後の文節に係るからである．

\ref{sec:lex-model} 節に述べたように，
語彙モデルにおいてはいくつかの種類の統計情報を取り扱う．
ここでは，構文的な統計情報，
および語彙モデルにおいて考慮された語彙的な統計情報の
それぞれの曖昧性解消における効果を調べるために，
以下に述べる6種類のモデルを用意し，それらを比較した．
結果を表\ref{tab:res-lex} に示す．

\begin{description}
\item[BL:] ベースライン

  全ての文節の係り先を，
  (1)全ての文節は係り得る文節の中でできるだけ近いものに係る，
  (2)一文中における文節の係り受け関係は互いに交差しない，
  として決定するモデルである．

\item[Syn:] 従属係数を無視したモデル

  $D(W|R)=1$ としたモデルである．
  すなわち，構文モデル$P(R)$ で学習した統計情報のみを用いて
  曖昧性解消を行う．

\item[F:] 格要素となる名詞に関する従属係数のみを用いたモデル

  $D(W|R)$ として，式(\ref{eq:ldp-filler})によって
  与えられる従属係数のみを考慮したモデルである．



\item[M:] 用言に係る助詞に関する従属係数のみを用いたモデル

  $D(W|R)$ として，式(\ref{eq:ldp-marker-multi2})によって
  与えられる従属係数のみを考慮したモデルである．

\item[P:] 体言に係る助詞に関する従属係数のみを用いたモデル

  $D(W|R)$ として，式(\ref{eq:ldp-josi})によって
  与えられる従属係数のみを考慮したモデルである．


\item[all:] 全ての従属係数を用いたモデル

  上記全ての従属係数を考慮したモデルである．
\end{description}

\begin{table}[tbp]
  \begin{center}
    \caption{文節の正解率}
    \label{tab:res-lex}

    \begin{tabular}{|l||r|r|} \hline
      & \makebox[15mm]{後置詞節} &
        \makebox[15mm]{全ての文節} \\ \hline\hline
      BL     & 62.92\% & 61.68\% \\ \hline
      Syn    & 69.63\% & 73.38\% \\ \hline
      F      & 71.36\% & 74.69\% \\ \hline
      M      & 78.19\% & 78.55\% \\ \hline
      P      & 84.06\% & 82.22\% \\ \hline
      all    & 86.30\% & 84.34\% \\ \hline
    \end{tabular}
  \end{center}
\end{table}

表\ref{tab:res-lex} から，
語彙モデルにおいて考慮した語彙的な統計情報のうち，
体言に係る助詞に関する従属係数(モデルP)が正解率の向上に
一番大きく貢献することがわかる．
すなわち，助詞が用言に係っているか否かの違いが
その生成確率に大きく影響し，
その違いを考慮することによって
曖昧性解消の精度を大きく向上させることができた．
また，表\ref{tab:res-lex} における``後置詞節''とは，
``彼女-が''，``パイ-を''など，
用言の格要素および表層格を表わす可能性のある文節を指す\footnote{
  この後置詞節には，``太郎-の''など，
  実際には体言を修飾する文節も含まれる．
  }．
テスト文全体における2,975個の文節のうち，
1,788個がこの後置詞節に相当する．
この後置詞節のみで評価した場合，
全ての文節で評価した場合に比べて，
語彙的な統計情報を考慮したモデル(F,M,P,all)と
構文的な統計情報のみを考慮したモデル(Syn)との
文節の正解率の差が大きくなっている．
これは，今回の実験で用いた語彙モデルにおいては，
語彙的な統計情報の中でも用言の格関係に注目しているため，
語彙モデルが``後置詞節''の係り先の曖昧性解消に
特に有効に働いているためと考えられる．

構文モデルと全ての語彙的従属関係を考慮した語彙モデルを
組み合わせて曖昧性解消に用いた場合(all)，
構文モデルのみを用いた場合(Syn)と比べて
文節の正解率が10.96\%向上し，
また構文モデルのみを曖昧性解消に用いたときの
ベースラインとの文節の正解率の差が11.70\%であることから，
文節の係り受け解析の精度向上において，
語彙モデルは構文モデルと同程度の貢献をしていると考えられる．
本研究で提案した統合的確率言語モデルにおいては，
語彙的な統計情報を局所化し構文的な統計情報とは独立に学習しているが，
このようなアプローチにおいても，語彙的な統計情報は
曖昧性解消の精度向上に十分大きく貢献すると期待できる．

最後に，本研究で提案する統合的確率言語モデルを用いた解析結果と
KNPパーザ\cite{kurohasi:94:a} による解析結果との比較を行った．
KNPパーザは形態素解析システムJUMAN\cite{matumoto:94:a} の
形態素解析結果を入力とし，
文節の区切りを認定してから文節の係り受け解析を行う．
そこで，\ref{sec:result-stat} 節の実験で用いた
500個のテスト文のうち，
JUMANの形態素解析結果による形態素区切り
およびKNPパーザによる文節区切りの結果が
コーパスと一致した388文を対象に，
両者の係り受け解析結果の比較を行った．
結果を表\ref{tab:comp-knp} に示す．

\begin{table}[htbp]
  \begin{center}
    \caption{KNPパーザとの比較}
    \label{tab:comp-knp}

    \begin{tabular}{|l||r|r|} \hline
      & \makebox[15mm]{後置詞節} &
        \makebox[15mm]{全ての文節} \\ \hline\hline
      本手法    & 86.57\% & 84.53\% \\ \hline
      KNPパーザ & 86.79\% & 85.71\% \\ \hline
    \end{tabular}
  \end{center}
\end{table}

本手法はKNPパーザよりも文節の正解率で1\%程度劣っている．
今回の実験では，統合的確率言語モデルに組み込む語彙的従属関係として，
格要素と動詞との従属関係，助詞と係り先用言との従属関係，
格間の従属関係などを考慮した．
しかしながら，これ以外にも，
曖昧性解消に有効であると考えられる語彙的従属関係が数多く存在する．
特に，今回の実験では連体修飾に関しては語彙的従属関係を
何も考慮していないので，そのことによる解析誤りが多かった．
例えば，「彼女の紫色の帽子が風に飛ばされた」という文においては，
文節``彼女-の''が(a)``紫色-の''に係る，
(b)``帽子-が''に係るという2つの解釈がある．
ところが，連体修飾する``彼女''については
語彙的従属関係を考慮していないので，
より近い文節に係る解釈(a)に高い確率が与えられてしまう．
これを回避するためには，以下のような従属係数を学習し
語彙モデルに加えればよい．
\begin{equation}
  \label{eq:dp-rentai-noun}
  D(n_1 | N[n_2]) = \frac{P(n_1 | N[n_2])}{P(n_1 | N)}
\end{equation}
式(\ref{eq:dp-rentai-noun})の分子$P(n_1 | N[n_2])$ は，
ある名詞$N$ が$n_2$ を連体修飾しているとき，
その名詞として単語$n_1$ が生成される確率を表わしている．
このような従属係数を考慮することにより，
``彼女''は``紫色''よりも``帽子''を連体修飾することが多い，
すなわち$D(彼女 | N[紫色]) << D(彼女 | N[帽子])$ であると
考えられるので，正しい解釈(b)に高い確率を与えると期待できる．
このように，
統合的確率言語モデルに新たな種類の語彙的従属関係を反映させるときには，
それに対応した従属係数を新たに語彙モデルに加えるという形で
容易に対処できる．
これは，
語彙的従属関係を局所化して構文的優先度などの他の統計情報と
独立に学習するように，
また異なる種類の語彙的従属関係は
異なる従属係数として独立に学習するように
モデルを設計したことに依る．

一方，後置詞節のみで評価した場合には，
本手法とKNPパーザの文節の正解率はほぼ等しい．
とはいえ，後置詞節の係り先の特定に失敗する場合も少なくない．
我々は現在その原因を調査中であり，
その一部については既に報告している\cite{sirai:97:d}．
今後，曖昧性解消に有効な統計情報を新たに組み込んだり，
また解析誤りの原因を調査しそれらに対処することにより，
係り受け解析の精度向上を図っていきたい．
\section{おわりに}
\label{sec:conclusion}

本研究では，形態素解析・構文解析を同時に行う際に，
構文的な統計情報と語彙的な統計情報を組み合わせて曖昧性を解消する
ひとつの手法を提案した．
我々の手法の特徴は，
構文的優先度，隣接する品詞間の共起関係，距離に関する優先度といった
構文的な統計情報を構文モデル$P(R)$ として，
単語の出現頻度および単語の共起関係を語彙モデル$P(W|R)$ として，
それぞれ独立に学習する点にある．
このことは，個々の統計情報を異なる言語資源から学習できるだけでなく，
曖昧性解消時における個々の統計情報の働きを容易に分析することができる．
実際に，京大コーパスを用いて構文モデルを，
RWCコーパスやEDRコーパスを用いて語彙モデルを学習した．
また，これらの確率モデルを用いた日本語文の文節の係り受け解析実験の結果，
構文的な統計情報と語彙的な統計情報のそれぞれが
曖昧性解消に大きく貢献することを確認した．

最後に今後の課題について述べる．
まず，統合的確率言語モデルが本来想定している
形態素解析と構文解析を同時に行い，
その有効性を実験的に確認することが挙げられる．
また，今回の実験では文長の比較的短い文を対象にしたが，
文長の長い文の係り受け解析を行うことにより，
統合的確率言語モデルの特性をさらに調査する必要がある．
文長の長い文においては，二重格を取りにくいなどの
格間の従属関係がさらに有効に働くのではないかと予想される．
最後に，統合的確率言語モデルと他の統計的構文解析に関する研究とを
実験的に比較することが挙げられる．
特に今回の実験は日本語を対象にしたが，
構文的な統計情報と語彙的な統計情報を独立に学習するアプローチが
英語などの他の言語においても本当に有効であるのかどうかは
今後調査していく必要があると思われる．

\bigskip

\acknowledgment

本研究にあたり，日本語語彙体系を提供して下さいました
NTTコミュニケーション科学研究所知識処理研究部
翻訳処理研究グループに感謝いたします．



\bibliographystyle{jnlpbbl}
\bibliography{nlp-str,jpaper}


\begin{biography}
\biotitle{略歴}
\bioauthor{白井 清昭}{
  1993年東京工業大学工学部情報工学科卒業．
  1995年同大学院理工学研究科修士課程修了．
  1998年同大学院情報理工学研究科博士課程修了．
  同年同大学院情報理工学研究科計算工学専攻助手，現在に至る．
  博士 (工学)．
  統計的自然言語解析に関する研究に従事．
  情報処理学会会員．}

\bioauthor{乾 健太郎}{
  1990年東京工業大学工学部情報工学科卒業．
  1992年同大学大学院理工学研究科修士課程修了．
  1995年同大学大学院理工学研究科博士課程修了．
  同年同大学院情報理工学研究科計算工学専攻助手．
  1998年九州工業大学情報工学部知識情報工学科助教授，現在に至る．
  博士 (工学)．
  自然言語処理に関する研究に従事．
  人工知能学会，体系機能言語学会，各会員．}

\bioauthor{徳永 健伸}{
  1983年東京工業大学工学部情報工学科卒業．
  1985年同大学院理工学研究科修士課程修了．
  同年(株)三菱総合研究所入社．
  1986年東京工業大学大学院博士課程入学．
  現在, 同大学大学院情報理工学研究科計算工学専攻助教授．
  博士 (工学)．
  自然言語処理, 計算言語学に関する研究に従事．
  情報処理学会, 認知科学会, 人工知能学会, 計量国語学会, 
  Association for Computational Linguistics, 各会員．}

\bioauthor{田中 穂積}{
  1964年東京工業大学工学部情報工学科卒業．
  1966年同大学院理工学研究科修士課程修了．
  同年電気試験所(現電子技術総合研究所)入所．
  1980年東京工業大学助教授．
  1983年東京工業大学教授．
  現在, 同大学大学院情報理工学研究科計算工学専攻教授．
  博士(工学)．
  人工知能，自然言語処理に関する研究に従事．
  情報処理学会, 電子情報通信学会，認知科学会, 人工知能学会,
  計量国語学会, Association for Computational Linguistics, 各会員．}

\bioreceived{受付}
\biorevised{再受付}
\bioaccepted{採録}

\end{biography}

\end{document}
