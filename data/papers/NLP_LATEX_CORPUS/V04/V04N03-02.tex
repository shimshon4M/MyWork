\documentstyle[jnlpbbl]{jnlp_j}

\setcounter{page}{27}
\setcounter{巻数}{4}
\setcounter{号数}{3}
\setcounter{年}{1997}
\setcounter{月}{7}
\受付{1996}{8}{29}
\再受付{1996}{10}{30}
\再々受付{1997}{1}{17}
\採録{1997}{3}{27}

\setcounter{secnumdepth}{2}

\title{シソーラス上に動的に構成される標本空間における\\
動詞の多義性解消}
\author{内山 将夫\affiref{U} \and 板橋 秀一\affiref{I}}

\headtitle{シソーラス上に動的に構成される標本空間における動詞の多義性解消}
\headauthor{内山 将夫・板橋 秀一}

\affilabel{U}{筑波大学 大学院 工学研究科}{Doctoral Program in Engineering, University of Tsukuba (現在，信州大学工学部電気電子工学科, Department of Electrical and Electronic Engineering, Faculty of Engineering, Shinshu University)}
\affilabel{I}{筑波大学 電子・情報工学系}{Institute of Information Sciences and Electronics, University of Tsukuba}

\jabstract{本稿では，語義の尤度パラメータの標本空間を，シソーラスに沿っ
て動的に拡張することにより，動詞の多義性を解消する手法を提案する．提案
手法では，尤度1位の語義と2位の語義とを比較し，尤度差が統計的に有意なら
ば，1位の語義を選ぶ．有意でなければ，シソーラスに沿って標本空間を一段
拡張し，多義性解消を試みる．もし，最大の標本空間でも尤度差が有意でなけ
れば，語義は判定しない．本稿の実験では，EDR日本語コーパスから頻度500以
上の動詞74語を抽出し，延べで約89,000の動詞について多義性を解消した．こ
のとき，最頻の語義を常に選ぶ場合の適合率は0.65，判定率は1.00であった．
ただし，判定率とは，多義性の解消を試みたなかで，実際に語義が判定された
割合である．クラスベースの手法と提案手法とを比較すると，分類語彙表を利
用した場合には，適合率は共に0.71であったが，判定率は，クラスベースの手
法が0.68，提案手法が0.73であった．EDR概念体系を利用した場合には，適合
率は共に0.70であったが，判定率は，クラスベースの手法が0.76，提案手法が
0.87であった．両者の判定率を比べると，提案手法の方が統計的に有意に高く，
その有効性が示された．}

\jkeywords{統計的な自然言語処理，事例に基づく自然言語処理，シソーラスの利用，動詞の多義性解消}

\etitle{Verb Sense Disambiguation in Dynamically\\
Constructed Sample Spaces on a Thesaurus}

\eauthor{Masao Utiyama \affiref{U} \and Shuichi Itahashi \affiref{I}}

\eabstract{This paper proposes a method which disambiguates verb
senses using co-occurrence-based likelihood parameters whose sample
spaces are extended according to a thesaurus. The method selects the
most plausible sense if its likelihood is significantly greater than
that of the second most plausible one. If not, the sample space is
extended and the significance test is tried again. If it cannot be
extended anymore, the system gives up disambiguation.  The method was
applied to 74 polysemous verbs (about 89,000 instances) extracted from
the EDR Japanese Corpus.  When the most frequent sense was selected,
the precision was 0.65 and the applicability, i.e. the ratio of the
disambiguated verbs to the treated verbs, was 1.00. The proposed
method was compared with a class-based method.  With {\it
Bunruigoihyou\/}, the precisions of both the methods were 0.71, but
the applicabilities of the proposed method and the class-based method
were 0.73 and 0.68, respectively. With the EDR Concept Classification
Dictionary, the precisions of both the methods were 0.70, but the
applicabilities of the proposed method and the class-based method were
0.87 and 0.76, respectively. The applicability of the proposed method
is significantly higher than that of the class-based method, which
shows the plausibility of the proposed method.  }

\ekeywords{statistically-based NLP, example-based NLP, use of thesaurus, verb sense disambiguation}

\begin{document}
\maketitle
\section{はじめに}
\label{sec:introduction}
単語の多義性を解消するための技術は，機械翻訳における訳語の選択や仮名漢
字変換における同音異義語の選択などに応用できる．そのため，さまざまな手
法\cite{Nagao96}が研究されているが，最近の傾向ではコーパスに基づいて多
義性を解消するものが多い．

コーパスに基づく手法では，単語と単語や語義と語義との共起関係をコーパス
から抽出し，抽出した共起関係に基づいて入力単語の語義を決める．しかし，
抽出した共起関係のみでは全ての入力には対応できないというスパース性の問
題がある．

スパース性に対処するための一つの方法は，シソーラスを利用することである．
シソーラスを使う従来手法には，クラスベースの手法
\cite{Yarowsky92,Resnik92,Nomiyama93,Tanaka95a}や事例ベースの手法
\cite{Kurohashi92,Iida95,Fujii96a}がある．クラスベースの手法では，シス
テムに入力された単語(入力単語)の代りに，その上位にある，より抽象的な節
点を利用する\footnote{本章では単語と語義と節点とを特には区別しない．}．
一方，事例ベースの手法では，このような抽象化は行わない．すなわち，入力
単語がコーパスに出現していない場合には，出現している単語(出現単語)のう
ちで，入力単語に対して,シソーラス上での距離が最短の単語を利用する．

ところで，シソーラス上では，2単語間の距離は，それらに共通の上位節点
\footnote{「二つの節点に共通の上位節点」といった場合には，共通の上位節
点のうちで最も深い節点，すなわち，根から最も遠い節点を指す．}の深さに
より決まる．つまり，共通の上位節点の深さが深いほど，2単語間の距離は短
くなる．したがって，事例ベースの手法では，シソーラス上における最短距離
の出現単語ではなくて，最短距離の出現単語と入力単語とに共通の上位節点を
利用しているとも考えられる．こう考えると，どちらの手法も，入力単語より
も抽象度の高い節点を利用している点では，共通である．

二つの手法の相違は，上位節点の決め方と その振舞いの解釈である．まず，
上位節点の決め方については，クラスベースの手法が，当該の入力単語とは独
立に設定した上位節点を利用するのに対して，事例ベースの手法では，入力単
語に応じて，それに最短距離の出現単語から動的に決まる上位節点を利用する．
次に，上位節点の振舞いについては，クラスベースの手法では，上位節点の振
舞いは，その下位にある節点の振舞いを平均化したものである．一方，事例ベー
スの手法では，上位節点の振舞いは，入力単語と最短距離にある出現単語と同
じである．

このため，クラスベースの手法では，クラス内にある単語同士の差異を記述で
きないし，事例ベースの手法では，最短距離にある出現単語の振舞いが入力単
語の振舞いと異なる場合には，当該の入力の処理に失敗することになる．これ
は，一方では平均化により情報が失なわれ\cite{Dagan93}，他方では個別化に
よりノイズに弱くなる\cite{Nomiyama93}という二律排反な状況である．

クラスベースの手法でこの状況に対処するためには，クラスの抽象化の度合
を下げればよい．しかし，それには大規模なコーパスが必要である．一方，事
例ベースの手法では，最短距離の出現単語だけではなくて，適当な距離にある
幾つかの出現単語を選び，それらの振舞いを平均化して入力単語の振舞いとす
ればよい．しかし，幾つ出現単語を選べば良いかの指針は，従来の研究では提
案されていない．

本稿では，平均化による情報の損失や個別化によるノイズを避けて，適当な抽
象度の節点により動詞の多義性を解消する手法を提案する．多義性は，与えら
れた語義の集合から，尤度が1位の語義を選択することにより解消される．そ
れぞれの語義の尤度は，まず，動詞と係り受け関係にある単語に基づいて計算
される．このとき，尤度が1位の語義と2位の語義との尤度差について，その信
頼下限\footnote{確率変数の信頼下限というときには，その推定値の信頼下限
を意味する．確率変数$X$の(推定値の)信頼下限とは，$X$の期待値を$\langle
X \rangle$，分散を$var(X)$とすると$\langle X \rangle - \alpha
\sqrt{var(X)}$である．また，信頼上限は$\langle X \rangle + \alpha
\sqrt{var(X)}$である．$\alpha$は推定の精度を左右するパラメータであり，
$\alpha$が大きいと$X$の値が実際に信頼下限と信頼上限からなる区間にある
ことが多くなる．}が閾値以下の場合には語義を判定しないで，信頼下限が閾
値よりも大きいときにのみ語義を判定する．語義が判定できないときには，シ
ソーラスを一段上った節点を利用して多義性の解消を試みる．この過程を根に
至るまで繰り返す．根においても多義性が解消できないときには，その係り受
け関係においては語義は判定されない．

提案手法の要点は，従来の研究では固定的に選ばれていた上位節点を，入力に
応じて統計的に動的に選択するという点である．尤度差の信頼下限は，事例ベー
スの手法において，「幾つ出現単語を選べば良いか」を決めるための指標と考
えることができる．あるいは，クラスベースの手法において，「平均化による
情報の損失を最小にするクラス」を，入力に応じて設定するための規準と考え
ることができる．

以下，\ref{sec:model} 章では動詞の多義性の解消法について述べ，
\ref{sec:experiment} 章では提案手法の有効性を実験により示す．実験では，
主に，提案手法とクラスベースの手法とを比較する．\ref{sec:discussion} 章
では提案手法とクラスベースの手法や事例ベースの手法との関係などを述べ，
\ref{sec:conclusion} 章で結論を述べる．

\section{動詞の多義性の解消法}
\label{sec:model}

提案手法では，シソーラスに沿って段階的に入力単語を抽象化し，それぞれの
段階で動詞語義の尤度を計算する．動詞語義の尤度は尤度パラメータ(確率変
数)の値なので，シソーラスに沿った段階的な抽象化は，尤度パラメータの標
本空間(定義域)を，シソーラスに沿って段階的に拡張することで実現する．

このように，提案手法では，尤度パラメータの標本空間は可変なのであるが，
説明の順番としては，まず，固定された標本空間での多義性の解消法について
述べたあとで，可変の標本空間での多義性の解消法について述べ，最後に，
\cite{Dagan94}の手法を変形した手法について述べる．

本章で述べる方法は，一つの係り受け関係において動詞語義を決定する方法で
ある．複数の係り受け関係がある場合には，関係ごとに尤度1位の語義と2位の
語義との尤度差の信頼下限を得て，その値が最大の係り受け関係に従って動詞
の語義を決める．ただし，全ての関係において尤度差の信頼下限が閾値以下で
ある場合には，語義は判定されない．

\subsection{固定された標本空間における動詞の多義性解消}
\label{sec:solid}

関係$r$ にある単語$W$と動詞$V$とが与えられ，それぞれの語義集合が
$W=\{w_1,w_2,\ldots\}$，$V=\{v_1,v_2,\ldots\}$であるとき，語義$v_i$の
尤度パラメータ$F_i$の標本空間を定義する．まず，語義$w_h$と$v_i$とが関
係$r$で共起することを$r(w_h,v_i)$で表し，その共起頻度を$n(r(w_h,v_i))$とする．
このとき，単語$W$と語義$v_i$の共起頻度
$n_i = n(r(W,v_i)) = \sum_{w_h \in W} n(r(w_h,v_i))$に基づいて語義
$v_i$の尤度を決める．$n(r(W,v_i))$は，標本空間が
$\{r(W,v_1),r(W,v_2),\ldots\}$である確率変数$N(r(W,v_i))$の観測値であ
る．$N(r(W,v_i))$の標本空間は$F_i$の標本空間でもある．

本稿では，この標本空間における共起頻度の分布が一般化超幾何分布
\footnote{ある母集団が$k$ 種類の個体からなるとき，それぞれの種類の個体
数を$N_1,N_2,...,N_k$とする$(N=N_1 + \cdots + N_k)$．$n$個の個体を非復
元抽出したとき，それぞれの種類の個体が$n_1,n_2,...,n_k(n=n_1 + \cdots +
n_k)$だけ選ばれる確率は，一般化超幾何分布$ h(n_1 \cdots n_k|N_1 \cdots
N_k) = \left(
      \begin{array}{l}
        N_1 \\
        n_1
      \end{array}
    \right)
    \cdots
    \left(
      \begin{array}{l}
        N_k \\
        n_k
      \end{array}
    \right)
    \left(
      \begin{array}{l}
        N \\
        n
      \end{array}
    \right)^{-1} $で表される．なお，$k=2$の場合が超幾何分布である．}に
従うと仮定する．つまり，標本空間を
$\{r(W,v_1),r(W,v_2),\ldots,r(W,v_k)\}$としたとき，$W$と$v_i$との共起
頻度を表す確率変数$N(r(W,v_i))$の値には$0 \le N(r(W,v_i)) \le N_i$とい
う制限があり，$N_1 + N_2 + \cdots + N_k = N$ であるとする．

このとき，語義$v_i$の尤度パラメータ$F_i$を以下のように定義する．
\begin{equation}
  \label{D1}
  F_i = \frac{N_i - n_i}{N - n}.
\end{equation}
ただし，$n = n_1 + n_2 + \cdots + n_k$．すると，$F_i$の期待値，分散，
および，$F_i$と$F_j$の共分散は以下の通りである(付録A参照)．
\begin{eqnarray}
  \label{D2}
  \langle F_i \rangle & = &(n_i + 1)/(n + k), \\
  \label{D3}
  var(F_i) & = & p_i(1-p_i)/(n + k + 1), \\
  \label{D4}
  cov(F_i,F_j) & = & -p_i p_j /(n + k + 1).
\end{eqnarray}
ただし，$p_i = \langle F_i \rangle$である．

動詞の語義を判定するかしないかは，$D = F_1 - F_2$の信頼下限($Pl$)に基
づいて決める．ここで，$\langle D \rangle = \langle F_1 \rangle -
\langle F_2 \rangle$，$var(D) = var(F_1) + var(F_2) - 2 cov(F_1,F_2)$
である．ただし，動詞の語義を適当に並べかえて，$i \ge j$ ならば $n_i
\ge n_j$であるようにする．

推定精度を左右する$\alpha$と閾値$\theta$を適当に選んで，$Pl(v_1) =
\langle D \rangle - \alpha \sqrt{var(D)} > \theta$である場合には$v_1$
を語義とする．そうでない場合には関係$r$においては語義を判定しない．な
お，$\alpha$と$\theta$の値は\ref{sec:experiment} 章で述べる．

複数の関係がある場合には，前述のように，最大の$Pl$である関係(信頼下限
最大の関係)に基づいて語義の判定を行う．これは，\ref{sec:variable} 節と
\ref{sec:dagan} 節で述べる手法についても同様である．なお，以後，特に断
わらない限り，信頼下限とは，尤度1位の語義と2位の語義との尤度差($D$)の
信頼下限($Pl$)のことである．

\paragraph{例}

「初めて理由を聞いた」における「聞く」の多義性を解消する．「聞く」の語
義としては，「音を耳に感じとる(HEAR)」と「質問する(ASK)」とを考える．
なお，以下では，$F_i$という表記の代りに$F(HEAR)$や$F(ASK)$という表記を
用いる．また，共起頻度はEDR日本語コーパス\cite{EDR95}の一部における共
起頻度である．

「初めて理由を聞いた」には，「副詞(初めて，聞く)」と「を(理由，聞く)」
という二つの係り受け関係\footnote{本稿での係り受け関係の種類は
\ref{sec:data} 節で述べる．}があるので．それぞれについて信頼下限を求め
ると以下のようになる．まず，「初めて」は「聞く」との共起回数は 1 回で，
HEAR と共起している．このとき，$\alpha = 1$とすると，(\ref{D2})式から
$\langle F(HEAR) \rangle = (1 + 1)/(1 + 2) \simeq 0.67$，$\langle
F(ASK) \rangle = (0 + 1)/(1 + 2) \simeq 0.33$である．分散は(\ref{D3})
式から$var(F(HEAR)) = var(F(ASK)) \simeq 0.056 $であり，共分散は
(\ref{D4})式から$cov(F(HEAR),F(ASK)) \simeq - 0.056$である．以上より，
$\langle D \rangle = \langle F(HEAR) \rangle - \langle F(ASK) \rangle
\simeq 0.33$，かつ，$var(D) = var(F(HEAR)) + var(F(ASK)) - 2
cov(F(HEAR),F(ASK)) \simeq 0.22$，$\sqrt{var(D)} \simeq 0.47$である．
よって，$Pl(HEAR) = \langle D \rangle - \sqrt{var(D)} \simeq - 0.14$で
ある．次に，「理由」は関係「を」では「聞く」との共起回数は 5 回で，
HEAR と 0 回，ASK と 5 回共起している．よって，上と同様な計算により，
$Pl(ASK) \simeq 0.47$となる．

「聞く」の語義は，$\theta = 0$とすると，以下のように決まる．まず，「副
詞(初めて，聞く)」では，$Pl(HEAR) \simeq - 0.14 \le \theta$であるので，
語義は判定されない．一方，「を(理由，聞く)」では，$Pl(ASK) \simeq 0.47
> \theta$であるので，ASKが語義として選択される．語義が判定された関係は
「を」のみであるので，全体では ASK が語義として選択される．

この例では，信頼下限最大の係り受け関係に従って語義を判定した結果が成功
している．失敗する例については次節で述べる．

\vspace{\baselineskip}

本節で述べた手法は，単語と動詞語義との共起頻度に基づいて動詞の多義性を
解消する手法であるが，この手法は，容易にクラスベースの手法に拡張できる．
すなわち，あるクラスが与えられたときには，そのクラスと動詞語義との共起
頻度に基づいて動詞の多義性を解消すればよい．このとき，クラスと動詞語義
との共起頻度を得るには，そのクラスに属する語義の全てについて動詞語義と
の共起頻度を得て，それらの和をとればよい．これは，\ref{sec:dagan} 節で
述べる手法についても同様である．

\subsection{可変の標本空間における動詞の多義性解消}
\label{sec:variable}

前節で述べた手法は，標本空間$\{r\} \times \{w_1,w_2,\ldots\} \times
\{v_1,v_2,\dots\}$を縮小した標本空間$\{r(W,v_1),r(W,v_2),\ldots\}$にお
ける共起頻度の分布についての手法である．ここでは，標本空間をシソーラス
に沿って拡張することを考える．標本空間を段階的に拡張し，各段階において
信頼下限を求め，信頼下限が閾値より大となった時点で語義を判定し，判定の
プロセスを終える．

以下では，まず，標本空間の拡張の仕方について述べ，次に，信頼下限の求め
方について述べる．最後に例を示す．

\subsubsection{標本空間の拡張の仕方}

ここで考える標本空間は$\{r\} \times U_i \times \{v_1,v_2,\ldots\}$であ
る．$U_i$は，単語$W$の語義集合$W=\{w_1,w_2,\ldots\}$を，シソーラス
\footnote {本稿では，シソーラスとは，一つの根を有するDAG(Directed
Acyclic Graph)であるとする．シソーラスの節点のうちで，根は，それに接続
する枝の終点となることはなく，かつ，根からは全ての節点に対して有向道が
ある．また，そこから出ていく枝がないような節点を葉と呼ぶ．さらに，ある
節点の支配下の節点とは，その節点から到達できる節点である．}の構造に従っ
て拡張したものである．$U_i$は$U_{ij}$の和集合として定義されるので，
$U_{ij}$を定義してから，$U_i$を定義する．まず，$U_{ij}$は，根から$w_j$
までの道上の節点において，根からの距離が$i$にある節点が支配する葉の集
合\footnote{任意の単語の任意の語義は葉で表現されると仮定する．この場合
には各々の語義は互いに支配関係にない．分類語彙表\cite{Kokken64}とEDR概
念体系とを，\ref{sec:experiment} 章では，実験に用いるのであるが，分類語
彙表の場合には，この仮定が成立する．しかし，EDR概念体系の場合には，語
義にあたる概念が葉であるとは限らないため，その語義にあたる節点が別の語
義にあたる節点を支配している場合がある．その場合には，ある節点における
葉の数が，その節点が支配する語義の数と一致しない．そのため(\ref{U2a})
式や(\ref{U2b})式において考慮されない語義がでる．本稿ではこの問題は無
視し，全ての語義が葉に相当するとして尤度を計算した．}として定義される．
このとき，根から$w_j$までの距離を$l_j$とすると，
\begin{equation}
  \label{U_0j}
  U_{0j} \supseteq U_{1j} \supseteq \cdots \supseteq U_{l_j j} = \{w_j\}
\end{equation}
である．なお，$k>l_j$のときには$U_{kj}=\phi$である．次に，$U_i$を以下の
ように定義する．
\begin{equation}
  \label{U_i}
  U_i = \bigcup_{w_j \in W} U_{ij}.
\end{equation}
$i \le j$のときには，(\ref{U_0j})式と同様に，$U_i \supseteq U_j$が成立
する．このとき，標本空間は，$l=\max_{w_j \in W} l_j$とすると，$U_l$か
ら順に，$U_{l-1}, U_{l-2}, \ldots, U_0$と拡張される．

たとえば，図\ref{fig:U_i} で$W = \{w_4,w_5\}$とすると，$U_0 =
\{w_1,w_2,w_3,w_4,w_5,w_6\}$, $U_1 = U_2 = \{w_4,w_5,w_6\}$, $U_3 =
\{w_5\}$である．このとき，標本空間は，$U_3, U_2, U_1, U_0$の順に拡張さ
れる．

複数の親を持つ節点の場合には，根からの距離として複数の道の中で最長のも
のを選択すれば，$i \le j$のときに$U_i \supseteq U_j$となる．たとえば，
根$a$から葉$e$までの二つの道が，$a \rightarrow b \rightarrow c
\rightarrow d \rightarrow e$と$a \rightarrow b \rightarrow f
\rightarrow e$であるとき，それぞれの節点の根からの距離は，
$a=0,b=1,c=2,d=3,e=4,f=2$，とする．

標本空間の拡張の仕方は他にも考えられるが，DAGを対象とする場合には，上
述の方法が簡明であると考える．

\begin{figure}[htbp]
  \begin{center}
    \leavevmode
    \atari(88,64)
  \end{center}
  \caption{$U_i$の例}
  \label{fig:U_i}
\end{figure}

\subsubsection{信頼下限の求め方}

関係$r$ にある単語$W$と動詞$V$について，それぞれの語義集合を
$W=\{w_1,w_2,\ldots\}$, $V=\{v_1,v_2,\ldots\}$とする．ここで考える標本
空間は，$I = \{r\} \times U_i \times V$である．

語義$v_j$の尤度パラメータ$F(W^\prime,v_j|I)$は，$W^\prime = W \cap
U_i$とすると，次のように定義される．
\begin{eqnarray}
  \label{U1}
  F(W^\prime,v_j|I) &= &F(v_j | I) F(W^\prime|v_j,I) \nonumber \\
  & = & F(v_j | I) \sum_{w \in W^\prime} F(w | v_j,I).
\end{eqnarray}
$F(W^\prime,v_j|I)$は，図\ref{fig:I} に示されるような標本空間の構造，す
なわち，まず，動詞の語義を選び，次に，その語義のもとで単語の語義を選ぶ
という構造を反映している．

\begin{figure}[htbp]
  \begin{center}
    \leavevmode
    \atari(108,72)
  \end{center}
  \caption{標本空間$I$の構造}
  \label{fig:I}
\end{figure}

$F(W^\prime,v_j|I)$の期待値は以下の通りである．
\begin{equation}
  \label{U3}
  \langle F(W^\prime,v_j|I) \rangle = \langle F(v_j | I) \rangle \sum_{w \in W^\prime} \langle F(w | v_j,I) \rangle.
\end{equation}
ただし，$F(v_j|I)$と$F(W^\prime | v_j,I)$とは確率的に独立であるとみな
した．なお，分散や共分散は付録Bで与える．

(\ref{U1})式における$F(v_j|I)$や$F(w | v_j,I)$は，前と同じように一般化
超幾何分布に従う確率変数であり， それらの期待値は以下の通りである．
\begin{eqnarray}
  \label{U2a}
  \langle F(v_j | I) \rangle & = & \frac{\sum_{u \in U_i} n(r(u,v_j)) + 1}{\sum_{u \in U_i,v \in V} n(r(u,v)) + |V|}\;\;, \\
  \label{U2b}
  \langle F(w | v_j,I) \rangle & = & \frac{n(r(w,v_j)) + 1}{\sum_{u \in U_i} n(r(u,v_j)) + |U_i|}\;\;.
\end{eqnarray}
ただし，$n(r(u,v))$は関係$r$における共起頻度を表す．分散や共分散も
(\ref{D3})，(\ref{D4})式と同様に得られる．

(\ref{U2a})式と(\ref{U2b})式に使われる数値のなかで，まず，$|V|$は動詞
の語義の数である．また，$|U_i|$は標本空間の可変な部分の大きさを表わす
数値である．$|U_i|$はシソーラスの構造から決まるので，シソーラスの節点
にあらかじめ記録しておくことにより，実行時の計算量を減らす．たとえば，
図\ref{fig:U_i} で$|U_2|$の値を求めるときには，二つの節点$u_{24}$と
$u_{25}$に記録されている$|U_{24}|=1$と$|U_{25}|=2$の和をとる．同様に，
$\sum_{u \in U_i} n(r(u,v_j))$の値も，各節点に，支配下の葉と$v_j$との
共起頻度の和を記録しておき，それを利用して求める\footnote{共起頻度の和
は葉から根に再帰的に共起頻度を伝播することで記録する．たとえば，図
\ref{fig:U_i} では，$u_1$に記録される値は$u_{24}$と$u_{25}$に記録されて
いる値の和である．多重継承があるときには，この伝播の過程で，一つの節点
に複数の親がある場合がある．その場合には，その節点に記録されている共起
頻度の値を均等に親に分ける．}．図\ref{fig:U_i} の例では，$U_2$について，
$\sum_{u \in U_2} n(r(u,v_j)) = n(r(w_4,v_j)) + n(r(w_5,v_j)) +
n(r(w_6,v_j))$を求めるためには，$u_{24}$に記録されている
$n(r(w_4,v_j))$の値と$u_{25}$に記録されている$n(r(w_5,v_j)) +
n(r(w_6,v_j))$の値との和をとる．また，(\ref{U2a})式や(\ref{U2b})式の値
は，$U_i$ごとに計算され，$U_i$は最大でシソーラスの高さだけの数しかない
ので，これらの値を計算することは計算量の面で困難ではない．

動詞の語義を判定するかしないかは，$D = F(W^\prime,v_1|I) -
F(W^\prime,v_2|I)$の信頼下限($Pl$)に基づいて決める．ただし，動詞の語義
を適当に並べかえて，$\langle F(W^\prime,v_1|I) \rangle \ge \langle
F(W^\prime,v_2|I) \rangle \ge,\ldots$であるようにする．なお，$D$の期待
値は$\langle D \rangle = \langle F(W^\prime,v_1|I) \rangle - \langle
F(W^\prime,v_2|I) \rangle$である．また，分散$var(D)$は付録Bで与える．

語義を判定するために，$\alpha$と$\theta$を適当に選んで，$Pl(v_1) =
\langle D \rangle - \alpha \sqrt{var(D)} > \theta$である場合には$v_1$
を語義とし，語義判定のプロセスを終える．そうでない場合には，$U_i$の段
階では語義の判定をせずに，標本空間をシソーラスに沿って拡張した
$U_{i-1}$で再び語義の判定をする．$U_0$においても判定ができないときには
関係$r$においては語義の判定を行わない．なお，$\alpha$と$\theta$の値は
\ref{sec:experiment} 章で述べる．

\paragraph{例}

「私は関係者にいきさつを聞いた」における「聞く」の多義性を解消する．
「聞く」の語義としては，前節と同様に，ASKとHEARとを考える．また，
$\alpha=1,\theta=0$とする．なお，計算に必要なその他の詳細は省略する．

「私は関係者にいきさつを聞いた」には，「は(私，聞く)」「に(関係者，聞
く)」「を(いきさつ，聞く)」という三つの係り受け関係があるので，それぞ
れについて信頼下限を計算し，語義を求める．

「私」は関係「は」では「聞く」との共起回数は6回で，HEARと5回，ASKと1回
共起している．そのため，標本空間を拡張するまでもなく$Pl(HEAR) = 2.1
\times 10^{-1} > \theta$となった．

「関係者」は関係「に」では2回共起し，HEARと1回，ASKと1回共起している．
しかし，このHEARでの共起はタグ付けの誤りであり，ASKと共起すべきもので
あった．とにかく，この段階ではHEARとASKとで尤度差はない．しかし，シソー
ラス(分類語彙表)を2段階あがった時点($U_4$)では，$U_4$全体でHEARとの共
起は2回，ASKとの共起は9回である．このとき，$|U_4| = 69$であり，
$Pl(ASK) = 5.5 \times 10^{-4} > \theta$となり，ASKが語義として選択され
る．これは，タグ付けの誤りを回避した例である．

「いきさつ」は関係「を」では「聞く」との共起頻度は0である．一段シソー
ラスを上ったときの標本空間全体ではASKと1回，HEARと0回共起する．しかし，
この段階では$Pl(ASK) < \theta$であるので語義は判定されない．そのま
ま標本空間を拡張していくと，$U_1$で頻度の分布が逆転し，HEARで171回，
ASKで104回共起している．しかし$|U_1| = 26984$と標本空間の大きさが大き
いので信頼下限は閾値$\theta$を超えない．結局，この係り受け関係では語義
は判定されない．これは，シソーラス上での最短距離の語義に従えば成功して
いた例である．

三つの係り受け関係のうちで「は(私，聞く)」が最も信頼下限$Pl$が大きい．
一般に，シソーラスを上ると標本空間の大きさは指数的に大きくなるので，尤
度は指数的に小さくなる．そのため，標本空間が小さいときの信頼下限は，そ
れが大きいときに比べて大きい．

この例では，「は(私，聞く)」に従って語義を選択するので，HEARが語義に選
ばれる．これは失敗である．なお，最大の信頼下限に基づく語義選択の妥当性
は，\ref{sec:experiment} 章で実験により確かめる．

\subsection{Daganの手法}
\label{sec:dagan}

多義性を解消するときに，語義の判定が可能なものだけを判定するという手法
は，\cite{Dagan94}でも採用されている．\cite{Dagan94}では，機械翻訳にお
ける訳語の選択を目的としているが，ここでは，その手法を，動詞語義の選択
のために修正したものについて述べる．以下では，この手法を単にDaganの手
法と呼ぶ．また，本節で用いられている記号のうちで，新たに定義されていな
い記号については，\ref{sec:solid} 節と同じ意味で用いられている．

標本空間についていえば，Daganの手法は，\ref{sec:solid} 節と同じ，固定さ
れた標本空間を使う．ただし，\cite{Dagan94}では共起頻度は多項分布をして
いると仮定している．

動詞の語義を判定するかどうかは，$\hat{p_1}$と$\hat{p_2}$との対数比
$\ln(\hat{p_1}/\hat{p_2})$に基づいて決める．ただし，
$\hat{p_1},\hat{p_2},\ldots$は，$n_1,n_2,\ldots$から最尤推定される
$r(W,v_1),r(W,v_2),\ldots$の確率であり，$\ln(\hat{p_1}/\hat{p_2}) =
\ln(n_1/n_2)$，$var(\ln(\hat{p_1}/\hat{p_2})) \simeq 1/n_1 + 1/n_2$で
ある．もし，$n_1$，$n_2$で$0$なるものがあれば，$0$の代りに$0.5$を用い
る．

$\alpha$と$\theta$を適当に選んで，$Pl(v_1) = \ln(\hat{p_1}/\hat{p_2}) -
\alpha \sqrt{var(\ln(\hat{p_1}/\hat{p_2}))} > \theta$である場合には
$v_1$を語義とする\footnote{\cite{Dagan94}では`$>$'ではなく`$ \ge$'
であるが，`$ \ge$'の場合には，$\theta = 0$としたときに $\hat{p_1} =
\hat{p_2}$であっても$v_1$が選ばれることになるため`$>$'とした．ただし実
際にはどちらを用いても同じことである．}．そうでない場合には関係$r$では
語義は判定されない．なお，\cite{Dagan94}では$\alpha = 1.282$, $\theta
= 0.2$が選ばれているので，\ref{sec:experiment} 章の実験でもそれに従っ
た．

\subsection{Daganの手法と提案手法との違い}

Daganの手法と提案手法との基本的な違いは，標本空間が固定か可変かという
ことである．提案手法が，尤度の比較に差を用いたり，分布に一般化超幾何分
布を仮定したりしているのは，可変の標本空間を上手く取扱うためである．

まず，尤度の比較に差を用いた場合には，標本空間を拡張するたびに尤度や尤
度差が指数的に小さくなるので，標本空間の大きさを信頼下限に直接反映させ
ることができる．Daganの手法のように(対数)比を用いた場合には，第１位の
語義の尤度と第２位の語義の尤度とはオーダとしては違わないため，標本空間
の大きさは直接には反映されない．

次に，一般化超幾何分布を用いている理由は，標本空間の大きさを明示的に取
扱うためである．提案手法では，標本空間を拡張するたびに，尤度パラメータ
の期待値や分散が変化することが必要である．一般化超幾何分布に従うと，
(\ref{U2b})式で示されるように，標本空間の可変な部分の大きさを$|U_i|$と
して明示的に取り扱える．

標本空間の大きさを信頼下限に反映させる理由は，標本空間が小さいときほど
語義の判定結果が信頼できると考えているためである．

\section{実験}
\label{sec:experiment}

実験のデータ／手法／結果について順に述べる．

\subsection{実験データ}
\label{sec:data}

\subsubsection{コーパスからの実験データの抽出}

EDR日本語コーパス\footnote{本稿で用いたEDR日本語コーパス，日本語単語辞
書，概念体系辞書はVersion1.5である．}から，動詞を「係り」または「受け」
とする係り受け関係を抽出した．

EDR日本語コーパスは，新聞・雑誌・辞典などの流通文書から1文単位でとられ
た約22万文からなるコーパスであり，各文は，人手により，形態素・構文・意
味解析されている．なお，EDR日本語コーパスにおける形態素解析の結果には，
動詞などの活用語の基本形は示されていない．そこで，本実験では，EDR日本
語コーパスでの形態素解析結果をもとに，JUMAN\cite{Matsumoto94}を用いて
動詞の基本形を同定した．

EDR日本語コーパスにおける解析結果のうちで，本稿で利用するものは，文節
間の係り受け関係と文節の主辞に付与された語義(概念識別子)とである．文節
間の係り受け関係は，構文解析の結果から得た．また，構文解析の結果には句
の主辞に相当するものに印が付いているので，それを利用して文節の主辞を得
た．

抽出したものは，コーパスにおいて主辞の印がついている動詞と，それと係り
受け関係にある文節の集合である．これは，主節か従属節か連体修飾節のいず
れかであるが，本稿では一括してセットと呼ぶ．多義性解消は各セットごとに
行なわれる．

なお，次のセットは抽出しなかった．
\begin{itemize}
\item 多義性解消の対象である動詞に概念識別子が割当てられていないもの．
\item 多義性解消の対象である動詞を主辞とする文節(動詞文節)が受身，ある
いは，使役であるもの．
\item 多義性解消の対象である動詞をJUMANにより基本形に変形できないもの．
\end{itemize}
また，セットに含まれる文節で，次のものは除いた．
\begin{itemize}
\item 動詞／形容詞／形容動詞／副詞／名詞／接尾語／数字 以外を主辞とする文節．
\item 動詞文節で，主辞である動詞をJUMANにより基本形に変形できないもの．
\end{itemize}

\newpage
係り受け関係としては図\ref{fig:rels} に示されている21種類を用いた．「*」
で示されているものは関係のグループにつけた名前であり，関係の名前ではな
い．「*係り」というのは，その品詞\footnote{「形容(動)詞」とは，形容詞
または形容動詞のことである．また，名詞／接尾語／数字は一括して「名詞」
とした．}の形態素を主辞とする文節に係っていくことを示している．なお，
係りの関係は関係名の先頭に「係：」を付けることで示す．「*受け」は，そ
の文節を受けることを示している．

「*受け」の関係で「*格助詞」で示されるのは，名詞に続く助詞列が格助詞を
含む場合である．なお，「φ」は，名詞文節を受ける際に，格助詞や係助詞な
どが介在しない場合である．また，関係が「*係助詞」で示されるのは，名詞
に続く助詞列が係助詞のみからなる場合である．複数の係助詞が共起する場合
には最後の係助詞により関係名が決まる．その他の場合は「助詞相当表現」と
して扱った．この例としては「に対して」などがある．

\begin{figure}[htbp]
  \begin{center}
    \leavevmode
    \atari(99,37)
  \end{center}
  \caption{21種類の係り受け関係}
  \label{fig:rels}
\end{figure}

抽出したセットから頻度500以上の動詞74語を選び，それらの動詞を含む約
8,9000セットを実験の対象とした．実験では頻度が5000を超える4語(ある，い
う，する，なる)については，無作為に抽出した5000のセットについて実験を
行った．その他の動詞については，抽出された全てのセットについて実験を行っ
た．ただし，頻度が10未満の語義を語義としてもつ動詞を多義性解消の対象と
するようなセットは実験のデータから除いた．なお，この74語には異なり語義
数が1の動詞は含まれていない．

\subsubsection{シソーラス}

実験に用いたシソーラスはEDR概念体系と分類語彙表である．EDR概念体系には
約40万の概念識別子があり，それらはDAGを構成している．分類語彙表には分
類番号がつけられた約3万7千の単語があり，それらは意味的に分類された6レ
ベルの木構造をなす．

\subsection{実験の手法}
\label{sec:method}

それぞれの動詞について，抽出したセットに対して，10分割のクロスバリデー
ション法で多義性解消の実験を行った．すなわち，抽出したセットの集合を10
個の均等な大きさの部分集合に分け，9個の部分集合を訓練データとして共起
頻度を得て，残りの部分集合をテストデータとして多義性の解消をするという
ことを10回繰り返した．

訓練の段階では，以下のようにして，動詞の語義(概念識別子)と，それに係る
(それを受ける)単語の語義との共起頻度を得た．まず，シソーラスとして分類
語彙表を使う場合には，その単語に割り当てられている$n$個の分類番号の全
てに対して，$1/n$を動詞語義との共起頻度とした．次に，シソーラスとして
EDR概念体系を使う場合には，共起の相手である単語に，コーパスにおいて概
念識別子が付与されていれば，その概念識別子に共起頻度1を与え，そうでな
ければ，分類語彙表の場合と同様に，単語に割り当てられている全ての概念識
別子に対して均等に共起頻度を割当てた．なお，テストの段階では，単語は全
て多義語として取り扱い，単語に付与されている概念識別子は利用しなかった．

コーパスのなかの単語には複合名詞もある．そのときには右からの最長一致に
より辞書引きをした．たとえば，「データ通信」が辞書にないときには「通信」
で引き，「通信データ」の場合には「データ」で引いた．このようにしても辞
書にない単語については共起として数えなかった．分類語彙表をシソーラスと
して使うときには，分類語彙表にない単語は未知語となる．また，EDR概念体
系をシソーラスとして使うときには，EDR日本語単語辞書にもEDR日本語コーパ
スにもない単語は未知語となる(字面が登録されていても概念識別子が登録さ
れていない場合は未知語である)．

\subsection{実験結果}
\label{sec:results}

\subsubsection{多義性解消の精度}

抽出したセットに対して，次の四つの要因を組み合わせて多義性解消の実験を
した．その正確な組合せ方は表\ref{tab:results} にある．1)標本空間は，
\ref{sec:model} 章で述べた意味において，固定または可変である．2)共起の
相手は，標本空間が固定の場合には単語かクラスであり，可変の場合には語義
である．ただし，クラスとは，分類語彙表の場合には，分類番号の上位3桁を
共有する分類番号の集合のことである．また，EDR概念体系の場合には，根か
らの距離が4である概念識別子が一つのクラスを代表する概念識別子であり，
それに支配される概念識別子の集合が一つのクラスである．3)シソーラスには，
分類語彙表かEDR概念体系かを用いた．4)多義性解消の際に動詞語義を判定す
る手法は，差に基づくもの(\ref{sec:solid}節や\ref{sec:variable}節の手法)
か比に基づくもの(Daganの手法)である．なお，表\ref{tab:results} の2行目
に$\alpha = 1.55$などとあるのは，\ref{sec:solid} 節と\ref{sec:variable}
節で述べた，差に基づく手法においてパラメータ$\alpha$の値を1.55などにし
たということである．ただし，$\theta$は0に固定した．また，比に基づく手
法の場合は$\alpha = 1.282$，$\theta = 0.2$と一定である．ここで，差に基
づく手法の$\alpha$の値は，比較の便宜のため，比に基づく手法と適合率の差
が1％未満になるように調整した場合の値である．

抽出したセットに対して，動詞ごとに多義性解消の実験を行い，判定率・適合
率を計算した．ある動詞に対する判定率・適合率の定義は以下の通りである．
\begin{equation}
  \label{app}
  \mbox{判定率} = \frac{\mbox{語義が判定されたセットの数}}{\mbox{その動詞を含むセットの数}},
\end{equation}
\begin{equation}
  \label{pr}
  \mbox{適合率} = \frac{\mbox{正解の数}}{\mbox{語義が判定されたセットの数}}.
\end{equation}
ただし，正解とは，プログラムにより選択された語義が，EDR日本語コーパス
において付与されている語義(概念識別子)と一致する場合をいう．

\begin{table}[htbp]
   \begin{center}
      \leavevmode
      \caption{判定率・適合率の平均値}
      \label{tab:results}
      \smallskip
      \begin{tabular}{|c|cc|cc|cc|cc|cc|c|}\hline
                   標本空間 & \multicolumn{8}{c|}{固定} & \multicolumn{2}{c|}{可変} &      \\
\cline{1-2}\cline{2-9}\cline{10-11}
                   共起対象 & \multicolumn{4}{c|}{単語($\alpha=1.55$)} & \multicolumn{4}{c|}{クラス($\alpha=1.95$)} & \multicolumn{2}{c|}{語義($\alpha=1$)} &      \\
\cline{1-2}\cline{2-5}\cline{6-9}\cline{10-11}
                  Thesaurus & \multicolumn{2}{c|}{分類} & \multicolumn{2}{c|}{EDR} & \multicolumn{2}{c|}{分類} & \multicolumn{2}{c|}{EDR} &  分類 &   EDR &    \raisebox{1.6ex}[0pt]{BASE}  \\
\cline{1-2}\cline{2-3}\cline{4-5}\cline{6-7}\cline{8-9}\cline{10-11}
                    判定手法 &    比 &    差 &    比 &    差 &    比 &    差 &    比 &     差 & \multicolumn{2}{c|}{差} &  \\ \hline \hline 
                            & .267 & .332 & .302 & .363 & .688 & .682 & .765 &  .764 & .726 & .865 & 1.000\\
\raisebox{1.6ex}[0pt]{全体} & .750 & .752 & .753 & .751 & .710 & .707 & .696 &  .696 & .713 & .695 & 0.652\\ \hline \hline 
                            & .132 & .163 & .153 & .179 & .370 & .365 & .517 &  .533 & .539 & .699 & 1.000\\
   \raisebox{1.6ex}[0pt]{group 1} & .541 & .544 & .563 & .567 & .483 & .473 & .445 &  .442 & .488 & .462 & 0.353\\ \hline 
                            & .194 & .250 & .214 & .269 & .510 & .497 & .587 &  .582 & .624 & .793 & 1.000\\
   \raisebox{1.6ex}[0pt]{group 2} & .665 & .675 & .671 & .665 & .608 & .607 & .603 &  .604 & .624 & .587 & 0.515\\ \hline 
                            & .307 & .366 & .356 & .411 & .808 & .795 & .874 &  .865 & .796 & .926 & 1.000\\
   \raisebox{1.6ex}[0pt]{group 3} & .734 & .733 & .725 & .726 & .686 & .685 & .672 &  .673 & .690 & .674 & 0.647\\ \hline 
                            & .331 & .406 & .374 & .444 & .850 & .844 & .905 &  .898 & .829 & .947 & 1.000\\
   \raisebox{1.6ex}[0pt]{group 4} & .861 & .859 & .859 & .854 & .831 & .832 & .825 &  .826 & .823 & .818 & 0.813\\ \hline 
                            & .380 & .485 & .419 & .523 & .917 & .925 & .954 &  .957 & .852 & .966 & 1.000\\
   \raisebox{1.6ex}[0pt]{group 5} & .964 & .963 & .962 & .960 & .957 & .956 & .953 &  .953 & .956 & .954 & 0.951\\ \hline 
       \end{tabular}
   \end{center}
   各欄には，上段に判定率の平均値，下段に適合率の平均値が記載されてい
る．表の2行目に$\alpha = 1.55$などとあるのは，\ref{sec:solid}節と
\ref{sec:variable}節で述べた，差に基づく手法においてパラメータ$\alpha$
の値を1.55などにしたということである．ただし，$\theta$は0に固定した．
また，比に基づく手法の場合は$\alpha = 1.282$，$\theta = 0.2$と一定であ
る．
\end{table}

判定率・適合率は動詞ごとに異なり，かつ，その異なりは最頻の語義の占める
割合と相関があると考えられる．たとえば，一つしか語義がない場合には適合
率は1である．そこで，動詞74語を，最頻の語義の占める割合の小さいものか
ら順に，五つの均等な大きさのグループに分け，各グループごとに判定率・適
合率の平均値を求めた．それらは，表\ref{tab:results} に，group 1〜5とし
て示されている．表の各欄の値は，上段が判定率の平均値であり，下段が適合
率の平均値である．なお，「全体」とある行には動詞全体についての平均値が
載せてある．また，「BASE」とある右端の列には，常に最頻の語義を選ぶ手法
の判定率と適合率がある．この手法は，多義性解消のための手法のベースライ
ンと考えられる\cite{Gale92}．BASEにおいては，全てのセットについて，最
頻の語義を語義とみなすので，判定率は1，適合率は最頻の語義の割合となる．

表\ref{tab:results} において，まず，共起対象として，単語を選んだ場合と
その他(クラスや語義)を選んだ場合とを比べると，シソーラスや判定手法の組
合わせにより違いはあるが，単語を選んだ場合の適合率が，その他の場合より
4〜5％程度高い．しかし，判定率を比べると，その他の場合の方が，35〜50％
程度，単語を選んだ場合より高い．このように単語を共起対象とした場合には，
クラスや語義を共起対象とする場合に比べて，適合率が高くなり判定率が低く
なるが，4〜5％程度の適合率の高さは，35〜50％程度の判定率の低さを埋め合
わせるほどではないと考える．

次に，固定された標本空間での結果について比較する．まず，EDR概念体系と
分類語彙表とを比べると，登録単語数の差を反映して，EDR概念体系の方が判
定率が高い．また，比に基づく手法と差に基づく手法とを比べると，単語を共
起の相手とした場合には，差に基づく手法の方が，6％程度，判定率が高い．
このことは，差に基づく手法の方が共起頻度の違いに敏感なことを示している．
クラスを共起の相手とした場合には，比に基づくものの方が多少判定率が高い
が，その差は1％未満である．この場合に，手法の違いが，共起の相手を単語
にした場合に比べて効かないのは，クラスを共起の相手とした場合には，単語
を共起の相手とした場合に比べて，語義ごとの共起頻度の情報が無視される度
合が強いためであると考えられる．

最後に，固定された標本空間におけるクラスの結果と可変な標本空間の結果と
を，差に基づく手法について比べると，可変な標本空間における方が，分類語
彙表の場合には4％程度，EDR概念体系の場合には10％程度，判定率が高い．前
述のように，クラスを共起の相手としたときには，比／差の手法において全体
の判定率の違いは1％未満である．それに比べて，4あるいは10％程度の違いは
大きいと考える．つまり，語義の判定に比を用いるか差を用いるかに比べて，
標本空間を可変にするか固定にするかは，より重要な違いであると考える．

なお，本稿程度の規模の実験では，1％程度の差があれば，EDR日本語コーパス
が日本語全体を良く代表していると仮定して，日本語全体でも同様な傾向が見
られるといえる．たとえば，固定された標本空間で，共起の相手としてクラス
を選び，分類語彙表をシソーラスとして使ったときには，比に基づく手法の判
定率と差に基づく手法の判定率との差は0.6％であるが，これは，符号検定に
よると，1％の有意水準で有意な差である．

\subsubsection{最大の信頼下限による語義判定の有効性}

表\ref{tab:results} では，動詞ごとに判定率と適合率を得て，それらの平均
を示した．動詞ごとに適合率などを得たのは，動詞ごとのデータ数の違いを吸
収するためである．ところで，本節と次節では，係り受け関係を個別にみるの
で，後述する指標を動詞ごとに得て，それを平均するのは不適当である．なぜ
なら，動詞によっては，出現数の少い係り受け関係があるため，安定した数値
が得られないからである．

そのため，動詞ごとではなくて，抽出したセット全体における指標を調べるが，
この場合でも動詞ごとのデータ数の影響は除きたい．そこで，まず，各動詞に
ついて，前節と同様のセットに対して，\ref{sec:variable} 節で述べた提案手
法により，EDR概念体系を用い，共起の対象を語義とし，$\alpha=1$，
$\theta=0$で多義性の解消をした．次に，その解析結果の中から，各動詞につ
いて500セットを無作為に選び，全体では$500\times74=37,000$セットについ
て本節と次節での分析をした．このセットの中では，最頻の語義が占める割合
は 0.655，語義の判定されたセットの占める割合は0.865，判定されたセット
の中での正解の割合は0.720 である．

さて，提案手法では，複数の係り受け関係があるときには，係り受け関係ごと
に動詞語義と信頼下限とを求め，信頼下限最大の動詞語義を語義として選択し
ている．それが妥当なことは表\ref{tab:1and2} からわかる．

表\ref{tab:1and2} は，抽出されたセットの中で語義が判定されたセットにつ
いて，係り受け関係ごとに語義判定の正解／不正解を調べたときに，信頼下限
最大の係り受け関係での正解／不正解と2番目以降での正解／不正解との関連
を示したものである．各欄の数値は，そのようなセットが，語義の判定された
セットの中で占める割合である．なお，「2番目以降に正解なし」という場合
は，一つのセットにおいて，語義が判定された係り受け関係の数が1である場
合を含む．

表\ref{tab:1and2} から，信頼下限が2番目以降の係り受け関係に正解がある場
合は，0.46と少ないことがわかる．更に，信頼下限最大のものが不正解である
場合には，2番目以降に正解がある割合は$0.021/0.280 \simeq 0.075$のみで
ある．以上から，語義の判定された係り受け関係が複数あるときには，信頼下
限最大の係り受け関係に基づいて語義を判断するのが妥当であるといえる．

\begin{table}[htbp]
  \begin{center}
    \leavevmode
    \caption{信頼下限最大の係り受け関係での正解／不正解と2番目以降のものでの正解／不正解の関連}
    \label{tab:1and2}
    \bigskip
    \begin{tabular}{c|cc|c}
      & 2番目以降に正解あり & 2番目以降に正解なし & 合計 \\ \hline
      信頼下限最大の関係が正解   & 0.442 & 0.278 & 0.720 \\
      信頼下限最大の関係が不正解 & 0.021 & 0.259 & 0.280 \\ \hline
      合計        & 0.463 & 0.537 & 1.000
    \end{tabular}
  \end{center}
\end{table}

\subsubsection{個別の係り受け関係の多義性解消への寄与}

動詞の多義性解消における，係り受け関係ごとの寄与率を表
\ref{tab:contrib} に示す．表\ref{tab:contrib} において，係り受け関係の
「寄与率」とは，正解であったセットの中で，その係り受け関係の信頼下限が
最大であったセット(その関係に基づいて語義が判定されたセット)の割合であ
る．なお，表\ref{tab:contrib} の係り受け関係は，図\ref{fig:rels} に示し
たもののなかで，寄与率が1％以上のものである．また，表\ref{tab:contrib}
では寄与率が5％の上下で境界線を引いた．次に，「出現率」は，その係り受
け関係が出現するセットの数を全セット数で割ったものである．ただし，1セッ
ト中に同じ係り受け関係が複数回出現しても1回として計数した．「1位判定率」
というのは，その係り受け関係が出現したセットにおいて，その関係に基づい
て動詞語義が判定されたセットの割合である．「1位適合率」というのは，そ
の係り受け関係により語義が判定されたセットにおいて，正解であったセット
の割合である．「共出現数」とは，その係り受け関係が出現したセットにおい
て，その関係を含めた係り受け関係の個数の平均値である．

まず，寄与率をみると，「を」「に」「が」のような必須的な格，および，こ
れに準ずる「係：名詞」のものが，やはり，高い．しかし，必須でない係り受
け関係でも，「係：動詞」や「動詞」は寄与率が高い．これは，この二つの出
現率が高いためであろう．つまり，コーパス中には複文が多いためであると考
えられる．

次に，1位判定率は，共出現数が少いほど高くなる傾向がある．それは，判定
の際の競合相手が少くなるためである．そのため，$\mbox{1位判定率}
/(1/\mbox{共出現数}) = \mbox{1位判定率}\times\mbox{共出現数}$は，係り
受け関係の相対的な強さ，つまり，提案手法が，どの係り受け関係により語義
を判定するかの相対的な度合を示していると考えられる．この指標により，係
り受け関係を並べると，大きい方から，「副詞」「を」「動詞」「係：動詞」
「形容(動)詞」「φ」「が」「に」「係：名詞」．．．となる．

ところで，計算機用日本語基本動詞辞書IPAL\cite{IPAL87}のような格フレー
ム辞書には，名詞と動詞の格関係のみが記述されている．したがって，動詞と
動詞や副詞と動詞との共起関係は記述されていない．

ところが，上述のことからは，動詞と動詞や副詞と動詞の係り受け関係も，多
義性解消に高く貢献しているといえる．よって，格フレームだけでなく，動詞
と動詞や副詞と動詞の共起関係を記述しておくことも必要であると考える．

なお，1位適合率については次章で別に述べる．

\begin{table}[htbp]
   \begin{center}
      \leavevmode
      \begin{tabular}{l|ccccc}
係り受け関係 & 寄与率 & 出現率 &  1位判定率 &  1位適合率 & 共出現数\\ \hline
        を   &  0.212 &  0.346 &      0.495 &      0.769 &  2.738\\
  係：動詞   &  0.130 &  0.278 &      0.457 &      0.637 &  2.610\\
  係：名詞   &  0.117 &  0.272 &      0.367 &      0.731 &  2.324\\
      動詞   &  0.116 &  0.247 &      0.430 &      0.679 &  2.837\\
        に   &  0.089 &  0.245 &      0.325 &      0.695 &  2.786\\
        が   &  0.084 &  0.212 &      0.325 &      0.762 &  2.807\\
      副詞   &  0.050 &  0.100 &      0.426 &      0.727 &  3.255\\ \hline
        は   &  0.046 &  0.187 &      0.208 &      0.739 &  3.053\\
        φ   &  0.032 &  0.081 &      0.324 &      0.759 &  3.303\\
形容(動)詞   &  0.029 &  0.067 &      0.369 &      0.733 &  3.030\\
        で   &  0.027 &  0.116 &      0.200 &      0.737 &  3.167\\
助詞相当表現 &  0.014 &  0.075 &      0.162 &      0.727 &  3.003\\
        も   &  0.012 &  0.049 &      0.206 &      0.730 &  2.690\\
        と   &  0.011 &  0.038 &      0.251 &      0.744 &  2.540\\
      から   &  0.011 &  0.036 &      0.229 &      0.843 &  3.124\\
係：形容(動)詞 &  0.011 &  0.034 &      0.287 &      0.665 &  2.497\\
       \end{tabular}
   \end{center}
   \caption{係り受け関係の寄与率}
   \label{tab:contrib}
\end{table}

\section{考察}
\label{sec:discussion}

本章では，提案手法とクラスベースの手法や事例ベースの手法との関係，およ
び，今後の課題について述べる．

\subsection{提案手法とクラスベースの手法や事例ベースの手法との関係}

提案手法とクラスベースの手法とを比較すると，クラスベースの手法における
クラスは，本稿での場合のように，先験的に決めるか，あるいは，データに基
づいて決める\cite{Resnik92,Nomiyama93,Tanaka95a}必要がある．データに基
づく場合には，必要が生じた時点でクラスを変更する必要がある．しかし，提
案手法の場合には，入力に応じて動的に標本空間を定めるため，クラスの設定
自体が不要である．

提案手法と事例ベースの手法とを実験的に比較することは今後の課題であるが，
一つの係り受け関係から動詞の語義を決める場合については，定性的には以下
のことが言える．

まず，動詞語義の尤度についていえば，事例ベースの手法では，入力単語が動
詞語義に付与する尤度は，入力単語から，その動詞語義とコーパスで共起した
単語(出現単語)への，シソーラス上での最短距離に基づいている．一方，提案
手法では，動詞語義の尤度は，シソーラスの構造と動詞語義のシソーラス上で
の頻度分布と入力単語とにより決まる．これは，シソーラス上での距離を，コー
パスでの共起情報と入力単語とを利用して尤度に変換しているとみなすことも
できる．なお，これと同様なことは\cite{Shinnou96}でも行なわれている．し
かし，\cite{Shinnou96}は，単語間の一般的な類似性を，シソーラス上での単
語間の距離とコーパスでの共起頻度の分布とから設定することを目的としてい
て，多義性の解消は直接の目的とはしていない．

次に，提案手法において，\ref{sec:variable} 節で述べた$\alpha$の値を0に
すると，第1位と第2位の語義の尤度差が0でなくなった段階で，必ず，動詞の
語義が判定される．このときには，事例ベースの手法のように，シソーラス上
での最短距離の出現単語に基づいて動詞語義を判定していることになる．一方，
$\alpha$の値を大きくすると，動詞語義の頻度分布に大きな偏りがなければ，
語義は判定されない．つまり，$\alpha$の値を大きくすることは，最短距離以
外にある出現単語も考慮することを意味する．

このことは，\ref{sec:introduction} 章で述べた，「入力単語の振舞いを決め
るのに幾つ出現単語を用いるか」という問題を，$\alpha$の設定に帰着させた
と考えることもできる．

事例ベースの手法で，もし複数の出現単語を使うとしても，幾つ使うかを決め
るためには，シソーラスにおける動詞語義の頻度分布などを考慮しなければな
らないであろう．提案手法では，\ref{sec:variable} 節で述べたように，それ
が既に分散として数式中で考慮されているため，$\alpha$の値を決めることは，
幾つ出現単語を使うかを決めるよりは，容易であると考える．

\subsection{今後の課題}

表\ref{tab:contrib} にあるように，関係ごとの1位適合率は一様ではない．た
とえば，「を」は1位適合率が高く，「係：動詞」は1位適合率が低い．そこで，
1位適合率が高いものは，$\alpha$を小さくすることにより，判定率を高くし，
1位適合率が低いものは，$\alpha$を大きくすることにより，判定率を低くす
ることが考えられる．$\alpha$の設定の仕方は今後の課題である．なお，同様
な考え方として，\cite{Fujii96a}では，複数の格要素における語義の尤度を
足し合わせて全体の尤度とするときに，格ごとの曖昧性解消への貢献度に応じ
て，その格での尤度に重みを付ける手法が述べられている．

本稿では，複数の係り受け関係があっても，それらの間の関係は考慮せずに，
信頼下限が最大のものを選んで多義性の解消をしている．この方法は，表
\ref{tab:1and2} に示すように，有効である．しかし，複数の係り受け関係の
間にある依存関係を利用すれば，判定率や適合率が向上すると考えられる．そ
のような依存関係を取扱うことは今後の課題である．依存関係を考慮したもの
としては，既存の格フレームを利用したり\cite{Kurohashi92,Fujii96a}，格
フレームあるいは決定木を獲得したり\cite{Tanaka95b}，対数線型モデル
\cite{Matsuda88}により依存関係を推定する\cite{Bruce94}などの研究がある
ので，これらと提案手法との融合を検討したい．

\section{おわりに}
\label{sec:conclusion}

シソーラスの構造に従って標本空間を動的に拡張し，動詞の多義性を解消する
手法を提案した．シソーラスを使って多義性を解消する従来手法には，クラス
ベースの手法と事例ベースの手法とがあるが，前者には平均化により情報が失
われるという短所があり，後者には個別化によりノイズに弱くなるという短所
がある．提案手法は，入力に応じて抽象化の度合を統計的に変化させることに
より，情報の損失やノイズを避けながら多義性の解消をしようとしている．

実験では，EDR日本語コーパスから頻度500以上の動詞74語を抽出し，延べで約
89,000の動詞について多義性の解消をした．このとき，最頻の語義を常に選ぶ
場合の適合率は65％，判定率は100％であった．

クラスベースの手法と提案手法とを比較すると，分類語彙表をシソーラスとし
て利用した場合には，適合率は共に71％であったが，判定率は，クラスベース
の手法が68％，提案手法が73％であった．EDR概念体系を利用した場合には，
適合率は共に70％であったが，判定率は，クラスベースの手法が76％，提案手
法が87％であった．両者の判定率を比べると，提案手法の判定率の方が統計的
に有意に高く，提案手法の有効性が示された．

仮名漢字変換や情報検索などに提案手法を応用すること，複数の係り受け関係
の間にある依存関係をモデル化すること，などが今後の課題である．


\acknowledgment

本稿に対して適切な助言を下さった，本学 山本幹雄 講師に感謝する．

\bibliographystyle{jnlpbbl} 

\bibliography{disamb}

\section*{付録}
\label{sec:appendix}


\section*{Ａ 壷のなかに残された玉の割合の推定\footnote{超幾何分布については\cite[Chapter 6]{Jaynes96}にある．}}

\label{sec:apA}

壷の中に$k$ 種類の玉があり，それぞれ，$N_1$,$N_2$,...,$N_k$ 個であると
する($N = \sum_{i=1}^{k} N_i$)．ただし，それらの値は不明である．$n$個
の玉を壷から取り出したとき，それぞれの種類が，$n_1$,$n_2$,...,$n_k$だ
け取り出されたとする($n = \sum_{i=1}^{k} n_i$)．このとき，壷の中に残さ
れた玉$i$の割合 $F_i = (N_i - n_i)/(N - n)$ の期待値$\langle F_i
\rangle$と分散$var(F_i)$，および，$F_i$と$F_j$との共分散$cov(F_i,F_j)$
は$N \rightarrow \infty$ のとき次の通りである．
\begin{equation}
  \label{A1}
  \langle F_i \rangle = (n_i + 1)/(n + k)
\end{equation}
\begin{equation}
  \label{A2}
  var(F_i) = p_i (1 - p_i)/(n + k + 1)
\end{equation}
\begin{equation}
  \label{A3}
  cov(F_i,F_j) = - p_i p_j / (n + k + 1)
\end{equation}
ただし，$p_i = \langle F_i \rangle$ である．

\subsection*{導出の概略}

$N_1$の期待値$\langle N_1 \rangle$，分散 $var(N_1)$，および，$N_1$と
$N_2$との共分散 $cov(N_1,N_2)$ を求め，それを利用して$\langle F_1
\rangle$，$var(F_1)$，$cov(F_1,F_2)$ を求める．その他の$F_i$，$i =
2,\ldots,k$ については対称性から求まる．

$D$ により$k$ 種類の玉が，それぞれ，$n_1$,...,$n_k$個取り出されたという
事象を示す．$N$や$N_i$は$p(\ldots)$の内部で使われたときには，玉の数が
$N$や$N_i$であるという事象を示し，それ以外の場合には玉の数を示す．また，
$I$はこの問題に対する事前知識(標本空間など)を示す．
\begin{equation}
  \label{A4}
  \langle N_1 \rangle = \sum_{N_1 = 0}^{N} N_1 p(N_1 | D,N,I) 
\end{equation}
であるので，まず，$p(N_1 | D,N,I)$ を求める．

ベイズの定理から，
\begin{equation}
  \label{A5}
  p(N_1 | D,N,I) = p(N_1 | N,I) \frac{p(D | N_1,N,I)}{p(D | N,I)}.
\end{equation}
事前確率として次のような一様分布を設定する．
\begin{equation}
  \label{A6}
  p(N_1 | N, I) = \left\{
    \begin{array}{ll}
      1/(N + 1) & 0 \le N_1 \le N \\
      0 & \mbox{otherwise}
    \end{array}
  \right..
\end{equation}
$D$は一般化超幾何分布からの標本であるので，
\begin{equation}
  \label{A7}
  p(D | N_1,N,I) = \frac{
    \left(
      \begin{array}{l}
        N_1 \\
        n_1
      \end{array}
    \right)
    \sum_{N_2 + \cdots + N_k = N - N_1}
    \left(
      \begin{array}{l}
        N_2 \\
        n_2
      \end{array}
    \right)
    \cdots
    \left(
      \begin{array}{l}
        N_k \\
        n_k
      \end{array}
    \right)
    }{
    \left(
      \begin{array}{l}
        N \\
        n
      \end{array}
    \right)
    }.
\end{equation}
$p(D | N,I)$は正規化のため項であり，
\begin{equation}
  \label{A8}
  p(D | N,I) = \sum_{N_1 = 0}^N p(D | N_1,N,I) p(N_1 | N,I).
\end{equation}
(\ref{A5})式に，(\ref{A6})，(\ref{A7})，(\ref{A8})式を代入する
と次式が得られる．
\begin{equation}
  \label{A9}
  p(N_1 | D,N,I) = \frac{
    \left(
      \begin{array}{l}
        N_1 \\
        n_1
      \end{array}
    \right)
    \sum_{N_2 + \cdots + N_k = N - N_1}
    \left(
      \begin{array}{l}
        N_2 \\
        n_2
      \end{array}
    \right)
    \cdots
    \left(
      \begin{array}{l}
        N_k \\
        n_k
      \end{array}
    \right)
    }{
    \sum_{N_1 + \cdots + N_k = N}
    \left(
      \begin{array}{l}
        N_1 \\
        n_1
      \end{array}
    \right)
    \cdots
    \left(
      \begin{array}{l}
        N_k \\
        n_k
      \end{array}
    \right)
    }.
\end{equation}
ここで
\begin{equation}
  \label{A10}
  \sum_{N_1 + \cdots + N_k = N} 
  \left(
    \begin{array}{l}
      N_1 \\
      n_1
    \end{array}
  \right)
  \cdots
  \left(
    \begin{array}{l}
      N_k \\
      n_k
    \end{array}
  \right) =
  \left(
    \begin{array}{l}
      N + k - 1 \\
      n + k - 1
    \end{array}
  \right).
\end{equation}
よって，
\begin{equation}
  \label{A11}
  p(N_1 | D, N, I) =
  \left(
    \begin{array}{l}
      N_1\\
      n_1
    \end{array}
  \right)
  \left(
    \begin{array}{l}
      N - N_1 + k - 2\\
      n - n_1 + k - 2\\
    \end{array}
  \right)
  \left(
    \begin{array}{l}
      N + k - 1\\
      n + k - 1
    \end{array}
  \right)^{-1}
\end{equation}
(\ref{A11})式を(\ref{A4})式に代入すると
\begin{equation}
  \label{A12}
  \langle N_1 \rangle = \frac{(n_1 + 1)(N + k)}{n + k} - 1.
\end{equation}
$F_1 = (N_1 - n_1)/(N - n)$ とすると (\ref{A1})式が得られる．
また $p_1 = \langle F_1 \rangle$とすると，
\begin{equation}
  \label{A13}
  var(N_1) = \frac{p_1(1-p_1)}{n + k + 1}(N + k)(N-n)
\end{equation}
である．よって，
\begin{equation}
  \label{A14}
 var(F_1) = \frac{p_1(1-p_1)}{n + k + 1}\frac{N + k}{N-n} 
\end{equation}
であり，$N \rightarrow \infty$ とすれば(\ref{A2})式が得られる．

$N_1$と$N_2$との共分散は，$cov(N_1,N_2) = \langle (N_1 - \langle
N_1\rangle )(N_2 - \langle N_2\rangle )\rangle = \langle N_1
N_2\rangle - \langle N_1\rangle \langle N_2\rangle$である．
(\ref{A5})式から(\ref{A8})式までと同様な導出により
\begin{equation}
  \label{A15}
  p(N_1, N_2| D, N, I) =
  \left(
    \begin{array}{l}
      N_1\\
      n_1
    \end{array}
  \right)
  \left(
    \begin{array}{l}
      N_2\\
      n_2
    \end{array}
  \right)
  \left(
    \begin{array}{l}
      N - N_1 - N_2 + k - 3\\
      n - n_1 - n_2 + k - 3\\
    \end{array}
  \right)
  \left(
    \begin{array}{l}
      N + k - 1\\
      n + k - 1
    \end{array}
  \right)^{-1}
\end{equation}
であるので，$p_1 = (n_1 + 1)/(n + k)$，$p_2 = (n_2 + 1)/(n + k)$ とすると
\begin{equation}
  \label{A16}
  cov(N_1,N_2) = \frac{- p_1 p_2}{n + k + 1}(N + k)(N - n).
\end{equation}
$F_1 = (N_1 - n_1)/(N - n)$,$F_2 = (N_2 - n_2)/(N - n)$とすると
\begin{equation}
  \label{A17}
  cov(F_1,F_2) = \frac{- p_1 p_2}{n + k + 1}\frac{N + k}{N - n}
\end{equation}
である．$N \rightarrow \infty$ として(\ref{A3})式を得る．


\section*{Ｂ 差の分散}

$D = F(W^\prime,v_1 | I) - F(W^\prime,v_2 | I)$としたとき，$var(D)$は
(\ref{B2})，(\ref{B3})，(\ref{B4})式などから(\ref{B1})式により得られる．
$E(X)$ は 確率変数 $X$ の期待値である．$F(v_1 | I)$ や $F(w | v_1,I)$ 
などの期待値や分散などは\ref{sec:model}章を参照．また，(\ref{B2})式と
(\ref{B4})式では$F(v_1 | I)$と$F(v_2 | I)$以外は確率的に独立であるとし
た．(\ref{B1})と(\ref{B2})と(\ref{B3})式については\cite{Stuart87}，
(\ref{B4})式については\cite{Bohrnstedt69}を参照．

\begin{equation}
  \label{B1}
  var(D) = var(F(W^\prime,v_1|I)) + var(F(W^\prime,v_2|I)) - 2 cov(F(W^\prime,v_1|I),F(W^\prime,v_2|I))
\end{equation}

\begin{eqnarray}
  \label{B2}
  var(F(W^\prime,v_1 | I)) & = & var(F(v_1 | I) F(W^\prime | v_1,I)) \nonumber \\
  & = & var(F(v_1 | I)) var(F(W^\prime | v_1,I)) 
        + E(F(v_1 | I))^2  var(F(W^\prime | v_1,I)) \nonumber \\
  & + & var(F(v_1 | I)) E(F(W^\prime | v_1,I))^2 
\end{eqnarray}

\begin{eqnarray}
  \label{B3}
  var(F(W^\prime|v_1,I)) & = & \sum_{w \in W^\prime} var(F(w|v,I)) + \sum_{w,w^\prime \in W^\prime, w \ne w^\prime} cov(F(w|v,I),F(w^\prime | v,I)) \nonumber \\
\end{eqnarray}

\begin{eqnarray}
  \label{B4}
  cov(F(W^\prime,v_1 | I),F(W^\prime,v_2 | I)) & = & cov(F(v_1 | I) F(W^\prime | v_1, I),F(v_2 | I) F(W^\prime|v_2,I)) \nonumber \\
  & = & cov(F(v_1 | I),F(v_2 | I)) E(F(W^\prime|v_1,I)) E(F(W^\prime|v_2,I)) \nonumber \\
\end{eqnarray}


\begin{biography}
\biotitle{略歴}
\bioauthor{内山 将夫}{

筑波大学第三学群情報学類卒業(1992)．筑波大学大学院工学研究科博士課程修
了(1997)．信州大学工学部 電気電子工学科 助手(1997)．言語処理学会，情報
処理学会の会員．}

\bioauthor{板橋 秀一}{

東北大学 工学部 通信工学科 卒業(1964)．東北大学 大学院 工学研究科 電気
及び通信工学専攻 博士課程 単位取得退学(1970)．東北大学 電気通信研究所 
助手(1970). 通産省 工業技術院 電子技術総合研究所 技官(1972).同 主任研
究官(1974).ストックホルム王立工科大学 客員研究員(1977-78). 筑波大学 電
子・情報工学系 助教授(1982)．筑波大学 電子・情報工学系 教授(1987). 専
門は音声・自然言語・画像の処理・理解．1982年より，（社）日本電子工業振
興協会の音声入力方式分科会主査，音声入出力方式専門委員会委員長として音
声データベースの検討・構築に従事している．

}

\bioreceived{受付}
\biorevised{再受付}
\biorerevised{再々受付}
\bioaccepted{採録}

\end{biography}

\end{document}
