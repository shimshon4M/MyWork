<?xml version="1.0" ?>
<root>
  <title/>
  <author/>
  <jkeywords/>
  <section title="">*AcknowledgementWethankMr.~K.~Nakamura,Mr.~T.~Fujita,andDr.~K.~KobayashiofNECC&amp;CRes.~Labs.~fortheirconstantencouragement.WethankDr.~K.~YamanishiofC&amp;CRes.~Labs.~forhisvaluablecomments.WethankMs.~Y.~YamaguchiofNISforherprogrammingeffort.document</section>
  <section title="Introduction">Recentlyvariousmethodsforautomaticallyconstructingathesaurus(hierarchicallyclusteringwords)basedoncorpusdatahavebeenproposed.Therealizationofsuchanautomaticconstructionmethodwouldmakeitpossibletoa)savethecostofconstructingathesaurusbyhand,b)doawaywiththesubjectivityinherentinahandmadethesaurus,andc)makeiteasiertoadaptanaturallanguageprocessingsystemtoanewdomain.Althoughmanyoftheproposedmethodshaveprovedtobeeffective,thewordclusteringproblemisstillaproblemwhichneedsfurtherinvestigation.Inthispaper,weproposeanewmethodforautomaticconstructionofthesauruses.Specifically,weviewtheproblemofautomaticallyclusteringwordsasthatofestimatingajointdistributionovertheCartesianproductofapartitionofasetofnouns(ingeneral,anysetofwords)andapartitionofasetofverbs(ingeneral,anysetofwords),andproposeanestimationalgorithmusingsimulatedannealingwithanenergyfunctionbasedontheMinimumDescriptionLength(MDL)Principle.TheMDLPrincipleisawell-motivatedandtheoreticallysoundprinciplefordatacompressionandestimationfrominformationtheoryandstatistics.Asastrategyofstatisticalestimation,MDLisguaranteedtobenearoptimal.Weempiricallyevaluatedtheeffectivenessofourmethod.Inparticular,wecomparedtheperformanceofourmethodbasedontheMDLPrincipleagainstthatofonebasedontheMaxi-mumLikelihoodEstimator(MLEforshort).WefoundthattheMDL-basedmethodperformsbetterthantheMLE-basedmethod.Wealsoevaluatedourmethodbyconductingstructural(pp-attachment)disambiguationexperimentsusingathesaurusautomaticallyconstructedbyitandfoundthatdisambiguationresultscanbeimproved.Sincesomewordsneveroccurinacorpus,andthuscannotbereliablyclassifiedbyamethodsolelybasedoncorpusdata,weproposetocombinetheuseofanautomaticallyconstructedthesaurusandthatofahandmadethesaurusindisambiguation.Weconductedsomeexperimentsinordertotesttheeffectivenessofthisstrategy.Ourexperimentalresultsindicatethatcombininganautomaticallyconstructedthesaurusandahandmadethesauruswidensthe`coverage'ofdisambiguation,whilemaintaininghigh`accuracy'.</section>
  <section title="The Problem Setting">Manyofthemethodsofautomaticallyconstructingathesaurusbasedoncorpusdataconsistofthefollowingthreesteps:(i)Extractco-occurrencedata(e.g.,caseframedata,adjacencydata)fromacorpus,(ii)Startingfromasingleclass(oreachwordcomposingitsownclass),divide(ormerge)wordclassesbasedontheco-occurrencedatausingsomesimilarity(distance)measure.(Theformerapproachiscalled`divisive,'thelatter`agglomerative.')(iii)Repeatstep(ii)untilsomestoppingconditionismet,toconstructathesaurustree.Themethodweproposehereconsistsofthesamethreesteps.SupposeavailabletousaredatalikethoseinFigure~,whichareco-occurrencedatabetweenverbsandtheirobjects,extractedfromacorpus(step(i)).Wethenviewtheproblemofclusteringwordsasthatofestimatingaprobabilisticmodel(representingaprobabilitydistribution)thatgaverisetosuchdata.Wedefinetheprobabilisticmodelinthefollowingway.WefirstdefineanounpartitionP_NoveragivensetofnounsNandaverbpartionP_VoveragivensetofverbsV.AnounpartitionisanysetP_NsatisfyingP_N2^N,_C_iP_NC_i=Nand_i,C_jP_N,C_iC_j=.AverbpartitionP_Visdefinedanalogously.Inthispaper,wecallamemberofanounpartitiona`nouncluster,'andamemberofaverbpartitiona`verbcluster.'WerefertoamemberoftheCartesianproductofanounpartitionandaverbpartition(P_NP_V)simplyasa`cluster.'Wethendefineaprobabilisticmodel(orajointdistribution),writtenP(C_n,C_v),whererandomvariableC_nassumesavaluefromafixednounpartitionP_N,andC_vavaluefromafixedverbpartitionP_V.Withinagivencluster,weassumethateachelementisgeneratedwithequalprobability,i.e.,Figure~exhibitstwoexamplemodelswhichmighthavegivenrisetothedatainFigure~.Inthispaper,weassumethattheobserveddataaregeneratedbyamodelbelongingtotheclassofmodelsjustdescribed,andselectamodelwhichbestexplainsthedata.Asaresultofthis,weobtainbothnounclustersandverbclusters.Thisproblemsettingisbasedontheintuitiveassumptionthatsimilarwordsoccurinthesamecontextwithroughlyequallikelihood,asismadeexplicitinequation().Thusselectingamodelwhichbestexplainsthegivendataisequivalenttofindingthemostappropriateclassificationofwordsbasedontheirco-occurrence.1mm[htb]figure*[htb]figure*</section>
  <section title="Clustering with MDL">Wenowturntothequestionofwhatstrategy(orcriterion)weshouldemployinordertoestimatethebestmodel.OurchoiceistheMinimumDescriptionLength(MDL)Principle,awell-knownprincipleofdatacompressionandestimationfrominformationtheoryandstatistics.MDLstipulatesthatthebestprobabilitymodelforgivendataisthatmodelwhichrequirestheleastcodelengthforencodingthemodelitselfandthegivendatarelativetoit.Werefertothecodelengthforthemodelasthe`modeldescriptionlength'andthatforthedatathe`datadescriptionlength.'WeapplyMDLtotheproblemofestimatingamodelconsistingofapairofpartitionsasdescribedabove.Inthiscontext,amodelwithlessclusters,suchasModel2inFigure~,tendstobesimpler(intermsofthenumberofparameters),butalsotendstohaveapoorerfittothedata.Incontrast,amodelwithmoreclusters,suchasModel1inFigure~,ismorecomplex,buttendstohaveabetterfittothedata.Thus,thereisatrade-offrelationshipbetweenthesimplicityofamodelandthegoodnessoffittothedata.Themodeldescriptionlengthquantifiesthesimplicity(complexity)ofamodel,andthedatadescriptionlengthquantifiesthegoodnessoffittothedata.AccordingtoMDL,themodelwhichminimizesthesumtotalofthetwotypesofdescriptionlengthsshouldbeselected.Inwhatfollows,wewilldescribeindetailhowtocalculatethedescriptionlengthinourcurrentcontext,aswellasoursimulatedannealingalgorithmbasedonMDL.</section>
  <subsection title="Calculating Description Length">Wewillnowdescribehowtocalculatethedescriptionlengthforamodel.RecallthateachmodelisspecifiedbytheCartesianproductofanounpartitionandaverbpartition,andanumberofparameters.Hereweletk_ndenotethesizeofthenounpartition,andk_vthesizeoftheverbpartition.Then,therearek_nk_v-1freeparametersinamodel.GivenamodelManddataS,itstotaldescriptionlengthL(M)iscomputedasthesumofthemodeldescriptionlengthL_mod(M),theparameterdescriptionlengthL_par(M),andthedatadescriptionlengthL_dat(M)(wealsosometimesrefertoL_mod(M)+L_par(M)asthemodeldescriptionlength),namely,Weemploythe`binarynounclusteringmethod,'inwhichk_visfixedat|V|andwearetodecidewhetherk_n=1ork_n=2.Thisisasifweviewthenounsasentitiesandtheverbsasfeaturesandclassifytheentitiesbasedontheirfeatures.Sincethereare2^|N|subsetsofthesetofnounsN,andforeachbinarynounpartitionwehavetwodifferentsubsets(aspecialcaseofwhichiswhenonesubsetisNandtheothertheemptyset),thenumberofpossiblebinarynounpartitionsis2^|N|/2=2^|N|-1.Thusforeachbinarynounpartitionweneed-12^(|N|-1)=|N|-1bitstodescribeit.HenceL_mod(M)iscalculatedas(M)issubjective,anditdependsonthecodingschemeusedforthedescriptionofthemodels.L_par(M)iscalculatedbywhere|S|denotesthedatasize,andk_nk_v-1isthenumberoffreeparametersinthemodel.Asiswellknown,itisbesttouse-1|S|=12|S|bitstodescribeeachoftheparameters,sincethestandarddeviationofthemaximumlikelihoodestimationofeachparameterisoforder1|S|,andhencedescribingeachparameterusingmorethanO(12|S|)bitswouldbewastefulfortheestimationaccuracypossiblewiththegivendatasize.Finally,L_dat(M)iscalculatedbywheref(n,v)denotesthetotalobservedfrequencyofnounverbpair(n,v),andP(n,v)theestimatedprobabilityof(n,v),whichiscalculatedasfollows:wheref(C_n,C_v)denotestheobservedfrequencyofthenounverbpairsbelongingtocluster(C_n,C_v).Withthedescriptionlengthforamodeldefinedintheabovemanner,wewishtoselectamodelhavingtheminimumdescriptionlengthandoutputitastheresultofclustering.SincethemodeldescriptionlengthL_modisthesameforeachmodel,inpracticeweonlyneedtocalculateandcompareL'(M)=L_par(M)+L_dat(M).ThedescriptionlengthsforthedatainFigure~usingthetwomodelsinFigure~areshowninTable~.(Table~showssomevaluesneededforthecalculationofthedescriptionlengthforModel1.)ThesecalculationsindicatethataccordingtoMDL,Model1shouldbeselectedoverModel2.[htb]table*[htb]table*</subsection>
  <subsection title="A Simulated Annealing-based Algorithm">Wecouldinprinciplecalculatethedescriptionlengthforthedatausingeachmodelandselectamodelwiththeminimumdescriptionlength,ifcomputationtimewereofnoconcern.Sincethenumberofprobabilisticmodelsunderconsiderationisexponential,however,thisisnotfeasibleinpractice.Weemploythe`simulatedannealingtechnique'todealwiththisproblem.Figure~showsour(divisive)algorithmforhierarchicalwordclustering.[htb]figure*</subsection>
  <section title="Advantages of Our Method">Althoughtherehavebeenmanymethodsofwordclusteringproposedtodate,theirobjectivesappeardifferent.InTableandweexhibitasimplecomparisonbetweenourworkandrelatedwork.Perhapsthemethodproposedbyisthemostrelevantinourcontext.In,theyproposedamethodof`softclustering,'inwhich,eachwordcanbelongtoanumberofdistinctclasseswithcertainprobabilities.Softclusteringhasseveraldesirableproperties.Forexample,wordsenseambiguitiesininputdatacanberesolvednaturally.Here,werestrictourattentionon`hardclustering'(i.e.,eachwordmustbelongtoexactlyoneclass),inpartbecauseweareinterestedincomparingthesaurusesconstructedbyourmethodwithexistinghand-madethesauruses.(Notethatahandmadethesaurusisbasedonhardclustering.)[htb]table*[htb]table*Wenextelaborateonthemeritsofourmethod.Instatisticalnaturallanguageprocessing,usuallythenumberofparametersinaprobabilisticmodeltobeestimatedisverylarge,andthereforesuchamodelisdifficulttoestimatewithareasonabledatasizethatisavailableinpractice.(Thisproblemisusuallyreferredtoasthe`datasparsenessproblem.')Wecouldsmooththeestimatedprobabilitiesusinganexistingsmoothingtechnique(e.g.,),calculatesomesimilaritymeasureusingthesmoothedprobabilities,andthenclusterwordsaccordingtoit.Thereisnoguarantee,however,thattheemployedsmoothingmethodisinanywayconsistentwiththeclusteringmethodusedsubsequently.OurmethodbasedonMDLresolvestheclusteringproblemandthesmoothingprobleminaunifiedfashion.(Forexample,theprobabilityofthenounverbpair(rice,make)isestimated(smoothed)tobe0.05inModel1,althoughtheobservedoccurrenceofitis0(seeFigure~andFigure~).)Byemployingmodelsthatembodytheassumptionthatwordsbelongingtothesameclusteroccurwithequalprobability,ourmethodachievesthesmoothingeffectasasideeffectoftheclusteringprocess,wherethedomainsofsmoothingcoincidewiththeclustersobtainedbyclustering.Thus,thecoarsenessorfinenessofclusteringalsodeterminesthedegreeofsmoothing.Alloftheseeffectsfalloutnaturallyasacorollaryoftheimperativeofbestpossibleestimation,theoriginalmotivationbehindtheMDLPrinciple.Inourproblemsetting,wecouldalternativelyemploytheMaximumLikelihoodEstimator(MLE)ascriterionforestimationofthebestprobabilisticmodel,insteadofMDL.MLE,asitsnamesuggests,selectsamodelwhichmaximizesthelikelihoodofthedata,i.e.,P=_P_xSP(x).ThisisequivalenttominimizingthedatadescriptionlengthasdefinedinSection3,i.e.,P=_P_xS-P(x).WecanseeeasilythatMDLgeneralizesMLE,inthatitalsotakesintoaccountthecomplexityofthemodelitself.Inthepresenceofmodelswithvaryingcomplexity,MLEtendstooverfitthedata,andoutputamodelthatistoocomplexandtailoredtofitthespecificsoftheinputdata.IfweemployMLEascriterionfortheestimation,itwillresultinselectingaveryfinemodelwithmanysmallclusters,mostofwhichwillhaveprobabilitiesestimatedaszero.Thus,incontrasttoemployingMDL,itwillnothavetheeffectofsmoothingatall.Purelyasastrategy(criterion)ofstatisticalestimationaswell,thesuperiorityofMDLoverMLEissupportedbyconvincingtheoreticalfindings.Forinstance,thespeedofconvergenceofthemodelsselectedbyMDLtothetruemodelisknowntobenearoptimal.(ThemodelsselectedbyMDLconvergetothetruemodelapproximatelyattherateof1/swheresisthenumberofparametersinthetruemodel,whereasforMLEtherateis1/t,wheretisthesizeofthedomain,orinourcontext,thetotalnumberofelementsofNV.)`Consistency'isanotherdesirablepropertyofMDL,whichisnotsharedbyMLE.Thatis,thenumbersofparametersinthemodelsselectedbyMDLconvergetothatofthetruemodel.BothofthesepropertiesofMDLareempiricallyverifiedinourpresentcontext,aswillbeshowninthenextsection.Inparticular,wehavecomparedtheperformanceofemployinganMDL-basedsimulatedannealingagainstthatofonebasedonMLEinhierarchicalwordclustering.</section>
  <section title="Experimental Results">Wedescribeourexperimentalresultsinthissection.</section>
  <subsection title="Experiment 1: MDL v.s. MLE">[htb]figure*Asdescribedintheprevioussection,therearesometheoreticalfindingsverifyingthatemployingMDLperformsbetterthanemployingMLEinstatisticalestimation.Weempiricallytestifthisisthecaseinourcurrentcontext.Weartificiallyconstructedatruemodelofwordco-occurrence(seeFigure~),andthengenerateddataaccordingtoitsdistribution.Wethenusedthedatatoestimateamodel(hierarchicallyclusterwords)byemployingMDLandMLE,respectively.(ThealgorithmusedforMLEwasthesameasthatshowninFigure~,exceptthedatadescriptionlengthreplacesthetotaldescriptionlengthinStep2.)WeevaluatedthetwomethodsintermsofthenumberofnounclustersandtheKLdistance.Figure(a)plotstherelationbetweenthenumberofobtainednounclusters(leafnodesintheobtainedthesaurustree)versusthedatasize,averagedover10trials.(Notethatthenumberofnounclustersinthetruemodelis4.)Figure(b)plotstheKLdistanceversusthedatasize,alsoaveragedoverthesame10trials.TheresultsindicatethatMDLconvergestothetruemodelfasterthanMLE.Also,MLEtendstoselectamodelthatistoolarge(overfittingthedata),whileMDLtendstoselectamodelwhichissimpleandyetfitsthedatareasonablywell.Weconductedthesamesimulationexperimentsforsomeothermodelsandfoundthesametendencies.WeconcludethatitisbettertoemployMDLthanMLE,asacriterioninsimulatedannealing-basedhierarchicalwordclustering.[htb]figure*</subsection>
  <subsection title="Experiment 2: Qualitative Evaluation">0.5mm[htb]figure*Weextractedroughly180,000caseframesfromthebracketedWallStreetJournal(WSJ)corpusofthePennTreeBankasco-occurrencedata.Wethenconstructedanumberofthesaurusesbasedonthesedata,usingourmethod.Figure~showsanexamplethesaurusforthe20mostfrequentlyobservednounsinthedata,constructedbasedontheirappearancesassubjectsandobjectsofroughly2000verbs.Theobtainedthesaurusseemstoagreewithhumanintuitiontosomedegree.Forexample,`million'and`billion'areclassifiedinonenouncluster,and`stock'and`share'areclassifiedtogether.Notallofthenounclusters,however,seemtobemeaningfulintheusefulsense.Thisgeneraltendencyisalsoobservedinotherexamplethesaurusesobtainedbyourmethod.Pragmaticallyspeaking,however,whethertheobtainedthesaurusagreeswithourintuitioninitselfisonlyofsecondaryconcern,sincethemainpurposeistousetheconstructedthesaurustohelpimproveonadisambiguationtask.</subsection>
  <subsection title="Experiment 3: Disambiguation">Wealsoevaluatedourmethodbyusingaconstructedthesaurusinpp-attachmentdisambiguationexperiments.Weusedastrainingdatathesame180,000caseframesusedinExperiment2.Wealsoextractedasourtestdata172(verb,noun_1,prep,noun_2)patternsfromthedatainthesamecorpus,whichwerenotusedinthetrainingdata.Forthe150wordsthatappearinthepositionofnoun_2,weconstructedathesaurusbasedontheco-occurrencesbetweenheadsandslotvaluesoftheframesinthetrainingdata.Thisisbecauseinourdisambiguationexperimentsweonlyneedathesaurusconsistingofthese150words.Wethenappliedthelearningmethodproposedintolearncaseframepatternsusingtheconstructedthesauruswiththesametrainingdataasinput.WeformalizethecaseframepatternsasconditionaldistributionsoftheformP(Class|head,prep),whereClassvariesovertheinternalnodesinacertain`cut'inthethesaurustree.OurmethodselectstheoptimalcutinthethesaurustreeusingthegivendatainthesenseofMDL,thatis,acutwhichisfineenoughtocapturethetendencyintheinputdata,butiscoarseenoughtohaveareasonablysmallnumberofparameterstoestimate.ItalsoestimatesP(Class|head,prep)foreachClassinthecut(seeforfurtherdetail).Table~showssomeexamplecaseframepatternsobtainedbythismethod,andFigure~showstheleafnodesdominatedbytheinternalnodesappearinginthecaseframepatternsofTable~.[htb]#80:ground,wake,success,network,game,rest,art,organization,plane,output,television,benefit,letter,holder,support,nation,corporation,review,thousand,manufacturer,margin,man,meeting,customer,agent,help#122:reorganization,attitude,relief,competition,constitutionverbatimfigure*Wethenconductedpp-attachmentdisambiguationexperiments.WecompareP(noun_2|verb,prep)andP(noun_2|noun_1,prep),whicharecalculatedbasedonthecaseframepatterns,todeterminetheattachmentsiteof(prep,noun_2).Morespecifically,iftheformerislargerthanthelatter,weattachittoverb;andifthelatterislargerthantheformer,weattachittonoun_1;andotherwise(includingthecaseinwhichbothare0),weconcludethatwecannotmakeadecision.Table~showstheresultsoftheexperimentsintermsof`coverage'and`accuracy.'Here`coverage'referstotheproportion(inpercentage)ofthetestpatternsonwhichthedisambiguationmethodcanmakeadecision.`Base-Line'referstothemethodofalwaysattaching(prep,noun_2)tonoun_1.`Word-Based,'`MLE-Thesaurus,'and`MDL-Thesaurus'respectivelystandforusingword-basedestimates,usingathesaurusconstructedbyemployingMLE,andusingathesaurusconstructedbyourmethod.Notethatthecoverageof`MDL-Thesaurus'significantlyoutperformedthatof`Word-Based,'whilebasicallymaintaininghighaccuracy(thoughitdropssomewhat),indicatingthatusinganautomaticallyconstructedthesauruscanimprovedisambiguationresultsintermsofcoverage.Wealsotestedthecaseofusinganexistingthesaurus(insteadofanautomaticallyconstructedthesaurus)tolearncaseframes.Inparticular,weusedthismethodwithWordNetandthesametrainingdata,andthenconductedapp-attachmentdisambiguationexperimentusingtheobtainedcaseframepatterns.Werepresenttheresultofthisexperimentas`WordNet'inTable~.Wecanseethatintermsofcoverage,WordNetoutperformsMDL-Thesaurus,butintermsofaccuracy,MDL-ThesaurusoutperformsWordNet.Theseresultscanbeinterpretedasfollows:Anautomaticallyconstructedthesaurusismoredomaindependentandthereforecapturesthedomaindependentfeaturesbetter,andthususingitachieveshighaccuracy.Ontheotherhand,sincetrainingdatawehadavailableisinsufficient,itscoverageissmallerthanthatofahandmadethesaurus.Inpractice,itmakessensetocombinebothtypesofthesauruses.Thatis,anautomaticallyconstructedthesauruscanbeusedwithinitscoverage,andoutsideitscoverage,ahandmadethesauruscanbeused.Giventhecurrentstateofthewordclusteringtechnique(namely,itrequiresdatasizethatisusuallynotavailable,andittendstobecomputationallydemanding),thisstrategyispractical.Wetestedthisstrategy.Morespecifically,wecompareP(noun_2|verb,prep)andP(noun_2|noun_1,prep)calculatedfromcaseframepatternsobtainedusinganautomaticallyconstructedthesaurus;whenthetwoprobabilitiesareequal,includingthecaseinwhichbothare0,wecomparetheprobabilitiescalculatedfromcaseframepatternsobtainedusingWordNet.Table~representstheresultofthiscombinedmethodas`MDL-Thesaurus+WordNet.'Theexperimentalresultindicatesthatemployingthecombinedmethoddoesincreasethecoverageofdisambiguation.Wealsotested`MDL-Thesaurus+WordNet+LA+Default,'whichstandsforusingtheconstructedthesaurusandWordNetfirst,thenthelexicalassociationvalueproposedby,andfinallythedefault(i.e.,alwaysattachingprep,noun_2tonoun_1).Figure~showstheresults.Ourbestdisambiguationresultobtainedusingthislastcombinedmethodslightlyimprovestheaccuracyreportedin(84.3%).[htb]table*[htb]figure*</subsection>
  <section title="Concluding Remarks">Wehaveproposedamethodofautomaticallyconstructingathesaurus(hierarchicallyclusteringwords)basedoncorpusdata.Weconcludewiththefollowingremarks.OurmethodofhierarchicallyclusteringwordsbasedontheMDLPrincipleistheoreticallysound.OurexperimentalresultsindicatethatitisbettertoemployMDLthanMLEasestimationcriterioninhierarchicalwordclustering.Usingathesaurusconstructedbyourmethodcanimprovepp-attachmentdisambiguationresults.Giventhecurrentstateoftheartinstatisticalnaturallanguageprocessing,itisbesttouseacombinationofanautomaticallyconstructedthesaurusandahandmadethesaurusfordisambiguationpurpose.Thedisambiguationaccuracyobtainedthiswaywas85.5%.Inthefuture,hopefullywithlargertrainingdatasizes,weplantoconstructlargerthesaurusesaswellastotestotherclusteringalgorithms.</section>
</root>
