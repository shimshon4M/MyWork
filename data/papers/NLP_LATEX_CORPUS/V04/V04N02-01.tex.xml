<?xml version="1.0" ?>
<root>
  <title>文節間係り受け距離の統計的性質を用いた日本語文の係り受け解析</title>
  <author>張玉潔尾関和彦</author>
  <jabstract>日本語における2文節間の係り受け頻度は,その距離に依存することが知られている．すなわち,文中の文節はその直後の文節に係ることが最も多く,文末の文節に係る場合を除いては,距離が離れるにしたがってその頻度が減少する．この統計的性質は,日本語文の係り受け解析においてしばしば用いられるヒューリスティクス：「文中の文節は係り得る文節の中で最も近いものに係る」の根拠となっている．しかし,このヒューリスティクスは,日本語に見られるこのような統計的性質の一部しか利用していない．したがって,係り受け距離の頻度分布をもっと有効に利用することにより,解析性能が向上する可能性がある．本研究では,ATR503文コーパスから抽出した係り受け距離の頻度分布に基づいて2文節間の係り受けペナルティ関数を定義し,「総ペナルティ最小化法」を用いて係り受け解析実験を行なった．その結果を,上のヒューリスティクスに基づく決定論的解析法による解析結果と比較したところ,かなりの解析性能向上が認められた．また,係り文節を分類し,その種類別に抽出した係り受け頻度の情報を用いることにより,さらに解析性能を改善できることが明らかになった．</jabstract>
  <jkeywords>係り受け解析,係り受けの整合性,係り受け距離,係り受け規則,統計的言語知識,総ペナルティ最小化</jkeywords>
  <section title="まえがき">日本語文の表層的な解析には,係り受け解析がしばしば用いられる．係り受け解析とは,一つの文の中で,どの文節がどの文節に係る(広義に修飾する)かを定めることであるが,実際に我々が用いる文について調べて見ると,2文節間の距離とそれらが係り受け関係にあるか否かということの間に統計的な関係のあることが知られている．すなわち,文中の文節はその直後の文節に係ることがもっとも多く,文末の文節に係る場合を除いては距離が離れるにしたがって係る頻度が減少する．係り受け距離に関するこのような統計的性質は「どの文節も係り得る最も近い文節に係る」というヒューリスティクスの根拠になっていると思われる．しかし実際には「最も近い文節に係ることが多い」とは言え,「最も近い文節にしか係らない」というわけではない．したがって,係り受け距離の統計的性質をもっと有効に利用することにより,係り受け解析の性能を改善できる可能性がある．本論文では,総ペナルティ最小化法を用いて,係り受け距離に関する統計的知識の,係り受け解析における有効性を調べた結果について報告する．総ペナルティ最小化法においては,2文節間の係り受けペナルティの総和を最小化する係り受け構造が解析結果として得られる．ここでは,係り受け距離に関する統計的知識を用いない場合と,そのような知識を用いて係り受けペナルティ関数を設定するいくつかの方法について,解析結果を比較した．また,「係り得る最も近い文節に係る」というヒューリスティクスを用いた決定論的解析法についても解析結果を求め,上の結果との比較を行った．学習データとテストデータを分離したオープン実験の結果や統計的知識を抽出するための学習データの量が解析結果に与える効果についても検討した．</section>
  <section title="係り受け距離の統計的知識">丸山らは新聞記事データベースを用いて係り受け距離とその頻度の関係を調査し,それを表す近似式を見い出した．我々はATR音声データベース(セットB)に含まれる503文コーパスについて丸山らと同様の調査を行った．コーパスの概要を表1に示す．全体の503文はA〜Jまでの10グループに分割されており,A〜Iのグループには各50文,グループJには53文が含まれている．各文節には,それを受ける文節との間の距離を表すラベルが付けられている．文の係り受け構造はこれらの値によって知ることができる．</section>
  <subsection title="係り受け距離の頻度分布">文x_1x_2...x_N(x_kは文節)においてx_iがx_jに係るとき,x_iとx_jの係り受け距離をj-iと定義する．また,N-iをx_iのレンジと呼ぶ．表2は係り受け距離の頻度分布を距離4以下について示したものである．レンジは考慮していない．距離が11以上の係り受けは存在しなかった．このコーパスでは距離が1の係り受けが全体の64.6%を占めている．文の構成要素(日本語における文節や英語における単語など)が,隣接する構成要素を修飾する傾向は他の言語においても見られる．例えば,英語においても,文中の単語が直後の単語を修飾する(RightAssociation)頻度は67%に及ぶことが報告されている．</subsection>
  <subsection title="係り受け距離の頻度を表す近似式">レンジごとに求めた係り受け距離の頻度分布を表3に示す．ただし,レンジが6以下のものだけを掲げた．このデータを用いて文献と同じ方法により距離頻度の近似式[*5mmP(k)＝.]*20mm(kは係り受け距離,rは係り文節のレンジ)*3.9mmをあてはめたところa=0.58,b=-1.886が得られた．これは,丸山らの結果a=0.54,b=-1.896と近いものである．</subsection>
  <subsection title="係り文節の種類による係り受け距離の頻度分布のちがい">前節に述べた係り受け距離の頻度分布は,文節の種類を考慮に入れずに求めた,全文節に対する平均的なものであるが,係り受け距離の頻度分布は係り文節の種類に依存することが予想される．そこで,ここでは係り文節をその末尾の形態素によって表4に示す基準により約100種類に分類し,その種類別に頻度分布を求めた．品詞属性はコーパスの説明書によった．*4mmまた,受け文節はそれが文末であるか非文末であるかによって区別した．そして係り文節の種類別に,また受け文節が文末か非文末かを区別して係り受け距離の頻度を求めた．具体的な計算は4.2の3に述べる定義式によって行なう．距離の頻度分布が係り文節の種類に大きく依存する例を表5に示す．格助詞「が」の係り受け距離の頻度分布は式(1)(a=0.58,b=-1.886)に近いが,接続助詞「が」の場合には距離2で頻度最大になり,ほかの距離の頻度は一様になっている．このような頻度分布を式(1)のような単調減少関数で近似することには無理がある．したがって,本研究では係り文節の種類別に求めた頻度分布を,そのまま係り受け解析のための情報として用いた．</subsection>
  <section title="係り受け解析法">係り受け距離の統計的性質を利用することにより,どの程度,解析性能が向上するかを調べるため,総ペナルティ最小化法を用いて係り受け解析の実験を行った．また,これと比較するため,決定論的解析法による解析も行った．ここではこれらの解析法について簡単に説明する．</section>
  <subsection title="総ペナルティ最小化法">文節列が“正しい”文を構成するためには文節間に以下の条件を満たす係り受けが存在する必要があると考えられている．総ペナルティ最小化法においては2文節間の整合性を程度の問題と考え,それをペナルティ関数で表す．そして,唯一性と非交差性を満たす係り受け構造の中でペナルティの総和が最小になるものすべてを解析結果として出力する．この計算は動的計画法の原理を用いることにより効率よく実行できる．ペナルティ関数を適切に設定することにより,2文節間の種々の関係を係り受け解析に利用することができると考えられる．本研究では,後で述べるように係り受け距離の頻度分布に基づいてペナルティ関数を設定した．</subsection>
  <subsection title="決定論的解析法">この解析法においては,整合性を程度の問題とは捉えず,整合するかしないかのいずれかであると考える．解析は文末から順に,その文節を受ける文節を決定することにより行われる．非交差性,整合性を満たす受け文節の候補が複数個ある時は,最も距離が近い文節を採用する．解析の途中で受け文節が見出せない時には,その時点で解析不能となる．本研究で用いたアルゴリズムを図1に示す．</subsection>
  <section title="実験と結果">係り受け距離の頻度情報に基づいて,いくつかのぺナルティ関数を定義し,総ペナルティ最小化法による係り受け解析を行った．結果を正解検出率,一意正解率,曖昧度減少率および平均候補数によって評価し,ぺナルティ関数による結果の違いを比較検討した．また,決定論的解析法による解析も行い,総ペナルティ最小化法の結果と比較した．</section>
  <subsection title="係り受け規則">文献を参考にして,2文節間の形態素による整合条件を表6に示すように定めた．*3mm実際には,この条件を,「受け文節の頭部の形態素」としての「名詞」,「名詞＋判定詞」,「動詞」のさまざまな変形に対処できるように補強して用いる．例えば,「二時半ごろだった」(「数詞＋接尾語＋名詞＋名詞＋助動詞＋助動詞」)は「名詞＋判定詞」として認識されるようにしている．ここではこの条件を係り受け規則と呼ぶ．この規則は総ペナルティ最小化法と決定論的解析法の両方において共通に使用した．</subsection>
  <subsection title="ぺナルティ関数の定義">係り文節x,受け文節yに対してぺナルティ関数F(x,y)を次のように定義する．[*5mmF(x,y)＝.]ここで,P(x,y)はxがyに係る頻度から定まる値である．その具体的な定め方は後で述べる．文節x,yが係り受け規則を満たさない場合にはP(x,y)=0と定義する．このときのぺナルティ値cは非零のP(x,y)で生成される最大ぺナルティ値より十分大きな値に設定する．これにより,文節x,yが係り受け規則を満たさない場合には大きなぺナルティが課せられる．また,係り受け規則を満たす場合には,P(x,y)が大きい程,ぺナルティは小さくなる．P(x,y)として次の4種類の関数を考え,それぞれに対して実験を行った．</subsection>
  <subsection title="解析実験">解析実験は学習データから係り受け距離の頻度情報を抽出する学習ステップと,その頻度情報に基づいて設定したペナルティ関数を用いてテストデータを係り受け解析し,結果を評価する解析ステップから成る．まず学習法と各種のパラメータ設定法について説明する．P_1を用いる場合には,頻度情報は全く使用しない,したがって学習は不要である．αはどのような値に設定しても解析結果は同じになる．P_2はパラメトリックな分布モデルである．学習ステップにおいては,学習データを用いてパラメータa,bを推定する．P_3はノンパラメトリックな分布モデルである．学習ステップにおいては,学習データを用いて,係り文節の種類mと係り受け距離kに対してS_l^m(k)とS_n^m(k)を求め,記憶する．解析ステップにおいては,各文節x,yに対し,S_l^m(x)(k)とS_n^m(x)(k)から定義式に基づいてP_3(x,y)の値を計算し,ペナルティ関数の値を設定する．P_4は基本的にはP_3から定まるので学習する必要はない．βの値は式(3)を満たす限りどのような値に設定しても結果に大きな差はないと考えられるので,式(3)を満たす値を任意に選んで使用した．ペナルティ関数を式(2)によって定義するとき,定数cを定める必要がある．これは文節xが文節yに係ることが係り受け規則により許されないとき,あるいは文節xが文節yに係る頻度が0になるとき(P_4によって補間されない限り)文節xが文節yに係ることを禁止するような大きなペナルティを与えるためのものである．したがって,これはとも言うべきものであり,十分大きな値に設定すれば,どのような値に設定しても解析結果に差はない．このような学習法およびパラメータ設定法を用いて次のような3種類の実験を行った．</subsection>
  <subsection title="解析結果">解析結果について述べるため,まず記法と用語を定義する．M:評価に用いるテスト文の総数;S_i:番号iのテスト文;L_i:文S_iの文節数;*7mm文S_iに対する係り受け構造の中の係り受けの総数はL_i-1に等しい．D_i:長さL_iの文節列上に存在し得る係り受け構造の総数;*7mmD_iはカタラン数と呼ばれる数列になり,次の式によって計算することができる:[D_i＝1L_i_2(L_i-1)C_(L_i-1)]*8mmここで_2(L_i-1)C_(L_i-1)は2(L_i-1)個のものから(L_i-1)個のものをとる組合せの数を表す．この式は再帰式[D_i=.]が成り立つことと,母関数の手法を用いることにより示すことができる．R_i:文S_iの解析結果,すなわち係り受け構造候補の集合;K_ij:文S_iに対するj(1≦j≦|R_i|)番目の解析候補の中でコーパスのラベルに示されるものと一致する係り受けの数．評価方法*1mm結果の評価は,(1)2文節間の係り受けがどの程度正しく検出されたか,(2)係り受け構造がどの程度正しく検出されたか,という二つの観点から行った．また,文の検出率が高くても一つの文に対する解析結果の候補数が多ければ良い解析法とは言えない．このため,候補数がどれだけ絞られるかを評価することとした．さらに,解析を行うことにより,情報理論的な曖昧さがどれだけ減少するかを調べ,全体的な解析効率を評価した．これらの評価を行うため,以下のようないくつかの評価尺度を定義した．(1)係り受けの正しさに着目した評価尺度[文S_iの係り受け検出率＝_j=1^|R_i|K_ij(L_i-1)×|R_i|][係り受け検出率＝_i=1^M_j=1^|R_i|K_ij_i=1^M(L_i-1)×|R_i|]係り受け検出率は,解析結果がラベルと部分的に一致する度合を示す数字である．このような部分的一致も,結果を意味理解に利用する場合などには有用と思われる．決定論的解析法を用いた解析実験においては,解析の途中で,ある文節を受ける文節が存在しない時には,直後の文節を受け文節として解析を続けた．その結果をもとにして係り受け検出率を計算した．*3mm(2)係り受け構造の正しさに着目した評価尺度(a)文検出数と文検出率[文検出数_k＝_i:|R_i|≦kI_i]ここで[I_i＝.]*2mm文検出数_kは,出力された解析結果の候補数がk以下であり,かつ,その中にコーパス中のラベルで指定される係り受け構造と一致するものが含まれるようなテスト文の数である．また,これをテスト文の総数に対する比で表わしたものを,文検出率_kと呼ぶ:[文検出率_k＝文検出数_kM]文検出数_,文検出率_をそれぞれ単に文検出数,文検出率という．また,文検出数_1,文検出率_1をそれぞれ一意正解数,一意正解率と呼ぶことにする．一意正解数は解析結果が一意的に決定し,それがコーパスのラベルと一致したテスト文の数である．(b)平均候補数[平均候補数＝_i=1^M|R_i|×I_i_i=1^MI_i](c)曖昧度減少率文S_iの係り受け構造の候補数の対数をその文の曖昧度と定義すると,*2mm[解析前の曖昧度＝D_i][解析後の曖昧度＝.]*1mm*36mm＝(D_i+(|R_i|-D_i)I_i)*2mmとなる．これを用いて,次の量を定義する．[曖昧度減少率＝_i=1^MD_i-_i=1^M(D_i+(|R_i|-D_i)I_i)_i=1^MD_i]曖昧度減少率は文の統語的な曖昧さが解析により減少する度合を表す．*1mm実験結果と分析*1mm(1)実験1の結果を表7に示す．また,同実験において候補数を制限したときの文検出数(率)を表8に示す．P_1を用いた場合は,他の場合と比べて,文検出率が高い反面,平均候補数が非常に大きい．また,表8から文検出率_k(k=10,1)はP_2〜P_4を用いた場合の方が,P_1を用いた場合より高い．したがって,係り受け距離の情報は候補数を絞るのに有効であることがわかった．P_3を用いた場合は,P_2を用いた場合と比較して,表7における文検出率,および表8における文検出率_k(k=50,10,1)が高い．したがって,係り文節の種類別に求めた係り受け距離の頻度情報は,係り文節を分類せず全体で求めた情報よりも,各文検出率_kを高めるのに有効である．また,その結果,曖昧度が減少している．P_4を用いた場合は,P_3を用いた場合と比べて,表7における係り受け検出率と表8における一意正解率が高くなっている．したがって,補間は係り受け検出率と一意正解率を上げる効果があることが検証された．文検出率は低下したが,P_2を用いた場合よりは高く,平均候補数はP_3を用いた場合の半分以下になっている．したがって,距離の頻度分布を補間することにより,ある程度係り受け規則の不完全さとコーパスの希薄性の問題を軽減できることがわかった．また,平均候補数は1.1まで絞られており,87.1%というかなり高い係り受け検出率が得られることから,この解析法を意味理解のための部分解析法として使用できる可能性がある．決定論的解析法を用いた解析実験と比較すると,P_1〜P_4を用いた場合は表7の平均候補数が大きくなったかわりに文検出率がかなり向上している．さらに,表8において,P_2〜P_4を用いた場合は決定論的解析法を用いた場合の一意正解率を越えるともに各文検出率_kが高くなっている．とくにP_4を用いた場合は決定論的解析法を用いた場合と平均候補数がほとんど等しく,各文検出率_k,曖昧度減少率,および係り受け検出率が高くなっている．したがって,係り受け距離の統計的知識を利用した総ペナルティ最小化法により,決定論的解析法を用いた場合に比べて解析性能を向上させることができる．(2)実験2の結果を表9に示す．この実験は学習データとテストデータを分離した,いわゆるオープン実験である．文検出率と一意正解率はクローズ実験である実験1の結果に比べると若干低下し,平均候補数もやや増加している．しかし,1.2という平均候補数は,この解析結果に対してさらに何らかの後続処理を行う場合,処理量の上で問題となる数ではなく,表7に示される決定論的解析法の結果と比べると,文検出数,一意正解数,曖昧度減少率など全てが高い．したがって,未知文の係り受け解析に対しても(1)と同様の結論が導かれる．(3)実験3の結果を表10に示す．これもオープン実験である．学習データの量が増加するに従って,一意正解数,曖昧度減少率が向上するともに,平均候補数は減少する．文検出数はほぼ一定に保たれる．したがって,学習データ量を増加させることはこのような解析法の性能向上に有効である．より大きなコーパスを使用することによって,更に解析性能が向上することが期待される．</subsection>
  <section title="むすび">本研究の結論は次のようにまとめることができる．係り受け距離の頻度分布は係り受け解析におけるヒューリスティクスとして有効である．係り受け距離の頻度分布は,係り文節の種類別に求めた方がよい．頻度分布の補間により,係り受け規則の不完全さとコーパスの希薄性をある程度補うことができる．頻度分布を抽出する学習データ量を増加することにより解析性能が向上する．ここで用いた学習データよりもっと多くのデータを使用することにより,解析性能がさらに向上する可能性がある．問題点としては,以下のことが挙げられる．以上の結果と問題点を踏まえて,今後は文節間の係り受けに関する言語現象を詳しく調査し,係り受け解析に対するより優れたぺナルティ関数の設定法について研究を進める予定である．document</section>
</root>
