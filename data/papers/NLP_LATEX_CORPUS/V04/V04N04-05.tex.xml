<?xml version="1.0" ?>
<root>
  <title>発話タイプ付きコーパスを用いた確率的対話モデルの自動生成</title>
  <author>北研二福井義和永田昌明森元逞</author>
  <jabstract>コーパスに基づく確率的言語モデルとして，従来は主に語彙統語論的なモデルが扱われてきた．我々は，より高次の言語情報である対話に対する確率的モデルを，コーパスから自動的に生成するための研究を行った．本研究で用いたコーパスは，ATR対話データベース中の「国際会議参加登録」に関する対話データであり，各発話文には，発話者のラベルおよび陳述・命令・約束などの発話行為タイプが付与されている．本技術資料では，これらのコーパスから，２種類の方法を用いて，確率的な対話モデルを生成する．まず初めに，エルゴードHMM(HiddenMarkovModel)を用いて，コーパス中の話者ラベルおよび発話行為タイプの系列をモデル化した．次に，ALERGIAアルゴリズムと呼ばれる，状態マージング手法に基づいた学習アルゴリズムを用いて，話者ラベルおよび発話行為タイプの系列をモデル化した．エルゴードHMMの場合には，確率モデルの学習に先立ち，モデルの状態数をあらかじめ決めておく必要があるが，ALERGIAアルゴリズムでは，状態の統合化を繰り返すことにより，最適な状態数を持つモデルを自動的に構成することが可能である．エルゴードHMMあるいはALERGIAアルゴリズムを用いることにより，話者の交替や質問・応答・確認といった会話の基本的な構造を確率・統計的にモデル化することができた．また，得られた確率的対話モデルを，情報理論的な観点から評価した．</jabstract>
  <jkeywords>対話モデル，発話行為タイプ，コーパス，隠れマルコフモデル，アルゴリズム</jkeywords>
  <subsubsection title="">*(1)接頭木アクセプタの作成学習データから接頭木アクセプタ(PrefixTreeAcceptor；PTA)を作る．なお，接頭木アクセプタとは，学習データ中のシンボル列を受理する決定性有限オートマトンであり，トライのようにシンボル系列の接頭部分が同じものを共通の状態によって表現したものである．例えば，学習データS=,00,10,110(は空列)に対するPTAは，図のようになる．</subsubsection>
  <section title="はじめに">話し言葉や対話における特徴として，旧情報や述語の一部が省略されるなど，断片的で不完全な発話が多く現れるという点をあげることができる．このような断片的あるいは不完全な発話を正しく認識／理解するためには，対話に対する適切なモデルが必要となる．また，話し言葉や対話の音声認識を考えた場合，認識候補の中には統語的にも意味的にも正しいが，対話の文脈の中では不適切な認識候補が存在する場合もある．例えば，文末の述語「〜ですか」と「〜ですが」は，お互いに誤認識されやすいが，対話モデルを用いることにより，このような誤認識を避けられたり，あるいは誤り訂正が可能となることが期待できる．文献では，発話行為タイプ(IllocutionaryForceType;IFT)のラベルが付いたコーパスから，IFTのマルコフモデルを学習し，このモデルが対話のエントロピーを大きく減少させることを示している．我々は，同様のIFT付きコーパスを用いて，対話構造を表す確率モデルを自動生成する研究を行なった．我々の研究においては，確率的対話モデルの生成に２種類の独立な方法を用いた．最初の方法では，IFT付きコーパスの話者ラベルおよび発話行為タイプの系列を，エルゴードHMM(HiddenMarkovModel)を用いてモデル化した．この方法では，モデルの構造(状態数)をあらかじめ定めておき，次にモデルのパラメータ(状態遷移確率，シンボル出力確率，および初期状態確率分布)を学習データから推定した．２番目の方法では，状態の統合化を繰り返すことにより，最適な状態数を持つモデルを自動的に生成することのできる状態マージング手法を用いた．近年，状態マージング手法に基づく確率モデルの学習アルゴリズムがいくつか提案されているが，我々はCarrascoらによるALERGIAアルゴリズムを用いた．以下では，２節でIFT付きコーパスの概要について説明する．３節でエルゴードHMMによる対話構造のモデル化について述べ，４節で状態マージング手法による対話構造のモデル化について述べる．</section>
  <section title="IFT 付きコーパス">対話モデル作成のための基礎データとして，発話行為タイプ(IllocutionaryForceType;IFT)付きコーパスを用いた．これは，ATR対話データベース中の「国際会議参加登録のタスク」の対話の各発話について，その発語内行為を分析し，陳述・命令・約束などの発話のタイプが付けられたコーパスである．このコーパスで用いられているIFTは，表層の統語的パターンと比較的直接的な対応がとれる表層IFT(SurfaceIllocutionaryForceType)と呼ばれるものである．また，各発話文には，発話者(事務局または質問者)を示すラベルが付与されている．IFT付きコーパスで用いられている表層IFTの種類および各IFTに属する例文を表に，IFT付きコーパスの例を図に示す．本研究における評価実験では，IFT付きコーパスの中から，モデル会話10対話(222文)とキーボード会話50対話(1686文)を用いた．</section>
  <section title="エルゴード HMM による対話構造のモデル化">IFT付きコーパスの各発話には，話者ラベルおよびIFTが付与されている．話者の交替や質問・応答・確認のような対話の基本的な構造を確率・統計的にモデル化するために，コーパス中の話者ラベルおよびIFTの系列をエルゴードHMMによりモデル化することを試みた．なお，エルゴードHMMとは，自己遷移も含めすべての状態間の遷移を許す全遷移型のHMMである．本実験では，あらかじめHMMの状態数を決めておき，Baum-Welchの再推定アルゴリズムにより，エルゴードHMMの学習を行なった．初期モデルとしては，初期状態分布確率を均等確率に，また状態遷移確率および出力確率は確率値の総計が１になるようなランダムな値で初期化した．エルゴードHMMの学習データとして，モデル会話およびキーボード会話中から，以下の２つの系列を抽出した．IFTのみの系列話者ラベルとIFTを組み合わせたラベルの系列IFTの総数は９個であり，対話コーパス中の発話者は２名(事務局あるいは質問者)であるので，上記(2)の場合のシンボル数は18個である．実験では，エルゴードHMMの構造として状態数2〜14のものを用いて学習を行なった．表に，状態数2,4,6,8,10,12,14の場合のモデルのエントロピーを示す．表で，IFTと示されているのはIFTのみの系列を用いたときの結果であり，SP-IFTは話者ラベルとIFTを組み合わせたラベルの系列を用いたときの結果である．一般的な傾向として，状態数が増えるに従いエントロピーが小さくなり，同じ状態数では話者ラベルを併用したものの方がエントロピー値が大きくなっている．文献の結果では，trigramモデルを使った場合，モデル会話でのSP-IFTのエントロピー値は1.26，キーボード会話でのSP-IFTの値は2.19と報告している．本実験では，12〜14状態のエルゴードHMMの場合が，trigramのエントロピー値とほぼ同等になっている．学習後のHMMの構造(状態数5の場合)を図および図に示す．図はIFTのみの系列から得られたモデルであり，図は話者ラベルとIFTを組み合わせたラベルの系列から得られたモデルである．図には，遷移確率および出力確率が0.1以上のもののみを記しており，矢印の太いものほど大きな遷移確率を持っていることを示している．状態遷移の一番上に書かれている確率が遷移確率であり，その下に各シンボル(IFT)の出力確率が記されている．図で，Sで始まるシンボルは事務局側の発話であることを，またQで始まるシンボルは質問者側の発話であることを示している．例えば，図では，状態1が初期状態であり，質問者が最初の発話「もしもし」を発話するとQphaticを出力する遷移をたどることになる．これは，状態1での自己ループあるいは状態1から状態2への遷移に対応している．「国際会議参加登録のタスク」では，事務局の「こちらは会議事務局です」という発話により対話が始まる場合もある．この場合にはSinformを出力する遷移である状態1での自己ループとなる．また，図では，状態遷移が事務局側の発話と質問者側の発話で比較的きれいに分かれている．例えば，状態3から状態2への遷移は質問者側の発話によって起こり，しかもこの遷移は事務局に対する質問や依頼に対応していることが分かる．この質問や依頼に対し，状態2から状態0の遷移で事務局が応答(Sresponse,Sinform)する確率が非常に高いことも読みとることができる．以上のように，発話行為タイプ付きコーパスから得られたエルゴードHMMは，質問・応答といった基本的な構造を抽出しているということができる．</section>
  <section title="状態マージング手法による対話構造のモデル化">エルゴードHMMによるモデル化では，確率モデルの学習に先立ち，モデルの構造(状態数)をあらかじめ決めておく必要がある．これに対し，近年，状態マージング手法を用いて，学習データに対し最適な構造を持つモデルを自動的に構築する研究がいくつか行なわれている．我々は，CarrascoらによるALERGIAアルゴリズムを用いて，対話構造のモデルを構築することを試みた．</section>
  <subsection title="ALERGIA アルゴリズム">ALERGIAアルゴリズムは，与えられた学習データを受理する確率決定性有限オートマトンを構成するアルゴリズムである．詳細なアルゴリズムは，文献に説明されている．以下では，ALERGIAアルゴリズムの概要について述べる．</subsection>
  <subsection title="ALERGIA アルゴリズムの動作例">ALERGIAアルゴリズムの動作を，簡単な例で説明する．いま，学習データとして，次の集合Sが与えられたとする．また，=0.8と仮定する．</subsection>
  <subsection title="対話構造のモデル化">上述のALERGIAアルゴリズムを用いて，IFT付きコーパスから対話構造をモデル化する実験を行なった．学習データとしては，キーボード会話50対話(1686文)を用いた．ALERGIAアルゴリズムでは，状態の等価性は式()により判定されるが，式()の右辺の値(定数の値)を変えることにより，様々な状態数を持つオートマトンを学習データから構成することができる．図に，ALERGIAアルゴリズムにより得られたオートマトンの状態数とパープレキシティの関係を示す．パープレキシティの値は，学習データに対するテストセット・パープレキシティを用いている．状態数の増加にともないパープレキシティは減少している．パープレキシティPとエントロピーHの間には，なる関係があるが，式()より，ALERGIAアルゴリズムで得られたモデルのエントロピーを算出してみると，エルゴードHMMと同程度の精度を達成するためには，エルゴードHMMの場合よりもはるかに多くの状態が必要となることが分かる．これは，HMMが非決定性の有限オートマトンと等価であるのに対し，ALERGIAアルゴリズムにより得られるモデルが決定性の有限オートマトンであるためである．図は，話者ラベルとIFTを組み合わせたラベルの系列から得られた30状態のオートマトンの一部(16個の状態)である．このオートマトンの初期状態は状態0であり，最終状態は状態22である．図の左側には，初期状態0から状態遷移する確率の高い11個の状態(状態0,4,7,9,10,11,12,17,20,27,28)が示されている．状態0から始まり再び状態0に至る状態遷移系列(例えば，0→7→4や0→7→27→28など)が，質問・応答・確認などの対話の基本サイクルを表していると考えることができる．また，図の右側に，最終状態22に状態遷移する確率の高い5個の状態(状態1,16,21,22,23)が示されている．例えば，状態27あるいは28で，expressive(例：「ありがとうございました」)に対応する発話が現れると，最終状態へ向かう状態遷移が選択されるということが分かる．しかし，国際会議参加登録のタスクでは，expressiveやphaticというIFTの出現頻度はIFT全体の数パーセントにしか過ぎないので，状態27あるいは28から状態21へ遷移する確率は低くなっている(図中，遷移確率の小さいものは破線で示されている)．*-3mm</subsection>
  <section title="おわりに">*-2mm本技術資料では，コーパスからの確率的対話モデルの自動生成に関する研究として，エルゴードHMMによる対話構造のモデル化とALERGIAアルゴリズムによる対話構造のモデル化の２種類の方法について述べた．モデル化実験では，ATR対話データベース中の「国際会議参加登録」に関する対話データの各発話文に発話者のラベルおよび陳述・命令・約束などの発話行為タイプを付与したものを用いた．エルゴードHMMおよびALERIGIAアルゴリズムを用いて，上記コーパス中の発話者のラベルおよび発話行為タイプの時系列のモデル化を行なうことにより，話者の交替や質問・応答・確認といった会話の基本的な構造を確率・統計的に反映した確率的対話モデルを構築した．今後は，同様の手法を用いて，対話における話題の遷移等をモデル化するための研究を行ないたいと考えている．*-2mmdocument</section>
</root>
