\documentstyle[epsf,jnlpbbl]{jnlp_j_b5}   

\setcounter{page}{21}
\setcounter{巻数}{4}
\setcounter{号数}{2}
\setcounter{年}{1997}
\setcounter{月}{4}
\受付{1996}{4}{24}
\再受付{1996}{6}{12}
\採録{1996}{7}{18}

\setcounter{secnumdepth}{2}

\title{コーパスに基づく動詞の多義解消}
\author{福本 文代\affiref{Yama} \and 辻井 潤一\affiref{Tokyo}}
\headauthor{福本 文代・辻井 潤一}
\headtitle{コーパスに基づく動詞の多義解消}

\affilabel{Yama}{山梨大学工学部電子情報工学科}
{Deptartment of Electrical Engineering and Computer Science, Yamanashi
University}
\affilabel{Tokyo}{東京大学理学部情報科学科}
{Department of Information Science, University of Tokyo}

\jabstract{
本稿では, コーパスから抽出した動詞の語義情報を利用し, 文中に含まれる多
義語の曖昧性を解消する手法を提案する.  先ずコーパスから動詞の多義解消
に必要な情報を抽出する手法について述べる.  本手法では, 多義を判定しな
がら意味的なクラスタリングを行なうことで多義解消に必要な情報を抽出する. 
そこで, 表層上は一つの要素である多義語動詞を, 多義が持つ各意味がまとまっ
た複数要素であると捉え, これを一つ一つの意味に対応させた要素 (仮想動詞
べクトルと呼ぶ) に分解した上でクラスタを作成するという手法を用いた. 本
手法の有効性を検証するため, 丹羽らの提案した単語ベクトルを用いた多義語
の解消手法と比較実験を行なった結果, 14種類の多義語動詞を含む1,226文に
対し, 丹羽らの手法が平均62.7\%の正解率に対し, 本手法では71.1\%の正解率
を得た.}

\jkeywords{コーパス, 統計手法, 語義の曖昧性解消, 意味} 

\etitle{Word-Sense Disambiguation Using the Extracted \\ Polysemous
Information from Corpora}
\eauthor{Fumiyo Fukumoto \affiref{Yama} \and Jun'ichi
Tsujii\affiref{Tokyo}}

\eabstract{
In this paper, we focus on a definition of polysemy in terms of
distributional behaviour of words in monolingual texts and propose a
method for disambiguating word-senses in sentences containing
occurrences of polysemous verbs.  We first discuss existing work on
some corpus-related approaches on word-sense disambiguation and show
the significance of our approach by comparing it with other related
work.  Then we give a definition of polysemy from the viewpoint of
clustering and propose a clustering method which {\it automatically}
recognises polysemous words.  Finally the information extracted by the
clustering method is shown to contribute to disambiguating word-senses
in sentences containing occurrences of polysemous verbs.  We report
the results of two experiments.  The first experiment, Disambiguation
Experiment, is conducted in order to see how the extracted polysemy
information can be used to disambiguate word-senses in actual texts.
The second, Comparative Experiment, is conducted in order to see
how our disambiguation technique is effective than other related
approach, Niwa's technique.  The results of experiments demonstrate
the applicability of our proposed method.}

\ekeywords{Corpus, Statistics, Word Sense Disambiguation, Semantics}

\begin{document}
\maketitle

\section{まえがき}

自然言語処理における重要な問題の一つに, 形態\hspace{-0.1mm}$\cdot$\hspace{-0.1mm}構文\hspace{-0.1mm}$\cdot$\hspace{-0.1mm}意味といっ
た言語に関する様々な曖昧性の問題がある.  一般に, 意味的な曖昧性を解消
するためには, 意味に関するさまざまな情報を規則化し記述しておく必要があ
る.  しかし, 意味は文脈に依存して決まるため, あらゆる文脈に対応できる
すべての意味を予め規則として網羅的に記述しておくことは難しい.  Collins
English Dictionary, Roget のシソーラス, 分類語彙表など, 機械可読辞書と
して電子化されたものがあるが, 辞書の記述は語の定義が言語学者によりまち
まちであるため, 現実の文に対処できる有用な意味情報を得ることは難しい. 
そこで, 意味的な曖昧性を解消するためには, 解消手法と同時に, 文脈に依存
した情報をどのように獲得するかが重要となる.

こうしたことを背景に, 最近コーパスから意味的に近い語群の情報や, 共起関
係の情報などを抽出する研究が盛んに行なわれている
\cite[など]{Church1991,Hindle1990,Tsujii1992,Sekine1992,Smadja1993}.  これらのアプローチは
知識獲得のためのアルゴリズムを提案することで, コーパスからその分野に依
存した知識を自動的に抽出するというものである.  

本稿では, 単一言語コーパスから抽出した動詞の語義情報を利用し, 文中に含
まれる多義語の曖昧性を解消する手法について述べる.  2章では, 関連した研
究について述べる.  3章ではコーパスから多義解消に必要な情報を抽出する手
法について述べる.  4章では得られた情報を基に, 文中に含まれる多義語の曖
昧性を解消する手法について述べる.  5章では丹羽らの提案した文脈ベクトル
を用いた名詞の多義解消手法 \cite{Niwa1994}を動詞に適用した結果と比較す
ることで, 本手法の有効性を検証する.

\section{関連した研究}

近年, 大量のコーパスが利用可能になったことを背景に, コーパスから得られ
た情報を用いて語義の曖昧性を解消する研究が多数行なわれている 
\cite[など]{Brown1991,Schutze1992,Zernik1991,Yarowsky1992,Niwa1994}.

Yarowsky らは, Roget のシソーラスカテゴリを利用し, 統計手法を用いるこ
とでテキスト中に現れる多義語の曖昧性を解消する手法を提案した.  彼らの
手法は, 統計情報を用いてシソーラスカテゴリに出現する単語に重み付けを行
なった後, その結果を利用して多義語の周辺語の重みの和から多義語がどのシ
ソーラスカテゴリに属するかを決定するというものである.  この手法を12 の
多義語名詞に適用し実験を行なった結果, 平均解消率92\% という高い正解率
が得られることが報告されている \cite{Yarowsky1992}.  しかし, Yarowsky 
らのシソーラスを用いる問題として, データスパースネスの問題が指摘されて
いる.  すなわち, シソーラスカテゴリに示されている語が抽象的な語で定義
されているため, 文書の種類によっては, その語が文書に出現しない場合があ
る\cite{Niwa1995}. また, Yarowskyらは彼らの手法が動詞の多義解消につい
ては名詞と同様の正解率が得られないことを指摘している.

丹羽らは, 文脈を構成する単語をベクトルで表現し, 文脈をそれらベクト
ルの和で表した.  任意の文脈 A における単語の意味は, 多義の各意味を表す
文脈例を各意味に応じてあらかじめ用意しておき, 各々の例と文脈 A におけ
る単語の意味との類似度(内積)を計算し, その値が最も大きい文脈が示す意味
であるとした.  この手法を名詞の多義判定に適用した結果, 平均80\%の正
解率が得られている \cite{Niwa1994}.

Brown らは対訳テキストを用い, 一方の言語の語義の曖昧性を他方の語の情報
を利用することで解消する手法を提案している \cite{Brown1991}.  彼らは実
際に英仏機械翻訳システムにこの手法を適用し, 検証を行なっている.  しか
し彼らは問題点として, (1) 多義語の持つ意味を予め高々2つに限定している. 
(2) 語が, ターゲット言語の2つの異なる訳に翻訳できないとき, 語義の解消
ができない. (3) 膨大な対訳テキストを必要とする, を挙げている.

Zernik や Sch\"{u}tze らは, 動詞の多義を判定するための情報として名詞と
動詞の共起関係を利用している.  任意の動詞がどの意味を持つかは, 動詞と
共起する名詞の集合に応じて決定される.  しかし, 名詞の集合を意味に応じ
て分割する処理は人手で行なっているため, 語の分類は人間の言語的な直観に
頼ることになってしまう.

本稿では, Yarowsky らがシソーラスカテゴリを利用しているのに対し, 単一
言語コーパスから抽出した動詞の語義情報を利用し, 文中に含まれる多義語の
曖昧性を解消する手法について述べる.  我々の手法は名詞の集合を意味に応
じて人手により分割する Zernik や Sch\"{u}tzeらの手法, と異なり, 多義解
消に必要な情報は, 与えられた多義語を含む動詞グループに対し, クラスタリ
ングアルゴリズムを適用することで自動的に得られるため, 人間の介在を必要
としない.  また, Brown らが多義語の持つ意味を予め高々2つに限定している
のに対し, 本手法では, 多義語を含む動詞グループに対し, クラスタリングア
ルゴリズムを適用するため, 2つ以上の意味を持つ語に対しても曖昧性の解消
が可能である.


\section{多義解消に必要な情報の抽出}

一般に, 意味的に近い2つの動詞は同じ名詞と共起して現れる.  

\vspace*{2mm}
\begin{tabular}{ll}
(s1) &\parbox[t]{12cm}{In the past, however, coke has typically \underline{taken} a
minority \underline{stake} in such ventures.} \\
(s1') &\parbox[t]{12cm}{Guber and Peters tried to \underline{buy} a
\underline{stake} in Mgm in 1988.} \\
\end{tabular}

\begin{tabular}{ll}
(s2) &\parbox[t]{12cm}{That process of sorting out specifies is likely
to \underline{take} \underline{time}.}  \\
(s2') &\parbox[t]{12cm}{We \underline{spent} a lot of \underline{time}
and money in building our group of stations.} \\
\end{tabular}
\vspace*{2mm}

\noindent
例えば, {\it Wall Street Journal} から抽出した例文(s1)$\sim$(s2')において, (s1), (s1')に現れる\underline{take} と
\underline{buy} は共に
\underline{stake} と共起して現れ, ほぼ同じ意味を持つ  \cite{Liberman1991}.  同様に(s2),
(s2')に現れる$\!$\underline{take}$\!$と$\!$\underline{spend}$\!$は共に$\!$\underline{time}$\!$と共起して現れ, 両者は同じ意味を持つ.  従って
多義語$\!$\underline{take}$\!$がもつ複数の意味は, 各意味に対応した動詞
\underline{buy}, \underline{spend}と共起して現れる名詞
\underline{stake}, \underline{time}と特徴づけて考えることができる.  す
なわち, 多義語を含む文において, もし多義語と共起する名詞のうち少なくと
も一つが多義語の意味を特徴づける名詞と同じ(あるいは名詞の集合に属する)
ならば, 文中の多義語の意味はその名詞と共起する動詞の意味に同定すること
ができる.  我々は文中に現れる多義語の曖昧性を, その語と共起する名詞を用いるこ
とで解消した.  

以下, 3.1節では仮想動詞について述べる.  3.2節では語の意味的な偏差を計
算する手法について述べ, 3.3節では3.2節で述べた偏差の値を用いてクラスタ
リングを行なうためのアルゴリズムについて説明する.  多義語を含む動詞グルー
プに対し, クラスタリングアルゴリズムを適用することで多義語の各意味を示
す動詞(仮想動詞)と共起する名詞の集合が, 動詞の個数分得られる.  3.4節で
は仮想動詞, 及びそれと共起する名詞との相互情報量を求める手法
について述べる.  仮想動詞と名詞の相互情報量は, 文中に現れる名詞が複数
の(名詞の)集合に含まれる場合にどの集合に含まれるかを一意に決定するため
に用いられる.



\subsection{仮想動詞}

本手法では, 多義を判定しながら意味的なクラスタリングを行なうことで多義
語の曖昧性解消に必要な情報, すなわち, 多義語の意味を特徴づける名詞の集合
を抽出する.  そこで, 表層上は一つの要素である多義語を, 多義が持つ各意
味がまとまった複数要素であると捉え, これを一つ一つの意味に対応させた要
素 (本稿ではこの要素を{\bf 仮想動詞}ベクトルと呼ぶ)に分解した上でクラスタを作成する
という手法を用いた.

我々は, 動詞をベクトルと捉え, 動詞と共起する$n$個の名詞を軸とする$n$次
元名詞空間上でこれを表した. 軸$i$(1 $\leq$ $i$ $\leq$ $n$)における動詞
ベクトルの長さは, $i$軸で示される名詞と動詞の相互情報量\hspace{-0.15mm}\cite{Church1990}\hspace{-0.1mm}の値を用いた. 仮に2つの動詞に多義性がなく, かつこの2
つの動詞が意味的に近いとすると, これらの動詞はこの空間上で互いに距離が
近いため, 同一のクラスタに含まれることになる. 一方, \hspace{-0.05mm}(s1)\hspace{-0.05mm}と\hspace{-0.05mm}(s2)\hspace{-0.05mm}に現れ
る\underline{take}は多義であるため, 各意味を表す動詞
ベクトル \underline{buy}, \underline{spend}のいずれともクラス
タを構成しなければならない.  そこで, ベクトル \underline{take}を各軸
に従って (この場合, \underline{stake}と\underline{time}の2軸) 分割する
ことを考える.  ベクトル \underline{take}を
\underline{stake}と\underline{time}の軸に従って分割した結果を図\ref{cluster1}に示す.

{
\begin{figure}[htbp]
\centerline{
\epsfile{file=cluster1.eps,height=45mm}}
\caption{ベクトル take の分割} \label{cluster1}
\begin{center}
\vspace*{-5mm}
Figure 1 The decomposition of the verb \underline{take} 
\end{center}
\end{figure}
}

\noindent
図\ref{cluster1}において, ベクトル \hspace{-0.2mm}\underline{take}は, \underline{stake}\hspace{-0.2mm}と\hspace{-0.2mm}\underline{time}の軸上でベクトル \hspace{-0.2mm}{\sf take1}\hspace{-0.3mm}と\hspace{-0.2mm}{\sf take2}に分割されている.  {\sf take1}と{\sf take2}を{\bf 仮想動詞}
ベクトルと呼ぶ.  図\ref{cluster1}は
仮想動詞ベクトルを導入することで, 各々意味的に近い要素を持つ2つのクラスタ 
\{{\sf take1}, buy\},
\{{\sf take2}, spend\}が得られることを示す.


\subsection{動詞グループの偏差}

クラスタリングアルゴリズムは動詞グループの意味的な偏差を比較し, 偏差の
少ない順にクラスタを生成する.  今$m$個から成る動詞グループをVG = \{$v_{1}$,
$\cdots$, $v_{m}$\}とすると, VG の偏差$Dev(\mbox{VG})$は式(\ref{22})で示される.  ただ
し, $n$ は動詞と共起する名詞の個数とする.

\begin{eqnarray}
Dev(\mbox{VG}) & = &
\frac{1}{\mid\bar{g}\mid(\beta \ast m +
\gamma)}\sqrt{\sum_{i=1}^{m}\sum_{j=1}^{n}(v_{ij}-\bar{g_{j}})^{2}} \label{22}
\end{eqnarray}

\noindent
(\ref{22})の$\bar{g_{j}}$ \ \hspace{-0.3mm}( \hspace{-0.3mm}= \ \hspace{-0.3mm}$\frac{1}{m}\sum_{i=1}^{m} v_{ij}$ )は, $j$軸での重心の値を示す. また, $\mid$ $\bar{g}$ $\mid$ \ \hspace{-0.3mm}( \hspace{-0.5mm}= \ \hspace{-0.5mm}$\frac{1}{m}\sqrt{\sum_{j=1}^{n}(\sum_{i}^{m}v_{ij})^{2}}$ ) は重心ベクトルの長さを示す.  (\ref{22})の$v_{ij}$は,

\begin{equation}
v_{ij}  = \left\{ \begin{array}{ll}
	 Mu(v_{i},n_{j}) & \mbox{if $Mu$($v_{i}$,$n_{j}$) $\geq$ $\alpha$} \\
		 0 &\mbox{otherwise} 
	\end{array} \right. \label{v}
\end{equation}

\noindent
とする.  ここで, $Mu(v_{i},n_{j})$は動詞$v_{i}$(1 $\leq$ $i$ $\leq$
$m$)と名詞$n_{j}$(1 $\leq$ $j$ $\leq$ $n$)の相互情報量の値を表し, 式
(\ref{church_mu})で示される.

\begin{eqnarray}
Mu(x,y) & = & log_{2}\frac{P(x,y)}{P(x)P(y)} \label{church_mu}
\end{eqnarray}

\noindent
$P(x)$, $P(y)$ は, $x$, $y$の頻度数$f(x)$, $f(y)$をそれぞれコーパスに
出現する語の総数$N$で正規化したものであり, \hspace{0.1mm}$P(x,y)$は$x$\hspace{-0.1mm}と\hspace{-0.1mm}$y$の共起頻
度数$f(x,y)$を\hspace{-0.1mm}$N$\hspace{-0.1mm}で正規化したものである. また, 式(\ref{v})における$\alpha$は閾値とする.  式(\ref{22})の\hspace{-0.1mm}$\beta$ $\ast$ $m$ + $\gamma$ は, 
動詞の偏差を示す値が動詞の個数に比例して増加することを防ぐために最小2
乗法を用いて行なった正規化である\footnote{{\it Wall Street Journal} を
用いた実験では, $\alpha$ を 3に設定し, $\beta$, $\gamma$ それぞれ 
0.964, -0.495 を得た.}.  式(\ref{22})はその値が小さいほどより偏差が少
ないことを示す.


\subsection{クラスタリング手法}


クラスタリングアルゴリズムは, non-overlapping と overlapping アルゴリ
ズムに大別できる.  本手法はoverlapping クラスタリングアルゴリズムに含
まれる.  Overlapping アルゴリズムの代表的なものとして 
$B_{k}$ ($k$ = 1,2,$\cdots$) 手法がある\cite{Jardine1968}. 

本手法と$B_{k}$手法との違いは, $B_{k}$手法では要素が複数のクラスタに属
すか否かは$k$の個数に依存して決まるのに対し, 我々の手法は, 複数のクラ
スタに属すか否かを判定する条件をアルゴリズムの中に導入している点が異な
る.  我々の手法では, 動詞ベクトルを分割して仮想動詞ベクトルを作成し, 
\ その仮想動詞ベクトルを含
むクラスタの偏差を比較することで, \ 要素が 複数のクラスタに属すか否か, す
なわち多義であるかどうかの判定を行なっている.  例えば,
\underline{take} が\underline{buy}\hspace{-0.1mm}と\underline{spend}\hspace{-0.1mm}の意味
を持つかどうかを判定するために, ベクトル\hspace{-0.1mm}\underline{take}\hspace{-0.1mm}を\underline{stake}\hspace{-0.1mm}と
\underline{time}\hspace{-0.1mm}の軸に従い分割し, 仮想動詞ベクトル {\sf take1}と{\sf take2}を
作成する.  \underline{take} が多義であるか否かは, \{{\sf take1},
buy\}, \{{\sf take2}, spend\} 及び, \{take,buy,spend\} のクラスタの偏差
を比較することにより決定される.

\subsubsection{Splitting と Lumping}

今 $v$と$w_{p}$を動詞とし, $w_{1}$, $\cdots$, $w_{n}$ を動詞, または仮想動詞とする.
また, $Dev(v, w_{i})$ $\leq$ $Dev(v, w_{j})$ (1 $\leq$ $i$ $\leq$ $j$
$\leq$ $n$) かつ, $Dev(v, w_{1})$ $\leq$ $Dev(v, w_{p})$とする.  本手
法では$v$が$w_{1}$と$w_{p}$ で示される2つの意味を持つか否かを判定する
ために, (\ref{split})と(\ref{lump})で示されるクラスタを作成し, それぞ
れの偏差を比較する.

\begin{eqnarray}
\{v_{x}, w_{p}\}, \{v_{y}, w_{1}, \cdots, w_{n}\} \label{split} \\
\{v, w_{1}, \cdots, w_{p}, \cdots, w_{n}\} \label{lump}
\end{eqnarray}

\noindent 
ただし, (\ref{lump})の $w_{1}$, $\cdots$, $w_{p}$, $\cdots$, $w_{n}$ 
は $Dev(v, w_{i})$ $\leq$ $Dev(v, w_{j})$ (1 $\leq$ $i$ $\leq$ $j$
$\leq$ $n$)を満たすとする.  (\ref{split})の $v_{x}$ と $v_{y}$ は$v$の
仮想動詞を示す.  以下では, (4)で示されるクラスタを作成するために,  \hspace{-0.1mm}$v$,  \hspace{-0.1mm}$w_{1}$, \hspace{-0.1mm}$w_{p}$を入力とし, 仮想動詞$v_{x}$\hspace{-0.1mm}と\hspace{-0.1mm}$v_{y}$を出力する関数 
$split$, 及び, \hspace{-0.1mm}(5)で示されるクラスタを作成する過程で仮想動詞$v_{x}$,
$v_{y}$が現れた場合にそれらをマージする関数 $lump$ を定義する.

\begin{enumerate}

\item 関数 $split$ は入力 $v$,
$w_{1}$, $w_{p}$に対し, $v_{x}$ と $v_{y}$ を出力する.  ただしベクトル 
$v$ は, ($v_{1}$, $\cdots$, $v_{n}$)で示されるとする.


\begin{eqnarray}
split(v,w_{p},w_{1}) & = & (v_{x},v_{y}) \label{sp} \\
\mbox{where} \ \ \ Dev(v, w_{1}) & \leq & Dev(v, w_{p}) 
\end{eqnarray}

\vspace*{-2mm}
\[ 
	v_{x}  = \left[ \begin{array}{l}
		v_{x1} \\
		v_{x2} \\
		\vdots \\
		v_{xn} 
		\end{array} \right] \  \hbox{s.t.} \ \
v_{xj} \  = \left\{ \begin{array}{lll}
	v_{j}  &\mbox{if $w_{pj}$ $\neq$ 0} &(8)   \nonumber \\
	0 & \mbox{otherwise} \hspace*{2.2cm} &(8') \nonumber
	\end{array}
           \right. 
\]
\[
 v_{y}  = \left[ \begin{array}{l}
		v_{y1} \\
		v_{y2} \\
		\vdots \\
		v_{yn} 
		\end{array} \right] \  \hbox{s.t.} \ \
v_{yj} \  = \left\{ \begin{array}{lll}
	v_{j}  &\mbox{if ($w_{1j}$ $\neq$ 0 or $w_{pj}$ = $w_{1j}$ =
0)} &(9)  \nonumber \\
	0 & \mbox{otherwise} &(9')   \nonumber
	\end{array}
           \right. 
\]
\addtocounter{equation}{2}

式(8), (9) において $v$と共起する$n_{j}$が, $w_{p}$と$w_{1}$の両方と共
起する場合には, $v_{xj}$ と $v_{yj}$ は共に $v_{j}$ \ = \
$Mu(v,n_{j})$ とした. また式(9) において $v$ と共起する$n_{j}$ が,
$w_{1}$ と $w_{p}$のいずれとも共起しない場合には, $v_{yj}$ の値は 
$v_{j}$ の値とした.  これは, $v_{j}$ が $v_{x}$ と$v_{y}$の両方に含ま
れない場合, \{$v_{y}$, $w_{1}$\} の偏差は常に, \{$v_{x}$, $w_{p}$\}よりも小さくな
る.  よって, $v_{x}$ と $v_{y}$ の偏差をできるだけ均等にするため,
$v_{yj}$の値は, $v_{j}$の値とした.

\item 関数 $lump$ は仮想動詞 $v_{x}$ と $v_{y}$ を入力とし $w$を出
力する.


\vspace*{-5mm} 
\begin{eqnarray}
lump(v_{x}, v_{y}) & = & w \label{lu}
\end{eqnarray}

\vspace*{-7mm} 
\begin{equation}
\hspace*{-3mm}  w = \left[ \begin{array}{l}
		w_{1} \\
		w_{2} \\
		\vdots \\
		w_{n} 
		\end{array} \right] \ \hbox{s.t.} \ w_{j}
\ = \left\{ \begin{array}{ll}
	v_{xj} + v_{yj}   &\mbox{if $v_{xj}$$\neq$$v_{yj}$} \\
	v_{xj}  & \mbox{if $v_{xj}$ = $v_{yj}$}
	\end{array}
           \right. 
\end{equation}

\end{enumerate}

\noindent
実験では, (\ref{split})で示される二つのクラスタの偏差の値が共に
(\ref{lump})で示されるクラスタの偏差の値よりも小さい場合に動詞$v$は多義
とみなした.


\subsubsection{クラスタリングアルゴリズム}


クラスタリングアルゴリズムの流れを図\ref{flow_algo} に示す.  図\ref{flow_algo} の`('
はその上で示される関数の処理を示す.

{\begin{figure}[htbp]
\begin{center}
\fbox{
\parbox{130mm}{
{\bf begin} \\ 
\hspace*{5mm}ICS := {\sf Make-Initial-Cluster-Set}(VG) \\
\[  \left( \begin{array}{l}
\mbox{VG} = \{v_{i} \mid i = 1,\cdots,m\} \\ 
\mbox{ICS} = \{Set_{1}, \cdots, Set_{\frac{m(m-1)}{2}}\} \\
ただし \ Set_{p} = \{v_{i},v_{j}\} \ と \ Set_{q} = \{v_{k},v_{l}\} \in
\mbox{ICS} \ (1 \leq p < q \leq m) は \hspace*{2cm}\\
Dev(v_{i},v_{j}) \leq Dev(v_{k},v_{l}) を満たす.
\end{array} \right. \]
\hspace*{5mm}{\bf for} \ $i$ := 1 {\bf to} $\frac{m(m-1)}{2}$ {\bf do} \\
\hspace*{10mm}
\parbox{125mm}{
{\bf if} \ CCS = $\phi$ \\
\hspace*{10mm}{\bf then} \ $Set_{\gamma}$ := $Set_{i}$ \\
\hspace*{10mm} \hspace*{10mm} i.e. $Set_{i}$ は新たに得られるクラスタ
として CCS に蓄積される. \\
{\bf else if} \ $Set_{\alpha}$ $\in$ CCS exists {\it
such that} $Set_{i}$ $\subset$ $Set_{\alpha}$  \\
\hspace*{10mm}{\bf then} \ $Set_{i}$ が ICS から削除され,
$Set_{\gamma}$ := $\phi$ となる. \\
{\bf else if} \\ 
\hspace*{10mm}{\bf for all} $Set_{\alpha}$ $\in$ CCS {\bf do} \\
\hspace*{15mm}{\bf if} \ $Set_{i}$ $\cap$ $Set_{\alpha}$ = $\phi$ \\
\hspace*{20mm}{\bf then} \ $Set_{\gamma}$ := $Set_{i}$ \\
\hspace*{15mm} \hspace*{15mm} i.e. $Set_{i}$ は新たに得られるクラスタ
として CCS に蓄積 \\
\hspace*{38mm}される. \\
\hspace*{15mm}{\bf end\_if} \\
{\bf else} $Set_{\beta}$ := {\sf
Make-Temporary-Cluster-Set}($Set_{i}$,CCS) \\
\hspace*{0.8cm}{\bf (}$Set_{\beta}$ := $Set_{\alpha}$ $\in$ \mbox{CCS} \ {\it such that} \
$Set_{i}$ $\cap$ $Set_{\alpha}$ $\neq$ $\phi$ \hspace*{4cm} \\
\hspace*{10mm}$Set_{\gamma}$ := {\sf
Recognition-of-Polysemy}($Set_{i}$,$Set_{\beta}$)  \\
{\bf end\_if} \\ 
{\bf end\_if} \\ 
{\bf end\_if}} \\ \\
\hspace*{10mm}{\bf if} \ $Set_{\gamma}$ = VG  \\
\hspace*{15mm}{\bf then} \ {\it for\_loop} を抜ける. \\
\hspace*{10mm}{\bf end\_if} \\
\hspace*{5mm}{\bf end\_for} \\
{\bf end}}}
\caption{クラスタリングアルゴリズムの流れ} \label{flow_algo}
Figure 2 The flow of the clustering algorithm
\end{center}
\end{figure}
}

\noindent
図\ref{flow_algo} において, 関数 {\sf Make-Initial-Cluster-Set} は, 動
詞グループ VG を入力とし, VG の任意の動詞対の組合せに対し, 意味的な偏
差の値を計算し, 任意の動詞対と偏差の値をその値が昇順になるように出力
する.  この結果をICS(Initial Cluster Set)と呼ぶ.

CCS(Created Cluster Set) は作成されたクラスタの集合を示す.  関数{\sf
Make-Temporary-Cluster-Set} は$Set_{i}$ のどちらか一方の動詞を含むクラ
スタを CCS から抽出する.  その結果である$Set_{\beta}$が関数 {\sf
Recognition-of-Polysemy}に渡される.  関数{\sf Recognition-of-Polysemy}
は動詞が多義か否かを判定する関数である.

今 $Set_{i}$ と $Set_{\beta}$の両方に属する動詞を$v$とする.  $v$が多
義であり$w_{p}$(ただし$w_{p}$は$Set_{i}$の要素とする) と$w_{1}$
(ただし $w_{1}$ は $Set_{\beta}$ の要素とする)の意味を持つか否かを判定
するために, (\ref{split})と(\ref{lump})で示されるクラスタが作成される. 
具体的には関数 (\ref{sp}) が $v$, $w_{1}$, と $w_{p}$ に適用され 
$v_{x}$ と$v_{y}$が作成される.  もし $v_{x}$ と $v_{y}$ が
(\ref{lump})で示されるクラスタを作成する過程で存在する場合, 関数 
(\ref{lu}) が $v_{x}$ と $v_{y}$に適用され, $w$ が作成される.

この処理は新しく得られるクラスタ$Set_{\gamma}$がVG と等しくなるか, あ
るいはICS 
の要素がなくなるまで適用される.


\subsection{仮想動詞と名詞の相互情報量}

多義語を含む動詞グループに対し, 前節で述べたアルゴリズムを適用すること
で, 多義語の各意味を示す動詞と共起する名詞の集合が動詞の個数分得られる. 

\vspace*{-0.5cm}
{\footnotesize
\begin{table}[htbp]
\begin{center}
\caption{\{take, obtain, spend, buy\} のクラスタリング結果}
\label{app_1}
Table 1 The clustering results of \{take, obtain, spend, buy\} \\
\begin{tabular}{l||l|r|r|c|r|c}  \hline \hline
\multicolumn{2}{}{} &\multicolumn{3}{|l|}{クラスタリング結果得られる値}
&\multicolumn{2}{l}{(\ref{re_cal}), (\ref{church_mu})より得られる値} \\ \hline 
$v_{i}$ &$n_{ij}$ &$f(n_{ij})$ &$f(v,n_{ij})$ &$Mu(v,n_{ij})$ &$f(v_{i})$ &$Mu(v_{i},n_{ij})$ \\ \hline
{\sf take1} &columbia  &418 &\hspace*{2mm}5  &3.543 &214 &7.330 \\ 
(buy) &equity  &510 &\hspace*{2mm}6   &3.519 &214 &7.306 \\ 
 &lot  &610 &\hspace*{2mm}8  &3.676 &214 &7.463 \\ 
 &note  &936 &14 &7.653  &214 &3.866 \\ 
 &option &640 &\hspace*{2mm}7  &3.414 &214 &7.201 \\ 
 &order  &1004 &\hspace*{2mm}9  &3.127 &214 &6.914 \\ 
 &part  &1664 &27 &7.770   &214 &3.983 \\ 
 &property  &505 &\hspace*{2mm}7  &3.756 &214 &7.543 \\ 
 &stake  &1081 &28  &4.658 &214 &8.445 \\ 
 &thrift  &494 &\hspace*{2mm}9  &4.150  &214 &7.937 \\ \hline  
{\sf take2} &hour &443 &\hspace*{2mm}9 &4.307 &270 &7.759 \\ 
(spend) &lot &610 &\hspace*{2mm}8  &3.676 &270 &7.127 \\ 
 &minute &197 &\hspace*{2mm}5  &4.628 &270 &8.080 \\ 
 &money &1569 &19  &3.561 &270 &7.012 \\ 
 &month &3546 &39  &3.422 &270 &6.874 \\ 
 &time &2866 &45  &3.936 &270 &7.387 \\ 
 &week &2647 &26  &3.259  &270 &6.710 \\ \hline 
{\sf take3} &drug &1164 &11  &3.203 
&\hspace*{2mm}41 &9.374 \\ 
(obtain) &loan &1369 &12  &3.095
&\hspace*{2mm}41 &9.265 \\ \hline  
\multicolumn{7}{c}{} \\ 
\multicolumn{7}{c}{} \\  \hline 
\multicolumn{2}{}{} &\multicolumn{3}{|l|}{クラスタリング結果得られる値}
&\multicolumn{2}{l}{(\ref{re_cal_oh}), (\ref{church_mu})より得られる値} \\ \hline 
\footnotesize{$v_{r}$} &\footnotesize{$n_{rj}$} &\footnotesize{$f(n_{rj})$} &\footnotesize{$f(v,n_{rj})$}
&\footnotesize{$Mu(v,n_{rj})$} &\footnotesize{$f(v_{r})$}
&\footnotesize{$Mu(v_{r},n_{rj})$} \\ \hline
residue &account &375 &33   &6.422
&\hspace*{-2mm}2429 &6.704 \\ 
 &action &560 &52  &6.500
&\hspace*{-2mm}2429 &6.782 \\ 
 &\multicolumn{6}{l}{etc. total 102} \\ 
\end{tabular} 
\end{center} 
\end{table}
} 

\noindent
表 \ref{app_1} は, 多義語 take を含む動詞グループ\{take, obtain,
spend, buy\} に対し, クラスタリングアルゴリズムを適用した結果を示す. 

クラスタリングの結果得られるこのテーブルを{\it pvn} (polysemous verb
noun)テーブルと呼ぶ.  $v_{i}$ は仮想動詞{\sf take1}, {\sf take2}, {\sf
take3}を示し, それぞれ, `buy', `spend', `obtain' を示す.  $v_{r}$ 
は $v_{i}$ 以外の意味を示す仮想動詞`residue'を示す.  $n_{ij}$ は, 仮想
動詞$v_{i}$と共起する名詞を示し, $n_{rj}$は仮想動詞$v_{r}$
と共起する名詞を示す.  $f(n_{ij})$ と$f(n_{rj})$はそれぞれ
$n_{ij}$, $n_{rj}$の頻度を示し, $f(v,n_{ij})$ と$f(v,n_{r
j})$はそれぞれ`take'と$n_{ij}$, `take' と$n_{rj}$の共起頻度数を
示す.

文中に現れる動詞の多義解消は基本的に名詞$n_{ij}$ 及び$n_{rj}$を用いて行なわれる. 
すなわち, 文中に現れる動詞と共起する名詞が表\ref{app_1}に示されているとき, 文中の動
詞は, その名詞
と共起する仮想動詞の意味となる.  例えば, (s3) において,
\underline{stake} は表\ref{app_1}に示されている.  従って(s3)の
\underline{taken}の意味は, {\sf take1}が示す意味である`buy' と判定される.

\vspace*{2mm}
\begin{tabular}{ll}
(s3) &\parbox[t]{12cm}{In the past, however, Coke has typically
\underline{taken} a minority \underline{stake} in such ventures.} \\
\end{tabular}
\vspace*{2mm}

\noindent 
名詞の中には, 例えば表\ref{app_1}の `lot' のように複数の集合に属する名
詞が存在する. この場合は, 各仮想動詞と `lot' との相互情報量の中で
大きい値を持つ仮想動詞の意味とした.  ただし, 表 \ref{app_1} の $Mu(v,
n_{ij})$ 及び $Mu(v,n_{rj})$は, `take' と各名詞との相互情報量を示す.  そこで, 仮想動
詞$v_{i}$及び$v_{r}$と各名詞との相互情報量$Mu(v_{i},n_{ij})$及び
$Mu(v_{r},n_{rj})$を以下のよう
にして求めた.

\begin{enumerate}
\item $v_{i}$ \ (1 $\leq$ $i$ $\leq$ $k$) を仮想動詞とし, $v_{r}$ を 
$v$ における各仮想動詞以外の意味を示す仮想動詞とする.  $num(i)$ $(1
\leq i \leq k)$ を$v_{i}$と共起する名詞の個数とし, $n_{ij}$ $(1 \leq i
\leq k, \ 1 \leq j
\leq num(i))$を$v_{i}$\hspace{-0.1mm}と共起する$j$軸の名詞とする.
\hspace{-0.1mm}$v_{i}$の頻度$f(v_{i})$と$v_{r}$の頻度$f(v_{r})$ は以下の式で示される.

\begin{eqnarray}
f(v_{i}) & = & f(v) \times
\frac{\sum_{j=1}^{num(i)}f(v,n_{ij})}{\sum_{p=1}^{k}(\sum_{q=1}^{num(p)}f(v,n_{pq}))}
\label{re_cal} \\
f(v_{r}) & = & f(v) - \sum_{i=1}^{k}f(v_{i}) \label{re_cal_oh}
\end{eqnarray}

\item 式(\ref{re_cal})と(\ref{church_mu}), 及び(\ref{re_cal_oh})と(\ref{church_mu})を用い
て, $Mu(v_{i},n_{ij})$ と$Mu(v_{r},n_{rj})$を求める.

\end{enumerate}

\noindent
表\ref{app_1}の$Mu(v_{i}, n_{ij})$ と$Mu(v_{r},n_{rj})$は 
それぞれ仮想動詞$v_{i}$と名詞$n_{ij}$, 仮想動詞$v_{r}$と名詞
$n_{rj}$との相互情報量を示す.


\section{多義語の解消}

文中の多義語 $v$の意味は, $v$の {\it pvn}テーブルを用いて以下のように決定され
る.

\begin{enumerate}
\item[\bf 1.] $v$の後方5語以内に出現する名詞を$x$とすると, $x$が{\it 
pvn} テーブルに存在する場合:

\begin{enumerate}
\item[\bf 1-1.] $x$が一つのみ存在する場合, $v$の意味は, $x$と共起する仮想動詞の意味
とする.

\item[\bf 1-2.] $x$が二つ以上存在する場合, $v$の意味は, $x$と共起する仮想動
詞のうち, $x$との相互情報量の値が最も高い仮想動詞の意味とする.

\end{enumerate}

\item[\bf 2.] $x$ が {\it pvn} テーブルに存在しない場合,
$rel(v_{i},x)$の値が最大になるような仮想動詞$v_{i}$を求める.  $v$の意
味は, $v_{i}$の意味とする.
\end{enumerate}

\noindent
$rel(v_{i},x)$は, $v_{i}$と$x$の意味的な関係を示す式であり, 以下の
ように定義した.

\begin{eqnarray}
rel(v_{i},x) & = & \max_{y \in N_{i}}(\frac{Mu(v_{i},y)}{Dis(x,y)}) \ \ \ (1 \leq
i \leq k) \label{co}
\end{eqnarray}

\noindent
式(\ref{co})において, $N_{i}$は$v_{i}$と共起する名詞の集合を示す.  $Dis(x,y)$ は, $x$と{\it pvn}テーブルに登録されている名詞$y$\hspace{-0.05mm}との偏差を示
す. すなわち, \hspace{-0.1mm}式(\ref{22})\hspace{-0.1mm}において$m$を2とし, $v_{1j}$と$v_{2j}$をそれぞれ,
$x$\hspace{-0.05mm}, $y$\hspace{-0.05mm}とする. さらに式(\ref{22})\hspace{-0.1mm}中の動詞と共起する名詞の個数を名詞$x$\hspace{-0.05mm}及
び$y$\hspace{-0.05mm}と共起する動詞の個数に置き換えることにより$Dis(x,y)$が得られる.  

\vspace*{-2mm}
\section{実験}

実験では, 14の動詞グループに対しクラスタリングアルゴリズムを適用した結果得られた
$pvn$ テーブルを用い, $pvn$ テーブルが曖昧性の解消にどの程度有効である
かの検証を行なった.  さらに丹羽らの提案した文脈ベクトルを用いた名詞の
多義解消手法を動詞に適用した結果と, 本手法とを比較することで, 
本手法の有効性を検証した.  先ず, 実験で用いたデータについて述べ, 実験
とその結果を示す.  次に丹羽らの多義解消手法の概略を示し, 比
較を行なった結果について述べる.

\vspace*{-2mm}
\subsection{データ}

コーパスはタグ付けされた{\it Wall Street Journal} であり, 182,992文, 
総数2,878,688語(総異なり数73,225語)から成る
\cite{Liberman1991}.  実験では, このコーパスからウィンドウサイズを5語にとり, 総数
5,940,193個から成る任意の2語対 (総異なり数2,743,974組)を得た.  ここで単語 $x$ と $y$ のウィ
ンドウサイズが 5語であるとは, $x$ の出現位置から$x$ の後方 5語以内に現
れる単語$y$ と $x$ との組を示す.

我々は, 動詞$x$と名詞$y$の組を使用した.  これは5語という比較的小さいウィンド
ウサイズでは, 動詞と目的語という観点から動詞と名詞の意味的な関係が顕著
に現れると考えられるためである.  また, 動詞の中には, 特定の副詞, 例え
ば様態を示す副詞と共起することで, その動詞の意味が決まる場合も存在する. 
そこで, 名詞と動詞の組で正解が得られなかった多義(表
\ref{disam}の(11)$\sim$ (14)のグループ)に対しては, 動詞$x$と副詞$y$の組を使
用することで正解が得られた{\it pvn}テーブルを用いて文中における多義語
の解消を行なった.  総異なり数2,743,974組に対し相互情報量を計算し, 一定
の閾値(動詞と名詞, 及び動詞と副詞の共起頻度数の閾値を5, 相互情報量
の閾値を3)以上である動詞と名詞, 動詞と副詞の組を抽出した結果, それぞれ
6,768, 1,200の組を得た.

実験では14種類の多義語を用いた.  テスト文として, 各々の多義語に対しラ
ンダムに100文, 総計1,400文を抽出し, これらから{\it delexical usage}, 
イディオム, メタファ, 多義語の意味が曖昧で人間が一意に決定できないものを除く
1,226文を対象とし実験を行なった.

\subsection{曖昧性解消実験の結果}

実験で用いた動詞グループと実験結果を 表\ref{disam} に示す.

\vspace*{-0.6cm}
{\footnotesize
\begin{table}[htbp]
\begin{center}
\caption{曖昧性解消実験の結果} \label{disam}
\vspace*{-0.2cm}
Table 2 The results of Disambiguation Experiment \\
\begin{tabular}{c||c|c} 
\multicolumn{3}{l}{\bf{(1) \{ \underline{close}, open, end \}}} \\
\hline \hline
Procedures  &disambiguated &correct (\%)  \\ \hline \hline
$within$ $the$ $pvn$ $table$ &\hspace*{1mm}34  &26 (26/99 = 26.2 ) \\ \hline
$without$ $the$ $pvn$ $table$ &65  &31 (31/99 = 31.3 ) \\ \hline
{\bf Total} &99  &57 (57/99 = 57.5 ) \\ \hline
\multicolumn{3}{c}{} \\
\multicolumn{3}{l}{\bf{(2) \{ \underline{take}, spend, buy, obtain
\}}} \\ \hline \hline
Procedures &disambiguated &correct (\%) \\ \hline \hline
$within$ $the$ $pvn$ $table$  &\hspace*{2mm}6  &5 (5/42 = 11.9) \\ \hline
$without$ $the$ $pvn$ $table$  &36 &28 (28/42 = 66.6)  \\ \hline
{\bf Total}  &42 &33 (33/42 = 78.5)  \\ \hline
\multicolumn{3}{c}{} \\
\multicolumn{3}{l}{\bf{(3) \{ \underline{lose}, win, miss \}}} \\
\hline \hline
Procedures  &disambiguated &correct (\%) \\ \hline \hline
$within$ $the$ $pvn$ $table$  &\hspace*{1mm}51  &41 (41/97 = 42.2) \\ \hline
$without$ $the$ $pvn$ $table$  &46 &36 (36/97 = 37.1)  \\ \hline
{\bf Total} &97  &77 (77/97 = 79.3)  \\ \hline
\multicolumn{3}{c}{} \\
\multicolumn{3}{l}{\bf{(4) \{ \underline{get}, receive, gain \}}} \\
\hline \hline
Procedures  &disambiguated &correct (\%)  \\ \hline \hline
$within$ $the$ $pvn$ $table$   &\hspace*{1mm}30  &25 (25/83 = 30.1) \\ \hline
$without$ $the$ $pvn$ $table$  &53 &24 (24/83 = 28.9) \\ \hline
{\bf Total} &83 &49 (49/83 = 59.0) \\ \hline
\multicolumn{3}{c}{} \\
\multicolumn{3}{l}{\bf{(5) \{ \underline{give}, provide, impose \}}}
\\ \hline \hline
Procedures &disambiguated &correct (\%) \\ \hline \hline
$within$ $the$ $pvn$ $table$  &\hspace*{1mm}33  &31 (31/85 = 36.4) \\ \hline
$without$ $the$ $pvn$ $table$  &52 &26 (26/85 = 30.6)  \\ \hline
{\bf Total}  &85 &57 (57/85 = 67.0)  \\ \hline
\multicolumn{3}{c}{} \\
\multicolumn{3}{l}{\bf{(6) \{ \underline{make}, earn, build \}}} \\
\hline \hline
Procedures  &disambiguated &correct (\%)  \\ \hline \hline
$within$ $the$ $pvn$ $table$ &\hspace*{1mm}65  &63 (63/93 = 67.7) \\ \hline
$without$ $the$ $pvn$ $table$  &28  &15 (15/93 = 16.1) \\ \hline
{\bf Total} &93 &78 (78/93 = 83.8) \\ \hline
\end{tabular}
\end{center}
\end{table}
}


{\footnotesize
\begin{table}[htbp]
\begin{center}
\begin{tabular}{c||c|c} 
\multicolumn{3}{l}{\bf{(7) \{ \underline{bring}, take, cause \}}} \\
\hline \hline
Procedures &disambiguated &correct (\%)  \\ \hline \hline
$within$ $the$ $pvn$ $table$ &\hspace*{1mm}26  &24 (24/83 = 28.9) \\ \hline
$without$ $the$ $pvn$ $table$ &57 &49 (49/83 = 59.0) \\ \hline
{\bf Total} &83  &73 (73/83 = 87.9)  \\ \hline
\multicolumn{3}{c}{} \\
\multicolumn{3}{l}{\bf{(8) \{ \underline{leave}, go, receive \}}} \\
\hline \hline
Procedures  &disambiguated &correct (\%) \\ \hline \hline
$within$ $the$ $pvn$ $table$  &\hspace*{1mm}26  &25 (25/93 = 26.8) \\ \hline
$without$ $the$ $pvn$ $table$ &67 &48 (48/93 = 51.6) \\ \hline
{\bf Total} &93 &73 (73/93 = 78.4) \\ \hline
\multicolumn{3}{c}{} \\
\multicolumn{3}{l}{\bf{(9) \{ \underline{run}, operate, move \}}} \\
\hline \hline
Procedures  &disambiguated &correct (\%) \\ \hline \hline
$within$ $the$ $pvn$ $table$  &\hspace*{1mm}57  &56 (56/97 = 57.7) \\ \hline
$without$ $the$ $pvn$ $table$  &40 &19 (19/97 = 17.5) \\ \hline
{\bf Total} &97  &73 (73/97 = 75.2)  \\ \hline
\multicolumn{3}{c}{} \\
\multicolumn{3}{l}{\bf{(10) \{ \underline{set}, fix, put \}}} \\
\hline \hline
Procedures  &disambiguated &correct (\%)  \\ \hline \hline
$within$ $the$ $pvn$ $table$ &\hspace*{1mm}58  &51 (51/91 = 56.0) \\ \hline
$without$ $the$ $pvn$ $table$ &33 &21 (21/91 = 23.0) \\ \hline
{\bf Total} &91  &72 (72/91 = 79.1) \\ \hline
\multicolumn{3}{c}{} \\
\multicolumn{3}{l}{\bf{(11) \{ \underline{see}, look, know \}}} \\
\hline \hline
Procedures &disambiguated &correct (\%)  \\ \hline \hline
$within$ $the$ $pvn$ $table$  &53 &41 (41/100 = 41.0) \\ \hline
$without$ $the$ $pvn$ $table$  &47 &9 (9/100 = 9.0) \\ \hline
{\bf Total} &100  &50 (50/100 = 50.0) \\ \hline
\multicolumn{3}{c}{} \\
\multicolumn{3}{l}{\bf{(12) \{ \underline{come}, go, become \}}} \\
\hline \hline
Procedures &disambiguated &correct (\%)  \\ \hline \hline
$within$ $the$ $pvn$ $table$  &44  &41 (41/74 = 55.4) \\ \hline
$without$ $the$ $pvn$ $table$  &30 &7 (7/74 = 9.4) \\ \hline
{\bf Total} &74 &48 (48/74 = 64.8) \\ \hline
\multicolumn{3}{c}{} \\
\multicolumn{3}{l}{\bf{(13) \{ \underline{find}, receive, see \}}} \\
\hline \hline
Procedures &disambiguated &correct (\%) \\ \hline \hline
$within$ $the$ $pvn$ $table$ &60  &50 (50/92 = 54.3) \\ \hline
$without$ $the$ $pvn$ $table$ &32 &10 (10/92 = 10.8) \\ \hline
{\bf Total} &92 &60 (60/92 =65.2) \\ \hline
\end{tabular}
\end{center}
\end{table}
}

\clearpage
{\footnotesize
\begin{table}[htbp]
\begin{center}
\begin{tabular}{c||c|c} 
\multicolumn{3}{l}{\bf{(14) \{ \underline{leave}, retire, remain \}}}
\\ \hline \hline
Procedures &disambiguated &correct (\%)  \\ \hline \hline
$within$ $the$ $pvn$ $table$  &63 &60 (60/96 = 65.6) \\ \hline
$without$ $the$ $pvn$ $table$  &33 &12 (12/96 = 12.5) \\ \hline
{\bf Total} &96 &72 (72/96 = 75.0) \\ \hline
\end{tabular}
\end{center}
\end{table}
}

\noindent
表\ref{disam}で示される動詞グループにおいて, 多義語は下線で示されてい
る.  `Procedures' の`{\it within the pvn table}'と`{\it without
the pvn table}' はそれぞれ4章で示した決定法における{\bf 1}と
{\bf 2} を示す.  `disambiguated' は各 
`Procedures' で正しく判定できる文数を示す.  `correct'は実際に正しく判定
できた文数を示す.

\subsection{他手法との比較}

本手法の有効性を検証するため, 丹羽らの提案した文脈ベクトルを用いた名詞
の多義解消手法を動詞に適用した結果と, 本手法とを比較する.  先ず, 
丹羽らの手法の概略を示し, 次に比較実験の結果を示す.


\begin{enumerate}

\item 単語 $w$の文脈

$C$: $\cdots$,$w_{-N}$, $\cdots$, $w_{-1}$, $w$, $w_{1}$, $\cdots$, $w_{N'}$,$\cdots$,

に対する文脈ベクトル$V(C)$を

\begin{eqnarray}
V(C) & = & \sum_{i=-N}^{N'} V(w_{i}) \nonumber 
\end{eqnarray}

と定義する. ここで, $V(w_{i})$ は,

\[ V(w_{i}) = \left( \begin{array}{c}
		   I(w_{i},O_{1}) \\
		   I(w_{i},O_{2}) \\
		   \vdots \\
		   I(w_{i},O_{m}) \\
		   \end{array} \right) \]

で示される.  $I(x,y)$ は$x$ と$y$ の相互情報量であり, 基準単語と呼ばれ
る $O_{1}$, $\cdots$, $O_{m}$ は, ACL CD-ROM所収の Collins English
Dictionary の語義文における頻度をカウントし, 最上位50単語を除いて抽出
した1000語を示す.


\item 二つの文脈ベクトルの類似度は正規化されたベクトルの内積で表す.

\begin{eqnarray}
sim(C_{1},C_{2}) & = & \frac{V(C_{1})}{\mid V(C_{1}) \mid} \frac{V(C_{2})}{\mid V(C_{2}) \mid}  \label{niwa1}
\end{eqnarray}

式(\ref{niwa1})において, $sim(C_{1}, C_{2})$ の値が大きいほど, 文脈 
$C_{1}$, $C_{2}$ は類似していることを示す.

\item 今, 単語$w$\hspace{-0.01mm}が複数の意味\hspace{-0.01mm}$s_{1}$,$s_{2}$,$\cdots$,$s_{m}$\hspace{-0.25mm}を持ち,  \hspace{-0.1mm}各
意味に対して次のような文脈例が与えられているとする. (各 $C_{ij}$ が文
脈例)

{
\begin{center}
\begin{tabular}{ccccc}
意味 &\multicolumn{4}{c}{文脈リスト} \\ \hline
$s_{1}$ &$C_{11}$ &$C_{12}$ &$\cdots$ &$C_{1n_{1}}$ \\
$\vdots$ &$\vdots$ &$\vdots$ &$\vdots$ &$\vdots$ \\
$s_{m}$ &$C_{m1}$ &$C_{m2}$ &$\cdots$ &$C_{mn_{m}}$ \\
\end{tabular}
\end{center}
}

この時, 任意の文脈$C$ における単語$w$の意味は, 類似度
$sim(C,C_{ij})$が最大となる文脈例$C_{ij}$を持つ意味$s_{i}$に決定される.

 
\end{enumerate}

丹羽らの手法を用いた実験では, $I(x,y)$ を求めるときに使用する$x$と
$y$のウィンドウサイズは前後50語とした.  多義語の各意味を示す文脈例とし
て {\it Wall Street Journal} から, 各意味ごとに 10例ずつ抽出し, 文脈リ
ストを作成した.  {\bf 文脈サイズ}は, 5語と10語を用いた. ここで, 例え
ば文脈サイズが \hspace{-0.05mm}10\hspace{-0.05mm}であるとは, \hspace{-0.05mm}多義性を解消しようとする語の前後10語
を文脈として用いたことを意味する.  実験結果を表
\ref{poly_result1}に示す.




{
\begin{table}[htbp]
\begin{center}
\caption{比較実験の結果} \label{poly_result1}
Table 3 The results of comparative experiment \\
\vspace*{2mm}
\begin{tabular}{r|l|c||c|c|c} \hline \hline
\raisebox{-1.5ex}{Num} &\raisebox{-1.5ex}{Word} &\raisebox{-1.5ex}{Sentence} &\raisebox{-1.5ex}{\it Hypo. Verb} &\multicolumn{2}{c}{Co-occurrence vector} \\ \cline{5-6}
 & & & &{\bf 10} &{\bf 5} \\ \hline 
(1) &close &99 &57 &39 &42 \\ \hline
(2) &take &42 &33 &30 &30 \\ \hline
(3) &lose &97 &77 &67 &75 \\ \hline
(4) &get &83 &49 &48 &48 \\ \hline
(5) &give &85 &57 &67 &71 \\ \hline
(6) &make &93 &78 &52 &49 \\ \hline
(7) &bring &83 &73 &64 &56 \\ \hline
(8) &leave &93 &73 &32 &37 \\ \hline
(9) &run &97 &73 &66 &80 \\ \hline
(10) &set &91 &72 &61 &58 \\ \hline
(11) &see &100 &50 &43 &46 \\ \hline
(12) &come &74 &48 &48 &52 \\ \hline
(13) &find &92 &60 &50 &60 \\ \hline
(14) &leave &96 &72 &70 &65 \\ \hline
\multicolumn{2}{c|}{Total} &1,226 &872(71.1\%) &730(60.0\%) &712(62.7\%) \\ \hline
\end{tabular}
\end{center}
\end{table}
}

\noindent
表\ref{poly_result1} において, `Num' は表\ref{disam} で示した動詞グルー
プの各番号を表す.  `Word' は動詞グループに含まれる多義語を示し,
`Sentence' はテスト文の総数を示す.  `{\it Hypo. Verb}' は本
手法による正解数を示し, `Co-occurrence vector' は丹羽の提案した手法を
用いた実験結果の正解数を示す. `Co-occurrence verctor' における {\bf
10}, {\bf 5} は文脈サイズを示す.


\section{考察}
\subsection{曖昧性解消実験}

表\ref{disam}によると, 4章で示した決定法の{\bf 2} は解消に重要な役割を
果たし, 名詞間に意味的な近さを示す尺度を導入する必要があることを示して
いる.  また総正解数は, 総数1,226文のうち872文であり正解率が71.1\%に達
していること, 特に`{\it within the pvn table}'の正解は, 総数606文のう
ち539文であり, 正解率が88.9\%に達していることから, クラスタリングの結
果得られた情報が有効であることを示す.

{\bf 1} と{\bf 2}における正解率を比較すると, 全ての動詞のグループに対し,
{\bf 2} の方が正解率が低かった.  例えば, (1) の動詞グループ
\{{\bf \underline{close},open,end}\}において, {\bf 1} である `$within$
$the$ $pvn$' における正解率が76.4\% (26/34 = 76.4)であるのに対し, {\bf
2} である`$without$ $the$ $pvn$' における正解率は47.6\% (31/65 =
47.6\%)であった.  式(\ref{co})中の$Dis(x,n)$を用いて偏差を計算した結果
例を表
\ref{noun}に示す.

{\footnotesize
\begin{table}[htbp]
\begin{center}
\caption{2語間の偏差の値} \label{noun}
Table 4 Semantic dissimilarity of two nouns \\
\begin{tabular}{rlc|rlc|rlc} \hline \hline
No. &$n$ &{\footnotesize (month,$n$)} &No. &$n$ &{\footnotesize
(loss,$n$)} &No. &$n$ &{\footnotesize (stake,$n$)}
\\ \hline
1. &Monday    &0.542  &1.  &profit   &0.410  &1. &equity   &0.427 \\
2. &week      &0.563  &2.  &earnings &0.461  &2. &interest &0.468 \\
3. &August    &0.578  &3.  &income   &0.462  &3. &cash     &0.531 \\
4. &end       &0.586  &4.  &net      &0.479  &4. &shares   &0.547 \\
5. &Tuesday   &0.589  &5.  &gain     &0.487  &5. &amount   &0.560 \\
6. &agreement &0.593  &6.  &result   &0.492  &6. &asset
&0.582 \\ 
7. &Wednesday &0.603  &7.  &decline  &0.528  &7. &value    &0.592 \\
8. &yesterday &0.614  &8.  &revenue  &0.578  &8. &option
&0.595 \\ 
9. &office    &0.626  &9.  &cent     &0.605  &9. &stock    &0.628 \\
10. &year     &0.637  &10. &increase &0.620  &10. &dividend &0.634 \\ \hline
 &     &\multicolumn{1}{r}{}  & & &\multicolumn{1}{r}{}  & & & \\
\hline \hline
No. &$n$ &{\footnotesize (profit,$n$)} &No. &$n$ &{\footnotesize
(loan,$n$)} &No. &$n$ &{\footnotesize (money,$n$)} \\ \hline
1. &earnings    &0.335 &1. &tax         &0.543 &1.  &cash &0.611 \\
2. &result      &0.405 &2. &use         &0.563 &2.  &tax &0.616 \\
3. &loss        &0.410 &3. &debt        &0.571 &3.  &control &0.637 \\
4. &income      &0.492 &4. &computer    &0.586 &4.  &dollar &0.654 \\
5. &revenue     &0.496 &5. &payment     &0.587 &5.  &power &0.657 \\
6. &decline     &0.540 &6. &investment  &0.589 &6.  &position &0.659 \\
7. &gains       &0.565 &7. &shareholder &0.598 &7.  &time &0.661 \\
8. &growth      &0.566 &8. &proposal    &0.606 &8.  &drug &0.663 \\
9. &operating   &0.571 &9. &fund        &0.613 &9.  &lot &0.666 \\
10. &net        &0.580 &10. &benefit    &0.628 &10. &loan &0.674 \\ \hline
\end{tabular}
\end{center}
\end{table}
}

\noindent
表\ref{noun}は, `month', `loss', `stake', `profit', `loan', `money'と
の偏差が少ない語をそれぞれ上位10語抽出した結果を示す.  数値は偏差の値
を示す.  表\ref{noun}によると, $Dis(x,n)$によりほぼ意味的に近いものを
抽出できていることがわかる.  このことから, 式(\ref{co})中の$Dis(x,n)$
は, 妥当であると言える.  {\bf 2}における正解率が{\bf 1}よりも低かった
原因として, 積関数である式(\ref{co})が考えられる.  すなわち, {\bf 2} 
において, 文中に現れる名詞$x$が$pvn$ に存在しない場合, $pvn$ に示され
る名詞の要素一つ一つに対して, 式(\ref{co})を適用し, $x$ との意味的な関
係を求めた.  しかし多義語の各意味は, 名詞の部分集合全体で特徴づけられ
ていることから, 文中における名詞$x$と部分集合全体との偏差を考慮に入れるよ
う式(\ref{co})を改良する必要がある.

\subsection{他手法との比較}

表\ref{poly_result1}の結果から, 本手法の正解率が 71.1\%であるのに対し, 
丹羽らの提案した手法は, 62.7\% (文脈サイズ5)であることから, 本手法の方が
良い正解率が得られることがわかる.  一般にある文章の話題抽出には名詞が
用いられていることから, 名詞同士の意味的な関係は広いウィンドウサイズが
適切である.  一方, 動詞と名詞の意味的な関係は比較的狭いウィンドウサイ
ズを用いた方が(動詞と目的語という観点から)顕著に現れる.  このことは, 
表
\ref{poly_result1} においてウィンドウサイズが{\bf 10}のときよりも{\bf
5}の方が良い結果が得られていることからも明らかである.  ところがウィン
ドウサイズを狭くとるとデータスパースネスの問題が生じる.  すなわち丹羽
らの手法では文脈中の各単語と基準単語との相互情報量の値を用いてベクトル
の内積を計算しているが, ウィンドウサイズを狭くとると基準単語と共起する
単語数が相対的に減少する.  その結果, 内積がゼロになり類似度が計算でき
ない場合が生じた.  さらに丹羽らの手法では, 基準単語は Collins English
Dictionary の語義文における頻度をカウントし, 最上位50単語を除いて1000
語を抽出しこれを用いている.  しかし, これは辞書から得られた一般的な情
報であり, {\it Wall Street Journal} のような分野依存のコーパスにおいて
同様に高頻度に現れるとは限らない.  実際, Collins English Dictionary の
見出し語約6万2千語の内, 少なくとも{\it Wall Street Journal} に一回以上
出現した単語は約半数であり, 単語と基準単語1000語との総組数のうち, 実際
に一回以上共起したのは約15.8\%であった.  丹羽らの手法において, このこ
とが本手法よりも高い正解率が得られなかった要因と考えられる.

\section{むすび}

本稿では, コーパスから抽出した動詞の語義情報を利用し, 文中に含まれる多
義語の曖昧性を解消する手法を提案した.  本手法の基本的なアイデアは, 表
層上は一つの要素である多義語動詞を, 多義が持つ各意味がまとまった複数要
素であると捉え, これを一つ一つの意味に対応させた要素に分解した上でクラ
スタを作成すれば, 多義を判定しながら意味的なクラスタリングが行なえると
いうことである.  本手法の有効性を検証するため, 丹羽らの提案した単語ベ
クトルを用いた多義語の解消手法と比較した結果, 14種類の多義語動詞を含む
1,226文に対し, 丹羽らの手法が平均62.7\%の正解率に対し, 本手法では,
71.1\%の正解率を得た.

本手法では動詞の多義を判定するため, 動詞を $n$ 次元 ($n$ は名詞の個数) 
名詞空間で, ベクトルとして表現した.  しかし, 名詞にも多義性があること
を考慮していない.  軸となる名詞の多義性をどのように扱うかは今後の課題
である.



\bibliographystyle{jnlpbbl}
\bibliography{main}
\begin{biography}
\biotitle{略歴}
\bioauthor{福本 文代}{
1986年学習院大学理学部数学科卒業.  同年沖電気工業
(株)入社. 総合システム研究所勤務.  1988年より1992年まで(財)新
世代コンピュータ技術開発機構へ出向.  1993年マンチェスター工科大学
計算言語学部修士課程終了.  同大学客員研究員を経て1994年より山梨大学
工学部助手, 現在に至る.  自然言語処理の研究に従事.  情報処理学会, ACL
各会員.}

\bioauthor{辻井 潤一}{
1971年, 京都大学工学部電子工学科卒業, 1973年,
同大学院工学研究科修士過程修了. 同年, 京都大学工学部電気第二工学科助手,
同助教授を経て, 1988年英国UMIST (マンチェスタ理工科大学: University of
Manchester Institute of Science and Technology)教授. 同大学計算言語学
研究センタ (Centre for Computational Linguistics: CCL) 所長, および, 
言語工学科主任教授を経て, 1995年より東京大学大学院理学系研究科情報科学
専攻教授. 1981年から1982年まで, フランス・CNRS招待研究者として, グルノー
ブル大学自動翻訳研究所 (GETA) に滞在. 工学博士. 国際計算言語学委員
(ICCL)メンバー, 1996年, 国際計算言語学会(Coling 96)プログラム委員長,
NATO機械翻訳プロジェクト (トルコ) 技術顧問など. 人工知能学会等会員.}
\bioreceived{受付}
\biorevised{再受付}
\bioaccepted{採録}
\end{biography}
\end{document}             



