\documentstyle[epsf,nlpbbl]{jnlp_e}
\input{emptyset.tex}

\setcounter{page}{3}
\setcounter{巻数}{4}
\setcounter{号数}{3}
\setcounter{年}{1997}
\setcounter{月}{7}
\受付{August}{1}{1996}
\再受付{October}{13}{1996}
\採録{December}{12}{1996}

\setcounter{secnumdepth}{2}

\title{}
\author{}
\jkeywords{}

\etitle{Generation in Machine Translation:
  the right place\\
  to choose between translation equivalents}

\eauthor{John D. Phillips\affiref{YamagutiUniv}}

\headauthor{John D. Phillips}
\headtitle{Generation \& Translation Equivalents}

\affilabel{YamagutiUniv}
          {Department of Linguistics, Yamaguchi University}
          {Department of Linguistics, Yamaguchi University}

\eabstract{
This paper looks at the problem of choosing between alternative lexical
translation equivalents in machine translation.
It argues that the knowledge base used for making choices between
alternative translations is part of the target language grammar and that
it should be applied and the translation chosen as part of generation of
the target language text.
This contrasts with previous work on the problem, which has assumed that
these choices should be made in analysis or transfer.
Various types of knowledge base could be used to make the choice between
alternatives. The method outlined here uses information about
stereotypical contexts of use of words, stored as part of an ontological
network.
}

\ekeywords{machine translation, lexical choice, DRT, knowledge representation}

\newcommand{\lsb}{}
\newcommand{\rsb}{}


\begin{document}

\maketitle

\section{Introduction}

Lexical choice in machine translation divides into two rather separate parts,
the choice of what to say and the choice of how to say it.
The latter is the problem, common to most applications involving
text-generation, of choosing between alternative morphological and syntactic
ways of expressing the same literal meaning. Examples in English are the
choice between verb or nominalisation, active or passive clause,
adjective or relative clause. The lexical choice here is the choice
between alternative realisations of the same lexeme. The factors involved
in these choices are mostly poorly understood, and often the internal
representation of such alternations in a natural language processing
system will be the same at some level. A separate and prior problem is the
choice of what literal meaning to express.
In machine translation, what to say is specified by the source text, but there will be many possible interpretations of the source text.
As a consequence, in machine translation, this problem manifests itself
as the selection of one of many possible translation equivalents for each
source-language morpheme or non-compositional structure. It is to this latter
problem that this paper is addressed.

Usually, machine translation systems make the choice between lexical
translation
equivalents either by setting up target-language-dependent word senses in
the source language or interlingua and making the choice during analysis,
or else by putting constraints on transfer rules. This paper will argue that
both theoretically and practically the best place to make the
choice is in generation. The paper will go on to discuss methods of
making the choice and will present one method in detail.

\section{An example}

Most of the examples in this paper will concern translation from English
into Japanese.
This is because the work began in an English-Japanese machine
translation project, and because those parts of the system here
described which have been implemented use these two languages.
None of the examples depend on special properties of English or Japanese.

The most straightforward type of example to which this paper addresses
itself is one where a single source-language word has many possible
translations into the target language. The English word {\it paper\/}
for instance has a couple of dozen translations into Japanese in a
medium-sized dictionary. Some of them are

\begin{flushleft}
  \begin{tabular}{ll@{\hspace*{10mm}}ll}
    {\it kami\/} & the substance paper &
      {\it mondaiyousi\/} & an exam question paper \\[-2mm]
    {\it ronbun\/} & an academic paper or thesis &
      {\it touan\/} & an exam answer paper \\[-2mm]
    {\it sinbun\/} & a newspaper &
      {\it binsen\/} & letter-writing paper \\[-2mm]
    {\it sinbunsya\/} & a newspaper company &
      {\it kabegami\/} & wallpaper \\[-2mm]
    {\it syorui\/} & papers, documents &
      {\it sindansyo\/} & a doctor's paper \\[-2mm]
    & & & (medical certificate) \\
  \end{tabular}
\end{flushleft}

In most contexts, none of these Japanese translations are interchangeable.
Though an English speaker will very often call a newspaper a paper,
{\it kami\/}
is not used in this sense in Japanese, and neither
{\it kami\/}
nor
{\it sinbun\/}
can be used to refer to a newspaper company, for instance in
translating the question ``{\it Which paper do you work for?\/}''
Thus, though
{\it kami\/}
translates the most central meaning of
{\it paper},
it cannot be used as a general-purpose translation equivalent; the most
suitable translation equivalent must
be chosen on each occasion of use.

Looking  up the content words of a couple of pages of Japanese text
in a medium-sized Japanese-English dictionary showed that the dictionary
gave most words at least three translations, on average about seven
translations for each word. The same experiment
with German and English gave about six translations for each word on average,
showing that the problem is not confined to words with a
particularly wide semantic range, or to translation between culturally
distant languages.


\section{Current techniques}

Current commercial machine translation systems ignore the problem of
lexical choice by allowing only one translation to be specified for each
source-language content word.
Even where a system contains several dictionaries (e.g. for different subject
fields), the dictionaries simply mask each other so that only one translation
for each word will be available in the processing of any particular text.
This approach has obvious limitations. It works reasonably well for a
technical text concerned with
a narrow subject area, because many words will be technical terms with only one
sensible translation in that subject area. Because most machine translation
to date has dealt with such texts, lexical choice has not been seen as a
central problem. However even in technical texts, many everyday words with
multiple possible translations are used and once the constraints on text
type are removed, lexical choice becomes the main problem for translation. 

Solutions to the problem of lexical choice have generally taken one of
two forms. An approach with a long history is that of selectional or
collocational restrictions (e.g. \cite{lehrberger:82:a}). In this approach, a
source-language word with many translations is treated as so many homographs.
The English word
{\it paper},
for instance, in translating into Japanese, would be treated as ambiguous
between paper$_1$ (= Japanese {\it kami}),
paper$_2$ (=
{\it ronbun\/}),
paper$_3$ (=
{\it sinbun\/}),
$\ldots$ , paper$_{10}$ (=
{\it sindansyo\/}).
In the English grammar then, predicates are syntactically restricted as
to the types of argument they are allowed to take. For instance in a
translation system intended for use with texts about conferences,
verbs such as
{\it submit},
{\it present},
and
{\it publish\/}
would have their syntactic objects restricted to paper$_2$ (amongst other
things), so that the only syntactically acceptable analysis of a verb
phrase such as {\it submit a paper\/} will contain paper$_2$.
`{\it A4 paper\/}' though would be analysed as containing paper$_1$ due to
selectional restrictions on
{\it A4},
and `{\it travel papers\/}' would be analysed as containing paper$_5$
due to selectional restrictions on
{\it travel\/}.
Generation of the correct translation equivalent is then trivial since each
predicate of the source-language analysis corresponds to a single
target-language lexeme.

This method is an improvement on having just a single translation
available, presumably just
{\it ronbun\/}
in the example, but there are both theoretical and practical objections to
it. A major practical objection is that the source-language
grammar and lexicon must be different for each target language and for each
direction of translation: there can be no multilingual or reversible
translation.
It is also difficult to make selectional restrictions work in practice:
the grammar rules must make them void in some contexts, e.g. some negatives and
intensional contexts, and allow them to be overridden in metaphors etc.
It is often difficult to decide what the appropriate restrictions are
--- in the 
{\it paper\/}
example above, there is no reason why someone should not publish an
examination paper or submit it to a conference, but if selectional
restrictions allow this collocation, they will not produce a unique
analysis of the example.
From a theoretical point of view, this type of analysis of a source
language, involving word senses
based on target-language lexemes, has no linguistic justification at all.
It might be argued that the word senses actually used are mostly intuitively
sensible ones anyway, and could be justified monolingually, but really this is
unlikely to be the case. 
\cite{jorgensen:90:a} argues that the division of
the range of meaning of a word into many discrete senses is psychologically
unfounded to start with: subjects asked to classify examples of the use
of words generally
divide them into only two or three senses, and the classification varies
from subject to subject, and from day to day with the same subject.

An interlingua can be seen as a multilingual extension of this approach
using homographs. An 
interlingua will contain enough predicates to represent all possible
lexical translation equivalences in the languages of the world. But apart
from the difficulty of designing an adequate interlingua, there is the
problem then that any analysis of a source language in order to translate
into a target language will be massively redundant --- senses will be
distinguished in the interlingual representation of the source language
text which are distinguished in other languages but irrelevant for
translation into the target language.

An alternative to stating selectional restrictions in the source lexicon is
to state the same information in the form of conditions on transfer rules.
\cite{pentheroudakis:90:a} presents a recent example.
One of Pentheroudakis'
examples is the following pair of rules for translating the English verb
{\it draft\/} into Korean:
\begin{flushleft}
  \quad
  \small
  \begin{tabular}{l}
    draft $\Leftrightarrow$ cingcipha- {\it iff the patient is human\/}
    (e.g. `draft a secretary to another department')\\
    draft $\Leftrightarrow$ koanha- {\it iff the patient is abstract\/}
    (e.g. `draft a letter')\\
  \end{tabular}
\end{flushleft}
Though the source-language grammar and lexicon are here freed from dependence
on the target-language lexicon, the knowledge base for lexical choice must
still be reformulated for each pair of languages in each direction of
translation: the information still seems to be used in the wrong way.

\section{Lexical choice in generation}

The knowledge used in lexical choice is in reality part of the target
language. In Pentheroudakis' example above, the type of object usual with
the Korean verbs
{\it cingcipha-\/}
and
{\it koanha-\/}
is a fact about Korean, not about the relationship between Korean and English
(which is how Pentheroudakis treats it) or about English (which is how it
would be treated in the homograph approach). Intuitively, the knowledge
base for
lexical choice belongs in the description of the target language, in that
the knowledge is about the semantic relationships between target-language
words. Of course there is also a practical advantage to making this
knowledge part of the description of the target language: it need only be
stated once regardless of the number of languages and directions of
translation. Furthermore, the same knowledge base will be useful for
other types of natural language processing as well as in machine translation.

It follows that in machine translation, lexical choice must be part of
generation. Take a standard machine translation system consisting of three
parts, analysis, transfer, and generation. If word senses are not
distinguished in the source language, or
at least are distinguished only as far as can be justified on grounds
internal to the source language, the output of analysis will contain only
genuine source-language predicates. In transfer, each of these predicates will
presumably have many possible target-language equivalents, all of which must
be output. It will be the job of the generation component to apply the
target-language knowledge base to choose between the many possible
translations suggested by the transfer module. The same sequence of operations
will apply in the type of partly interlingual system argued
for by~\cite{eynde:92:a}
and partly implemented in Eurotra, where lexical items are treated by
transfer but grammatical structure and relations are treated interlingually.

The target-language knowledge base used in generation for lexical choice
could take a variety of forms.
Stochastic methods immediately spring to mind of course, either the purely
statistical language model described by~\cite{brown:90:a},
or the type of mixed linguistic and statistical data which could be
derived from linguistically analysed corpora
such as the Penn Treebank. Simple selectional restrictions stated in the
target-language grammar would also do the job. However, the method to be
outlined below is a preference mechanism similar to
that of~\cite{wilks:75:a},
but incorporating some more recent results from psycholinguistics and
formal logic.

First, an outline of the system architecture assumed in subsequent
discussion and examples. 

\section{A framework for machine translation}

\begin{figure}[htbp]
  \begin{center}
    \leavevmode
    \atari(141,68)

    \caption{The proposed framework}
  \end{center}
\end{figure}

The framework assumed here for machine translation is diagrammed in Figure 1.
The double lines between
{\it parsing\/}
and
{\it disambiguation},
and between
{\it transfer\/}
and
{\it lexical choice},
represent possible multiple outputs.
{\it LF\/}
is the indexed logic described below, also used for the knowledge bases.
The module with which this paper is concerned is
{\it lexical choice}.
The task of the
{\it lexical choice\/}
module is to produce from the output of
{\it transfer\/}
(see \cite{phillips:93:c,phillips:93:b})
a single, non-disjunctive logical form from which a target-language
sentence can be straightforwardly generated using the algorithm
described by~\cite{phillips:93:a} and improved by ~\cite{lager:94:a}.

For purposes of exposition here, a simple logical formalism is used,
representing
a sentence as a network of semantic dependencies. The basic units of the
formalism are
{\it indices},
{\it properties},
and
{\it relations}.
A logical form consists of an unordered set of
{\it terms};
each term is either a property predicated of an index, or a relation between
two indices. The written notation depicts properties and relations as
unary and binary functors, respectively, and indices as their arguments,
i.e. within brackets. A logical form representing `John saw Mary' might
be
\begin{quote}
  \smallskip
  \small
  e : john(j) \& see(e) \& mary(m) \& tense(e,past) \&
  actor(e,j) \& patient(e,m)
\end{quote}
It can be seen that the concepts or entities mentioned in the sentence
are labelled with indices, and the semantic relationships specified in
the sentence to hold between them are shown by relations. Properties
show the natural language classes the English names of which were used
to refer to the entities in the sentence.
The indices will serve as discourse referents when the logical form is
incorporated into a discourse representation~\cite{kamp:81:a}.
The root node of the network is specified at the beginning of the
representation, in this case
{\it e},
the node representing the seeing event. In terms of discourse
representation theory,
{\it e\/}
is the discourse referent of which the logical form is a
description. Figure 2 depicts the same dependency network as a feature
structure.

\begin{figure}[htbp]
  \begin{center}
    \leavevmode
    \small
    e
    \begin{tabular}[b]{|@{\hspace*{-1.5mm}}clc@{\hspace*{-1.5mm}}|}
      \cline{1-1}\cline{3-3}
      & predicate : see &\\
      & tense : past & \\
      & actor : $_{\rm j}\,\!$[ predicate : john ] & \\
      & patient : $_{\rm m}\,\!$[ predicate : mary ] & \\
      \cline{1-1}\cline{3-3}
    \end{tabular}
    \caption{Feature-value representation of `John saw Mary'}
  \end{center}
\end{figure}

The advantages of using indexed logic for semantic representation have
been widely recognised at least since Kamp proposed its use for the
representation of discourse anaphora.
Kamp's paper dealt only with nominal discourse entities, but for an
adequate representation situational discourse entities, introduced by
verbs, adjectives and perhaps prepositions, are also needed --- the
case for this is well put by~\cite{parsons:90:a},
and from a different point of view by~\cite{asher:93:a}.

The logical form above is simplified, mainly in that the predicate
{\it tense\/}
here stands for what should be a complex representation of tense and
aspect, related anaphorically to the preceding discourse.
A similarly simplified representation of the Japanese translation of the
example, `Jon ga Mearii wo mita', might be
\begin{quote}
  \smallskip
  \small
  e : jon(j) \& kouisya(e,j) \& mearii(m) \&
  judousya(e,m) \& miru(e) \& tense(e,past)
\end{quote}
Again this is a description of an event
{\it e},
involving two other discourse entities,
{\it j\/}
and
{\it m},
described as
{\it jon\/}
and
{\it mearii\/}
respectively. The event
{\it e\/}
is a seeing event ({\it miru\/} is the Japanese verb);
{\it j\/}
stands in the
{\it kouisya\/}
(actor) relation to it, and
{\it m\/}
in the
{\it judousya\/}
(patient) relation.

What is required for machine translation is not an accurate
representation of `meaning', whatever that may be, but an unambiguous
representation of the entities involved in the discourse (objects,
events, etc.) and the relations between them. This is the purpose of the
representation we use here; it is important that the indices,
properties, and relations are justifiable in the language being
represented.

Properties in the formalism are simply the contentful (open-class,
lexical) morphemes of the language.
This follows from what was said above, though there can of course be no
objection to differentiating senses where there is justification in the
language being described.

Relations are a different matter, representing not morphemes but
grammatical structure.
The purpose of analysing a source text is to make explicit the semantic
relationships between its parts, so that a translation can be made which
specifies the same relationships (or translationally equivalent ones)
between the same discourse entities.
The relations of the formalism represent these semantic relationships,
in the sense that they instantiate demonstrable grammatical concepts of
the language being represented.
So the relations
{\it actor\/}
and
{\it patient\/}
in the example are not implication-based thematic roles of the type
rejected by~\cite{dowty:89:a} but are grammatical relationships
which can be justified on syntactic grounds, using such tests as literal
synonymy between sentences of different grammatical structure
(e.g. passive and active voice).

For representing ambiguity, disjunction of atoms is used. 
{\it \{see;saw\}{\rm (}e{\rm )}\/}
means that
{\it e\/}
is either a seeing event or a sawing event.
{\it with{\rm (}\{m;e\},t{\rm )}\/}
states a relation between
{\it t\/}
and either
{\it m\/}
or
{\it e\/}
(e.g. in the analysis of
{\it John saw Mary with a telescope}).
These disjunctions are purely abbreviatory, a way of representing
multiple outputs succinctly.

For transfer,
the English and Japanese logical forms shown above can be related
declaratively by rules such as
\begin{quote}
  \smallskip
  \small
  kouisya(X,Y) $\Leftrightarrow$ actor(X,Y)\\
  miru(E) $\Leftrightarrow$ see(E)
\end{quote}
The use of this type of rule reflects the basically compositional nature
of translation and works well for probably a large majority of the basic
structures and morphemes of any pair of languages.
With some translation examples, compositional translation initially
seems inadequate, but on closer inspection the analysis required on
language-internal grounds enables compositional translation at the level
of syntactic or semantic analysis, although the surface structures seem
different.
For instance there are many cases where arguments to a verb, absent in
one language, must be provided in another language.
Welsh, like many other languages, uses resumptive pronouns:
{\it the man I spoke to\/}
is
{\it y dyn y siaredais ag ef},
`the man that I spoke to him',
the preposition having an explicit object.
The Japanese sentence
{\it kesa atta\/}
might be translated
{\it I met him this morning},
although neither of the pronouns is explicit in the Japanese either
lexically or in the morphology of the verb.
It is uncontroversial that the absent noun-phrase in the Welsh case is
implicitly present and that the reference should be made explicit during
analysis; the same seems true in the Japanese case
(e.g. \cite{gunji:87:a} assumes a `zero pronoun' analysis
of this type of sentence without comment).
Once the referents are made explicit, translation becomes compositional.

There are though, as is well-known, as number of cases where a
compositional treatment is inadequate.
Some of these cases are just extensions of examples of
non-compositionality in monolingual analysis.
An adequate monolingual analysis, even though it is non-compositional,
will carry over into translation. Idioms, for instance, are essentially
non-compositional: in a monolingual analysis, an idiom must at some
level be treated as a single unit.
Likewise in translation, translation equivalence must be defined between
the idiom and an equivalent idiom or literal word or phrase in the other
language.
Rather similar are problems with the monolingual analysis of phrasal verbs
(e.g.
{\it look up\/}
in `look a word up in a dictionary'), and with support verbs (e.g.
{\it have\/}
in `have a bath').
Such cases must, for obvious semantic reasons, be treated as a unit in
monolingual analysis, and likewise translated as a unit.

However in other cases, there is no language-internal basis for anything
other than a compositional analysis, but the compositional analyses of
translationally-equivalent units correspond only partly between
languages:
in other words, these examples show a lack of compositionality
specifically in the process of translation and must be dealt with in
transfer.
\cite{lindop:91:a} have collected and classified a large number of
examples of what they call `complex transfer', i.e. non-compositional
translations.
Examples showing non-compositionality specifically in the process of
translation fall into a small number of discrete classes. To exemplify
here how such cases can be handled in our formalism, we look at three
possible German translations of the English verb
{\it like}:
the verbs
{\it m\"ogen\/}
and
{\it gefallen},
and the adverb
{\it gern}.

{\it M\"ogen\/}
is an unproblematic translation of
{\it like\/}:
we can translate
{\it I like Mary\/}
as
{\it ich mag Maria\/}
using the transfer rule
\begin{quote}
  \smallskip
  \small
  like(E) $\Leftrightarrow$ m\"ogen(E)
\end{quote}
In Figure 3, the relevant parts of the translation are lined up to show
the domains of application of separate transfer rules, i.e. five rules
are used.

{\renewcommand{\baselinestretch}{}
\begin{figure}[htbp]
  \begin{center}
    \small
    \leavevmode
    \begin{tabular}{rc@{\hspace*{-1.5mm}}ccc@{\hspace*{-1.5mm}}c@{~~}l}
      \cline{2-2}\cline{5-5}
      me(me)      ~& & \lsb & \rsb & & & ich(me) \\
        \cline{2-2}\cline{5-5}
      & & & & & & \\\cline{2-2}\cline{5-5}
      like(l)     ~& & \lsb & \rsb & & & m\"ogen(l) \\
        \cline{2-2}\cline{5-5}
      & & & & & & \\\cline{2-2}\cline{5-5}
      actor(l,me) ~& & \lsb & \rsb & & & actor(l,me) \\
        \cline{2-2}\cline{5-5}
      & & & & & & \\\cline{2-2}\cline{5-5}
      patient(l,m)~& & \lsb & \rsb & & & patient(l,m) \\
        \cline{2-2}\cline{5-5}
      & & & & & & \\\cline{2-2}\cline{5-5}
      mary(m)     ~& & \lsb & \rsb & & & maria(m)\\
        \cline{2-2}\cline{5-5}
    \end{tabular}

    \bigskip
    \caption{The translation of {\it like\/} as {\it m\"ogen\/}}
  \end{center}
\end{figure}
}

{\it Gefallen\/}
requires its arguments in a different order to those of
{\it like\/}:
\begin{quote}
  \smallskip
  \small
  like(L) \& actor(L,A) \& patient(L,P) $\Leftrightarrow$
  gefallen(L) \& actor(L,P) \& patient(L,A)
\end{quote}
In the translation
{\it Maria gef\"allt mir},
Mary is the syntactic subject and I am a dative object.
Again, Figure 4 shows the relevant parts of the logical forms aligned.
This time three transfer rules are used.

{\renewcommand{\baselinestretch}{}
\begin{figure}[htbp]
  \begin{center}
    \small
    \leavevmode
    \begin{tabular}{rc@{\hspace*{-1.5mm}}ccc@{\hspace*{-1.5mm}}c@{~~}l}
      \cline{2-2}\cline{5-5}
      me(me)      ~& & \lsb & \rsb & & & ich(me) \\
        \cline{2-2}\cline{5-5}
      & & & & & & \\\cline{2-2}\cline{5-5}
      like(l)     ~& & \lsb & \rsb & & & gefallen(l) \\
      & & \lsb & \rsb & & & \\
      actor(l,me) ~& & \lsb & \rsb & & & patient(l,me) \\
      & & \lsb & \rsb & & & \\
      patient(l,m)~& & \lsb & \rsb & & & actor(l,m) \\
        \cline{2-2}\cline{5-5}
      & & & & & & \\\cline{2-2}\cline{5-5}
      mary(m)     ~& & \lsb & \rsb & & & maria(m)\\
        \cline{2-2}\cline{5-5}
    \end{tabular}
    
    \bigskip
    \caption{The translation of {\it like\/} as {\it gefallen\/}}
  \end{center}
\end{figure}
}

Finally, if
{\it like\/}
is translated as
{\it gern},
an adverb, e.g. in
{\it ich schwimme gern},
`I like swimming', the
{\it actor\/}
of English
{\it like\/}
is the
{\it actor\/}
of
{\it like}'s
{\it patient\/}
in the German:
\begin{quote}
  \smallskip
  \small
  like(L) \& actor(L,A) $\Leftrightarrow$
  gern(L) \{ hs(L,E) \& patient(L,E) \}
\end{quote}
The forms in curly brackets \{~\} are conditions on the transfer, to be
verified relative to the discourse record when the transfer rule is
used, but not forming part of the discourse record.
{\it Hs},
mnemonic for `head-switch', is defined in such a way that the emboldened
indices in the alignment in Figure 5 are translated non-compositionally.

{\renewcommand{\baselinestretch}{}
\begin{figure}[htbp]
  \begin{center}
    \small
    \leavevmode
    \begin{tabular}{rc@{\hspace*{-1.5mm}}ccc@{\hspace*{-1.5mm}}cl}
      \cline{2-2}\cline{5-5}
      know        ~& & \lsb & \rsb & & & wissen(k) \\
        \cline{2-2}\cline{5-5}
      & & & & & & \\\cline{2-2}\cline{5-5}
      patient(k,{\bf l}) ~& & \lsb & \rsb & & & patient(k,{\bf s}) \\
        \cline{2-2}\cline{5-5}
      & & & & & & \\\cline{2-2}\cline{5-5}
      me(me)      ~& & \lsb & \rsb & & & ich(me) \\
        \cline{2-2}\cline{5-5}
      & & & & & & \\\cline{2-2}\cline{5-5}
      like(l)     ~& & \lsb & \rsb & & & gern(l) \\
      & & \lsb & \rsb & & & \\
      actor(l,me) ~& & \lsb & \rsb & & & \{ hs(l,s) \& patient(l,s) \}\\
        \cline{2-2}\cline{5-5}
      & & & & & & \\\cline{2-2}\cline{5-5}
      patient(l,s)~& & \lsb & \rsb & & & patient(l,s)\\
        \cline{2-2}\cline{5-5}
      & & & & & & \\\cline{2-2}\cline{5-5}
      swim(s)     ~& & \lsb & \rsb & & & schwimmen(s)\\
        \cline{2-2}\cline{5-5}
      & & & & & & \\\cline{2-2}\cline{5-5}
      actor(s,me) ~& & \lsb & \rsb & & & actor(s,me)\\
        \cline{2-2}\cline{5-5}
    \end{tabular}

    \bigskip
    \caption{The translation of to {\it know that I like swimming\/}
             as {\it wissen da\ss ~ich gern schwimme\/}}
  \end{center}
\end{figure}
}
The effect of
{\it hs\/}
is to shift the head of the embedding structure: the condition
{\it hs(E,F)\/}
requires that any relation of the form
{\it P(X,E)\/}
in the logical form to which the left-hand side of the transfer rule
relates, corresponds to
{\it P(X,F)\/}
in the logical form to which the right-hand side relates.
This is a development of the type of rule used by~\cite{watanabe:90:a}.

Detailed discussion and examples of the treatment of various types of
non-compositional translation in this formalism are presented by
\cite{phillips:93:c}, \cite{antona:93:a}, and \cite{phillips:93:b}.


\section{A method for choosing between alternative translations}

Given the above framework, the transfer component might be expected to
produce some output such as
\begin{quote}
  \smallskip
  \small
  $\ldots$ \{kami;ronbun;sinbun;sinbunsya;syorui;mondaiyousi;
  touan;binsen;kabegami;sindansyo\}(p)\\
  \& teisyutu(s) \&
  patient(s,p) \& recipient(s,j) \& zassi(j) \& $\ldots$
\end{quote}
when set to translate the English {\it I'd like to submit a paper to the
journal\/} into Japanese. ({\it Zassi\/} is `journal',
{\it teisyutu\/}
is `submit' in this sense --- of course in reality there would be other
possible translations of these words as well.) The problem is, how
to choose between the several translations of
{\it paper}.
The basic idea is that the translation including
{\it ronbun},
`an academic paper or thesis', describes the most stereotypical situation
in the context and is therefore the right translation. The correct
translation is the most stereotypical one.
Translation is equated with interpretation, and the correct
interpretation is the most stereotypical one.
Had the speaker or writer intended any other meaning, he would have had
to be more specific or his original English sentence would have been
misinterpreted by its recipient.

If ``stereotypical'' can be taken to be roughly the same as
``statistically commonest'' (which of course it is not) then a
stochastic algorithm can be used to choose the best translation based on
a language model derived from a statistical analysis of a large corpus
of Japanese text.
If the language model was based on a linguistically analysed corpus, it
would be applied to logical forms to choose the most likely combination
of relations between indices with particular properties.
That is, the language model would be based on probabilities of
particular relations between indices with particular properties.
Alternatively, a language model based on unanalysed corpora would be
used during generation of the target-language from the logical form to
choose the most likely target-language output.

However, ``stereotypical'' and ``common'' do not mean the same thing,
and the method to be outlined here uses a quite different idea of
stereotypicality.
Each of the source and target languages is provided with a lexical
knowledge base in which are entered stereotypical links between the
contentful morphemes of the language
--- i.e. the
{\it properties\/}
of the logical form.
For the example logical form above,
in the system's Japanese knowledge base is stated a stereotypical
connexion between the logical predicate
{\it zassi\/}
(`journal') and the predicate
{\it ronbun},
a connexion which does not exist for the other translations of
{\it paper}.
The discourse entity
{\it j\/}
is an instance of
{\it zassi\/}
and if
{\it p\/}
is an instance of
{\it ronbun\/}
the stereotypical link is verified via
{\it s},
an instance of
{\it teisyutu}
(`submit, present'). If
{\it p\/}
is an instance of any of the alternatives to
{\it ronbun},
the stereotypical link cannot be verified.

The justification for the above comes from Grice's work on the pragmatics of
linguistic communication.
\cite{grice:75:a} proposed several generalised
conversational implicatures, or maxims of conversation.
Grice's second Maxim of Quantity tells a speaker (or writer)
``Do not make your contribution more informative than is required.''
The hearer (or listener) will flesh out the text using knowledge of
stereotypical or default situations where it is underspecified.
\cite{levinson:87:a} has re-formulated the maxim as follows.

\begin{quote}
  \setlength{\baselineskip}{12pt}
  \smallskip

  \noindent
  Speaker's Maxim: the {\bf Maxim of Minimisation}\\
  `Say as little as necessary', i.e. produce the minimal linguistic
  information sufficient to achieve your communicational ends (bearing
  the first maxim in mind).
  \smallskip

  \noindent
  Recipient's Corollary: the {\bf Enrichment Rule}\\
  Amplify the informational content of the speaker's utterance, by
  finding the most {\sc specific} interpretation, up to what you judge
  to be the speaker's m-intended point.
  \smallskip
  \smallskip

  \noindent
  Specifically:\\
  (a) Assume that stereotypical relations obtain between referents or
  events, \\
  {\sc unless} (i) this is inconsistent with what is taken for
  granted, (ii) the speaker has broken the maxim of Minimisation by
  choosing a prolix expression.\\
  (b) $\ldots$ \\
  (c) $\ldots$

  \smallskip
\end{quote}

To Levinson's Enrichment Rule, it seems reasonable to add a direction
that words are used in stereotypical meanings. Caveat (i) of the rule
ensures that the context will be taken into account in deciding on any
actual interpretation, so the effect of the rule is that a word is
interpreted with a sense as stereotypical as the context will allow.
Hence if there are several possible interpretations (i.e., for our purposes,
translations) of a text, the most stereotypical is to be preferred.


\section{Knowledge base --- content}

It has often been observed that human translators need not be experts in
the subject matter of the texts they translate, though they do need a
certain amount of background knowledge.
This observation can be interpreted as meaning that a translator need
not acquire any deep understanding of a text to translate it, the
translator needs only knowledge of the way language is used in
discussing the text's subject area.
I myself have worked as a conference interpreter, and a large part of
the knack of the job initially is in learning not to understand and
absorb what is said consciously.
This claim, that a very shallow level of `understanding' of text is
sufficient for translation, is supported by the work of~\cite{gerloff:89:a}.
She studied professional translators at work and found them to translate
mechanistically by and large, using just local structures and local
context.
Her subjects used deliberate understanding and explicit reference to
larger units of text only when they experienced conscious difficulty.

Just this type of shallow processing for initial interpretation of text
is posited by the psycholinguistic theory of Naive Semantics
\cite{dahlgren:89:a,dahlgren:88:a}.
Part of the claim of Naive Semantics is that there exists
a discrete level of cognitive understanding of word meaning, corresponding
roughly to the idea behind selectional restrictions, but expressed in a
more general way as stereotypical relations between words. The following
quotation is from~\cite{dahlgren:89:a}:

\newpage
\begin{quote}
  \small
  The reader of a text actively constructs a rich picture of the objects,
  events, and situation described. The text is a vague, insufficient, and
  ambiguous indicator of the world that the writer intends to depict.
  The reader draws upon world knowledge to disambiguate and clarify the
  text, selecting the most plausible interpretation from among the
  infinitely many possible ones. In principle, any world knowledge
  whatsoever in the reader's mind can affect the choice of an
  interpretation. Is there a level of knowledge that is general and
  common to many speakers of a natural language? Can this level be the
  basis of an explanation of text interpretation? Can it be identified in
  a principled, projectable way? Can this level be represented for use in
  computational text understanding? We claim that there is such a level,
  called Naive Semantics, which is common-sense knowledge
  associated with words. Naive semantics identifies words with concepts,
  which vary in type. Nominal concepts are categorisations of objects
  based upon naive theories concerning the nature and typical description
  of conceptualised objects. Verbal concepts are naive theories of the
  implications of conceptualised events and states. Concepts are
  considered naive because they are not always objectively true.
\end{quote}
\vspace*{2mm}
It is claimed that there is a coherent, discrete level of speakers'
understanding of the meanings of words, and that it is this knowledge
which a reader draws upon to disambiguate and clarify a text and select
the most plausible interpretation from among the many possible ones.
While in the last resort any background knowledge can be made use of in
interpreting a text, this does not happen unless the naive semantics
fail to produce a coherent interpretation, in which case conscious
difficulty is experienced in understanding the text. The naive semantic
knowledge
is attached to individual lexical items and includes such things as
typical contexts of use,
attributes, preconditions, results, and ontological information.
The knowledge base originally comprised information elicited
experimentally from subjects.
An example (from \cite{dahlgren:89:a}, p. 154) is shown in Figure 6.

\begin{figure}[htbp]
  \begin{center}
    \leavevmode
    \small
    \begin{tabular}{ll}
      buy( \hspace*{2mm} &
      \begin{minipage}[t]{110mm}
        \{\\
        whatEnabled(can(afford(subj,obj))),\\
        how(with(X) \& money(X)),\\
        where(in(Y) \& store(Y)),\\
        cause(need(subj,obj)),\\
        whatHappenedNext(use(subj,obj))\\
        \},\\[2mm]
        \{\\
        goal(own(subj,obj)),\\
        consequenceOfEvent(own(subj,obj)),\\
        selectionalRestriction(sentient(subj)),\\
        implies(merchandise(obj))\\
        \})
      \end{minipage}
    \end{tabular}
    \bigskip

    \caption{Naive Semantic knowledge associated with `buy'}
  \end{center}
\end{figure}

Figure 6 (figure 10 of \cite{dahlgren:89:a}) shows the information
associated with
the verb {\it buy\/}. There are two lists of features, the first
giving typical properties of the word, the second inherent properties.
Dahlgren's implemented system has a total of forty-eight possible features,
of which about fifteen appear on a typical lexical item. Constraints which
naturally obtain in the generic knowledge mean that most features are
irrelevant to most words. For instance, only the features {\it cause,
goal, whatEnabled, whatHappenedNext, consequenceOfEvent, where,
when, implies, how, selectionalRestriction\/} can appear on verbs; the
features {\it sex\/} and {\it behaviour\/} are relevant only with animate
objects. The lexical frames fit into an ontological structure which
also specifies generic information on higher concepts (such as
{\it sentient\/} and {\it merchandise\/} in Figure 6) ---
this aspect of the system is described by~\cite{dahlgren:88:a}.
The amount of information required in the lexicon is
large, but not inordinately so: it is probably comparable in quantity
with the syntactic information required for reasonably full coverage of
a language.

As far as
this paper is concerned, the important claims of Naive Semantics are
(i) that such a database can in practice be compiled, (ii) that it
contains a manageably small amount of information, and yet (iii) that
it can represent all the information that is likely to be needed to
interpret a text (iv) using only the local context, i.e. the sentence
currently being processed.
Though the propounders of the theory apply it only to
the analysis of text, it obviously has wider applications in natural
language generation and in machine translation.
In particular, for machine translation, if we equate interpretation of a
text with translation (i.e. translation into Japanese rather than
mentalese), a Naive Semantic database should be able to make the choices
between alternative translations (i.e. interpretations) of lexical
items.


\section{Knowledge base --- representation}


\cite{thomason:92:a} present a formalisation of a monotonic
inheritance network with roles, where the roles are inheritable partial
functions from individuals to individuals.
Query answering in a network of this type corresponds to parsing a
fragment of the network with a context-free grammar whose non-terminals
are inference rules and whose terminals are links in the network.
Such a proof system is tractable and the resulting networks are easily
comprehensible, or `picture-able' as Thomason and Touretzky put it.
This type of network seems ideal for the representation of the
stereotypical knowledge needed for lexical choice.

Naive Semantics provides an ontology of concepts corresponding to
monolingually justifiable word senses.
The English word senses in the published literature on Naive Semantics
are in fact skewed towards particular technical applications and the
ontology could be much simplified for translation applications. Anyway,
the ontology can form the backbone of an inheritance network,
constructed by the
{\bf\small IS (=ISA)}
links of the network.
Typical and inherent properties of nodes in the network are represented
as roles and relations of various sorts.

Figure 7 shows the part of the network relevant to showing that journals
stereotypically have papers associated with them.

\begin{figure}[htbp]
  \begin{center}
    \leavevmode
    \atari(72,51)

    \caption{{\it `Journals have papers in them'\/}}
  \end{center}
\end{figure}

A distinction is drawn in the network between individuals (boxes) and
kinds (ellipses), and between independent (white) and dependent (shaded)
nodes, where a dependent node is one that is created by a role
attachment.
The ontology, then, consists of the white nodes only: shaded nodes are there
to show relations between kinds in the ontology.
Links in the network representing roles are marked with the name of the
role, crossed links represent identity (e.g. in Figure 10),
other links are {\bf\small IS\/} links.
Both inherent and stereotypical properties are represented by roles in
the network:
links representing inherent properties are solid, those representing
stereotypical properties are broken.
In Figure 7, {\it zassi\/} and {\it ronbun\/} are names of kinds,
{\it j\/} and {\it p\/} are individual entities,~
i.e. a specific journal and paper, and
{\it zr\/}
is a node showing a stereotypical relationship between
{\it zassi\/}
and
{\it ronbun\/}
({\it zr\/} is mnemonic for `{\it zassi\/}'s {\it ronbun\/}').

Although the network contains representations of both inherent and
stereotypical
properties, it remains monotonic and queries can be answered efficiently by
context-free parsing.
This is because the difference between the two types of role ---
inherent and stereotypical --- is used only to divide the network into
two parts which can be queried separately or together.
Queries can be of two types: `does the whole network support this fact?'
and `does the network excluding stereotypical links support this fact?'
In the first case, the query is a question about whether something is
stereotypically so,
and is answered by parsing the network treating inherent and
stereotypical links as though they were identical. In the second case,
the query is a question about whether something is inherently so and is
answered by parsing the network excluding stereotypical links. In both
cases, inheritance is monotonic.
Stereotypical links are stereotypical because of the way the translation
system uses answers to queries involving them, not because of the way
they are treated in the network.

Looking at the relevant part of the logical form for the
{\it `submitting a paper to a journal'\/}
example,
\begin{quote}
  \smallskip
  \small
  $\ldots$
  \{kami;ronbun;sinbun;sinbunsya;syorui;mondaiyousi;touan;binsen;
  kabegami;sindansyo\}(p)\\
  \& teisyutu(s) \& patient(s,p) \& recipient(s,j) \& zassi(j) \& $\ldots$
\end{quote}
it can be seen that we need to find some link between one of the
alternative translations of
{\it paper\/}
and some other predicate in the logical form. From the piece of network
in figure 7, it can easily be shown that
{\it ronbun\/}
are typically associated with
{\it zassi},
or alternatively, that if
{\it j\/}
and
{\it p\/}
are attached into the network as shown in the figure, a stereotypical
link exists between them.
The proof of the former uses two context-free `network structure' rules,
Thomason \& Touretzky's rules 21 and 24~\cite{thomason:92:a}:
\begin{quote}
  \smallskip
  \small
  \begin{tabular}{ll@{~~}c@{~~}l}
    21: & $\theta$(fF,x,G,y) & $\Rightarrow$ &
          x $\stackrel{\rm f}{\to}$ z ~
          $\theta$(F,z,G,y) \\
    24: & IS(\myemptyset,x,\myemptyset,p) & $\Rightarrow$ &
          x $\to$ p\\
  \end{tabular}
  \smallskip
\end{quote}

In the rule notation, `phrasal' categories are written with a link type (
{\it IS\/}
in rule 24, $\theta$, a variable ranging over non-role link types in 21)
and four arguments: a sequence of role links, the node at the beginning
of that sequence, another sequence of role links, the node at the
beginning of that sequence. `Lexical'
categories are actual links in the network
\\(e.g. zassi $\stackrel{\rm\small haspart}{\longrightarrow}$ zr
\ in Figure 7).
Variables F and G range over sequences of roles, f over roles, x, y, and
z over nodes, and p over kind nodes.
Hence rule 21 says that if you have an f-role link from x to z in the
network, and can proves some relation (i.e. $\theta$) between z's F and
y's G, then you have the same relation between x's fF and y's G.
In the case of the example, and referring to Figure 7, we have a
{\it haspart\/}
role link from
{\it zassi\/}
to
{\it zr},
and can prove an
{\rm\small IS}
relation between
{\it zr\/}
and
{\it ronbun},
therefore we have the same
{\rm\small IS}
relation between
{\it zassi\/}'s {\it haspart\/} and {\it ronbun}.
The relevant parse tree is shown in Figure 8.

\begin{figure}[htbp]
  \begin{center}
    \leavevmode
    \atari(51,44)

    \caption{Parse tree for `{\it zassi\/}'s {\it haspart\/}
             is a {\it ronbun\/}'}
  \end{center}
\end{figure}

Hence we can show that stereotypically there is a
{\it haspart\/}
relation between
{\it zassi\/}
and
{\it ronbun}.
Further, we can relate the
{\it haspart\/}
relation to a state of
{\it having-a-part},
a hyponym of
{\it have},
as shown in Figure 9.
In other words, if we know that stereotypically there is a
{\it haspart\/}
relation between
{\it zassi\/}
and
{\it ronbun},
we know that stereotypically {\it zassi\/} have {\it ronbun}.

\begin{figure}[htbp]
  \begin{center}
    \leavevmode
    \atari(66,54)

    \caption{Relating the {\it haspart\/} role to
             the state of {\it hav\/}ing}
  \end{center}
\end{figure}

From the piece of network in Figure 10, it can be shown that a similar
link is actually described by the example sentence.
The submitting event
{\it s},
being a type of giving event, has by inheritance a postcondition the
actor of which is the same as the recipient in
{\it s}.
In other words, as a consequence of submitting the paper to the journal,
the journal will have the paper.

\begin{figure}[htbp]
  \begin{center}
    \leavevmode
    \atari(129,57)
    
    \caption{The consequences of {\it submit\/}ting something}
  \end{center}
\end{figure}

Hence the choice of
{\it ronbun\/}
gives a logical form describing a more stereotypical situation than
would choosing one of the other possible translations which have no
stereotypical association with
{\it zassi}.

In fact of course, this example is oversimplified.
In reality the English word
{\it journal\/}
also has several possible translations, as does
{\it submit\/}.
It would of course be possible just to take each possible combination of
choices in turn and compare the number of stereotypical links found for
each.
Where there are more than two sets of choices, it will be more efficient
to use a modification of a constraint-satisfaction algorithm to iterate
through the possible combinations non-redundantly.
In the example, links would be found between
{\it zassi\/}
and
{\it ronbun},
and between
{\it teisyutu\/}
and those translations of
{\it paper\/}
which refer to privately produced documents, since
{\it teisyutu\/}
is most typically, though not invariably, used of handing in or
presenting a document one has prepared oneself.
The most stereotypical sentence would therefore be the one containing
{\it ronbun}, {\it zassi}, and {\it teisyutu}.


\section{Discourse representation}

Using the local context in determining word choice is appropriate only
for the first reference to a discourse entity.
On subsequent reference, the same translation will be used.
Imagine (12) as a reply to (11)
\begin{quote}
  \begin{itemize}
  \item[(11)] I'd like to submit a paper to the journal.
  \item[(12)] What is the paper about?
  \end{itemize}
\end{quote}
`The paper' in (12) refers to the paper mentioned in (11). There is
little context in (12) itself to help in deciding the translation of
`paper', but the fact of coreference between the papers in (11) and (12)
determines the ontological type, and therefore the translation, of
`paper' in (12).

To handle examples of this type, intersentential cohesive links
(anaphora, ellipsis, etc.) must be analysed and explicitly represented.
This can be done using techniques derived from discourse representation
theory (\cite{kamp:81:a}, \cite{asher:93:a} and much other work).
During translation, as analysis and generation progress, two
{\it discourse records\/} are built up, one each for the source and target
languages.
As each new logical index is used in the source-language analysis and
target-language generation, it is noted in the respective discourse
record along with its ontological type.
This will ensure that the translation of coreferring expressions is
consistent.
Of course the discourse record will contain other information as well:
information on topic structure and on the syntactic and thematic
structure of the immediately preceding clause and sentence will be
needed to handle anaphora and ellipsis in analysis and to produce
appropriate sentence types in generation.
The use of a discourse record allows translation to proceed
clause-by-clause or sentence-by-sentence, syntactic, semantic, and
pragmatic continuity between successive translation units being
maintained using the discourse record.

When (12) is translated in the context of (11), the index associated
with the `paper' will already appear in the target-language discourse
record, along with a note of its ontological type. The correct
translation as
{\it ronbun},
`academic paper', will therefore be the only possible one.
In fact, were the ontology implemented as sorted indices, this
translation would be forced automatically at the transfer stage, rather
than having to be checked explicitly subsequently, as part of lexical
choice.

Even when the local context suggests otherwise, an anaphoric noun phrase
will always have its translation determined by the discourse record.
In (13), if the definite noun phrase {\it the paper} is taken to be
anaphoric, say to an examination essay just discussed,
{\it touan\/}
`exam paper', rather than
{\it ronbun\/}
`academic paper', will be the correct translation despite the local
context.
\begin{quote}
  \begin{itemize}
  \item[(13)] I'd like to submit the paper to a journal.
  \end{itemize}
\end{quote}

In other examples, an anaphoric link may provide the local context for
choosing the translation of another word. In (14), the fact that, in the
Japanese translation, the reflex of `it' refers back to the
{\it ronbun\/}
`academic paper' will ensure that an appropriate translation of
{\it accept\/} is selected.
\begin{quote}
  \begin{itemize}
  \item[(14)] I have submitted a paper to the the journal.
    When will I know if it has been accepted?
  \end{itemize}
\end{quote}
{\it Accept\/} has a large number of translations into Japanese,
for example

\begin{flushleft}
  \small
  \quad
  \begin{tabular}{ll@{\hspace{10mm}}ll}
    {\it uketoru\/} & accept, take delivery &
      {\it kanju\/}   & accept (fate), submit tamely to \\
    {\it junou\/}   & accept (a gift) &
      {\it junnou\/}  & accept (a rule), conform \\
    {\it judaku\/}  & accept (a suggestion), agree &
      {\it saiyou\/}  & accept (an article or paper) \\
    {\it ukeireru\/}& accept, enlist, take in, enregister &
      {\it ukeru\/}   & accept, suffer, be given \\
    {\it mitomeru\/}& accept, approve, recognise &
      etc. & \\
  \end{tabular}
\end{flushleft}
Choice between many of the translations seems to be largely determined
by the ontological type of the patient --- here an academic paper.

Relational nouns can be seen as another relevant form of anaphora. (15)
shows that choice of the correct translation of the noun can depend on
the anaphoric link.
\begin{quote}
  \begin{itemize}
  \item[(15)] A car crashed. The driver was killed.
  \end{itemize}
\end{quote}
In (15), it is understood that the
{\it driver\/}
is the driver of the car mentioned in the first sentence. If (15) is
translated into Japanese, a choice must be made between half a dozen
translations of
{\it driver\/}:

\begin{flushleft}
  \small
  \quad
  \begin{tabular}{ll@{\hspace{10mm}}ll}
    {\it gyosya\/} & driver of a horse-drawn carriage &
      {\it usioi\/} & driver of cows \\
    {\it untensyu\/} & driver of a motor vehicle &
      {\it umakata\/}   & driver of horses \\
    {\it kikansyu\/} & driver of a train &
      {\it doraibaa\/} & driver of a computer peripheral \\
  \end{tabular}
\end{flushleft}
If the relational noun
{\it driver\/}
is given an analysis of the form {\it driver of it},
where {\it it\/} refers to the car, the translation follows naturally.

Ellipsis is relevant in examples such as (16).
\begin{quote}
  \begin{itemize}
  \item[(16)]
    Last night Mr.$\;$J. Smith gave a recital on the harpsichord.
    The harpsichord was a particularly beautiful antique instrument with
    paintings of children at play on the lid.
    Mr.$\;$Smith played for two hours $\ldots$
  \end{itemize}
\end{quote}
The translation of the first occurrence of
{\it play\/}
is dealt with as above. The translation of the second occurrence into Japanese should be
{\it hiku\/} `to play a stringed instrument', but could in general be

\begin{flushleft}
  \small
  \quad
  \begin{tabular}{ll@{\hspace{10mm}}ll}
    {\it fuku\/}   & play a wind instrument &
      {\it suru\/}   & play a sport \\
    {\it tataku\/} & play a percussive instrument &
      {\it enjiru\/} & play a role \\
    {\it asobu\/}  & amuse oneself &
      {\it kakeru\/} & play a record \\
    & & etc. \\
  \end{tabular}
\end{flushleft}
None of the mechanisms so far mentioned could provide a correct
translation in this case, since the local context gives no clues at all.
Indeed the previous sentence gives a misleading clue.
It may be that a correct analysis of the rhetorical structure of the
text could show that the second sentence was an aside, and that the use
of Mr.$\;$Smith's name in the third sentence marked a return to the main
subject, allowing a complex inference to relate
{\it play\/}
to
{\it harpsichord\/}
via Mr.$\;$Smith as a musician. A simpler way of looking at the relation
between {\it play\/}
and the first sentence, in the framework proposed here, is to see the
third sentence as containing an ellipsis:
``Mr.$\;$Smith played [the harpsichord] for two hours.''
If the lexical rule making an intransitive verb out of the basically
transitive verb {\it play\/}
marked it as having an elided object, the link with which would be
established during analysis, the correct translation could be obtained
in the normal way, using the local context and semantic network, though
in this case, as in (14),
the `local' context would be an anaphoric link to information in the
discourse record.


\section{Conclusion}

This paper has tried to do two things.
Firstly, it has tried to convince the reader that the best time to make
choices between alternative lexical translation equivalents is during or
immediately preceding target-language generation, not during analysis of
the source-language text or during transfer or by using an interlingua.
It has argued for this point of view on the theoretical grounds that
semantic relationships between target-language words are a matter for
the target-language grammar, not for source-language analysis rules or
for transfer rules, and on the practical grounds that making this
information part of the description of the target language means less
work when a machine translation system is built.

Secondly, the paper has tried to sketch a particular linguistically and
psycholinguistically based method of making lexical choices in
generation. Whether or not this method has been here adequately
justified is quite independent of whether or not lexical choice is best
done as part of generation. The basic approach is to choose the most
plausible of several possible target-language sentences in context, and
many other good methods exist for doing this.
In the short term, for the same amount of effort expended by system
builders, a stochastic approach, based on statistics gleaned from large
corpora, is likely to give better results.
A simple system of semantically based syntactic selectional restrictions
can also give good results with restricted text types.
In both these cases, no explicit choice need be made between alternative
translations, there need be no separate `lexical choice' module: the
sentence generator will either stochastically prefer one generated
sentence over the others, or it will fail to generate any sentences not
permitted by the selectional restrictions.
Neither of these methods can in the end take context fully into account.
Yet another possibility is a combination of methods: the method
described in this paper will sometimes fail to produce a unique
preferred translation, either because the source text is ambiguous or
because of deficiencies in the system's knowledge base, and in such
cases it would be convenient to choose between the presented
alternatives stochastically.



\bibliographystyle{nlpbbl}
\bibliography{epaper}

\begin{biography}

\biotitle{}

\bioauthor{John D. Phillips}
{
John Phillips graduated from the University of Wales in 1978.
After working first as a translator and interpreter then
as a computer programmer, he joined the staff of
Edinburgh University's Department of Artificial Intelligence
as a Research Associate in 1983,
where he worked on the Alvey NL Tools and speech recognition projects.
In 1988 he moved to the Seminar f\"ur Nat\"urliche-sprachliche
Systeme of T\"ubingen University,
and in 1990 to the University of Manchester Institute of Science \&
Technology's Centre for Computational Linguistics.
Since 1992 he has lived and worked in Japan, holding first
a JSPS Fellowship at the National Language Research Institute in Tokyo,
and currently an Associate Professorship at Yamaguchi University.
His current research interests include discourse representation
theories, and both linguistic and stochastic MT.
}

\bioreceived{Received}
\biorevised{Revised}
\bioaccepted{Accepted}

\end{biography}

\end{document}
