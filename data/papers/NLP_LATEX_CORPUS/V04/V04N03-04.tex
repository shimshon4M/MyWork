\documentstyle[jnlpbbl]{jnlp_j}

\setcounter{page}{71}
\setcounter{巻数}{4}
\setcounter{号数}{3}
\setcounter{年}{1997}
\setcounter{月}{7}
\受付{1996}{11}{21}
\再受付{1997}{1}{17}
\採録{1997}{3}{27}

\setcounter{secnumdepth}{2}

\title{確率的言語モデルに基づく多言語コーパスからの\\
言語系統樹の再構築}
\author{北 研二\affiref{TU}}

\headauthor{北 研二}
\headtitle{確率的言語モデルに基づく多言語コーパスからの言語系統樹の再構築}

\affilabel{TU}{徳島大学工学部 知能情報工学科}
{Faculty of Engineering, Tokushima University}

\jabstract{
本論文では，言語のクラスタリングに関する新しい手法を提案する．
提案する手法では，まず各言語の言語データから確率的言語モデルを
構築し，次に確率的言語モデルの間に導入した距離に基づき，
元の言語に対するクラスタリングを実行する．
本論文では，以上の手法を $N$-gram モデルの場合について
詳しく述べている．
また，提案した手法を用いて，ECI 多言語コーパス
(European Corpus Initiative Multilingual Corpus)中の 19 ヶ国語の
テキスト・データから，言語の系統樹を再構築する実験を行った．
本実験で得られた結果は，言語学で確立された言語系統樹と非常に
似ており，提案した手法の有効性を示すことができた．}

\jkeywords{言語モデル，$N$-gram モデル，多言語コーパス，
クラスタリング，言語系統樹}

\etitle{Reconstructing the Language Family Tree\\
from Multilingual Corpus Based on\\
Probabilistic Language Modeling}
\eauthor{Kenji Kita \affiref{TU}}

\eabstract{
This paper proposes a new method for automatically clustering languages.
The basic idea of this method involves
developing a probabilistic model for each language from the given linguistic data,
and then computing the distances between languages
according to the distance measure defined on the language models.
Clustering is performed based on this distance measure.
The paper embodies this idea
when the $N$-gram language model is concerned.
The effectiveness of the proposed method
has been confirmed by evaluation experiments
using multilingual texts of nineteen different languages
from the ECI Corpus (European Corpus Initiative Multilingual Corpus).
The results were very encouraging.
They were very close to the family tree of languages
established in linguistics.}

\ekeywords{language model, $N$-gram model, multilingual corpus,
clustering, language family tree}

\begin{document}
\maketitle



\section{はじめに}

近年，大量の機械可読なテキスト(コーパス)が利用可能になったことや，
計算機の性能が大幅に向上したことから，コーパス・データを利用した
確率的言語モデルの研究が活発に行われてきている．
確率的言語モデルは，従来，
自然言語処理や音声処理などの工学分野で用いられ，
その有効性を実証してきたが，
比較言語学，方言研究，言語類型論，社会言語学など，
言語学の諸分野においても有用な手法を提供するものと思われる．

本稿では，言語学の分野での確率的言語モデルの有用性を示す一例として，
言語のクラスタリングを取り上げる．
ここでは，言語を文字列を生成する情報源であるとみなし，
この情報源の確率・統計的な性質を確率モデルによりモデル化する．
次に，確率モデル間に距離尺度を導入し，
この距離尺度に基づき言語のクラスタリングを行なう方法を提案する．

以下では，まず２節で先行研究について概説し，
３節で確率的言語モデルに基づく言語のクラスタリング手法を提案する．
４節では，提案した手法の有効性を示すために行った実験について述べる．
ここでは，ECI 多言語コーパス
(European Corpus Initiative Multilingual Corpus)中の 19 ヶ国語の
テキスト・データから，言語の系統樹を再構築する．
また，実験により得られた結果を，言語学的な観点から考察する．
最後に，他分野への応用および今後の課題などについて述べる．


\section{従来の研究}

統計的手法に基づき，
言語の比較を計量的に行う研究は，従来から広く行われてきている．
Kroeber および Chr\'{e}tien は，1930 年代に，
音韻や語形等の言語的特徴から言語間の相関係数を求め，
これに基づきインド・ヨーロッパ諸言語 9 ヶ国語およびヒッタイト語
の間の類似性を求める研究を行っている\cite{Kroeber37,Kroeber39}．
また，クラスター分析に基づき，自動的に言語や方言を分類する研究
に関しても，いくつかの先行研究がある．
文献\cite{Yasumoto95Book}には，これらの研究の古典的な諸方法が概説されている．
また，数学的手法に基づく言語のクラスタリングに関する最近の研究として，
Batagelj らの研究\cite{Batagelj92}があり，
そこでは 65 ヵ国語の言語に対するクラスタリング結果が示されている．

ここで，従来研究では，言語間の距離(あるいは類似度)を導入するために，
どのような方法が用いられてきたかを若干紹介する．
文献\cite{Yasumoto95Book}の４章で述べられている方法では，
インド・ヨーロッパ諸言語の一致度を調べるために，
２つの言語の対応する数詞の最初の子音が一致しているか否かを
調べている．
たとえば，ドイツ語とスペイン語の数詞 ``1'' は
それぞれ ``eins'' および ``uno'' であるが，これらの２単語において，
最初に出現する子音は共に ``n'' であるので，
数詞 ``1'' に関しては，ドイツ語とスペイン語は一致していると考える．
10 個の数詞のうち，何個の数詞について最初の子音が一致しているかを
調べ，これをもとに言語間の距離を導入している．

また，Batagelj らの研究では，２つの言語の対応する単語の
文字列間距離に基づき，言語間の距離を定義している．
いま，２つの文字列 $u$ および $v$ が与えられたとき，
文字列 $u$ 中に文字を追加したり，あるいは $u$ 中の文字を削除したりして，
文字列 $u$ を文字列 $v$ に変換することを考える．
このとき，文字列 $u$ を文字列 $v$ に変換するために必要な
最小の追加および削除文字数で，２つの文字列間の距離を定義する．
たとえば，$u = \mbox{``belly''}$, $v = \mbox{``bauch''}$ に対しては，
まず $u$ から ``elly'' を削除し，次に ``auch'' を追加することにより，
$u$ を $v$ に変換することができるので，
この場合の文字列間距離は８(削除４文字＋追加４文字)である．
以上の文字列間距離を各言語から抽出した 16 個の単語について求め，
これらの距離の和により，言語間距離を定義している
\footnote
{
Batagelj らは，文字の追加・削除に加え置換を考慮した文字列間距離など，
他の文字列間距離についても議論しているが，本稿では省略する．
}．

以上のように，従来の研究では，
あらかじめ人間が言語を分類する上で有用であると思われる
音韻や語形等の言語的特徴を抽出したり，
あるいは比較のための基礎語彙を選定するなどの作業が必要であった．
また，言語間距離の定義にも恣意的な部分が残されていたと
いうことができる．

\section{確率モデルに基づく言語のクラスタリング}

本稿で提案する方法の概略を図 \ref{Fig:OurApproach} に示す．
この方法では，まず各言語の言語データから
確率的言語モデルを自動的に学習し，
次に確率モデル間に距離を導入することにより，言語間の距離を定義する．
このように，本稿の方法は，自己組織的(self-organizing)であり，
あらかじめ人間が各言語の言語的特徴を抽出したり，
基礎語彙を選定する必要はない．
また，本稿の方法の利点として，
各言語のデータを独立に選ぶことができるという点をあげることができる．
たとえば，言語によって違うジャンルのテキストであったり，
あるいはデータのサイズが異なっていても，
これらのデータの揺れを確率モデルの中に吸収することができる．

確率モデルとしては，様々なものが考えられるが，
４節で述べる評価実験では文字の trigram モデルを用いた．
trigram モデルは，$N$-gram モデルの特別な場合($N=3$ の場合)であり，
以下では $N$-gram モデルについて簡単に説明する．
$N$-gram モデルに関する詳細な説明は，
たとえば文献\cite{Jelinek90,Kita96Book}などを参照せよ．

\begin{figure}
\begin{center}
  \atari(80,98)
\end{center}
\caption{確率モデルに基づく言語のクラスタリング}
\label{Fig:OurApproach}
\end{figure}

\subsection{$N$-gram モデル}

たとえば，英語では文字 q には文字 u が後続するとか，
ドイツ語においては文字 c に後続するのは h や k であるなど，
文字の連鎖には確率・統計的な性質が存在する．
$N$-gram モデルは，このような文字の連鎖をモデル化するために適した
確率モデルである．

文字の $N$-gram モデルは，文字の生起を $N-1$ 重マルコフ過程により
近似したモデルであり，
文字の生起は直前に出現した $N-1$ 文字にのみ依存すると考える．
すなわち，
$n$ 文字から成る文字列 $c_{1}, \cdots ,c_{n}$ に対し，
\begin{equation}
        P(c_n|c_1, \cdots, c_{n-1}) \approx P(c_n|c_{n-N+1}, \cdots, c_{n-1})
        \label{Eq:NgramDef}
\end{equation}
となる．

$N$-gram モデルを用いた場合，文字列 $c_{1}, \cdots ,c_{n}$ の生成確率は，
次のようにして計算することができる．
\begin{eqnarray}
        P(c_1, \cdots, c_n) & = & \prod_{i=1}^{n} P(c_i|c_1, \cdots ,c_{i-1}) \nonumber\\
                & \approx & \prod_{i=1}^{n} P(c_i|c_{i-N+1}, \cdots, c_{i-1})
\end{eqnarray}
上式において，最初の等式は，確率論の基本定理から導かれる．
また，２番目の近似式は，式(\ref{Eq:NgramDef})による．

いま，文字列 $c_{1}, \cdots ,c_{n}$ が言語データ中に出現する回数を
$F(c_{1} \cdots c_{n})$ で表すことにする．
$N$-gram の確率は，言語データ中に出現する文字の $N$ 個組と $(N-1)$ 個組の
出現回数から，次のように推定することができる．
\begin{equation}
        P(c_n|c_{n-N+1}, \cdots, c_{n-1})
        = \frac{F(c_{n-N+1}, \cdots, c_n)}{F(c_{n-N+1}, \cdots, c_{n-1})}
        \label{Eq:NgramTraining}
\end{equation}
$N$ の値が大きい場合には，統計的に信頼性のある確率値を
コーパスから推定することが難しくなるため，
通常は $N=3$ あるいは $N=2$ のモデルが用いられることが多い．
なお，$N=3$ の場合を trigram モデル，
$N=2$ の場合を bigram モデル，
$N=1$ の場合を unigram モデルと呼ぶ．

\subsection{$N$-gram モデルのスムージング}

$N$-gram の確率値は，式(\ref{Eq:NgramTraining})に示すように，
言語データ中の文字列の頻度から推定することができる．
しかし，与えられた言語データが少ない場合には，
精度のよい確率値を推定することが難しくなる．
この問題に対処するために，我々の実験では，線形補間法と呼ばれる方法を用いて，
$N$-gram モデルのスムージング(平滑化)を行った．

線形補間法では，
$N$-gram の確率値を低次の $M$-gram $(M < N)$ の確率値と線形に補間する．
trigram の場合には，次のようになる．
\begin{equation}
  P(c_{n}|c_{n-2} c_{n-1})
 = \lambda_{1} P(c_{n}|c_{n-2} c_{n-1})
   + \lambda_{2} P(c_{n}|c_{n-1}) + \lambda_{3} P(c_{n})
\label{Eq:NgramLinearInterpolation}
\end{equation}
ここで，$\lambda_{1},\lambda_{2},\lambda_{3}$ は，
それぞれ trigram, bigram, unigram に対する重み係数であり，
$\displaystyle \sum_{i} \lambda_{i} = 1$ となるように設定される．
式(\ref{Eq:NgramLinearInterpolation})の補間では，
学習データ中に三つ組 $c_{n-2}, c_{n-1}, c_{n}$ が出現しない場合には，
bigram と unigram から $P(c_{n}|c_{n-2}, c_{n-1})$ の値を推定している．
二つ組 $c_{n-1}, c_{n}$ も出現しない場合には，
unigram の値によって近似している．
なお，$\lambda_{i}$ の値は，削除補間法と呼ばれる方法によって推定した．
削除補間法については，
たとえば文献\cite{Jelinek80,Kita96Book}などを参照されたい．


\subsection{言語モデル間の距離}

次に，言語モデル間に距離を導入する．
我々の用いた距離は，文献\cite{Juang85,Rabiner93}において提案されて
いるものと同一である．
上記文献においては，隠れマルコフ・モデル(Hidden Markov Model; HMM)間の距離として
定義されているが，一般の言語モデルに対しても同様に用いることができる．

いま，言語 $L_1$ および言語 $L_2$ の言語データとして，
それぞれ $D_1$，$D_2$ が与えられているとする．
$D_i$ $(i=1,2)$ は，文字列データであり，
その長さ(文字数)を $|D_i|$ と表記する．
また，言語データ $D_i$ から作成された言語モデルを $M_i$ で表す．

まず，言語モデル $M_1$ および $M_2$ に対し，
距離尺度 $d_0(M_1, M_2)$ を次のように定義する．
\begin{equation}
        d_0(M_1, M_2) = \frac{1}{|D_2|}
                \left [
                \log P(D_2|M_2) - \log P(D_2|M_1)
                \right ]
        \label{Eq:DistanceMeasure0}
\end{equation}
式(\ref{Eq:DistanceMeasure0})では，
言語 $L_1$ と $L_2$ の間の距離を，
言語 $L_1$ のモデル $M_1$ からデータ $D_2$ が生成される確率と，
言語 $L_2$ のモデル $M_2$ から同一のデータ $D_2$ が生成される確率の差に
基づいて決めている．
もし，言語 $L_1$ と $L_2$ が類似していれば，
モデルからのデータの生成確率も似た値になるので距離は小さくなるし，
類似していなければ，データの生成確率が大きく違うので距離は大きくなる．

式(\ref{Eq:DistanceMeasure0})は，言語モデル $M_1$ および $M_2$ に対し，
非対称である(すなわち $d_0(M_1, M_2) \neq d_0(M_2, M_1)$)．
対称形にするために，$d_0(M_1, M_2)$ と $d_0(M_2, M_1)$ の平均を取る．
従って，言語モデル $M_1$ と $M_2$ の間の距離 $d(M_1, M_2)$ は，
最終的に次のように定義される．
\begin{equation}
        d(M_1, M_2) = \frac{d_0(M_1, M_2) + d_0(M_2, M_1)}{2}
        \label{Eq:DistanceMeasure}
\end{equation}

\section{評価実験}

\subsection{言語データ}

以上で提案した方法の有効性を実証するために，
ECI 多言語コーパス
(European Corpus Initiative Multilingual Corpus)中の
言語データを用いて，言語の系統樹を再構築する実験を行った．
ECI コーパスは，ELSNET (European Network in Language and Speech)
から CD-ROM により提供されているもので，総語数約１億語から成る．
ECI コーパス中には，主要なヨーロッパ各国語および
トルコ語，日本語，ロシア語，中国語，マレー語等の言語データが
含まれている．
本実験では，このうち，ISO Latin-1 文字セットでコード化されている
19 言語のデータを用いた．

表 \ref{Tab:ECI_data} は，本実験で用いた言語の種類，
各言語データの ECI コーパス中での識別子，
言語データのジャンルを示している．
表のジャンル欄において，「並行テキスト」と記されているのは，
同一の内容を多言語で記述したものであることを示している．

ECI コーパス中のテキストは SGML によりコード化されているが，
本評価実験では，まず SGML のタグを除去し，テキスト部分のみを抽出した．
次に，多言語の言語データ間に均質性を持たせるために，
単語表記中にアルファベット大文字が使われている場合は小文字に変換し，
言語によってはウムラウトやアクセント記号等を表す特殊符号が入っていたが，
英語式アルファベット 26 文字以外の特殊文字は，
すべて対応するアルファベットに変換した．
たとえば，\~{a} は a に変換した．
また，文字の trigram は，表\ref{Tab:ECI_data} の識別子欄に示されている
テキストの最初の 1,000 単語を用いた．

\begin{table}
\caption{実験で用いた言語の種類・言語データの識別子・テキストのジャンル} \label{Tab:ECI_data}
\begin{center}
\begin{tabular}{c|c|c}
\hline
言語            & ECI コーパスでの識別子        &       ジャンル        \\
\hline
\hline
アルバニア語    & alb01b        &       小説            \\ \hline
チェコ語        & cze01a01      &       新聞            \\ \hline
ラテン語        & lat01a01      &       詩              \\ \hline
リトアニア語    & lit01a        &       フィクション    \\ \hline
マレー語        & mal01a01      &       技術文書        \\ \hline
ノルウェー語    & nor01a01      &       フィクション    \\ \hline
トルコ語        & tur02a        &       新聞            \\ \hline

クロアチア語    & cro18a        &       小説            \\ \cline{1-2}
セルビア語      & ser18a        &       (並行テキスト)  \\ \cline{1-2}
スロベニア語    & slo18a        &                       \\ \hline

デンマーク語    & dan16a        &                       \\ \cline{1-2}
オランダ語      & dut16a        &                       \\ \cline{1-2}
英語            & eng16a        &                       \\ \cline{1-2}
フランス語      & fre16a        &       技術文書        \\ \cline{1-2}
ドイツ語        & ger16a        &       (並行テキスト)  \\ \cline{1-2}
イタリア語      & ita16a        &                       \\ \cline{1-2}
ポルトガル語    & por16a        &                       \\ \cline{1-2}
スペイン語      & spa16a        &                       \\ \hline

ウズベク語      & mul13a        &       小説            \\ \hline
\end{tabular}
\end{center}
\end{table}

\subsection{実験結果および考察}

上記により作成した文字 trigram モデルに対し，
階層的(凝集型)クラスター分析を行ない，
言語のデンドログラム(dendrogram; 樹状図)を作成した．
クラスタリング・アルゴリズムには，
群平均法 UPGMA(Unweighted Pair-Group Method using Average)\cite{Washio89Book}
と呼ばれる方法を用いた．
群平均法は，広い範囲においてよい結果を与えるクラスター分析法で
あるといわれている．

図 \ref{Fig:ExprResult} に，19 言語のクラスタリング結果を示す．
言語名の左側の樹状図が実験により得られた結果であり，
右側には各言語のおおまかな分類を記している．
以下では，言語学的な観点から，クラスタリング結果の妥当性について
考察する．
なお，言語の分類および諸言語間の関係に関しては，
文献\cite{GengoBook}を参考にした．
まず，評価実験で用いた言語は，以下のように大きく分類される．
\begin{itemize}
\item[(A)]      インド・ヨーロッパ語族
        \begin{itemize}
        \item[(A-1)] アルバニア語派 (アルバニア語)
        \item[(A-2)] スラブ語派 (チェコ語，クロアチア語，セルビア語，スロベニア語)
        \item[(A-3)] バルト語派 (リトアニア語)
        \item[(A-4)] イタリック語派 (ラテン語，フランス語，ポルトガル語，スペイン語，イタリア語)
        \item[(A-5)] ゲルマン語派
                \begin{itemize}
                \item[(A-5-1)] 北ゲルマン語派 (ノルウェー語，デンマーク語)
                \item[(A-5-2)] 西ゲルマン語派 (オランダ語，ドイツ語，英語)
                \end{itemize}
        \end{itemize}
\item[(B)]      アルタイ諸語 (トルコ語，ウズベク語)
\item[(C)]      オーストロネシア語族 (マレー語)
\end{itemize}
図 \ref{Fig:ExprResult} の右側に示すように，
実験により得られた結果は，上記の大分類を反映したものになっている．

\begin{figure}[p]
\setlength{\baselineskip}{0.25\baselineskip}
\begin{small}
\begin{center}
\begin{verbatim}
　　　　　　　　┌─────────────　アルバニア語　──　アルバニア語派
　　　　　　　　│
　　　　　　　　│　　　　　┌───────　チェコ語　　　─┐
　　　　　┌──┤　　　　　│　　　　　　　　　　　　　　　　│
　　　　　│　　│　┌───┤　　　　　┌─　クロアチア語　　│
　　　　　│　　│　│　　　│　┌───┤　　　　　　　　　　│　スラブ語派
　　　　　│　　│　│　　　└─┤　　　└─　セルビア語　　　│
　　　　　│　　└─┤　　　　　│　　　　　　　　　　　　　　│
　　　┌─┤　　　　│　　　　　└─────　スロベニア語　─┘　
　　　│　│　　　　│　
　　　│　│　　　　└───────────　リトアニア語　──　バルト語派
　　　│　│　
　　　│　│　　　┌────────────　トルコ語　　──┐　アルタイ諸語
　　　│　└───┤　　　　　　　　　　　　　　　　　　　　　│　チュルク語族
　　　│　　　　　└────────────　ウズベク語　──┘
　┌─┤
　│　│　　　　　　┌───────────　ラテン語　　──┐
　│　│　　　　　　│　　　　　　　　　　　　　　　　　　　　│
　│　│　　┌───┤　　　┌───────　フランス語　　　│
　│　│　　│　　　│　┌─┤　　　　　　　　　　　　　　　　│
　│　│　　│　　　│　│　│　┌─────　ポルトガル語　　│　イタリック語派
　│　│　　│　　　└─┤　└─┤　　　　　　　　　　　　　　│
　│　│　　│　　　　　│　　　└─────　スペイン語　　　│
　│　└──┤　　　　　│　　　　　　　　　　　　　　　　　　│
　│　　　　│　　　　　└─────────　イタリア語　──┘
─┤　　　　│　
　│　　　　│　　　　　　　┌───────　ノルウェー語　─┐
　│　　　　│　┌─────┤　　　　　　　　　　　　　　　　│　北ゲルマン　─┐　ゲ
　│　　　　│　│　　　　　└───────　デンマーク語　─┘　語派　　　　　│　ル
　│　　　　└─┤　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　│　マ
　│　　　　　　│　　　┌─────────　オランダ語　　─┐　　　　　　　　│　ン
　│　　　　　　│　┌─┤　　　　　　　　　　　　　　　　　　│　　　　　　　　│　語
　│　　　　　　└─┤　└─────────　ドイツ語　　　　│　西ゲルマン　─┘　派
　│　　　　　　　　│　　　　　　　　　　　　　　　　　　　　│　語派
　│　　　　　　　　└───────────　英語　　　　　─┘
　│　
　└────────────────────　マレー語　　　──　オーストロネシア語族
　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　西部オーストロネシア語派
\end{verbatim}
\end{center}
\end{small}
\caption{ECI 多言語コーパスより得られたクラスタリング結果} \label{Fig:ExprResult}
\end{figure}

\clearpage

次に，言語間のより細かな関係について調べる．
まず，実験結果では，スラブ語派に属するクロアチア語とセルビア語を，
最初に一つのクラスタとしてまとめている．
クロアチア語とセルビア語はともに，南スラブ語群に属し，
両者の差異は方言的なものであるとされている．
従って，両者を一つのクラスタとすることは，
きわめて妥当であるといえる．
また，実験結果では，スラブ語派とバルト語派を併合した後に，
これをアルバニア語派と併合している．
スラブ語派とバルト語派の諸言語には，多くの類似点が見られ，
バルト・スラブ祖語の存在を考えている研究者もいる．
アルバニア語は，同一の語派に属する言語がなく，
１言語で１語派の扱いを受けているが，
南スラブ語等の言語からの影響を受けている．
実験結果は，以上の点を反映しているということができる．
西ゲルマン語派に関しては，オランダ語とドイツ語を，まず併合しているが，
ドイツ語学では，オランダ語をドイツ語の１方言，低地フランク語として扱っており，
この２言語はきわめて類似している．
以上のように，実験結果は，言語の細分類に関しても，かなりの
部分で言語学での分類と一致しており，
提案したクラスタリング手法が有効なものであることを示している．

また，
文字 trigram モデルにおけるスムージングの効果を調べるために，
スムージングを行わない場合についても実験を行った．
スムージングを行わない場合には，
トルコ語，ウズベク語，マレー語が１つのクラスタを形成するという
結果が得られた
(図 \ref{Fig:ExprResult} から分かるように，スムージングを行った
場合には，マレー語は単独で１つのクラスタを形成している)．
しかし，それ以外には，クラスタを形成する順序が多少違うだけで，
結果に大きな違いを見いだすことはできなかった．
この結果より，言語系統樹の構築では，言語モデルを多少精密化しても，
精密化の影響を受けるまでには至らなかったということができる．


\subsection{言語識別の実験}

上記実験により，
確率的言語モデルは言語のクラスタリングにきわめて有効で
あることを示したが，クラスタリング以外の分野での
確率的言語モデルの有用性を示すために，
追加実験として言語識別の実験を行った．
この実験では，上記で作成した各言語の文字 trigram モデルを用いて，
未知のテキストから，そのテキストの使用言語を特定するということを行った．
以下に，本実験の手順を示す．
\begin{itemize}
\item[(1)]
各言語に対し，２つの未知テキストを評価データとして用意する．
未知テキストは，表 \ref{Tab:ECI_data} に示したものとは別のテキスト・データである．
言語によっては，表 \ref{Tab:ECI_data} 以外のテキスト・データがないものもあり，
言語識別用の評価データは 26 個(13 言語)となった．
なお，未知テキストとして用いた言語データは，
以下の通りである．
チェコ語(cze01a02, cze01a03)， 
 ラテン語(lat01a02, lat01a03)， 
 マレー語(mal01a02, mal01a03)， 
 ノルウェー語(nor01a02, nor01a03)， 
 セルビア語(ser01a01, ser01a02)， 
 デンマーク語(mda12a, mda12b)， 
 オランダ語(dut01a01, dut01a02)， 
 英語(eng01a, eng01b)， 
 フランス語(fre01a01, fre01a02)， 
 ドイツ語(ger02a, ger02b)， 
 イタリア語(ita01a, ita03a)， 
 ポルトガル語(por01a, por01b)， 
 スペイン語(spa02a, spa02b)．

\item[(2)]
各未知テキストから，4.1 節に示した手順と同様にして，
未知テキストに対する言語モデル(文字 trigram モデル)を作成する．

\item[(3)]
3.3 節で導入した距離を用いて，
未知テキストの言語モデルと各言語の言語モデルとの間の距離を計算する．
最も小さな距離を与える言語を，未知テキストの使用言語と判断する．
\end{itemize}

上記による実験において，
未知テキストの最初の 10 単語，20 単語，30 単語，40 単語，50 単語，100 単語，1000 単語
を用いた場合について，言語識別率を求めた．
実験結果を，図 \ref{Fig:Rate} に示す．
図から分かるように，未知テキストからの使用言語の特定には，
20 単語程度あれば十分であるということができる．
20 単語を用いたときには，26 個の未知テキストのうち，
25 個についてその言語を正確に推定できた(識別率 96.2\%)．
なお，識別に失敗したものは，例えばセルビア語のテキストをクロアチア語と間違うなど，
近親関係の言語間での間違いが主であった．

なお，$N$-gram に基づいた言語識別に関する研究としては，
Cavnar および Trenkle の研究\cite{Cavnar94}などがある．
Cavnar らは，テキスト中に頻出する $N$-gram 文字列の出現順位に
基づいた距離を用いて，高い精度で言語の自動識別ができることを示している．
彼らの実験では，8 ヶ国語のネットニュースの記事を用いており，
未知テキストが 300 文字以上与えられた場合には，
99.8\% の識別率を達成したと報告している．
実験に用いた言語データや対象言語の数などが異なることから，
本論文の結果と直接比較することはできないが，
Cavnar らの結果も本論文の結果も，
$N$-gram が言語の自動識別に非常に有効であることを示している．

\begin{figure}
\begin{center}
  \atari(113,81)
\end{center}
\caption{未知テキスト中の単語数と言語識別率}
\label{Fig:Rate}
\end{figure}

\section{おわりに}

本稿では，確率的言語モデルに基づいた言語のクラスタリング手法を提案した．
また，ECI 多言語コーパス中の 19 ヶ国語の
テキスト・データから言語の系統樹を再構築する実験を行ない，
実験結果を言語学での分類と比較することにより，
提案した手法の有効性を示した．

本稿では，確率的言語モデルとして文字の $N$-gram モデルを用いたため，
文字の連鎖という観点からのクラスタリング結果が得られた．
言語類型論の分野では，言語の語順等により諸言語間の比較を行うことなどが
行われているが，語順等に対する言語モデルを設定することができれば，
言語類型性という観点から見たクラスタリングを行うことができるであろう．
今後の課題として，
(1) 他の言語モデルを用いたときの言語のクラスタリング，
(2) より多くの言語を対象としたクラスタリング実験
を行いたいと考えている．
また，本稿で述べた実験では，
クラスタリング手法として群平均法 UPGMA を用いたが，
クラスタリング手法には他にも様々なものがあり，
他の手法を用いた場合についても検討していく必要があろう．

本稿では，言語のクラスタリングを中心に扱ったが，
提案した手法はテキストの分類(Text Categorization)などにも
応用可能であると考えられる．
また，本稿で述べた基本的な手法は，
比較言語学，方言研究，言語類型論，社会言語学などの
幅広い分野で役立つものと期待される．
本論文で提案した手法を，これらの分野において広く用いて頂くために，
プログラムおよび実験に用いたデータを公開する．
\verb+ kita@is.tokushima-u.ac.jp +宛に電子メイルで
問い合わせられたい．



\bibliographystyle{jnlpbbl}
\bibliography{kita}

\begin{biography}
\biotitle{略歴}
\bioauthor{北 研二}{
1957年生．1981年早稲田大学理工学部数学科卒業．
1983年から 1992年まで沖電気工業(株)勤務．
この間，1987年から 1992年まで ATR 自動翻訳電話研究所に出向．
1992年 9月から徳島大学工学部勤務．現在，同助教授．工学博士．
確率・統計的自然言語処理，音声認識等の研究に従事．
情報処理学会，電子情報通信学会，日本音響学会，日本言語学会，
計量国語学会，ACL 各会員．}

\bioreceived{受付}
\biorevised{再受付}
\bioaccepted{採録}

\end{biography}

\end{document}
