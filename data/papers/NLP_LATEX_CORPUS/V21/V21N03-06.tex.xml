<?xml version="1.0" ?>
<root>
  <addtolength>-height1pt</addtolength>
  <addtolength>-height1pt</addtolength>
  <section title="Introduction">Indocument-levelsentimentclassification,earlystudiesexploitedlanguage-basedclues(e.g.,n-grams)extractedfromtextualcontent,followedbymorerecentstudiesthatadapttheclassifiertoreviewswrittenbyaspecificuserorwrittenonaspecificproduct.Althoughuser-andproduct-awaremethodsexhibitedbetterperformanceoverthosebasedonpurelytextualclues,mostofthemuseonlytheuserinformation,ortheyassumethattheuserandproductofatestreviewareseeninthetrainingdata.Theseassumptionsheavilylimittheirapplicabilityinareal-worldscenariowherenewusersandnewproductsareconstantlyemerging.Thispaperproposesamethodofcollectivesentimentclassificationthatisawareoftheuserandproductofthetargetreview.Ourmethodbenefitsfromthebiaseddistributionsofpolaritylabelsintherealworld;intolerantuserstendtoreportcomplaintswhilepopularproductsarelikelytoreceivepraise.Weintroduceglobalfeaturestoencodethebiasofauserandofaproduct(referredtoasuserleniencyandproductpopularity),andthencomputetheglobalfeaturesalongwithtesting.Inthisway,theglobalfeaturesarecollectivelycomputedwithrespecttothelabelsofothertestreviews.Ourmethodisthereforeapplicabletoreviewswrittenbyemergingusersandonemergingproductsthatarenotobservedinthetrainingdata.Themajordifficultyinrealizingourcollectivesentimentclassifierisindecoding.Becauseglobalfeaturesdependontestreviewlabelsandlabelsconverselydependontheglobalfeatures,weneedtooptimizeagloballabelconfigurationforthetestreviews.Inthisstudy,wetacklethisproblembyresortingtotwoapproximatedecodingalgorithms,easiest-firstandtwo-stagestrategies.Wealsoempiricallycomparethespeedandaccuracyofthesetwostrategies.Weevaluateourmethodsonthreedatasetswithuserand/orproductinformation.Experimentalresultsdemonstratethatwhenuser-orproduct-biasexists,ourcollectivemethodcanimproveclassificationaccuracyagainststate-of-the-artmethods.Theremainderofthispaperisorganizedasfollows.Section~discussesrelatedworkthatexploitsuserandproductinformationinasentimentclassificationtask.Section~proposesamethodthatcollectivelyclassifiesagivensetofreviews.Section~reportsexperimentalresults.Finally,Section~concludesthisstudyandaddressesfuturework.</section>
  <section title="Related Work">Document-levelsentimentclassificationfocusesonlabelingagivenreview.Normally,thecontentofthereview,theuserwhowrotethereview,andtheproductonwhichthereviewiswrittenareconsideredcontributivetothisresearch.Inwhatfollows,webrieflyglanceattraditionalapproachesbasedonpurelytextualcontent,andintroduceuser-orproduct-awareapproachesindepth.</section>
  <subsection title="Text-based Methods">Earlystudiesconsideronlytextualcontentforclassifyingthesentiment.Panget~al.(2002)developedasupervisedsentimentclassifierthattakesonlywordn-gramsasfeatures.Hu:2004HuandLiu(2004)analyzedthesentimentofproducts'aspects.Blitzer:2007Blitzeret~al.(2007)builtaclassifierthatlearnsweightsoftextualfeaturesdependingontheproductdomain.andconsideredstructuralinteractionbetweenwordstocapturecomplexintra-sententialphenomenasuchaspolarityshifting.usedabootstrapmethodtoexpandsentimentlexicon,whichgreatlyhelpstoidentifythesentiment.</subsection>
  <subsection title="User- or Product-Aware Methods">Recently,user-generatedcontenthasbeenthefocusofconsiderableattentionstimulatingresearcherstoexploretheeffectivenessofuserandproductinformation.Tanet~al.(2011)andSperiosuet~al.(2011)exploitedausernetworkbehindasocialmedia(Twitter,intheircase),anddevelopedagraph-basedmethodundertheassumptionthatfriendsgivesimilarratingstowardsthesameproducts.However,suchusernetworksarenotalwaysavailableintherealworld.Seroussiet~al.(2010)computedthesimilaritiesamongusersonthebasisoftextandtheirratinghistories.Then,theyclassifiedagivenreviewbyreferringtotheratingsgivenforthesameproductbyotheruserswhoweresimilartotheuserinquestion.Liet~al.(2011)incorporateduser-orproduct-dependentn-gramfeaturesintoaclassifier.Theyarguedthatusersemployuser-specificlanguagetoexpresstheirsentiment,whilethesentimenttowardaproductisdescribedinproduct-specificlanguage.Theseapproaches,however,assumethatthetrainingdatacontainsreviewswrittenbythetestusersoronthetestproducts.Thisisanunrealisticassumption,sinceweneedtolabelreviewsrequiredforeveryemerginguserorproduct.Inthisstudy,weintendtohandlereviewswrittenbyemergingusersoronemergingproductsbycapturingtheircharacteristicsfromthetestreviews.Aswelaterconfirminexperiments,ourmethodimprovesclassificationaccuracyevenwhenonlyafewreviewsareavailablefortheusersorproductsinquestion.</subsection>
  <section title="Method">Thissectiondescribesourmethodofcollectivesentimentclassificationthatusesuserleniencyandproductpopularity.</section>
  <subsection title="Overview">Ourtaskis,givenasetofNreviewsR,toestimatelabelsY,wherey_r+1,-1foreachgivenreviewrR,+1and-1representpositiveandnegativepolarity,respectively.Eachreviewlabelisestimatedbasedonthefollowingscoringfunction,wherex_risfeaturevectorrepresentationofthereviewr,andwisaweightvectorthatwelearnfromlabeleddata.Withthisscoringfunction,thelabelisestimatedasfollows:[y_r=(score(x_r))=.]Asdiscussedintheintroduction,ouraimistoexploituserleniencyandproductpopularitytoimprovesentimentclassification.Wethereforeencouragereviewswrittenbythesameuseroronthesameproducttoreceivethesamepolaritywhentheirpolaritylabelsarefoundtobebiased.Werealizethisbyencodingsuchbiasesastwoglobalfeaturesinadditiontolocaltextualfeatures,asdetailedinSection.Sinceglobalfeaturesmakeitimpossibletoestimatereviewlabelsindependently,weexploreusingtwoapproximatedecodingstrategiesinSection.Notethathereweassumeeachreviewtobeassociatedwitheithertheuserwhowrotethatreview,theproductonwhichthereviewwaswritten,orboth.Thisassumptionisnotunrealistic,sincetheuserorproductcanbeidentifiedinmanyreviewwebsites.Weshouldemphasizethatourmethoddoesnotrequireuserprofiles,productdescriptions,oranytypeofextrinsicknowledgeoftheusersorproducts,andthereforeitcanhandlereviewswrittenbyemergingusersoronemergingproducts.</subsection>
  <subsection title="Features">Ourfeaturescanbedividedintolocalandglobal,suchthatx_r=(x_r^l,x_r^g).Thelocalfeatures(x_r^l)areconventionalwordn-grams(n=1andn=2)withbinaryvaluesthatindicatetheexistenceofthen-grams.Theglobalfeatures(x_r^g)aretheuserleniencyandproductpopularitythatarerepresentedasrealvalues.Ourglobalfeaturesaredecomposedas:[x_r^g=(f_u^+(r),f_u^-(r),f_p^+(r),f_p^-(r)),]wheref_u^+(r)=|r_j,|,y_j=+1,r_jS_u(r)||S_u(r)|,	f_u^-(r)=|r_j,|,y_j=-1,r_jS_u(r)||S_u(r)|,_p^+(r)=|r_j,|,y_j=+1,r_jS_p(r)||S_p(r)|,	f_p^-(r)=|r_j,|,y_j=-1,r_jS_p(r)||S_p(r)|.gather*Here,S_u(r)istheuser-relatedneighborsetofr,whichcontainsthereviewswrittenbythesameuseruasr,whileS_p(r)istheproduct-relatedneighborsetofr,whichcontainsreviewswrittenonthesameproductpasr.IfS_u(r)(orS_p(r))isempty,wesetf_u^+(r)andf_u^-(r)(orf_p^+(r)andf_p^-(r))tobe0.Weusef_u^+(r)andf_u^-(r)tocaptureuserleniency,i.e.,howlikelytheuseristowritepositiveandnegativereviews,respectively,whileweusef_p^+(r)andf_p^-(r)tocaptureproductpopularity,i.e.,howlikelypositiveandnegativereviewsarewrittenontheproduct,respectively.</subsection>
  <subsection title="Two Approximate Decoding Strategies">Theglobalfeaturesmakeitdifficulttoperformdecoding(i.e.,labelingreviews)sinceeachreviewcannolongerbelabeledindependently.Exactdecodingalgorithmsbasedondynamicprogrammingarenotfeasibleinourcasebecausethesearchspacegrowsexponentiallyasthenumberoftestreviewsincreases.Instead,weexploreandempiricallycomparetwoapproximatealgorithms,easiest-firstandtwo-stagedecodingstrategy.Theeasiest-firstdecodingisslowerbutexpectedtobemoreaccuratethanthetwo-stagedecoding.Algorithm~depictstheeasiest-firstdecodingalgorithm.Thisstrategyiterativelydeterminesreviewlabelsonebyone.Ineachiteration,thereviewthatiseasiesttolabel,i.e.,reviewr_maxwiththehighestabsolutescorescore(x_r),ischosen(lineinAlgorithm~),andthenlabeled(lineinAlgorithm~).Thisprocessisrepeateduntilallthereviewsarelabeled.Theglobalfeaturesareincrementallyupdatedusingthereviewlabelsthatarealreadyassigned.Thatis,atthebeginningofdecoding,allglobalfeaturesaresetto0;whenthelabelingprocessproceeds,theglobalfeaturesbecomemoreaccurateasmorelabelsareusedtocomputethem.Algorithm~depictsatwo-stagedecodingalgorithm.Thisstrategyperformsdecodingtwice.Inthefirststage(lines--inAlgorithm~),weuseonlylocalfeaturestoclassifythereviews.Inthesecondstage(lines--inAlgorithm~),thoselabelsareusedtocomputeglobalfeatures,andthelabelsarereassignedusingtheadditionallycomputedglobalfeatures.Inourcase,thetwo-stagedecodingatfirstonlyuseswordn-gramfeaturestoestimatethelabels.Thereafter,thoselabelsareusedtocomputeglobalfeaturesinthesecondstage.[t][t]232pt232ptheight0.25ptheight0.25ptminipage[t]178pt178ptheight0.25ptheight0.25ptminipagealgorithmXThemajordifferenceinthetwoalgorithmsisinthewaytheycomputeglobalfeatures.Theeasiest-firststrategyuseslabelsestimatedbylocalfeaturesandpreviouslycomputedglobalfeatures,whilethetwo-stagestrategyuseslabelsestimatedonlybylocalfeatures.Weexpecttheeasiest-firstdecodingwillexhibitbetterclassificationaccuracyoverthetwo-stagestrategy,whichwewillconfirmlaterinexperiments.TimeComplexityWehereanalyzethetimecomplexityofthetwodecodingstrategieswithrespecttothenumberoftestreviews,N.Intheeasiest-firststrategy,twoprocessesconsumemostofthecomputationtime,oneofwhichischoosingtheeasiestreviewtolabel(lineinAlgorithm~).Thearg,maxoperationspendsO(N)timeineachiteration,usingaheapstructuretomaintainthescores.Thus,thetimecomplexityofthisstepisO(NN)forNiterations.Anotherbottleneckisscorerecomputation(lineinAlgorithm~).ToupdatethescoreforeachreviewrS_u(r_max)S_p(r_max),weneed|S_u(r_max)S_p(r_max)|timesdeleteandinsertoperationstotheheap.Ifwecanassumethemaximalnumberofreviewsforeachuseroreachproduct,|S_u(r_max)S_p(r_max)|isupper-boundedbyaconstantC._u(r_max)S_p(r_max)|isweaklyrelatedtoN.TheoveralltimecomplexityaddsuptoO(N(N+CN))=O(NN).Inthetwo-stagestrategy,thecomplexityisO(N)forbothstages.ThenthetotalcomplexityisalsoO(N),whichisthesameasthebaselinemethodthatusesonlylocaltextualfeatures.</subsection>
  <subsection title="Training">Itisstraightforwardtotraintheparametersofthescoringfunctionforthetwodecodingalgorithms.WetrainabinaryclassifierasthescoreestimationfunctioninEq.~,consideringwordn-gram,userleniencyandproductpopularityfeatures.Thevaluesofglobalfeaturesarecomputedusingthegoldlabelsoftrainingdata.Thisclassifierisusedfortheeasiest-firstdecodingandsecondstageofthetwo-stagedecoding.Aclassifierusedinthefirststageofthetwo-stagedecodingistrainedonlywithwordn-gramfeatures.</subsection>
  <section title="Experiments">Inthissection,weevaluateourmethodofcollectivesentimentclassificationonthreereal-worldreviewdatasetswithuserand/orproductinformation.</section>
  <subsection title="Setting">WepreprocessedeachreviewinthedatasetsusingOpenNLPtoolkittodetectsentenceboundariesandtotokenizesentences.FollowingPanget~al.(2002),weinducedwordunigramsandbigramsaslocalfeatureswhiletakingnegationintoaccount.Weignoredthen-gramsthatappearedlessthanapredefinednumberoftimes(wesetthisnumbertobesix)inthetrainingdatatolimitthefeaturesize.Weusedanonlinelinearclassifiercalledconfidence-weightedinourmethods.Weshouldemphasizeherethattheconfidence-weightedalgorithmisreportedtoperformaswellasSupportVectorMachineinadocument-levelsentimentclassification,andittherebyconstructsastrongbaseline.Foreachconfidence-weightedclassifier,wetunethetwohyper-parameters(confidenceparameterandthenumberofiterationsfortraining)onthetrainingdata.Confidence-weightedlearningadjustsamultivariateGaussiandistributionovertheweightparameterswherecontrolstheupdaterateofthevarianceandmean.Givenalarger,thevariancedecreasesfasterandthemeanisupdatedmoregradually.Thenumberofiterationscontrolshowmanytimeseachtraininginstanceisusedtoupdatetheparameters.Wedividedthetrainingdataintotwoequal-sizedparts.Intuning,onepartisusedastrainingdataandtheotherasdevelopmentdata.Theparameterischosenbetween1,2,5,10,20,50andthenumberofiterationsischosenbetween1,2,5,10,15,20,25.Aftertuning,weusedthetunedhyper-parameterstotrainaclassifierwiththeentiretrainingdata.</subsection>
  <subsection title="Datasets">Panget~al.(2004),Blitzeret~al.(2007),andMaaset~al.(2011)collectedthreedatasetsthatcontainuserand/orproductinformation.Allofthepolarities(positiveandnegativelabels)inthesedatasetsarebalanced.Table~summarizesthestatisticsofthesedatasets.Pang:Thisdatasetisasmallsubsetofreviewsmanuallychosenfromamoviereviewarchivecollectedfromadiscussionnewsgrouponartmovies.2,000reviewsarerandomlychosenfromalargearchivethatcontainsover30,000reviews.Blitzer:Thisdatasetiscollectedfromashoppingwebsiteonvariousdomainsofproducts.Weusedpartofitstotal780~kreviews,tobeconsistentwiththeothertwodatasets.Weautomaticallydeletedreplicatedreviewswrittenbythesameauthoronthesameproduct(resultingin740~krawreviews).Then,thereviewsarebalancedforpositiveandnegativelabels(over90~kreviewsforeach,byrandomlysamplingtheequalnumberofpositiveandnegativereviews).Maas:ThisdatasetiscollectedfromthesamemoviereviewwebsiteasthePangdatasetexceptthatthereviewsarenotconstrainedonanydiscussionnewsgroup.Thechoosingprocessisautomaticallyperformedbycollecting(upper-boundednumberof)reviewsforeachproduct(movie).Weautomaticallyrecoveredtheuserandproductinformation(implicitly)includedinthedatasets.ThePangandBlitzerdatasetsareaccompaniedbytheoriginalhtmlfiles,fromwhichweautomaticallyextractedtheuserandproductforeachreview.WeusedaURL(linktothemovietitle)providedbytheMaasdatasetforeachreviewastheidentifieroftheproduct,inthiscase,amovie.BecauseuserinformationcannotbefullyrecoveredintheMaasdataset,weonlyconsidertheproductpopularityinthisdataset.Whenwesplitthedatasetsforcross-validation,wemaintainedtheorderofthePangdatasetandshuffledtheBlitzerandMaasdatasetsfortrainingandtestingbeforesplitting.Sinceourmethodtakesadvantageoftheuserandproductinformation,themorereviewseachuserorproducthas,thehigheraccuracyourmethodisexpectedtoachieve(aswewilllaterconfirminSection).IntheBlitzerandMaasdatasets,thereviewswereoriginallyorderedbytheuserandproduct,respectively.Therefore,ifwenaivelyusetheoriginalordergivenbythedatasetswithoutshufflingwhensplittingthem,theaveragenumberofreviewsforeachuserorproductbecomesunnaturallyhigh,whichgeneratesadvantagestoourmethod.Inordertopreventtheseeminglyunfairaccuracygaininthisparticularsplitting,weshuffledthereviewsbeforeanyexperimentratherthanusingthesplitprovidedbytheauthors.Weperformedatwo-foldcross-validationonallthreedatasets.</subsection>
  <subsection title="Results">Wecomparedtheaccuracyofourmethodwithtwoothermethods:abaselinemethodusingaconfidence-weightedlinearclassifierwithn-gramfeaturesandanexistinguser-awaresentimentclassifierproposedbySeroussietal.(2010).Forreference,wealsolistedtheresultsreportedinmaas:2011Maasetal.(2011),whichwasevaluatedusingadifferenttwo-foldsplitting.Seroussietal.(2010)proposedaframeworkthatcombinesscoresgivenbyclassifierstrainedonotherusers,accordingtothesimilaritytothetargetuser.Webuildapersonalizedclassifierforeachuseronhis/hertrainingreviewsifhe/shehasmore(positiveandnegative)reviewsthanapredefinedthreshold.Foranypairofusers,theycomputethesimilarityasthejacquarddistanceofwordn-gramsfromtheir(testingandtraining)reviews(called``AIT,''whichperformedbestintheirpaper).Toclassifyareviewwrittenbyagivenuser,theycombinethescoresgeneratedbytheotherusers'personalizedclassifiersweightedbythesimilaritiesbetweenthoseusersandthegivenuser.Ifwesettheaforementionedthresholdtoohigh,manywordn-gramfeatureswillbelostbecausemanyreviewswillbeignored.Wetunedthethreshold(from1to5)usingtheidenticalmethodweusedtotunethehyper-parametersfortheconfidence-weightedclassifier.Inourdatasets,manytestusershadasimilarityof0totheusersinthetrainingdatabecausethenumberofreviewswrittenbyeachuserismuchsmallerthanthatintheSeroussi'sdataset.Forlabelingreviewswrittenbysuchtestusers,weconstructedandusedadefaultclassifiertrainedonallthetrainingdata.Table~showstheexperimentalresults.OurmethodimprovesaccuraciesontheBlitzerandMaasdatasetsagainstthebaselineclassifiers.AlargerimprovementisachievedontheMaasdataset,probablybecausetheaveragenumberofreviewsforeachproductishigherthanthatoftheBlitzerdatasetsuchthatwecouldestimatemorereliableglobalfeatures.OnthePangdataset,however,ourmethodhadnoadvantage.Wewillfurtheranalyzethereasoninthefollowingparagraphs.@makefnmarkThisresultuseddifferenttwo-foldsplittingfromours.Undertheirsplitting,ouraccuracieswere90.79%,92.39%,and92.27%forbaseline,easiest-first,andtwo-stagestrategies,respectively.BothstrategieseasilybeatMaaset~al.(2011)'saccuracy,88.89%.Ourbaselineissuperiortotheirmethod,partlybecauseofthefeaturesweused.Theyusedonlyunigramfeatures,whereasweusedunigramandbigram(whichconsidersnegation)asfeatures.Withonlyunigramfeatures,ourbaselineclassifierachieved87.80%accuracy.OntheBlitzerdataset,userleniencywasmorehelpfulthanproductpopularity.ThisisprobablybecausetheBlitzerdatasetincludesallthereviewswrittenbyeachuser.Ontheotherhand,productinformationplaysanimportantrolebecausetheMaasdatasetincludesallthereviewsforeachproduct.Amongthetwodecodingmethods,theeasiest-firstdecodingconsistentlyachieveshigheraccuracy.Thisconfirmsourexpectationthateasiest-firstdecodingismorecautiousthantwo-stagedecoding.However,easiest-firstdecodinghasitsownweaknessinspeed.Seroussietal.(2011)performedbadlybecausethenumberofreviewsforeachuserinourdatasetswaslowerthantheirs,hence,thepersonalizedclassifierslearnedonlimitedinstanceswouldbeunreliable.</subsection>
  <subsection title="Analysis">Inthissection,weanalyzeourexperimentalresultsonaccuracy.WefirstinvestigatetheimpactonaccuracywhenwechangethetestingdatasizeinAnalysis1.Next,weshowhowmuchimprovementwillbegainedbyourmethodifauser(oraproduct)hasadifferentnumberofreviewsinAnalysis2.Then,weshowthebiasesofeachdataset,andtheperformanceofourmethodonbothemergingandexistingusers(andproducts)inAnalysis3and4.Finally,inAnalysis5and6,weshowthelearningcurvesandsomeexamples.Analysis1:ImpactoftestingdatasizeonspeedandaccuracyFirst,weinvestigatetheimpactofthenumberoftestreviewsonspeedandaccuracyinourcollectivesentimentclassification.WeusetheBlitzerdatasetforevaluationbecauseofitslargersize.Userleniencyandproductpopularityarebothconsidered.Weusethefixedhyper-parameters(=1.0,#iterations=10)foralltheconfidence-weightedclassifiersusedinthisexperiment.Toillustratetheimpactofthesizeoftestdataonclassificationaccuracy,wechangedthenumberoftestreviewsprocessedatonce.Here,insteadofdecodingthewholetestingdata,wesplitthetestreviewsintoequal-sizedsmallersubsetsandapplyourclassifierindependentlytoeach.Weaccumulatetheresultsforallthesubsetstocomparetheaccuracyfortheentiretestdata.Fig.~showstheexperimentalresults.Whenweprocessalargernumberofreviewsatonce,wehavemorereviewsperuserorperproducttocomputetheglobalfeatures.Thecomputedglobalfeaturestherebybecomemorestatisticallyreliableandaccuratelycaptureuserleniencyandproductpopularity,whichresultsinhigherclassificationaccuracy.WewillconfirmthisinAnalysis2.Wethenmeasuredthetestingspeedusingthesamesettingastheaboveexperiment,whileevaluatingtheaveragetimeconsumedbyonesinglesubset.AsshowninFig.~,thespeedoftheeasiest-firstdecodingdrasticallyslowsdownasthenumberofprocessedreviewsgrows,whereasthespeedofthetwo-stagedecodingincreaseslinearly.Meanwhile,theaccuracyofthetwostrategiesarecompetitive,asshowninFig.~.Basedontheseobservations,thekeyfactorinachievingbetteraccuracyisnotthechoiceofdecodingstrategy,buttheamountoftestdataprocessedatonce.Weconcludethatwhenwehavetoomuchtestdatafortheeasiest-firstdecodingtoprocessinapracticaltimeweshouldadoptthetwo-stagedecodingstrategytoinduceandexploitmorereliableglobalfeatures.Otherwise,wecanchoosetheeasiest-firstdecodingtoenjoyamodestgaininaccuracy.Analysis2:AccuracyintermsofsizeofneighborsTheaccuracygainisrootedinglobalfeatures,whileglobalfeaturesarecomputedbyreferringtolabelsof(user-andproduct-related)neighboringreviews,S_u(r)andS_p(r).Whenonlyonesuchneighborisavailable,globalfeaturesmaybeunreliablecomparedwiththosecomputedfrommanyneighbors.Wetheninvestigatehowthenumberof(user-andproduct-related)neighboringreviewsaffectaccuracyimprovement.BothuserleniencyandproductpopularityfeaturesshownoimprovementonthePangdataset,asinTable~,becauseofthelimitedreviewsize,asweillustratedinAnalysis1.Itisalsoprobabllybecausethebiasesoftheuserandproductarenotsufficientinthisdataset,whichwillbeshowninAnalysis3.Table~showsthatuserleniencyfeaturesgreatlycontributetotheimprovement,whileproductpopularityhaslimitedinfluenceontheBlitzerdataset.PopularityfeaturesplayanimportantroleontheMaasdataset,asshowninTable~.Ingeneral,weexpectfurtherimprovementifwecollectsomeunlabeledreviewsfortheuser(orproduct).Wenoticedthatwhenthenumberofreviewswrittenbyauseroronaproductislargeenough(3|S_u(r)|7intheBlitzerdatasetand2|S_p(r)|5intheMaasdataset)havingmorereviewsforsuchusersandproductsdoesnotimprovetheaccuracyanyfurther.Consideringthatalarger|S_u(r)|or|S_p(r)|resultsinlowerspeedofeasiest-firstdecoding,asshownearlier,wecouldboundthenumberofreviewswrittenbyeachuseroroneachproducttosavecomputationwithoutlosingaccuracy.Analysis3:PolaritybiasintermsofuserorproductSinceourmethodtakesadvantageofbiaseddistributionsoverpolaritylabelsintermsofauserorproduct,largerthebiasinthedata,greaterourmethodcouldimproveclassificationaccuracy.Wethencomputethepolaritybiasintermsofusers(inshort,user_bias)andproducts(product_bias)asfollows:user_bias(u)&amp;=|N^+(u)-N^-(u)|N^+(u)+N^-(u),_bias(p)&amp;=|N^+(p)-N^-(p)|N^+(p)+N^-(p),align*where,N^+/-(u)orN^+/-(p)arethenumberofreviewswrittenbyuseruorwrittenonproductpwithpolarity+,-.Withthisdefinition,forinstance,useruwhoonlywritespositivereviewswillhaveuser_bias(u)=1whileuseruwhowritespositiveandnegativereviewsevenlywillbeassignedasuser_bias(u)=0.Highertheuser_biasandproduct_biasvaluesthatexist,themorepotentialourmethodhastoimproveaccuracy.Becauseofthedifferentcollectingmethods,thethreedatasetsweusedshowdifferentbiasproperties.Thedistributionsofuser_biasandproduct_biasvaluesareshowninFig.~andFig.~,respectively.Possibly,becausethePangdatasetiscollectedfromadiscussionnewsgroup,theusersinitarelessbiasedthanthoseintheBlitzerdataset,whichiscollectedfromageneraldomain.Assuch,theuserleniencyfeaturesextractedfromthePangdatasetmightbeunreliablesinceusersdonothavemuchbias.TheproductsintheMaasdatasetaremorebiasedthanthoseintheothertwodatasets.ThiscouldbeareasonuserinformationintheBlitzerdatasetandproductinformationintheMaasdatasetcontributethemosttoimprovement.AsillustratedinFig.~,whenthenumberofreviewsissmall,theaccuracyofourmethoddecreases.Thus,thesmallsizeofthePangdataset(Table~)andlowuserbiasvaluesseemtobethereasonsourmethodperformedbadlyonthisdataset.Analysis4:Impactoftrainingreviewswrittenbytestusers(ontestproducts)Aimingatrevealinghowwellourmethodworksforreviewswrittenbyunseen(emerging)usersoronunseen(emerging)products,weinvestigatedclassificationaccuracydependingonwhetherweobservedthesameuser(orproduct)inthetrainingdata.WeuseuserleniencyandproductpopularityfeaturesonthePangandBlitzerdatasets,whileweconsideronlyproductpopularityfeaturesontheMaasdataset.Thebaselineclassifierisexpectedtoestimatethelabelsofreviewswrittenbyseenusersoronseenproductsbetterthanthoseunseenones'becausetheclassifierlearnsn-gramsspecifictotheseusersortheproducts(andthusismoreeffectiveinclassification).Ontheotherhand,ourmethodperformedwellwhenmorereviewswereavailableforusersandproductsinthetestdata,sowecanexpectconsistentimprovementforseenandunseenusers(products).OnthePangdataset,asshowninTable~,thesmallernumberoftrainingdataresultedinpoorclassificationaccuracy,particularlyonunseenusersandunseenproducts.Unlikeintheothertwodatasets,lackofbiasesandthesmallnumberofreviewsseemtoberesponsible.AsshowninTable~,alargerimprovementwasobservedonreviewswrittenbytheseenusersintheBlitzerdataset.Wefoundthattheaveragenumberofreviewswrittenbyauserwasextremelylow(1.04reviews),andnoglobalfeatureswerefiredinmostofthesereviews.Weconsiderthismaybethemainreasonforthepoorimprovementinaccuracyonreviewswrittenbyunseenusers.OntheMaasdataset,asshowninTable~,theimprovementonthereviewswrittenonunseenproductsissignificantlylargerthanthereviewsonseenproducts.Thismayseemcounterintuitivesincewehaveasmallernumberofreviewswrittenontheunseenproducts(whichmeansfewerreliableglobalfeatures).Thereasonisprobablythatbaselineclassifierperformedpoorlyonreviewswrittenonunseenproducts,andhenceleftourmethodlargerspaceforimprovement.Analysis5:LearningcurvesUsingthesamesettingasinAnalysis1,wedividedthetrainingdataandinvestigatedtheeffectonaccuracy.Fig.~showstheaccuracywhenwechangethesizeofthetrainingdata.Ourmethodhasaclearadvantageoverthebaselinemethod,evenwhenthesizeofthetrainingdataissmall(1,800reviews).Inotherwords,wedonotneedmuchtrainingdatatolearnthecorrelationbetweenthelabelandglobalfeatures.Ourmethodtrainedwithhalfofthetrainingdataachievedahigheraccuracy(90.53%)thanthebaselinemethodtrainedontheentiredata(90.31%).Analysis6:ExamplesSomeexamplesaregiventoexplainhowourmodelworks.AsshowninTable~,ourmethodsuccessfullyclassifiessomereviewsthatarehardtoclassifycorrectlywhenonlytextualfeaturesareused.Inthefirsttwoexamples,weaknegativetextualfeaturesarefoundinthetestreview.However,sincethetwousersarelenientandproductofthefirstreviewisrelativelypopular(thesecharacteristicsarecapturedbyourproposedmethod),thetworeviewsshouldstillbegivenpositivelabels.Frequently,sentimentexpressedinsideareviewisnotobviousiftheclassifierdoesnotknowthemeaningofthewords(sometimes,evenahumanfindsithardtoidentifysentimentfromwords).AswecanseeinthethirdexampleinTable~,thebaselineclassifiercouldrecognizenoobvioussentimentevidencefromthetextualfeatures,whileourmethodclassifieditasnegativebydetectingthatitisaboutanotoriousproductandtheuseriscritical.Theseexamplesillustratethatourmodelcansuccessfullyexploittheuserandproductbiasestoimproveaccuracyofsentimentclassification.</subsection>
  <section title="Conclusion">Wepresentedamethodofcollectivesentimentclassificationthatcapturesandutilizesuserleniencyandproductpopularity.Differentfrommostofthepreviousstudiesthatareawareoftheuserandproductofthereview,ourmodeldoesnotassumethetrainingdatatocontainreviewswrittenbythesameuseroronthesameproductinthetestreviews.Todeterminealabelconfigurationforagivensetofreviews,weadoptedandcomparedtwostrategies,namely,easiest-firstdecodingandtwo-stagedecoding.Weconductedexperimentsonthreereal-worldreviewdatasetstocomparewithexistingmethods.Theproposedmethodperformedmoreaccuratelythanthebaseline,whichusesonlywordn-gramsasfeatureswhentheusersandproductsarebiasedonsentiment(whichisoftentrueinthereal-world).Italsooutperformedthestate-of-the-artmethodthatcombinespersonalizedclassifiers.Themorereviewsperuserorperproductareavailable,largertheimprovementourmethodgains.Thetwo-stagestrategyrunsintimethatislineartothenumberoftestreviews(expectedtobethesameorderofspeedasthebaselineclassifiers),whileachievingslightlylessaccuracycomparedwiththeeasiest-firststrategy.Weconsiderourproposedmethodasafirststeptowardmodelingmorecomplexpropertiesofreviews.Afutureextensionofthisworkistodetectauser'spreferenceforacertainkindofproduct.Wealsoplantousedualdecompositionasanadvanceddecodingstrategyforourcollectivesentimentclassification.authorswouldliketothankthereviewersfortheirvaluablecomments.Thisworkisbasedontheconferencepaperwithextendedexperimentanddetailedresultanalysis.document</section>
</root>
