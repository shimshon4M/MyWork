<?xml version="1.0" ?>
<root>
  <jtitle>階層的複数ラベル文書分類におけるラベル間依存の利用</jtitle>
  <jauthor>村脇有吾</jauthor>
  <jabstract>階層的複数ラベル文書分類においては，あらかじめ定義されたラベル階層の利用が中心的な課題となる．本稿では，複数の出力ラベル間の依存関係という，従来研究が用いてこなかった手がかりを利用する手法を提案する．これを実現するために，まずはこのタスクを構造推定問題として定式化し，複数のラベルを同時に出力する大域モデルと，動的計画法による厳密解の探索手法を提案する．次に，ラベル間依存を表現する枝分かれ特徴量を導入する．実験では，ラベル間依存の特徴量の導入により，精度の向上とともに，モデルの大きさの削減が確認された．</jabstract>
  <jkeywords>文書分類，構造推定問題，木，動的計画法，オンライン学習</jkeywords>
  <section title="はじめに">電子化されたテキストが利用可能になるとともに，階層的文書分類の自動化が試みられてきた．階層的分類の対象となる文書集合の例としては，特許，医療オントロジー，Yahoo!やOpenDirectoryProjectのようなウェブディレクトリが挙げられる．文書に付与すべきラベルは，タスクによって，各文書に1個とする場合と，複数とする場合があるが，本稿では複数ラベル分類に取り組む．階層的分類における興味の中心は，あらかじめ定義されたラベル階層をどのように自動分類に利用するかである．そもそも，大量のデータを階層的に組織化するという営みは，科学以前から人類が広く行なってきた．例えば，伝統社会における生物の分類もその一例である．そこでは分類の数に上限があることが知られており，その制限は人間の記憶容量に起因する可能性が指摘されている．階層が人間の制約の産物だとすると，そのような制約を持たない計算機にとって，階層は不要ではないかと思われるかもしれない．階層的分類におけるラベル階層の利用という観点から既存手法を整理すると，まず，非階層型と階層型に分けられる．非階層型はラベル階層を利用しない手法であり，各ラベル候補について，入力文書が所属するか否かを独立に分類する．ラベル階層を利用する階層型は，さらに2種類に分類できる．一つはラベル階層を候補の枝刈りに用いる手法（枝刈り型）である．典型的には，階層を上から下にたどりながら局所的な分類を繰り返す．枝刈りにより分類の実行速度をあげることができるため，ラベル階層が巨大な場合に有効である．しかし，局所的な分類を繰り返すことで誤り伝播が起きるため，精度が低下しがちという欠点が知られている．もう一つの手法はパラメータ共有型である．この手法では，ラベル階層上で近いラベル同士は似通っているので，それらを独立に分類するのではなく，分類器のパラメータをラベル階層に応じて部分的に共有させる．これにより分類精度の向上を期待する．これらの既存手法は，いずれも複数ラベル分類というタスクの特徴を活かしていない．複数ラベル分類では，最適な候補を1個採用すればよい単一ラベル分類と異なり，ラベルをいくつ採用するかの加減が人間作業者にとっても難しい．我々は，人間作業者が出力ラベル数を加減する際，ラベル階層を参照しているのではないかと推測する．例えば，科学技術文献を分類する際，ある入力文書が林業における環境問題を扱っていたとする．この文書に対して，「林業政策」と「林業一般」という2個のラベルは，それぞれ単独でみると，いずれもふさわしそうである．しかし，両者を採用するのは内容的に冗長であり，よりふさわしい「林業政策」だけを採用するといった判断を人間作業者はしているかもしれない．一方，別のラベル「環境問題」は「林業政策」と内容的に競合せず，両方を採用するのが適切を判断できる．この2つの異なる判断は，ラベル階層に対応している．「林業政策」と「林業一般」は最下位層において兄弟関係にある一方，「林業政策」と「環境問題」はそれぞれ「農林水産」と「環境工学」という異なる大分類に属している．このように，我々は，出力すべき複数ラベルの間にはラベル階層に基づく依存関係があると仮定する．そして，計算機に人間作業者の癖を模倣させることによって，（それが真に良い分類であるかは別として）人間作業者の分類を正解としたときの精度が向上することを期待する．本稿では，このような期待に基づき，ラベル間依存を利用する具体的な手法を提案する．まずは階層型複数ラベル文書分類を構造推定問題として定式化し，複数のラベルを同時に出力する大域モデルと，動的計画法による厳密解の探索手法を提案する．次に，ラベル間依存を表現する枝分かれ特徴量を導入する．この特徴量は動的計画法による探索が維持できるように設計されている．実験では，ラベル間依存の特徴量の導入により，精度の向上とともに，モデルの大きさの削減が確認された．本稿では，節で問題を定義したうえで，節で提案手法を説明する．節で実験結果を報告する．節で関連研究に言及し，節でまとめと今後の課題を述べる．</section>
  <section title="問題設定">階層型複数ラベル文書分類では，与えられた文書に対して，それをもっともよく表すラベルの集合MLを返す．ここで，Lはあらかじめ定義されたラベルの集合である．Lは図のように木構造で組織化されているとする．また，付与対象のラベルは葉のみであり，内部ノードはラベルとならないとする．図の場合，AA，AB，BAおよびBBがラベル候補となる．いくつかの記法を整理しておく．leaves(c)は，cの子孫である葉の集合を返す．例えば，leaves(A)=AA,AB．ただし，c自身が葉の場合は，leaves(c)=c．pcは親pから子cへの辺を表す．path(c)はROOTとcを結ぶ辺の集合を返す．例えば，path(AB)=ROOTA,AAB．また，tree(M)=_lMpath(l)とする．これはMを被覆する最小の部分木に対応する．例えば，tree(AA,AB)=ROOTA,AAA,AAB．文書xは(x)により特徴量ベクトルに変換される．特徴量として，例えば，文書分類タスクで一般な単語かばん(bag-of-words)手法を用いることができる．本タスクは教師あり設定であり，訓練データT=(x_i,M_i)_i=1^Tが与えられる．Tを用いてモデルを訓練し，これとは別のテストデータによって性能を評価する．</section>
  <section title="提案手法"/>
  <subsection title="大域モデル">ラベル間依存を利用するための準備として，入力文書xに対して出力ラベル集合Mを同時に推定する大域モデルを提案する．具体的には，階層的複数ラベル文書分類を構造推定問題とみなし，Mが作る部分木に対してスコアを定義する．[score^global(x,M)=w^global^global(x,tree(M))]w^globalは重みベクトルであり，訓練データを用いて学習すべきパラメータである．w^globalは，辺に対応する局所的な重みベクトルを連結することにより構成される．例えば，図の場合は[w^global=w_ROOTAw_ROOTBw_AAAw_AABw_BBAw_BBB]となる．特徴関数^globalは，文書xとtree(M)を入力とし，w^globalと同次元のベクトルを返す．具体的には，各pctree(M)に対応する部分ベクトルに(x)を，残りの要素に0を入れた特徴量ベクトルを返す．したがって，score^global(x,M)は以下のように書き換えられる．[score^global(x,M)=_pctree(M)w_pc(x)]この定式化により，w^globalが与えられた時，部分木のスコアを最大化するMを探す問題となる．[_Mscore^global(x,M)]</subsection>
  <subsection title="動的計画法による解探索">[t]indent=1.2em11pt[1]文書x,木のノードpラベル集合M,スコアsUpの各子ccが葉UU(c,w_pc(x))(M^,s^)(x,c)UU(M^,s^+w_pc(x))R(M^,s^)U|s^&gt;0Rが空R(M^,s^)ただし，(M^,s^)はUのなかでs^が最大のものM_(M^,s^)RM^s_(M^,s^)Rs^(M,s)algorithmicalgorithm大域モデルの，現在のパラメータw^globalのもとでの厳密解は，動的計画法により効率的に求められる．Algorithm~に動的計画法の擬似コードを示す．(x,p)は，pを根とする部分木の集合から，スコアが最大のものを再帰的に探索する．したがって，我々が呼び出すのは(x,ROOT)である．子cは，(1)cを根とするスコア最大の部分木を作るラベル集合，および(2)そのスコアとひも付けされている．ただし，葉のスコアは0である．pから見たcのスコアは，cの部分木のスコアと辺pcのスコアの和である（3--8行目）．pの部分木のスコアを最大にするには，正のスコアを持つcをすべて採用すればよい（10行目）．いずれの子も正のスコアを持たない場合は，最大のスコアを持つ子を1個採用する（11--13行目）．採用された子の集合により，pのラベル集合とスコアが決定される（14--15行目）．このアルゴリズムの拡張としては，上位N個の候補集合を出すというものが考えられる．木に対する動的計画法としては，構文解析よりもはるかに簡単なため，上位N個への拡張もさほど難しくない．</subsection>
  <subsection title="ラベル間依存の利用">以上の準備により，ラベル間依存を利用する条件が整った．ラベル間依存の捕捉は，大域モデルに対する特徴量の追加により実現される．具体的には，あるノードがいくつの子を採用しやすいかを制御する枝分かれ特徴量を導入する．枝分かれ特徴量は^BF(p,k)により表される．ここでpは根あるいは内部ノードであり，kはpが採用する子の数である．ただし，あらゆるkの値に対して特徴量を設けると疎になるため，あるRについて，R+1個(1,,Rもしくは&gt;R)の特徴量に限定する．さらに，ノードごとの特徴量だけでなく，すべての根あるいは内部のノードが共有するR+1個の特徴量も設ける．つまり，追加される特徴量は(I+1)(R+1)個であり，各ノードに対して2個の特徴量が発火する．ここで，Iはラベル階層における根および内部ノードの個数とする．[t]indent=1.2em11pt[1]9rUをsにより降順にソートした配列R^,s^0,M^k=1..(M,s)r[k]s^s^+s,M^M^MR^R^(M^,s^+w^BF^BF(p,k))(M,s)R^のなかでスコアsが最大の要素algorithmicalgorithmこの枝分かれ特徴量は，動的計画法による厳密解探索が維持できるように設計されている．この特徴量を組み込むには，Algorithm~の10--15行目をAlgorithm~で置き換えればよい．枝分かれ特徴量のスコアw^BF^BF(p,k)はkのみに依存する．そこで，まずは採用する子の数kによって候補をグループ分けし，各グループのなかでスコアが最大の候補を選ぶ（12--16行目）．最後に，異なるグループ同士を比較し，スコアが最大となる候補を採用する（17行目）．グループ内でスコアが最大の候補を選ぶには，子をスコア順に並べ，上位k個を採用すれば良い．候補のスコアは，pから見た各子のスコアと枝分かれ特徴量のスコアの和となる（15行目）．枝分かれ特徴量の導入により，ラベルの採否の判断が，ラベル同士の相対的な比較によって行われるようになる．節で触れた，「林業政策」と「環境問題」というラベルが付与された文書を再び例に挙げる．この文書に対して「林業一般」というラベルはそれほど不適切には見えないが，枝分かれ特徴量を持たないモデルは，「林業一般」を付与しない理由を，(x)に対応する重みですべて説明しなければならない．節で示すように，枝分かれ特徴量の重みは，一般に，負の値を持ち，ペナルティとして働く．また，子の数が増えるにつれてペナルティが増えるように学習される．したがって，子を2個採用するとよりペナルティがかかるので，「林業一般」に対応する重みを無理に引き下げることなく，相対的により適切な「林業政策」のみを採用することが可能となる．</subsection>
  <subsection title="大域訓練">大域モデルの訓練手法をここでは大域訓練と呼ぶ．本稿では，パーセプトロン系のオンライン学習アルゴリズムを採用する．具体的には，構造推定問題に対するPassive-Aggressiveアルゴリズムを用いる．Passive-Aggressiveを採用した理由としては，実装の簡便さ，バッチ学習と異なり，大量の訓練データに容易に対応可能なオンライン学習であること，次節で述べるように並列分散化が容易に実現できることが挙げられる．ただし，これは提案手法がパーセプトロン系アルゴリズムでしか実現できないことを意味せず，構造化SVM~を含む他の構造学習アルゴリズムの導入も検討に値する．[t]indent=1.2em11pt[1]訓練データT=(x_i,M_i)_i=1^T重みベクトルw^globalw^global0n=1..NTをシャッフル(x,M)TM_M,score^global(x,M)1-2|MM|/(|M|+|M|)&gt;0lscore^global(x,M)-score^global(x,M)+C,l^global(x,tree(M))-^global(x,tree(M))^2w^globalw^global+(^global(x,tree(M))-^global(x,tree(M)))algorithmicalgorithm大域モデルの場合の擬似コードをAlgorithm~に示す．ここで，Nは訓練の反復数を表し，パラメータCは1.0とする．現在のパラメータにおける厳密解は上述の動的計画法により求まる（5行目）．予測を誤った場合，正解ラベル集合を出力する方向に重みを更新する（10行目）．ここで，コストはモデル予測の誤り度合いを表し，重みの更新幅を変化させる．は，正解ラベル集合とシステムの出力の一致の度合いに基づいている．</subsection>
  <subsection title="大域訓練の並列分散化">大域訓練には学習が非常に遅いという欠点がある．ラベル集合の分類はラベル1個の2値分類とは比較にならないほど遅い．しかも，大域訓練はモデルを一枚岩とするため，モデルを局所分類器に分割して並列化することができない．そこで，繰り返しパラメータ混ぜ合わせ法~を用いて並列分散化を行う．基本的な考えは，モデルを分割する代わりに，訓練データを分割することで並列化を行うというものである．別々の訓練データ断片から学習されたモデル群を繰り返し混ぜ合わせることで収束性を保証している．Algorithm~に繰り返しパラメータ混ぜ合わせ法の擬似コードを示す．ここでN^は繰り返しパラメータ混ぜ合わせ法の反復数，Sは訓練データの分割数を表す．繰り返しパラメータ混ぜ合わせ法では，断片ごとに並列に訓練を行う．各反復の最後に，並列に訓練された複数のモデルを平均化する．次の反復では，この平均化されたモデルを初期値として用いる．繰り返しパラメータ混ぜ合わせ法はパーセプトロン向けに提案されたものである．しかし，が言及している通り，Passive-Aggressiveアルゴリズムに対しても収束性を証明することができる．[t]indent=1.2em11pt[1]訓練データT=(x_i,M_i)_i=1^T重みベクトルw^globalTをT_1,T_Sに分割w^global0n=1..N^s=1..Sw_s^global非同期的にAlgorithmを呼び出す．ただしいくつかの修正を加える．TをT_sで置き換える．w^globalを0ではなくw^globalで初期化する．反復数をN=1とする．非同期処理の終了を待つw^global1S_s=1^Sw_s^globalalgorithmicalgorithm</subsection>
  <section title="実験"/>
  <subsection title="データ">評価データとしてJSTPlusを用いる．JSTPlusは科学技術振興機構が作成している科学技術文献のデータベースである．各文書は，標題，抄録，著者一覧，ジャーナル名，分類コード一覧や，その他数多くの項目からなる．文書例を図に示す．実験では，標題と抄録を文書分類に用いるテキストとし，分類コードを付与すべきラベルとみなす．また，2010年の文献のうち，日本語の標題と日本語の抄録の両方を含むものを実験の対象とした．その結果，455,311件の文書を得た．これを409,892件の訓練データと45,419件の評価データに分割した．ラベル（分類コード）は3,209個からなり，これは4,030個の辺に対応する．ラベル階層は，根を除いて，最大で5階層となっている．ただし，いくつかの辺は中間層を飛ばす（例えば，第2層のノードの子が第4層にある場合がある）．各文書は平均で1.85個のラベルが付与されている（分散は0.85）．文書ごとの最大ラベル数は9である．文書の特徴関数(x)には以下の2種類の特徴量を用いる．ジャーナル名．2値特徴量で，各文書につき1個の特徴量が発火する．標題と抄録中の内容語．値は頻度．ただし，標題中の内容語の頻度は2倍する．内容語抽出には，形態素解析器JUMANおよび構文解析器KNPを用いた．まずJUMANによって各文を単語列に分割し，次にKNPが持つ規則を使って内容語にタグ付けした．各文書は平均で380文字を含んでいた．これは内容語としては120語に相当する．</subsection>
  <subsection title="モデル設定">大域訓練で訓練された大域モデル(GM-GT)について，枝分かれ特徴量(BF)を用いた場合と用いなかった場合を比較する．大域モデルの繰り返しパラメータ混ぜ合わせ法については，訓練データを10個の断片に分割し，反復数はN^=10とする．枝分かれ特徴量について，R=3とする．その他の比較対象として，従来研究を参考にして以下のモデルを用いる．</subsection>
  <subsubsection title="非階層型">非階層型(FLAT)はラベル階層を無視し，各ラベルlを文書xに付与すべきか否かを独立に決定する．そのために各lに対して2値分類器を用意する．分類器の実装手法としては，ナイーブベイズ，ロジスティック回帰，サポートベクタマシンなどが用いられてきたが，本稿では，提案手法との比較のためにPassive-Aggressiveアルゴリズム~を用いる．ラベルlに対する2値分類器は重みベクトルw_lを持つ．スコアw_l(x)が正のとき，lをxに付与する．ただし，文書に対して最低1個のラベルを付与する．そのために，いずれのラベルも正のスコアを取らない場合は，一番高いスコアを持つラベルを1個採用する．w_lを訓練するために，元の訓練データTを以下のようにしてT_lに変換する．[T_l=(x_i,y_i)|._i=1^T]各文書はラベルlを持つとき正例，そうでなければ負例となる．擬似コードをAlgorithm~に示す．ここで，パラメータCは1.0とする．訓練の反復数はN=10とする．なお，各2値分類器は独立なので，訓練は容易に並列化できる．[t]indent=1.2em11pt[1]訓練データT_l=(x_i,y_i)_i=1^T重みベクトルw_lw_l0n=1..NT_lをシャッフル(x,y)T_ll0,1-y(w_l(x))l&gt;0C,l(x)^2w_lw_l+y(x)algorithmicalgorithm</subsubsection>
  <subsubsection title="枝刈り型">枝刈り型(PRUNE)はラベル階層を利用する手法であり，ラベル階層に対応する2値分類器の集合を持つ．各2値分類器はラベル階層上の辺pcとひも付けされ，重みw_pcを持つ．w_pc(x)&gt;0は，xをpのいずれかの子孫に割り当てるべきであることを表す．これらの2値分類器も並列に訓練できる．パラメータCの値，訓練の反復数は非階層型と同じとする．枝刈り型には誤り伝播とよばれる問題が知られている．すなわち，階層上位の分類器による誤りから回復する手段がないため，累積的に誤りが作用する．誤り伝播を軽減するために様々な手法が提案されているが，煩雑さを避けるため，本稿では，Algorithm~に示す単純な実装を採用する．各ノードpにおいて，局所分類器が正のスコアを返す子すべてを採用する（4--7行目）．ただし，いずれの子も正のスコアを得ない場合は，一番高いスコアを得た子を1つ採用する（8--10行目）．この操作を葉に到達するまで繰り返す．2値分類器の訓練データT_pcの構築方法としては，以下の2種類を試す．ALL全訓練データを利用する．[T_pc=(x_i,y_i)|._i=1^T]各文書はcのいずれかの子孫のラベルが割り当てられていれば正例，そうでなければ負例となる．SIB正例はALLと同じだが，負例をcの兄弟の子孫が割り当てられている場合に限定する．[T_pc=(x,y)|.]こうすることで，全体として小さなモデルが学習される．なぜなら，数の多い階層下位の分類器に与えられる訓練データが小さくなるからである．従来研究ではSIBを採用する場合が多い．[t]indent=1.2em11pt[1]文書xラベル集合Mq[ROOT],,Mqが空でないpqの最初の要素を取り出す,,tpの子であるctt(c,w_pc(x))U(c,s)t|s&gt;0Uが空U(c,s),ただしcはpの子のなかで一番高いスコアsを持つ(c,s)Ucが葉MMccをqに追加algorithmicalgorithm</subsubsection>
  <subsection title="評価尺度">複数ラベル分類に対する評価尺度は数多く存在するが，大きく2種類に整理できる．1つは，文書を単位とした評価尺度で，しばしば用例ベースの尺度とよばれる．文書単位の尺度として，適合率(EBP)，再現率(EBR)およびF値(EBF)が以下のように定義される．EBP=1T_i=1^T|M_iM_i||M_i|EBR=1T_i=1^T|M_iM_i||M_i|EBF=1T_i=1^T2|M_iM_i||M_i|+|M_i|gather*ここでTはテストデータ中の文書数，M_iはi番目の文書の正解ラベル集合，M_iはそれに対応するシステムの出力とする．もう一つは，ラベルを単位とした評価尺度で，通常の適合率，再現率およびF値が用いられる．ただし，複数のラベルの集計方法としてマクロ平均とマイクロ平均がある．そのため合計で，LBMaP，LBMaR，LBMaF，LBMiP，LBMiRおよびLBMiFの6種類の尺度を用いる．最後に階層的な評価も行う．これは，出力ラベルがラベル階層上において正解と近いときに「部分点」を与えるものである．今回のように循環がない木構造を仮定した場合，適合率(hP)および再現率(hR)は以下のように定義される．hP=_i=1^T|tree(M_i)tree(M_i)|_i=1^T|tree(M_i)|hR=_i=1^T|tree(M_i)tree(M_i)|_i=1^T|tree(M_i)|gather*F値(hF)はhPとhRの調和平均として定義される．</subsection>
  <subsection title="結果">各種モデルの精度比較を表に示す．枝分かれ特徴量を組み込んだ大域モデル(GM-GT-BF)が7種類の尺度で最高精度を得た．枝分かれ特徴量なしのモデル(GM-GT)と比較すると，EBP，LBMaR以外の尺度でGM-GT-BFが上回り，すべてのF値を改善した．この改善は統計的に有意(p&lt;0.01)であった．大域モデルを非階層型(FLAT)と比較すると，適合率の改善が著しい一方，再現率に大きな差は見られない．2種類の枝刈り型(PRUNE)を比較すると，兄弟のみで訓練する場合()の方が全体的にやや良い精度が得られた．しかし，多くの尺度で非階層型に敗れており，従来研究の結果を再現する形となっている．誤り例を見ると，誤って採用したラベル，誤って採用しなかったラベルのいずれも，正解ラベルから離れて人間として改めて判断すると，必ずしも誤りとは言い切れない場合が少なくなかった．特に，該当文書にとって周辺的な話題を表すラベルをどこまで採用べきかを判断するのが難しかった．なお，モデル間の分類結果の差分からは，明確な誤り，改善の傾向をつかむのは困難であった．時間はテストデータの分類に要した時間であり，モデルの読み込み時間は含まない．予想される通り，枝刈り型が圧倒的に速い．GM-GT-BFはPRUNE-ALLと比較して約60倍の時間を要した．しかし，FLATと比較すると，階層を利用するにも関わらず，約18%の増加にとどまっている．これは，GM-GT-BFのモデルの大きさがFLATよりも約16%小さいことで説明できるかもしれない．モデルの大きさは重みベクトル中で，絶対値が10^-7より大きい要素の数とする．大きさはPRUNE-SIBが最小で，PRUNE-ALLが最大となった．GM-GT-BFがGM-GTよりも大きさを約9%削減したことは特筆に値する．訓練に用いたPassive-Aggressiveアルゴリズムには重みを0につぶそうとする仕組みがないことから，大きさが削減された理由は，学習過程で-GT-BFがGM-GTよりも予測を誤る回数が少なかったからと考えられる．このように，より小さなモデルでより高い精度が得られたことは，出力すべき複数ラベルの間にはラベル階層に基づく依存関係があるという我々の仮定を支持するものと考える．</subsection>
  <subsection title="議論">大域モデルの重みw^global自体は，大域訓練(GT)だけでなく，枝刈り型で用いた2値分類器群を連結することによっても構成できる(LT)．大域モデルの性質をさらに調べるために，こうしたモデルとの比較も行った．表に大域モデルの訓練方法の比較結果を示す．訓練データとしてSIBを用いた場合，極端に多くの候補を出力するようになり，その結果，極端に低い適合率と高い再現率を得た．SIBという限定されたデータで訓練された局所的な分類器に対して，大域モデルが未知の文書の分類を行わせたため，このような不安定な振る舞いとなった．一方，訓練データとしてALLを用いた場合，枝刈り型(PRUNE-ALL)から精度を大幅に向上させ，大域訓練とくらべても遜色のない精度が得られた．モデルの大きさや分類速度において大域訓練に劣りはするものの，大域モデルの最適化を行わずにこのような高精度が得られたことは興味深い．これは，訓練手法に改善の余地があることを示唆する．本稿では10並列による繰り返しパラメータ混ぜ合わせ法を用いたが，今後の最適化技術の発展が期待される．表に訓練データに対する精度を示す．訓練データに対しては非階層型(FLAT)が一番高い精度を示し，により局所訓練された大域モデル(GM-LT-ALL)がそれに続いた．大域訓練を行った場合(GM-GT-BF)との比較から，局所訓練が過学習をもたらしているとみられる．また，局所訓練と大域モデルの組み合わせにより，枝刈り型探索が誤りの主要因であることが確認できた．すなわち，PRUNE-ALLを訓練データに適用したところ，33%の文書について，PRUNE-ALLが出力したラベル集合よりも，正解ラベル集合の方が大域モデルにおいて高いスコアを持っていた．言い換えると，正しく探索を行えば犯さない誤りであった．ただし，この高い数値には過学習の影響も含まれており，同じ操作をテストデータに適用した場合は，割合は14%に下がったが正解Mよりも低いスコアを持つ場合，重みベクトルの更新が無効となってしまう．この問題に対処するための手法がいくつか提案されている~．．より詳細にモデルを調べるために，辺に分解した結果を示す．図はGT-BF，LT-ALL，LT-SIBの比較である．図(a)から(c)は辺に対応する局所ベクトルの大きさを示す．ここで，大きさの定義は表と同じである．辺を子の階層によって集約し，大きさを平均した結果を示す．一般に，上位階層ほど多数の有効な重みベクトルが必要となることが確認できる．GT-BFはLT-ALLよりも大きさが小さいが，辺ごとの大きさの比率は似通っている．LT-SIBと比較すると，GT-BFは上位階層では小さいが，下位階層では大きな有効重みベクトルを持つ．LT-SIBでは兄弟からの識別のみを考慮していたが，大域学習ではすべての辺が適切なスコアを返す必要があるため，有効重みベクトルがより大きくなったとみられる．図(d)から(f)は，各辺が得たスコアの絶対値の平均を表す．ここで，スコアは，テストデータに対するモデル出力から計算されたものである．これにより，どの階層の辺が強くモデル出力に影響しているかが推測できる．この結果から，上位階層ほど大きな影響を持つことがわかる．しかし，GT-BFは他とくらべて上位階層の影響が小さい．すなわち，GT-BFにおいては下位階層の辺が相対的に重要な役割を果たしている．枝分かれ特徴量に対応する重みを図にヒートマップとして示す．各要素の値は，親ノードに与えられた重み（ノードごとの重みと共有された重みの和）を平均したものである．平均化された値はすべて負となり，子の数が増えるにつれてペナルティが単調増加した．異なる階層間の重みの比較は，それらが重みベクトルの他の部分の値に依存するため難しい．しかし，下位ノードほど子の数に応じた重みの落差が大きいという結果は，階層上近いラベル候補同士ほど強い競合関係にあるという我々の仮説を支持しているようにみえる．最後に，訓練データおよび評価データの正解ラベルについて，正解ラベルを被覆する最小の部分木を作り，親が採用する子の数を調べた．採用した子の数が複数である割合は，根で34.9%，第1層で10.1%，第2層で4.6%，第3層で1.5%，第4層で0.6%であり，下位ノードほど強い競合関係にあることが確認できた．</subsection>
  <section title="関連研究">階層型文書分類において，枝刈り型が非階層型にしばしば敗れることが報告されており，誤り伝播を軽減するために様々な手法が提案されてきた．は枝刈り探索時の枝刈り基準を緩め，最後に候補の枝刈りを行う．すなわち，Algorithm~の7行目の閾値を0から-0.2などに引き下げて，より多くの候補を採用する．最後に，各候補について，根から葉までのパスの（シグモイド関数で変換された）局所スコアの和を取り，これに閾値を設定することによって出力ラベルを絞り込む．S-cut~は，一律の閾値を用いるのではなく，局所分類器ごとに閾値を設定する手法である．R-cutは上位r個の候補を採用する手法で，選び方には大域的手法と局所的手法がある．は採用されたラベル候補をメタ分類器にかけ，最終的な出力を決定する．メタ分類器の特徴量としては，根から葉までの局所スコアやその累積などを用いる．本稿ではこれらを総称して後付け補正とよぶ．後付け補正では，いずれもモデルあるいは探索が本質的に不完全であることを想定し，追加のパラメータによる補正を行なっている．そうしたパラメータは，人手で設定するか，あるいは訓練データとは別に開発データを用意して推定しなければならず煩雑である．一方，提案手法には後付け補正は不要であり，モデル自体の改善に専念できる．ラベル階層を下から上へ探索しながら候補を探すという点で，提案手法と似た手法がにより提案されている．しかし，彼らの手法では，大域モデルも大域訓練も用いられていない．代わりに，階層下位の分類器のスコアが上位の分類器のメタ特徴量として用いられている．分類器の訓練は局所的に行われ，煩雑な交差確認を必要とする．本稿ではあらかじめ定義されたラベル階層を利用した．そうした手がかりがない場合にラベル間依存を捉えるための手法も研究されている．は，出力すべきラベル集合中のラベルペアを特徴量に組み込んでいる．本稿のようにラベル階層が利用できる場合は，それをもとに限られた数のラベル同士の関係を考慮すればすむ．一方，ラベル階層がない場合は，モデルはすべてのラベルペアを考慮する必要があり，訓練および解探索に大きな計算コストを要する．こうしたモデルの検証は，ラベルの異なり数が数十程度のデータセットを用いて行われてきた．ラベルの異なり数が大きな場合について，は，ラベル集合を低次元の直交座標系に写像し，この空間上で非階層型の分類器を学習する手法を提案している．予測時には，分類器の出力を元の空間へ写像するという自明でない復号が必要となる．は，ラベル階層を組み込むために，木あるいは有向非循環グラフの制約を満たすような復号手法を提案している．</section>
  <section title="おわりに">本稿では，階層型複数ラベル文書分類を構造推定問題として定式化し，動的計画法による厳密解探索方法，大域訓練，ラベル間依存をとらえる枝分かれ特徴量を提案した．枝分かれ特徴量はモデルの大きさを削減するとともに精度の向上をもたらした．この結果は，人間作業者が複数のラベル候補から出力を選択する際，ラベル階層に基づいて，競合する候補の相対的な重要性を考慮していることを示唆する．今後の方向性としては，枝分かれ特徴量以外によってラベル間依存をとらえる方法を探究するというものが考えられる．例えば，「〜その他」や「〜一般」といったラベルは，他のラベルとの関係において特殊な振る舞いをすると予想される．また，本稿では葉のみが付与対象ラベルという問題設定を行ったが，従来研究には内部ノードも付与対象である場合を扱ったものがある~．こうした内部ノードの振る舞いも特殊である．内部ノードを採用するとき，その子孫へのラベル付与を行わないことが多い．さらに，木構造から有向非循環グラフへの提案手法の一般化も課題である．</section>
</root>
