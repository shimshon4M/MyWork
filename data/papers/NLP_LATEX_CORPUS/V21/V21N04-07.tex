    \documentclass[english]{jnlp_1.4_rep}

\usepackage{jnlpbbl_1.3}
\usepackage[dvipdfm]{graphicx}
\usepackage{hangcaption_jnlp}
\usepackage{udline}
\setulminsep{1.2ex}{0.2ex}
\usepackage{amsmath}
\usepackage{otf}
\usepackage{array}
\usepackage{ascmac}


\usepackage{url}
\newcommand{\word}[1]{}
\newcommand{\dom}[1]{}
    \newcommand{\bangou}[1]{}
    \newcommand{\Bangou}[1]{}
    \newcommand{\BAngou}[1]{}
    \newcommand{\BANgou}[1]{}

\Volume{21}
\Number{4}
\Month{September}
\Year{2014}

\received{2007}{12}{26}
\revised{2008}{4}{11}
\accepted{2008}{5}{11}

\setcounter{page}{817}

\etitle{Construction of a Domain Dictionary for Fundamental Vocabulary and its Application to Automatic Blog Categorization Using Dynamically Estimated \\ Domains of Unknown Words\footnotetext{\llap{*~}This is an augmented edition of \cite{Hashimoto:Kurohashi:2007,Hashimoto:Kurohashi:2008}; the authors augmented the material with additional experiments and detailed descriptions.}\footnotetext{\llap{**~}This article has been partially revised for better understanding of overseas readers.}}
\eauthor{Chikara Hashimoto\affiref{Author_1} \and Sadao Kurohashi\affiref{Author_2}} 
\eabstract{
The semantic relations between words are essential for natural language
understanding.
Toward deeper natural language understanding, we semi-automatically
 constructed a domain dictionary that represents the domain relations
 between fundamental Japanese words.
Our method does not require a document collection.
As a task-based evaluation of the domain dictionary,
we categorized blogs by assigning a domain for each
word in a blog article and categorizing it as the most dominant \mbox{domain.}
Thus, we dynamically estimated the domains of unknown words,
(i.e., those not listed in the domain dictionary),
resulting in our blog categorization achieving an accuracy of 94.0\%
(564/600).
Moreover, the domain estimation technique for unknown words achieved an 
accuracy of 76.6\% (383/500).
}
\ekeywords{domain, lexicon, blog, text categorization, unknown words' domain}

\headauthor{Hashimoto and Kurohashi}
\headtitle{Construction of Domain Dictionary and Blog Categorization}

\affilabel{Author_1}{}{National Institute of Information and Communications Technology}
\affilabel{Author_2}{}{Graduate School of Informatics, Kyoto University}

\Reprint[T]{Vol.~15, No.~5, pp.~73--97}

\begin{document}

\maketitle

\section{Introduction}

For deep semantic processing of language, a thesaurus is an
indispensable resource.
Thesauri represent is-a relations between words. 
An is-a relation is, so to speak, a ``vertical'' relation, and
we believe that ``horizontal'' semantic relationships between
words are also required to fully capture the meaning of words.
Accordingly, we propose the \emph{domain} relation in this paper.
For example, \word{textbook} and \word{teacher} belong to the
\dom{education} domain, whereas \word{kitchen knife} and
\word{surgical knife} belong to the \dom{diet} 
\dom{health} domains, respectively.
We have constructed a domain dictionary in which 
we have assigned domains to approximately 30,000
fundamental Japanese words are given appropriate domains.

Domain information enables a more natural semantic classification of
words. 
For example, a thesaurus would regard \word{textbook} as a book and
\word{teacher} as a profession; however both words are part of the same 
domain, i.e., the \dom{education} domain.
In contrast, a thesaurus would regard both \word{kitchen knife}
and \word{surgical knife} as knives; however, they should be distinguished 
by their domains, i.e., the \dom{diet} domain for the former and
the \dom{health} domain for the latter.

Domain information has been used for various natural language
processing (NLP) tasks.
We use it for blog categorization, but it has also been used for
document filtering
\cite{Liddy:Paik:1993},
word-sense disambiguation
\cite{Rigau:Atserias:Agirre:1997,Tanaka:Bond:Baldwin:Fujita:Hashimoto:2007},
and
machine translation
\cite{Yoshimoto:Kinoshita:Shimazu:1997,Lange:Yang:1999}.

Our method for constructing the domain dictionary is semi-automatic.
First, we automatically construct a domain dictionary by measuring 
the degree to which a target fundamental word and a domain keyword
are semantically related.
Then, we manually correct the automatically constructed results.
For measuring semantic relatedness, we exploit hit counts returned
by a web search engine.
We adopt a semi-automatic process because
semantic information about fundamental words is crucial for many NLP
tasks
and must therefore be highly accurate.
However, it is difficult to achieve such high accuracy using a
fully automatic process.
But, it is also undesirable to manually define all semantic relations
because of the high cost, low consistency, and low maintainability of
manual work.
Hence, we decided to manually correct reasonably accurate and
automatically constructed results.\footnote{
For a similar reason, Kyoto University Text Corpus (http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?Kyoto\linebreak[2]\%20University\%20Text\%20Corpus)
has been constructed in a semi-automatic way using a precise Japanese dependency parser 
called the Kurohashi-Nagao Parser (KNP) (http://nlp.ist.i.kyoto-u.ac.jp/EN/).}

Our domain dictionary is the first publicly available Japanese domain
dictionary.
Our domain dictionary construction method only requires an
access to a web search engine; no training data or any other language
resource is required.

We also propose a method for dynamically estimating a domain for
unknown words on-the-fly using a web search engine.

As a task-based evaluation of our domain dictionary construction method
and our domain estimation method for unknown words, we conduct automatic
blog categorization.
We categorize blog articles into one of the domains
assumed using the domain information of words in blog
articles. 
As a result, we achieve 94.0\% accuracy for blog
categorization and 76.6\% for unknown words' domain estimation.

In this study, our domain dictionary contains only fundamental words
used in daily life and does not contain technical
terms.\footnote{
To be more precise, about 30,000 content words from the JUMAN 
dictionary
\cite{Kurohashi:Nakamura:Matsumoto:Nagao:1994} (version 5.1)
were used for constructing our domain dictionary.}

The rest of the paper is organized as follows.
In Section \ref{sec:two-issues}, we describe issues related to 
the construction of the domain dictionary.
Section \ref{sec:domain-dictionary-construction} presents our domain
dictionary construction method.
Section \ref{sec:resulting-domain-dictionary} reports the details of the
domain dictionary we constructed.
In Section \ref{sec:blog-categorization}, we describe our blog
categorization method,
and in Section \ref{sec:domain-estimation-of-unknown-words}, we describe 
our domain estimation method for unknown words. 
In Section \ref{sec:evaluation}, we report evaluation results of our
blog categorization method and domain estimation method for unknown
words. 
In Section \ref{sec:related-work}, we compare our study with previous
ones, and in Section \ref{sec:conclusion},
we conclude the paper.

\section{Two Issues}
\label{sec:two-issues}

During creation of semantic domains, two issues must be addressed: which
domains to create and how to associate words with domains without
document collections.

Ideally, the choice of semantic relations should be based on how people
understand and categorize the real world, understanding of which is
really a challenging problem.
In this study, without getting too involved in the problem,
we adopt simple domain categories 
(listed in Table \ref{tab:domains})
that most people can agree upon.
It has been created based on web directories, such as the Open
Directory Project
(http://www.dmoz.org)
with some adjustments.
In addition, \dom{nodomain} was prepared for 
words not belonging to any particular domain.

\begin{table}[b]
\caption{Domains in this study}
\label{tab:domains}
\input{11table01.txt}
\end{table}

The association of words with domains can be done using standard keyword
extraction techniques;
identifying words that represent a domain from the document collection of
the domain using statistical measures like TF*IDF and matching between
those extracted words and the target fundamental words.
However, document collections of common domains, such
as those assumed in this study, are harder to obtain
than those of technical or specialized domains.
Web directories like the Open Directory Project
 or Yahoo!
JAPAN (http://dir.yahoo.co.jp) might seem like good sources of
 such document collections.
However, when we tried collecting web pages registered in Yahoo! JAPAN,
we found that most of them were index pages with little text
content from which reliable keywords could not be extracted.
Though we further tried following the links on the index pages to acquire
enough text content and extracting words from them,
some of those words turned out to be site-specific rather than
domain-specific because many pages were collected from a
particular web site.\footnote{
One more obstacles faced while using web directories as document
collections is the fact that 
on web pages, there are sometimes banner advertisement
links having little to do with a target domain.}
Therefore, we had to develop a method not requiring
document collections of domains.
The next section details our method.


\section{Domain Dictionary Construction}
\label{sec:domain-dictionary-construction}

To identify which domain a fundamental word is associated with, we use
manually prepared keywords for each domain
rather than document collections.

First, each domain is represented by 20 to 30 keywords, as
described in Section \ref{prepare_kw}.
Then, an association score between each fundamental word and a domain
($A_d$ score) is calculated by
summing up the association scores between the fundamental word and the
keywords of the domain ($A_k$ scores). 
After calculating $A_d$ scores between the fundamental word and all domains,
the fundamental word is associated with the domain with the highest $A_d$ score.
This process is repeated for each fundamental word.
Then,
some of the fundamental words
can be
reassociated with \dom{nodomain} if their $A_d$ scores are low.
This association process is described in Section \ref{associating}.
Finally, 
as described in Section \ref{manual-correction},
we corrected the association results for the association dictionary.
Figure~\ref{whole-process} illustrates the complete construction process.

\begin{figure}[b]
\begin{center}
\includegraphics{21-4ia11f1.eps}
\end{center}
\hangcaption{Whole Construction Process (``JFW'' and ``kw'' stand for Japanese fundamental word and keyword respectively)}
\label{whole-process}
\end{figure}

Note that the method for constructing the domain dictionary is
independent from the 12 domains specified in Table \ref{tab:domains};
you can use the same method with different domains.


\subsection{Preparing Keywords for each Domain}
\label{prepare_kw}

Keywords for each domain are collected manually from the
list of words appearing most frequently on the web.
To be precise, 
starting from the top of the list,
words are chosen as keywords of a domain if they
represent the domain.
This process is repeated
until 20 to 30 words are collected per domain.
If we are uncertain about which domain a word should belong to,
we ignore the word.\footnote{
This has been conducted by one of the authors.
In our future work, we plan to examine
how (in)consistent this judgment is between annotators.
}
Table \ref{ex_kw} shows examples of the keywords.

\begin{table}[b]
\caption{Examples of Keywords for each Domain}
\label{ex_kw}
\input{11table02.txt}
\end{table}

If you adopt different domains, you must collect appropriate keywords
for each domain yourself.
However, after that, the same procedure can be applied to them.


\subsection{Associating Fundamental Words with Domains}
\label{associating}

A fundamental word is associated with the domain with the highest $A_d$ score.
The $A_d$ score of the domain is calculated by summing up the top five $A_k$
scores for the domain.\footnote{
An experiment we conducted showed that this yielded a better
results than summing up all $A_k$ scores.}
An $A_k$ score, which is defined between a fundamental word and a keyword of a
domain, is a measure that shows how strongly the fundamental word and the keyword
are related.
Assuming that two words are related if they co-occur more often
than chance in a corpus, we adopt $\chi^2$ statistics to calculate
an $A_k$ score and use web pages as a corpus.
The number of co-occurrences is approximated by the number of search
engine hits when the two words are used as queries.\footnote{
We used Yahoo! JAPAN (www.yahoo.co.jp).} 
We chose to combine $\chi^2$ statistics and web pages because
\citeA{Sasaki:Sato:Utsuro:2006} reports that this
combination provides
good results during estimation of term relatedness.

Following Sasaki et al. \citeyear{Sasaki:Sato:Utsuro:2006}, 
the $A_k$ score between a fundamental word ($jw$) and a keyword ($kw$) is
\[
A_k(jw,kw)=\frac{n(ad-bc)^{2}}{(a+b)(c+d)(a+c)(b+d)} ,
\]
where $n$ is the total number of Japanese web pages\footnote{
We used 10,000,000,000 as $n$.} and
\begin{align*}
  a & = hits(jw\ \&\ kw), & b & = hits(jw) - a, \\
  c & = hits(kw) - a, & d & = n - (a + b + c). 
\end{align*}
Note that $hits(q)$ represents the number of search engine hits when $q$
is used as a query. 


\subsection{Reassociating Fundamental Words with NODOMAIN}
\label{nodomain}

Fundamental words not belonging to any particular domain, (i.e.,
fundamental words with low $A_d$ scores)
should be reassociated with \dom{nodomain}.
Accordingly, a low $A_d$ score threshold must be established,
below which fundamental words should be reassociated.
A pilot study we conducted showed that the threshold for a fundamental
word ($jw$)
needs to be changed according to $hits(jw)$;
the higher the $hits(jw)$, the higher the threshold should be.

To establish a function that takes $jw$ and returns the appropriate
threshold for it, the following semi-automatic process
is required after
all fundamental words are associated with domains:
\begin{itemize}
 \item[(i)]  Sort all tuples of $\langle$ $jw$, $hits(jw)$,
the highest $A_d$ of the $jw$ $\rangle$ by $hits(jw)$.\footnote{
Note that we acquired the number of search engine hits and the $A_d$
score for each $jw$ 
in the process \bangou{2} in Figure~\ref{whole-process}
}
 \item[(ii)] Segment the tuples.
We segmented them into 130.
 \item[(iii)] For each segment, manually extract tuples 
for which $jw$ should be associated with one of the 12 domains and those
for which $jw$
should be deemed as \dom{nodomain}.\footnote{
We extracted five tuples for those associated with one of the domains
and five for those associated with \dom{nodomain}, respectively.}
Note that the former tuples usually have higher $A_d$ scores than the
latter tuples.
 \item[(iv)] For each segment, identify a threshold that 
distinguishes
between the former tuples and the latter tuples by their $A_d$ scores.
\pagebreak
At this point, pairs of the number of hits (represented by each segment)
and the appropriate threshold for the number of hits should be obtained.
 \item[(v)] Approximate the relation between the number of hits and
its threshold by a linear function using the least square method.
This linear function provides the appropriate threshold for
each $jw$. 
\end{itemize}
Figure~\ref{reassociation-nodomain} illustrates the process from
\textbf{(i)} to \textbf{(iv)}.

\begin{figure}[t]
\begin{center}
\includegraphics{21-4ia11f2.eps}
\end{center}
\caption{Reassociating fundamental words with \dom{nodomain}: \textbf{(i)} to \textbf{(iv)}}
\label{reassociation-nodomain}
\end{figure}


\subsection{Performance of the Domain Association Method}

This subsection reports the performance of the proposed method.
We applied the method 
to fundamental words installed in the Japanese morphological analyzer JUMAN
\cite{Kurohashi:Nakamura:Matsumoto:Nagao:1994}.
To be more precise, we used 26,658 words comprising commonly used nouns and
verbs as fundamental words.

For our evaluation, we sampled 380 pairs of a fundamental words and domains,
and measured the accuracy of our method, i.e., we counted the number of
pairs for which the 
association was correct.

For comparison, a baseline was defined, which
was the accuracy when all fundamental words were associated with \dom{nodomain}.
This is because a pilot study we conducted showed that more than half of
fundamental words tended to be associated with \dom{nodomain}. 

As a result, the proposed method attained an accuracy of 81.3\%
(309/380), whereas the baseline provided an accuracy of 69.5\% (264/380).
This result shows that our method works very well.


\subsection{Allowing Multiple Domains for Fundamental Word}
\label{allow-multiple}

This subsection shows the extended version of the proposed method, in
which a fundamental word can be associated with more than one domain.
In fact,
some fundamental words should be associated with multiple domains.
For example,“大学院”(\textit{graduate school}) seems to be associated with both
\dom{education} and \dom{science}, and“登山”(\textit{climbing}) belongs to
both \dom{recreation} and \dom{sports}.
However, the proposed method is designed to associate a fundamental word
with only one domain.

The extended method associates a fundamental word with any domain that
meets the following two conditions:
\begin{itemize}
 \item[(i)] The domain's $A_d$ score is above the threshold described
in Section \ref{nodomain}.
 \item[(ii)] The domain's $A_d$ score is close to the highest $A_d$
score, as formalized as below.
\[
 \frac{\text{the highest}~A_d - \text{the domain's}~A_d}{\text{the highest}~A_d} < 0.01
\]
\end{itemize}

With the extended method, an additional 814 pairs of fundamental words
and domains were identified,
but the accuracy dropped to 78.6\% (308/392).\footnote{
To improve the domain dictionary using manual correction,
we adopted a method of associating a fundamental word with only one
domain for greater accuracy.
Fundamental words that should be associated with multiple domains were
then manually associated,
as described in Section \ref{manual-correction}.}


\subsection{Manual Correction}
\label{manual-correction}

Generally speaking, manual annotation of linguistic data can be
arbitrary and thus should be subject to guidelines.
Among the guidelines that we established, this subsection describes
the criteria for which words to associate with multiple domains,
how to deal with polysemous words, 
and which words to associate with \dom{nodomain}.

\paragraph{Words Belonging to Multiple Domains: }
Our principal policy is that 
simpler is better, and hence we avoid associating a fundamental word with
multiple domains
as much as possible.
Fundamental words to associate with multiple domains are restricted to those that
are EQUALLY relevant to more than one domain.
As mentioned in Section \ref{allow-multiple}, 
“大学院”(\textit{graduate school}) (\dom{education} and \dom{science}) and
“登山”(\textit{climbing}) (\dom{recreation} and \dom{sports}) are such
examples. 
In contrast,“ゴルフ”(\textit{golf}), which some people might consider
\dom{recreation} but is more directly related to
\dom{sports}, so we associate it with only \dom{sports}.
Similarly,“微分”(\textit{mathematical derivation}) is associated with
only \dom{education} and not \dom{science}.

\paragraph{Dealing with Polysemous Words: }
We associated each polysemous word with multiple domains, 
one for each meaning of the word. 
For example,“ボール”can be either
\textit{ball} or \textit{bowl}, which we associated with
\dom{sports} and \dom{diet}, respectively.

\paragraph{Criteria for NODOMAIN: }
We tried to be as conservative as possible.
We associated a word with \dom{nodomain}
if either 
\textbf{(1)} people are likely to disagree about which domain is
appropriate, or
\textbf{(2)} more than four domains could be
associated with it, rather than forcing them into a domain.
For example,“委員”(\textit{committee member}) is so vague that it can be
associated with \dom{government}, \dom{business}, \dom{education}, and more.
We associated words like this with \dom{nodomain}.


\section{Resulting Domain Dictionary \label{sec:resulting-domain-dictionary}}

Table \ref{breakdown_domain-dictionary} shows the breakdown of the
 words in the resulting domain dictionary.\footnote{
These numbers are drawn from our original Japanese journal paper
 published in 2008.
The breakdown of the domain information in the current
 JUMAN's dictionary is different.}
The most common domain is \dom{nodomain} due to the manual annotation
criteria described in the previous section.
The number of words associated with multiple domains is 787
(26.2\%). 

\begin{table}[b]
\caption{Breakdown of the Domain Dictionary}
\label{breakdown_domain-dictionary}
\input{11table03.txt}
\end{table}

We incorporated the resulting domain dictionary into the Japanese
morphological analyzer JUMAN, which is available at
http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN.
JUMAN uses the domain dictionary when performing the morphological
analysis, including tokenization, POS-tagging, and domain-tagging.


\section{Blog Categorization}
\label{sec:blog-categorization}

We categorized blog articles into one of our domains
(Table \ref{tab:domains}) as a task-based evaluation of our method of
constructing a domain dictionary.
Our blog categorization method is quite simple;
it annotates words in an article with domains and categorizes the article
as the most dominant domain.
The procedure is as follows (Figure~\ref{categorization-pic}):
\begin{itemize}
 \item[\Bangou{1}] Extract words from an article.
 \item[\Bangou{2}] Assign domains and IDFs to the words.
 \item[\Bangou{3}] Sum up IDFs for each domain.
 \item[\Bangou{4}] Categorize the article in the domain with the highest
	    IDF.\footnote{If the domain of the highest IDF is
	    \dom{nodomain}, the article is categorized as the second
	    highest domain.}
\end{itemize}

\begin{figure}[b]
\begin{center}
\includegraphics{21-4ia11f3.eps}
\end{center}
\caption{Categorization Process}
\label{categorization-pic}
\vspace{-1\Cvs}
\end{figure}

We used three kinds of words in our experiment (described in
Section \ref{sec:evaluation}):
\begin{itemize}
 \item Fundamental words only, such as“業者”(\textit{trader})
 \item Fundamental and unknown words, such as“コンプライアンス”
       (\textit{compliance})
 \item Fundamental, unknown, and compound words, such as“贈収賄-容疑”
       \footnote{The hyphen is a word boundary.}
       (\textit{bribery-allegation})
\end{itemize}
Compound words are not in the domain dictionary like unknown words.

In extracting fundamental words only, compound words are
split and fundamental words that constitute the compounds are extracted
separately. 
For example,“贈収賄容疑”is split into“贈収賄”(\textit{bribery}) and
“容疑”(\textit{suspicion}), and they are used for categorization
independently. 

As for \Bangou{2}, in which domains and IDFs are assigned, the IDF of
word ($w$) is calculated as follows:\footnote{
We used 10,000,000,000 as the total number.
}
\begin{equation}
\mathrm{IDF}(w)=\mathit{log}\frac{\text{Total \# of Japanese web pages}}{\text{\# of hits of}~w}
\end{equation}

Also, fundamental words are assigned their domains and
IDFs by the domain dictionary,\footnote{
We assigned each word of the
domain dictionary an IDF and domain in advance.} 
whereas those for unknown and compound
words are dynamically estimated  by the method
described in Section \ref{sec:domain-estimation-of-unknown-words}.

In what follows, we refer to both unknown and compound words as
``unknown words.'' 
When we need to distinguish between them, we call the former as
``simplex unknown words'' and the latter as ``complex unknown words.''


\section{Domain Estimation for Unknown Words}
\label{sec:domain-estimation-of-unknown-words}

We dynamically estimate the domain (and IDF)
of an unknown word using the web.
The intuition behind this is that the web shows how unknown words are
used and interpreted in the world, which provides important clues for
identifying the domains with which they should be associated with.
More specifically, we use Wikipedia articles and snippets in web
search results in addition to the domain dictionary.

The estimation process is as follows (Figure~\ref{estimation-pic}):
\begin{itemize}
 \item[\BAngou{1}] Search the web for an unknown word, acquire the top
	    100 records, and calculate the IDF of the
	    word.\footnote{Note that calculating the IDF of a word
	    requires only the number of hits for it, which the search
	    results provide.}
 \item[\BAngou{2}] Get the Wikipedia article about the word from the
	    search results if one exists, estimate the domain of the word 
	    using the Wikipedia-strict module (Section \ref{wikipedia}),
	    and exit the process.
 \item[\BAngou{3}] If no Wikipedia article about the word is found,
	    then obtain any Wikipedia article among the top 30 of the search
	    results, estimate the domain of the word using the
	    Wikipedia-loose module (Section \ref{wikipedia}), and exit
	    the process.
 \item[\BAngou{4}] If no Wikipedia article is found among the top 30 of the
	    search results, then remove all advertisement snippets from the
	    search results (Section \ref{snippets}).
 \item[\BAngou{5}] If a snippet remains in the search results, identify
	    the best domain for the word using the Snippets module
	    (Section \ref{snippets})
	    and then exit the process.
 \item[\BAngou{6}] If no snippet is left but the unknown word is a
	    compound word containing fundamental words, then identify
	    the best domain for the word using the Components module
	    (Section \ref{components}),
	    and then exit the process.
 \item[\BAngou{7}] If no snippet is left and the word is not a compound
	    word containing fundamental words, then the process is a
	    failure.
\end{itemize}

\begin{figure}[t]
\begin{center}
\includegraphics{21-4ia11f4.eps}
\end{center}
\caption{Domain Estimation Process}
\label{estimation-pic}
\end{figure}

The next three subsections describe the Wikipedia-strict,
Wikipedia-loose, Snippets, and Components modules in detail.
All of them share the idea that the domain to which an unknown word
should belong is the most
dominant domain in a text they deal with like a Wikipedia article,
snippets in search results, or text related to component words of the
unknown word.
Also, all of them use only fundamental words in the text to estimate
domains.


\subsection{Wikipedia-strict and Wikipedia-loose Modules}
\label{wikipedia}

The two Wikipedia modules follow this procedure:
\begin{itemize}
 \item[\BANgou{1}] Extract only fundamental words from the Wikipedia
	    article.
 \item[\BANgou{2}] Assign domains and IDFs to the fundamental words
	    using the domain dictionary. 
 \item[\BANgou{3}] Sum up IDFs for each domain.
 \item[\BANgou{4}] Assign the domain with the highest IDF to the unknown
	    word. If the domain with the highest IDF is \dom{nodomain},
	    the second highest domain is chosen for the unknown word
	    using the condition below:
    \begin{equation}
     \frac{\text{The second highest IDF}}{\text{\dom{nodomain}'s IDF}}>0.15
    \end{equation}
\end{itemize}

Figure~\ref{fig:Wikipedia-pic} shows the procedure of the domain estimation
for an unknown word using the Wikipedia-strict or Wikipedia-loose module.

Each of the two Wikipedia modules assigns a domain in about 10 seconds
(from \BAngou{1} to \BAngou{2} or \BAngou{3}).\footnote{We
used Dell PowerEdge 830 (Pentium D 3.00GHz).}

\begin{figure}[t]
\begin{center}
\includegraphics{21-4ia11f5.eps}
\end{center}
\caption{Domain Estimation by Wikipedia-strict or Wikipedia-loose Module}
\label{fig:Wikipedia-pic}
\vspace{-0.5\Cvs}
\end{figure}


\subsection{Snippets Module}
\label{snippets}

The Snippets module takes the snippets left in the
search results after advertisement websites (\BAngou{5}) are removed
as input.
We remove snippets in which the keywords in Table \ref{corporate-words}
appear more than once.
These words were collected from the error analysis of our preliminary
experimental result.
Removing advertisement snippets is indispensable
because they bias estimation towards
\dom{business}.

The Snippets module is the same as the two Wikipedia modules,
except that it extracts fundamental words from residual snippets in the
search result.

The Snippets module assigns a domain in about 6 seconds (from \BAngou{1}
to \BAngou{5}).\footnote{Though the Wikipedia modules are executed before
this module, the latter takes a shorter time. This is because it uses already
acquired snippets, while the Wikipedia modules have to retrieve an
article from the web.}


\subsection{Components Module}
\label{components}

This module is basically the same as the others except that it extracts
fundamental words from the unknown word itself.
For example, the domain of“金融市場”(\textit{finance market}) is
assigned based on the domain of“金融”(\textit{finance}) and“市場”
(\textit{market}). 

The Component module assigns a domain in about 4 seconds
 (from \BAngou{1} to
\BAngou{6}).\footnote{
This is the fastest because the text 
from which fundamental words are extracted (the unknown word itself) is
 rather small, and unlike the Wikipedia modules, it does not require
 additional web access.}

\begin{table}[t]
\caption{Keywords of Advertisement Snippets}
\label{corporate-words}
\input{11table04.txt}
\end{table}

\section{Evaluation}
\label{sec:evaluation}

We conducted blog categorization using the above-mentioned method
and measured the accuracy with which the method assigned domains to
blogs. 
Also, we measured the accuracy of the results of estimating unknown
words' domains that were obtained during the Blog categorization.


\subsection{Experimental Condition}

\paragraph{Data: }
We categorized 600 blog articles into 12 domains.
50 articles for each domain were collected from Yahoo!~Blog
(blogs.yahoo.co.jp).
In Yahoo!~Blog,
the authors of articles manually classify those articles into
categories, which correspond to what we call domains.
In this experiment, we selected appropriate categories for each domain
and collected 50 articles from the categories, as shown in
Table \ref{tab:domain-yahoo}.
Note that 
some articles (about 30\%) either do not fit in the categories 
into which they are classified\footnote{
For example, an article on a leisure trip is categorized as 
the science category.
} or contain only photos and no textual content.
We replaced those articles with more appropriate ones in advance.


\paragraph{Evaluation Method for Blog Categorization: }
We measured the accuracy of blog categorization and the domain
estimation for unknown words.
In blog categorization, we extracted three kinds of words 
from articles:
fundamental words (\textbf{F only} in Table \ref{bunrui-eval-result}),
fundamental and simplex unknown words
(\textbf{F + SU}), and
fundamental and all unknown words (both simplex and
complex, \textbf{F + AU}). 
We also measured the accuracy of not only the domain of the highest IDF
but also the top N domains (\textbf{Top N} in Table
\ref{bunrui-eval-result}).
Furthermore, we evaluated the performance when we used the domain
dictionary without manual correction and the performance when we used
the number of words for each domain as the score instead of the IDF
value. 

\paragraph{Evaluation Method for Domain Estimation for Unknown Words: }
During categorization, about 12,000 unknown words were found in 
600 articles, and the domain estimation for them was conducted
on-the-fly.
We then sampled 500 estimation results from them and measured the
accuracy of domain estimation.
Unknown words assigned to more than one domain were judged as correct
if they were assigned to one of the correct domains.
We also examined the number of times each estimation module was used and
how accurate they were.

\begin{table}[t]
\caption{Correspondence between Domains and Yahoo!~Blog's Categories}
\label{tab:domain-yahoo}
\input{11table05.txt}
\end{table}
\begin{table}[t]
\caption{Accuracy of Blog Categorization}
\label{bunrui-eval-result}
\input{11table06.txt}
\end{table}



\subsection{Result of Blog Categorization}

Table \ref{bunrui-eval-result} shows the accuracy of categorization.
The \textbf{F only} column indicates that a rather simple method like
the one in Section \ref{sec:blog-categorization} works well if
fundamental words are given good clues for categorization:
the domain in our case.
Furthermore, \textbf{F + SU} slightly outperformed \textbf{F only}, and
\textbf{F + AU} outperformed the others.
This shows that the domain estimation for unknown words moderately
improves blog categorization.

Errors are mostly due to the system's incorrect focus on
topics of secondary importance.
For example, in an article on a sightseeing trip, which should have been 
assigned the \dom{recreation} domain, the author frequently mentioned the
means of transportation. 
As a result, the article was wrongly categorized as
\dom{transportation}. 
Another example is an article that discussed software engineers working on
business management systems.
It should have been assigned the \dom{science} domain,\footnote{
Note that the \dom{science} domain includes computer science.
} 
but because it included numerous business keywords, 
it was assigned the \dom{business} domain. 

Table \ref{tab:bunrui-eval-result-autodic} shows the results when
we used the domain dictionary without manual correction.
The accuracy was over 80\% even without 
manual correction, which indicates that the method for constructing the
domain dictionary provides high performance.
In contrast, the fact that the accuracy was over 10\% lesser than when
we manually corrected the dictionary 
(0.82, compared to 0.94) indicates that our manual correction of the domain
dictionary was conducted accurately and that our resulting domain
dictionary is of high quality.

\begin{table}[t]
\setlength{\captionwidth}{205pt}
\begin{minipage}{205pt}
\hangcaption{Accuracy of Blog Categorization (using the domain dictionary without manual correction)}
\label{tab:bunrui-eval-result-autodic}
\input{11table07.txt}
\end{minipage}
\hfill
\begin{minipage}{205pt}
\hangcaption{Accuracy of Blog Categorization (using the number of words as the score)}
\label{tab:bunrui-eval-result-wordfreq}
\input{11table08.txt}
\end{minipage}
\end{table}

We used the sum of the IDF values for each domain as the score for blog
categorization. 
A simpler method would be to use the number of words for each domain
as the score, in which case the results are as described in Table
\ref{tab:bunrui-eval-result-wordfreq}.
The performance was lower than that of our method, which used the sum of
the IDF values.
This shows that our method of using the sum of the IDF values is superior
to the simpler method that uses the number of words.


\subsection{Result of Domain Estimation for Unknown Words}

The accuracy of the domain estimation for unknown words was
77.2\% (386/500).

Table \ref{domest-methods-results} shows the frequency in use and
the accuracy for each domain estimation module.
The Snippets module was used most frequently and achieved a reasonably
good accuracy of 78\%.
Though the Wikipedia-strict module showed the best performance, it was
used infrequently.
However, we expect that as the number of Wikipedia articles increases,
the best performing module will be used more frequently, thus
improving the accuracy of domain estimation as a whole.

\begin{table}[b]
\caption{Frequency and Accuracy for each Module}
\label{domest-methods-results}
\input{11table09.txt}
\end{table}

An example of newly coined words for which domains were estimated correctly
is“デイトレ,”which is an abbreviation of“デイトレード”
(\textit{day-trade}). 
It was correctly assigned the \dom{business} domain by the Wikipedia-loose
module.
Names of famous people also appear in blog articles one after another
and provide clues for blog categorization.
The Wikipedia-strict module correctly judged 
“レオナルド・ディカプリオ”(\textit{Leonardo DiCaprio})
as 
\dom{culture}, for instance.

Compound words for which domains were estimated correctly
include“支持-率”(\textit{approval rate}) and 
“運動-野”(\textit{the motor area of the brain}).
The former consists of“支持”(\textit{support}) and“率”
(\textit{rate}), both
of which belong to \dom{nodomain}, but the compound word as a whole
was correctly assigned the \dom{government} domain by the
Wikipedia-strict module.
Similarly, the latter term consists of“運動”(\textit{movement}) and
“野”(\textit{field}), both of which belong to \dom{nodomain}, but
the Snippets module correctly assigned the compound word 
to the \dom{health} domain.

Errors made in the experiment were mostly due to
the subtle boundaries between
\dom{nodomain} and the other particular domains.
For example, 
names of townships and persons that are common and popular were
assigned wrong domains.
Both of them should be assigned \dom{nodomain}.
However, the names of townships were often assigned the \dom{government}
domain because
most townships have their own local government websites,
which mislead the Wikipedia or Snippets modules.
Common and popular personal names are often sources of error because
virtually any person's name is linked to a particular domain on the web.
In other words, you will always find web sites of a
particular domain by searching the web for a common and popular
name.


\section{Related Work}
\label{sec:related-work}

\subsection{Related Work on Domain Dictionary}
\label{sec:related-work-on-domain-dictionary}

First of all, much lesser research has been carried out on the domain
relations than on is-a relations;
fewer resources have been constructed and fewer proposals on construction
methods have been made.

The domain information of a word is available in only a few lexical
resources such as HowNet \cite{HowNet:Dong:Dong:2006} and
WordNet. 
HowNet includes 32 domains such as \dom{economy}, \dom{industry},
\dom{agriculture}, \dom{education}.
In WordNet (2.0), domain relations hold between synsets.
For example, \textit{forehand}, \textit{rally}, and
\textit{match point} are associated with \textit{tennis}.
Some (human-oriented) dictionaries like LDOCE \cite{LDOCE:1987} describe
which domain (subject) a word belongs to. 
However, such resources are available only for a few languages,
such as English and Chinese.\footnote{
Some Japanese dictionaries occasionally describe domain
information,
but they cover only a few words.}

Thus,
towards deeper natural language understanding for languages other than
English or Chinese, an efficient way for
constructing a domain dictionary is required.
However,
most proposals on domain dictionary construction that have been
made so far rely on existing resources, like LDOCE or WordNet.
\citeA{guthrie91subjectdependent} exploited the domain
(subject) information of LDOCE to establish domain links between words.
To enrich WordNet with domain information,
\citeA{Magnini:Cavaglia:2000}
manually annotated upper synsets with domain information and then
automatically extended the manual assignments to all the reachable synsets
by exploiting WordNet relations.
\citeA{Agirre:Ansa:Martinez:Hovy:2001} extracted
from documents topically related words for each word sense in WordNet.
The documents were collected from the web by querying a search
engine.
To construct an effective query, they
used semantic information from WordNet.
\citeA{Chang:Huang:Ker:Yang:2002}
assigned domain tags to WordNet by exploiting WordNet relations.
They defined their domain tags on the basis of 
a small amount of existing domain information in WordNet
and also information from the Far East Dictionary.
As is seen, these methods are
not applicable to languages like Japanese, for which lexical resources
corresponding to LDOCE or WordNet are not available.\footnote{
The Japanese WordNet
http://nlpwww.nict.go.jp/wn-ja/index.en.html
has been available only recently and was not available when the
original journal paper of this article was published.
}

To construct a domain dictionary without relying on existing resources
like WordNet, you might use keyword extraction techniques that have
been developed in the context of information retrieval (IR) or term
recognition
\cite[and so on]
{Frantzi:Ananiadou:Tsujii:1998,Hisamitsu:Tsujii:2003,Nakagawa:Mori:Yumoto:2003}.
However, as we mentioned in Section \ref{sec:two-issues}, keyword extraction
techniques 
would not work well, because document collections
are hard to obtain for the common domains assumed in this
study.

Ultimately, our domain dictionary provides two innovations:
the first fully-available Japanese domain dictionary and a method for
constructing a domain dictionary that requires neither highly structured
existing lexical resources like WordNet nor document collections from
which to extract keywords.

Our domain dictionary is not tailored to any particular NLP application.
But among other things, domain information has been used for
document filtering \cite{Liddy:Paik:1993},
word-sense disambiguation \cite{Rigau:Atserias:Agirre:1997}, and
machine translation
\cite{Yoshimoto:Kinoshita:Shimazu:1997,Lange:Yang:1999}.


\subsection{Related Work on Text Categorization}
\label{sec:related-work-on-text-categorization}

Text-categorization methods that have been developed so far are mostly
based on machine learning, such as
$k$-nearest neighbor \cite{yang99evaluation},
decision tree \cite{lewis94comparison},
naive Bayes \cite{lewis98naive},
decision list \cite{li99text},
support vector machines \cite{Joachims:1999},
boosting \cite{schapire00boostexter}.

Methods based on machine learning require huge quantities of text to be
used as training data, which constitutes the bottleneck for those
methods.
Though there has been a growing interest in a technique using
a small number of annotated text and a large number of unannotated
text for machine learning \cite{Abney:2007}, 
methods based on this technique are still 
in the early phase of development.

In contrast, our method, which achieved an accuracy of 94\%,
requires no training data.
All you need is a manageable number of fundamental words with good clues
for categorization (into domains).
The construction of our domain dictionary also requires no
training data.
All you need is access to the web, and manual correction
(Section \ref{manual-correction}) is not labor-intensive.

Also note that our categorization method is NOT tailored to the 12
domains in Table \ref{tab:domains}, though we used them in this
study. 
If you want to categorize texts based on your own domains,
you must only construct your own domain dictionary using the
method described in Section \ref{sec:domain-dictionary-construction},
which is neither time-consuming nor painstaking.
Moreover, it is unlikely that the domains into which
texts are categorized will need to be changed frequently in practice.
Thus, you will likely have to construct your own domain dictionary only
once.

Another important feature of our method is the utilization of the
on-the-fly estimation of unknown words to domains, which achieved 
77\% accuracy.
This feature is useful for categorizing texts like blog articles,
which are updated on a daily basis and are filled with newly coined
words or neologisms.
In contrast, machine learning approaches need to collect huge quantities
of texts at short intervals in order to update classifiers for
frequently updated texts and the neologisms in them.

Consequently, as a result of our blog categorization research, 
we have developed a simple but high-performance text categorization
method that
\textbf{i)}~uses no machine learning, and thus requires no text
collection;
and \textbf{ii)}~can process unknown words dynamically.


\section{Conclusion}
\label{sec:conclusion}

Towards deeper natural language understanding, 
we constructed a domain dictionary, where about 30,000 fundamental words
are grouped by domain.
There are two issues in domain dictionary construction;
choosing domains and designing a method by which to associate words with
domains. 
We avoided being too involved in the former issue by adopting 12 domains
that were based on web directories like the Open Directory Project.
As for the latter issue, based on the work by Sasaki et al.
\shortcite{Sasaki:Sato:Utsuro:2006},
we developed a semi-automatic
method that does not require document collections or lexical resources
like WordNet.
As a result, 81.3\% of the target fundamental words were associated with
the correct domains. 
We further improved the domain dictionary by manually correcting the
association results.

We also presented a blog categorization method
that exploits our domain dictionary
and the dynamic
domain estimation for unknown words.
Our method categorized blogs with 94\% accuracy, and estimated domains
for unknown words with 77\% accuracy.

The contribution of this study is as follows.
First, we constructed the world's first publicly-available Japanese
domain dictionary.
Second, we developed the domain dictionary construction method for
fundamental words that requires neither training data nor
highly-structured language resources like WordNet.
Finally, we developed a blog categorization method that requires
no training data and flexibly assigns unknown words to domains.

In our future work, we plan to apply
our domain dictionary to word-sense
disambiguation and translation word selection, among other things.
Tanaka et al. used the domain information of words that appear in the
same sentence as a target word of word-sense disambiguation
\cite{Tanaka:Bond:Baldwin:Fujita:Hashimoto:2007}.
We will use domain information from broader contexts,
such as the domain information of words that appear in the neighboring
sentences as well.
Furthermore, we will exploit the domain information of unknown words for the
task. 



\acknowledgment

We would like to thank all the members of
the collaborative research group of Kyoto University and NTT
Communication Science Laboratories 
for their stimulating discussion.


\bibliographystyle{jnlpbbl_1.5}
\begin{thebibliography}{}

\bibitem[\protect\BCAY{Abney}{Abney}{2007}]{Abney:2007}
Abney, S. \BBOP 2007\BBCP.
\newblock {\Bem Semisupervised Learning for Computational Linguistics}.
\newblock Chapman \& Hall.

\bibitem[\protect\BCAY{Agirre, Ansa, Martinez, \BBA\ Hovy}{Agirre
  et~al.}{2001}]{Agirre:Ansa:Martinez:Hovy:2001}
Agirre, E., Ansa, O., Martinez, D., \BBA\ Hovy, E. \BBOP 2001\BBCP.
\newblock \BBOQ Enriching WordNet Concepts with Topic Signatures.\BBCQ\
\newblock In {\Bem Proceedings of the SIGLEX Workshop on ``WordNet and Other
  Lexical Resources: Applications, Extensions, and Customizations'' in
  conjunction with NAACL}.

\bibitem[\protect\BCAY{Chang, Huang, Ker, \BBA\ Yang}{Chang
  et~al.}{2002}]{Chang:Huang:Ker:Yang:2002}
Chang, E., Huang, C.-R., Ker, S.-J., \BBA\ Yang, C.-H. \BBOP 2002\BBCP.
\newblock \BBOQ Induction of Classification from Lexicon Expansion: Assigninig
  Domain Tags to WordNet Entries.\BBCQ\
\newblock In {\Bem Proceedings of COLING-2002 Workshop on SEMANET},
  \mbox{\BPGS\ 1--7}, Taipei.

\bibitem[\protect\BCAY{Dong \BBA\ Dong}{Dong \BBA\
  Dong}{2006}]{HowNet:Dong:Dong:2006}
Dong, Z.\BBACOMMA\ \BBA\ Dong, Q. \BBOP 2006\BBCP.
\newblock {\Bem HowNet and the Computation of Meaning}.
\newblock World Scientific Pub Co Inc.

\bibitem[\protect\BCAY{Frantzi, Ananiadou, \BBA\ Tsujii}{Frantzi
  et~al.}{1998}]{Frantzi:Ananiadou:Tsujii:1998}
Frantzi, K.~T., Ananiadou, S., \BBA\ Tsujii, J. \BBOP 1998\BBCP.
\newblock \BBOQ {T}he {C}-value/{NC}-value {M}ethod of {A}utomatic
  {R}ecognition for {M}ulti-word {T}erms.\BBCQ\
\newblock In {\Bem Proceedings of the Research and Advanced Technology for
  Digital Libraries: Second European Conference, ECDL'98}, \mbox{\BPGS\
  585--604}.

\bibitem[\protect\BCAY{Guthrie, Guthrie, Wilks, \BBA\ Aidinejad}{Guthrie
  et~al.}{1991}]{guthrie91subjectdependent}
Guthrie, J.~A., Guthrie, L., Wilks, Y., \BBA\ Aidinejad, H. \BBOP 1991\BBCP.
\newblock \BBOQ {S}ubject-{D}ependent {C}o-{O}ccurence and {W}ord {S}ense
  {D}isambiguation.\BBCQ\
\newblock In {\Bem Proceedings of the 29th Annual Meeting of the Association
  for Computational Linguistics}, \mbox{\BPGS\ 146--152}.

\bibitem[\protect\BCAY{Hashimoto \BBA\ Kurohashi}{Hashimoto \BBA\
  Kurohashi}{2007}]{Hashimoto:Kurohashi:2007}
Hashimoto, C.\BBACOMMA\ \BBA\ Kurohashi, S. \BBOP 2007\BBCP.
\newblock \BBOQ {C}onstruction of {D}omain {D}ictionary for {F}undamental
  {V}ocabulary.\BBCQ\
\newblock In {\Bem Proceedings of the 45th Annual Meeting of the Association
  for Computational Linguistics (ACL'07) Poster}, \mbox{\BPGS\ 137--140}.

\bibitem[\protect\BCAY{Hashimoto \BBA\ Kurohashi}{Hashimoto \BBA\
  Kurohashi}{2008}]{Hashimoto:Kurohashi:2008}
Hashimoto, C.\BBACOMMA\ \BBA\ Kurohashi, S. \BBOP 2008\BBCP.
\newblock \BBOQ Blog {C}ategorization {E}xploiting {D}omain {D}ictionary and
  {D}ynamically {E}stimated {D}omains of {U}nknown {W}ords.\BBCQ\
\newblock In {\Bem Proceedings of the 46th Annual Meeting of the Association
  for Computational Linguistics (ACL'08) Short paper, Poster}, \mbox{\BPGS\
  69--72}.

\bibitem[\protect\BCAY{Hisamitsu \BBA\ Tsujii}{Hisamitsu \BBA\
  Tsujii}{2003}]{Hisamitsu:Tsujii:2003}
Hisamitsu, T.\BBACOMMA\ \BBA\ Tsujii, J. \BBOP 2003\BBCP.
\newblock \BBOQ Measuring {T}erm {R}epresentativeness.\BBCQ\
\newblock In {\Bem Information Extraction in the Web Era}, \mbox{\BPGS\
  45--76}. Springer-Verlag.

\bibitem[\protect\BCAY{Joachims}{Joachims}{1999}]{Joachims:1999}
Joachims, T. \BBOP 1999\BBCP.
\newblock \BBOQ {T}ransductive {I}nference for {T}ext {C}lassification using
  {S}upport {V}ector {M}achines.\BBCQ\
\newblock In {\Bem Proceedings of the 16th International Conference on Machine
  Learning}, \mbox{\BPGS\ 200--209}.

\bibitem[\protect\BCAY{Kurohashi, Nakamura, Matsumoto, \BBA\ Nagao}{Kurohashi
  et~al.}{1994}]{Kurohashi:Nakamura:Matsumoto:Nagao:1994}
Kurohashi, S., Nakamura, T., Matsumoto, Y., \BBA\ Nagao, M. \BBOP 1994\BBCP.
\newblock \BBOQ Improvements of {J}apanese {M}ophological {A}nalyzer
  {JUMAN}.\BBCQ\
\newblock In {\Bem Proceedings of the International Workshop on Sharable
  Natural Language Resources}, \mbox{\BPGS\ 22--28}.

\bibitem[\protect\BCAY{Lange \BBA\ Yang}{Lange \BBA\
  Yang}{1999}]{Lange:Yang:1999}
Lange, E.~D.\BBACOMMA\ \BBA\ Yang, J. \BBOP 1999\BBCP.
\newblock \BBOQ Automatic Domain Recognition for Machine Translation.\BBCQ\
\newblock In {\Bem MT-VII}, \mbox{\BPGS\ 641--645}, Singapore.

\bibitem[\protect\BCAY{Lewis}{Lewis}{1998}]{lewis98naive}
Lewis, D.~D. \BBOP 1998\BBCP.
\newblock \BBOQ Naive ({B}ayes) at forty: The Independence Assumption in
  Information Retrieval.\BBCQ\
\newblock In N{\'{e}}dellec, C.\BBACOMMA\ \BBA\ Rouveirol, C.\BEDS, {\Bem
  Proceedings of {ECML}-98, 10th European Conference on Machine Learning},
  \mbox{\BPGS\ 4--15}, Chemnitz, DE. Springer Verlag, Heidelberg, DE.

\bibitem[\protect\BCAY{Lewis \BBA\ Ringuette}{Lewis \BBA\
  Ringuette}{1994}]{lewis94comparison}
Lewis, D.~D.\BBACOMMA\ \BBA\ Ringuette, M. \BBOP 1994\BBCP.
\newblock \BBOQ A Comparison of Two learning Algorithms for Text
  Categorization.\BBCQ\
\newblock In {\Bem Proceedings of {SDAIR}-94, 3rd Annual Symposium on Document
  Analysis and Information Retrieval}, \mbox{\BPGS\ 81--93}, Las Vegas, US.

\bibitem[\protect\BCAY{Li \BBA\ Yamanishi}{Li \BBA\ Yamanishi}{1999}]{li99text}
Li, H.\BBACOMMA\ \BBA\ Yamanishi, K. \BBOP 1999\BBCP.
\newblock \BBOQ Text classification using {ESC}-based stochastic decision
  lists.\BBCQ\
\newblock In {\Bem Proceedings of {CIKM}-99, 8th {ACM} International Conference
  on Information and Knowledge Management}, \mbox{\BPGS\ 122--130}, Kansas
  City, US. ACM Press, New York.

\bibitem[\protect\BCAY{Liddy \BBA\ Paik}{Liddy \BBA\
  Paik}{1993}]{Liddy:Paik:1993}
Liddy, E.\BBACOMMA\ \BBA\ Paik, W. \BBOP 1993\BBCP.
\newblock \BBOQ {D}ocument {F}iltering using {S}emantic {I}nformation from a
  {M}achine {R}eadable {D}ictionary.\BBCQ\
\newblock In {\Bem Proceedings of the ACL Workshop on Very Large Corpora},
  \mbox{\BPGS\ 20--29}.

\bibitem[\protect\BCAY{Magnini \BBA\ Cavagli\`{a}}{Magnini \BBA\
  Cavagli\`{a}}{2000}]{Magnini:Cavaglia:2000}
Magnini, B.\BBACOMMA\ \BBA\ Cavagli\`{a}, G. \BBOP 2000\BBCP.
\newblock \BBOQ Integrating Subject Field Codes into WordNet.\BBCQ\
\newblock In {\Bem Proceedings of LREC-2000}, \mbox{\BPGS\ 1413--1418}, Athens.

\bibitem[\protect\BCAY{Nakagawa, Mori, \BBA\ Yumoto}{Nakagawa
  et~al.}{2003}]{Nakagawa:Mori:Yumoto:2003}
Nakagawa, H., Mori, T., \BBA\ Yumoto, H. \BBOP 2003\BBCP.
\newblock \BBOQ {T}erm {E}xtraction {B}ased on {O}ccurrence and {C}oncatenation
  {F}requency.\BBCQ\
\newblock {\Bem Journal of Natural Language Processing}, {\Bbf 10}  (1),
  \mbox{\BPGS\ 27--45}.
\newblock (in {J}apanese).

\bibitem[\protect\BCAY{Proctor}{Proctor}{1987}]{LDOCE:1987}
Proctor, P. \BBOP 1987\BBCP.
\newblock {\Bem Longman Dictionary of Contemporary English}.
\newblock Longman Group.

\bibitem[\protect\BCAY{Rigau, Atserias, \BBA\ Agirre}{Rigau
  et~al.}{1997}]{Rigau:Atserias:Agirre:1997}
Rigau, G., Atserias, J., \BBA\ Agirre, E. \BBOP 1997\BBCP.
\newblock \BBOQ Combining Unsupervised Lexical Knowledge Methods for Word Sense
  Disambiguation.\BBCQ\
\newblock In {\Bem Proceedings of joint EACL/ACL 97}, Madrid.

\bibitem[\protect\BCAY{Sasaki, Sato, \BBA\ Utsuro}{Sasaki
  et~al.}{2006}]{Sasaki:Sato:Utsuro:2006}
Sasaki, Y., Sato, S., \BBA\ Utsuro, T. \BBOP 2006\BBCP.
\newblock \BBOQ {R}elated {T}erm {C}ollection.\BBCQ\
\newblock {\Bem Journal of Natural Language Processing}, {\Bbf 13}  (3),
  \mbox{\BPGS\ 151--176}.
\newblock (in {J}apanese).

\bibitem[\protect\BCAY{Schapire \BBA\ Singer}{Schapire \BBA\
  Singer}{2000}]{schapire00boostexter}
Schapire, R.~E.\BBACOMMA\ \BBA\ Singer, Y. \BBOP 2000\BBCP.
\newblock \BBOQ {BoosTexter}: {A Boosting-based System for Text
  Categorization}.\BBCQ\
\newblock {\Bem Machine Learning}, {\Bbf 39}  (2/3), \mbox{\BPGS\ 135--168}.

\bibitem[\protect\BCAY{Tanaka, Bond, Baldwin, Fujita, \BBA\ Hashimoto}{Tanaka
  et~al.}{2007}]{Tanaka:Bond:Baldwin:Fujita:Hashimoto:2007}
Tanaka, T., Bond, F., Baldwin, T., Fujita, S., \BBA\ Hashimoto, C. \BBOP
  2007\BBCP.
\newblock \BBOQ {Word Sense Disambiguation Incorporating Lexical and Structural
  Semantic Information}.\BBCQ\
\newblock In {\Bem Proceedings of the 2007 Joint Conference on Empirical
  Methods in Natural Language Processing and Computational Natural Language
  Learning (EMNLP-CoNLL)}, \mbox{\BPGS\ 477--485}.

\bibitem[\protect\BCAY{Yang}{Yang}{1999}]{yang99evaluation}
Yang, Y. \BBOP 1999\BBCP.
\newblock \BBOQ {A}n {E}valuation of {S}tatistical {A}pproaches to {T}ext
  {C}ategorization.\BBCQ\
\newblock {\Bem Information Retrieval}, {\Bbf 1}  (1/2), \mbox{\BPGS\ 69--90}.

\bibitem[\protect\BCAY{Yoshimoto, Kinoshita, \BBA\ Shimazu}{Yoshimoto
  et~al.}{1997}]{Yoshimoto:Kinoshita:Shimazu:1997}
Yoshimoto, Y., Kinoshita, S., \BBA\ Shimazu, M. \BBOP 1997\BBCP.
\newblock \BBOQ Processing of Proper Nouns and Use of Estimated Subject Area
  for Web Page Translation.\BBCQ\
\newblock In {\Bem TMI97}, \mbox{\BPGS\ 10--18}, Santa Fe.

\end{thebibliography}

\begin{biography}

\bioauthor[:]{Chikara Hashimoto}{
Chikara Hashimoto received Ph.D. in Linguistic Sciences from Kobe Shoin 
Graduate School in 2004 and Ph.D. in Informatics from Kyoto University 
in 2011. He is currently a research manager of Information Analysis
Laboratory at National Institute of Information and Communications 
Technology (NICT). He is interested in natural language processing.
 He received the Young Scientists' Prize (The Commendation for Science
and Technology by the Minister of Education, Culture, Sports, Science
and Technology) in 2014, IPSJ Best Paper Award 2011, and Best Paper 
Award 2008 given by the Association for Natural Language Processing.}

\bioauthor[:]{Sadao Kurohashi}{
Sadao Kurohashi received the B.S., M.S., and PhD in Electrical
Engineering from Kyoto University in 1989, 1991 and 1994,
respectively.  He has been a visiting researcher of IRCS, University
of Pennsylvania in 1994.  He is currently a professor of the Graduate
School of Informatics at Kyoto University. His research interests
include natural language processing, knowledge
acquisition/representation, and information retrieval.  He received
the 10th and 20th anniversary best paper awards from Journal of
Natural Language Processing in 2004 and 2014 respectively, 2009 Funai
IT promotion award, and 2009 IBM faculty award.}
\end{biography}


\biodate



\end{document}
