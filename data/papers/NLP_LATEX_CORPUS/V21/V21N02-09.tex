    \documentclass[japanese]{jnlp_1.4}
\usepackage{jnlpbbl_1.3}
\usepackage[dvips]{graphicx}
\usepackage{amsmath}
\usepackage{hangcaption_jnlp}
\usepackage{udline}
\setulminsep{1.2ex}{0.2ex}
\let\underline


\Volume{21}
\Number{2}
\Month{April}
\Year{2014}

\received{2013}{9}{23}
\revised{2013}{12}{1}
\accepted{2014}{1}{15}

\setcounter{page}{379}

\jtitle{長単位解析器の異なる品詞体系への適用}
\jauthor{
小澤　俊介\affiref{hatena} \and 内元　清貴\affiref{nict} \and 伝　　康晴\affiref{chiba}
} 
\jabstract{
言語研究において，新しい品詞体系を用いる場合には，既存の辞書やコーパス，
解析器では対応できないことが多いため，これらを再構築する必要がある．
これらのうち，辞書とコーパスは再利用できることが少なく，新たに構築する場合が多い．
一方，解析器は既存のものを改良することで対応できることが多いものの，
どのような改良が必要かは明らかになっていない．
本論文では，品詞体系の異なるコーパスの解析に必要となる
解析器の改良点を明らかにするためのケーススタディとして，
品詞体系の異なる日本語話し言葉コーパス（以下，CSJ）と現代日本語書き言葉均衡コーパス（以下，BCCWJ）を利用して，
長単位情報を自動付与した場合に生じる誤りを軽減する方策について述べる．
具体的には，CSJを基に構築した長単位解析器をBCCWJへ適用するため，
CSJとBCCWJの形態論情報における相違点に応じて，長単位解析器の学習に用いる素性やラベルを改善した．
評価実験により提案手法の有効性を示す．
}
\jkeywords{長単位解析，異なる品詞体系，話し言葉コーパス，書き言葉コーパス}

\etitle{Adaptation of Long-Unit-Word Analysis System to Different Part-Of-Speech Tagset}
\eauthor{
Shunsuke Kozawa\affiref{hatena} \and Kiyotaka Uchimoto\affiref{nict} \and Yasuharu Den\affiref{chiba}
} 
\eabstract{
Existing dictionaries, corpora, analyzers are not usually applicable to 
research using new part-of-speech tagset in the fields of linguistic research. 
Dictionaries and corpora are often newly constructed. 
On the other hand, existing analyzers can be reused by improving them. 
However, it is not clear how they could be improved. 
This paper describes how an analyzer constructed for analyzing a certain corpus 
can be applied to another corpus with a different part-of-speech tagset. 
In particular, we improved the features and labels used to train a long-unit-word analyzer 
based on Corpus of Spontaneous Japanese (CSJ)
by focusing on the differences between CSJ and Balanced Corpus of Comtemporary Written Japanese (BCCWJ)
and applied the analyzer to BCCWJ. 
The experimental results show the advantage of the proposed method.
}
\ekeywords{long-unit word, different part-of-speech tagset, spontaneous speech corpus, \break written corpus}

\headauthor{小澤，内元，伝}
\headtitle{長単位解析器の異なる品詞体系への適用}

\affilabel{hatena}{株式会社はてな}{Hatena Co., Ltd}
\affilabel{nict}{情報通信研究機構}{National Institute of Information and Communications Technology}
\affilabel{chiba}{千葉大学}{Chiba University}



\begin{document}
\maketitle


\section{はじめに}\label{intro}

近年，言語研究において，言語現象を統計的に捉えるため，コーパスを用いた研究が盛んに行われている．
コーパスを用いた研究は，語法，文法，文体に関する研究\cite{oishi2009,koiso2009}，
語彙に関する研究\cite{tanomura2010}，時代ごとの言語変化を調査する通時的な研究\cite{kondo2012}，
外国語教育へ適用する研究\cite{nakajo2006}など多岐にわたる．

コーパスを用いる研究では，新しい言語現象を調査するには新しいコーパスの構築が必要となる．
大規模なコーパスを構築する場合，人手でのアノテーションには限界があるため，
自動でアノテーションをする必要がある．
既存の言語単位や品詞体系を利用できる場合は，既存のコーパスや解析器を利用することにより，
他分野のコーパスに対するアノテーション作業を軽減できる\cite{kazama2004}．
また，対象分野のアノテーション済みコーパスがある程度必要なものの，
分野適応により，解析器の統計モデルを対象分野に適合するように調整することで，
他分野のコーパスに対しても既存のコーパスに対するものと
同程度の性能でアノテーションが可能となる\cite{jing2007,neubig2011}．
しかし，研究目的によっては適切な言語単位や品詞体系が異なるため，既存の言語単位や品詞体系が利用できないこともある．
例えば，国立国語研究所の語彙調査では，雑誌の語彙調査には$\beta$単位，
教科書の語彙調査にはM単位というように，どちらも形態素相当の単位ではあるが，調査目的に応じて設計し用いている．
これらの単位の概略は\cite{hayashi1982,nakano1998}に基づいている．
また，言語現象に応じて異なる場合もあり，日本語話し言葉コーパス\cite{csj}（以下，CSJ）
と現代日本語書き言葉均衡コーパス\cite{bccwj}（以下，BCCWJ）では異なる言語単位や品詞体系が定義されている．
新しい言語単位や品詞体系を用いる場合，分野適応の利用は難しく，
辞書やコーパス，解析器を再構築する必要がある．
これらのうち，辞書とコーパスは再利用できることが少なく，新たに構築する必要がある．
解析器に関しては，既存のものを改良することで対応できることが多いものの，
どのような改良が必要かは明らかではない．


本論文では，言語単位や品詞体系の異なるコーパスの解析に必要となる
解析器の改良点を明らかにするためのケーススタディとして，
品詞体系の異なるCSJとBCCWJを利用して長単位解析器を改良する．
CSJとBCCWJには，いずれも短単位と長単位という2種類の言語単位がアノテーションされている．
本論文ではこのうち長単位解析特有の誤りに着目して改善点を明らかにする．
そのため，短単位情報は適切にアノテーションされているものと仮定し，
その上で長単位情報を自動でアノテーションした場合に生じる誤りを軽減する方策について述べる．
評価実験により提案手法の有効性を示し，提案手法の異なる品詞体系への適用可能性について考察する．

本論文の構成は以下の通りである．
まず，\ref{csj_bccwj_diff}章で長単位解析器を改良するために重要となるCSJとBCCWJの形態論情報における相違点について述べ，
\ref{luw_analysis}章ではCSJに基づいた長単位解析手法を説明し，
CSJとBCCWJの形態論情報における相違点に基づいた長単位解析手法の改良点について述べる．
\ref{exp}章では，長単位解析手法の改良点の妥当性を検証し，改良した長単位解析手法を評価する．
\ref{comainu}章では，\ref{luw_analysis}章で述べた長単位解析手法を実装した
長単位解析システムComainuについて述べ，
\ref{conclusion}章で本論文をまとめる．


\section{CSJとBCCWJの形態論情報における相違点}
\label{csj_bccwj_diff}

一般に，解析器を学習したコーパスとは異なるコーパスに適用する場合，
各コーパスには同じ粒度のアノテーションが施してある必要がある．
アノテーションの粒度が異なる場合には精度よく解析できないため，解析器を改良し，学習しなおす必要がある．
解析器の改良にはコーパス間の相違点を把握することが重要となる．
本章では，CSJを基に構築した長単位解析器をBCCWJへ適用する上で必要な改良点を把握するため，
言語単位，品詞体系，言語現象，付加情報に着目して，CSJとBCCWJを比較し，形態論情報における相違点について述べる．


\subsection{言語単位}
\label{lang_unit_diff}

CSJとBCCWJではともに，短単位と長単位という2種類の言語単位が用いられている．
短単位は，原則として，現代語で意味を持つ最小の単位2つが1回結合したものであり，
その定義は一般的な辞書の見出しに近いものである．
長単位は，概ね，文節を自立語と付属語に分けたものであり，
1短単位からなるか，あるいは，複数の短単位を複合したものからなる．

短単位と長単位の例を表\ref{unit_example}に挙げる．
表\ref{unit_example}は「日本型国際貢献が求められています」という文における
文節，長単位，短単位の関係を表している．
例えば，「日本型国際貢献」という長単位は「日本」，「型」，「国際」，「貢献」の4短単位から構成される．

\begin{table}[b]
\caption{文，文節，長単位，短単位の関係}
\label{unit_example}
\input{ca13table01.txt}
\end{table} 

CSJとBCCWJではいずれも，短単位と長単位という同じ言語単位を利用しているが，
BCCWJの短単位・長単位の認定規定は，CSJの規定に修正を加えたものを利用しているため，
CSJとBCCWJの短単位・長単位は完全には一致しない．


\subsection{品詞体系}
\label{pos_tagset_diff}

CSJとBCCWJでは大きく異なる品詞体系が利用されている．
以下にCSJとBCCWJ，それぞれの品詞体系を示す．


CSJでは学校文法に基づく品詞体系が採用されており，
15種類の品詞，59種類の活用型，8種類の活用形で構成されている\cite{ogura2004}．
CSJの品詞体系は以下の通りである．
活用型の○にはア，カ，サ，タなどが入る．

\begin{itemize}
\item 品詞 （15種類）\\
名詞，代名詞，形状詞，連体詞，副詞，接続詞，感動詞，動詞，形容詞，助動詞，助詞，接頭辞，接尾辞，記号，言いよどみ
\item 活用型 （59種類）\\
○行五段，○行上一段，○行下一段，カ行変格，サ行変格，ザ行変格，
文語○行四段，文語○行上二段，文語○行下二段，文語カ行変格，文語サ行変格，文語ナ行変格，文語ラ行変格，
形容詞型，文語形容詞型1，文語形容詞型2，文語形容詞型3，文語
\item 活用形 （8種類） \\
未然形，連用形，終止形，連体形，仮定形，已然形，命令形，語幹
\end{itemize}

BCCWJは形態素解析用辞書UniDic\cite{den2007}\footnote{http://sourceforge.jp/projects/unidic/}
に準拠した品詞体系を利用しており，
品詞は「名詞-固有名詞-地名-一般」のように階層的に定義されている．
各階層は大分類，中分類，小分類，細分類と呼ばれる．
「名詞-固有名詞-地名-一般」の場合，「名詞」が大分類，「固有名詞」が中分類，「地名」が小分類，「一般」が細分類である．
品詞は4階層で定義され，大分類で15種類，細分類まで展開すると54種類ある．
活用型は3階層で定義され，大分類で20種類，小分類まで展開すると115種類ある．
活用形は2階層で定義され，大分類で10種類，中分類では36種類ある．
BCCWJの品詞体系のうち，大分類の体系は以下の通りである．

\begin{itemize}
\item 品詞 （15種類）\\
名詞，代名詞，形状詞，連体詞，副詞，接続詞，感動詞，動詞，形容詞，助動詞，助詞，接頭辞，接尾辞，記号，補助記号
\item 活用型 （20種類）\\
五段，上一段，下一段，カ行変格，サ行変格，
文語四段，文語上一段，文語上二段，文語下一段，文語下二段，文語カ行変格，文語サ行変格，文語ナ行変格，文語ラ行変格
形容詞，文語形容詞，助動詞，文語助動詞，無変化型
\item 活用形 （10種類）\\
語幹，未然形，意志推量形，連用形，終止形，連体形，仮定形，已然形，命令形，ク語法
\end{itemize}

CSJが階層のない単純な品詞体系を利用しているのに比べ，
BCCWJでは詳細な品詞体系が利用されている．
CSJの名詞とBCCWJの名詞を比較した例を表\ref{compare_noun}に示す．
BCCWJでは名詞は15種類に分類されている．
また，CSJでは短単位と長単位の品詞体系が一致しているのに対し，
BCCWJでは短単位と長単位の品詞体系は一部異なっている．
これはBCCWJの短単位では「名詞-普通名詞-サ変可能」や「名詞-普通名詞-形状詞可能」などの
曖昧性を持たせた品詞が設けられているのに対し，長単位ではこれらを設けていないためである．

\begin{table}[t]
\caption{CSJとBCCWJの名詞の比較}
\label{compare_noun}
\input{ca13table02.txt}
\end{table}


\subsection{言語現象}
\label{lang_diff}

CSJは話し言葉コーパスであるのに対し，BCCWJは書き言葉コーパスであるため，
言語現象として，話し言葉と書き言葉という大きな違いがある．
例えば，話し言葉の場合，フィラーや言いよどみなどが生じる．
一方，書き言葉の場合，著者によって使われる表記が異なるため，表記のバリエーションが多い．
話し言葉のコーパスであるCSJと書き言葉のコーパスであるBCCWJを比べると，
前者では人手で書き起こす際に表記揺れが吸収され表記は統一されており，
後者では著者の著したテキストがそのまま使われているため表記は不統一である．
次の節でこれらに関連してコーパスに付加された情報の違いを整理する．


\subsection{付加情報}
\label{additional_annotation}

CSJとBCCWJでは短単位に付与されている情報に多少違いがある．
以下に，CSJのみ，及び，BCCWJのみにしか付与されていない情報について述べる．

CSJには話し言葉特有の情報など，以下の情報が付与されている．
\begin{itemize}
\item フィラー：フィラーに対してタグ（F）が付与されている
\begin{quote}
（F えー），（F あのね），（F んーと）
\end{quote}
\item 言いよどみ：言いよどみに対してタグ（D）が付与されている
\begin{quote}
（D す）すると，（D テニ）昨日のテニスは，（D 情）情報が
\end{quote}
\item アルファベット：アルファベット，算用数字，記号の短単位に対してタグ（A）が付与されている．
\begin{quote}
（A シーディーアール；ＣＤ−Ｒ）
\end{quote}
\item 外国語：外国語や古語，方言などに対してタグ（O）が付与されている．
\begin{quote}
（O ザッツファイン）
\end{quote}
\item 名前：話者の名前や差別語，誹謗中傷などに対してタグ（R）が付与されている．
\begin{quote}
国語研の（R ××） です
\end{quote}
\item 音や言葉のメタ情報：音や言葉に関するメタ的な引用に対してタグ（M）が付与されている．
\begin{quote}
助詞の（M は）は（M わ）と発音
\end{quote}
\end{itemize}
一方，BCCWJではCSJには付与されていない以下の情報が付与されている．
\begin{itemize}
\item 語種情報：語種とは，語をその出自によって分類したものである．
BCCWJでは，短単位に以下の語種のいずれかが付与されている．
\begin{quote}
和語，漢語，外来語，混種語，固有名，記号
\end{quote}
\item 囲み情報：
BCCWJでは，丸付き数字（\textcircled{\footnotesize 1}，\textcircled{\footnotesize 2}）や丸秘などの丸で囲まれている文字は
内部の文字のみが短単位となっており，囲みの情報は別で付与されている．
例えば，「\textcircled{\footnotesize 1}林木の新品種の開発」は書字形では「１林木の新品種の開発」となっており，
囲みの情報は別で付与されている．
\end{itemize}


\section{長単位解析手法}
\label{luw_analysis}

本章では，まず，CSJに基づいて構築された長単位解析手法（従来手法）について述べ，次に，提案手法について述べる．
従来手法をBCCWJに適用するためには改良が必要であり，
提案手法では，CSJとBCCWJの形態論情報における相違点に着目した改良を行った．


長単位解析とは長単位境界，及び，長単位の語彙素，語彙素読み，品詞，活用型，活用形を同定するタスクである．
短単位解析では，辞書を用いることで，高精度に解析が行われてきた\cite{kudo,den}．
長単位解析でも長単位辞書を構築することによって高精度に解析できることが考えられるが，
短単位の組み合わせからなる長単位の語彙は膨大であり，辞書の構築には膨大な労力が必要となるため，効率的でない．
そのため，短単位情報を組み上げることにより長単位解析を行う．


\subsection{CSJに基づく長単位解析手法（従来手法）}
\label{uchimoto_method}

Uchimotoらは長単位を認定する問題を，入力された短単位列に対する系列セグメンテーション問題として捉え，
チャンキングモデルと後処理に基づいた長単位解析手法を構築した\cite{uchimoto2007}．
図\ref{flow}に長単位解析の流れを示す．
短単位列を入力とし，チャンキングにより長単位境界を認定する．
このとき，一部の長単位に対しては品詞情報も付与する．
次に，後処理によって長単位の品詞，活用型，活用形，語彙素，語彙素読みを付与する．

\begin{figure}[b]
\begin{center}
\includegraphics{21-2iaCA13f1.eps}
\end{center}
\caption{長単位解析の流れ}
\label{flow}
\end{figure}
\begin{figure}[b]
\begin{center}
\includegraphics{21-2iaCA13f2.eps}
\end{center}
\caption{短単位と長単位の例}
\label{fig:suw_luw_org}
\vspace*{-2\Cvs}
\end{figure}



\subsubsection{チャンキングモデル}
\label{chunking_model}

チャンキングモデルによって，長単位境界を認定し，一部の長単位に対しては品詞情報も付与するために，
Uchimotoらは下記の4つのラベルを定義している．

\begin{itemize}
\item[Ba] 長単位を構成する短単位のうち先頭の要素で，かつ，
その品詞，活用型，活用形が長単位のものと一致する．
\item[Ia] 長単位を構成する短単位のうち先頭以外の要素で，かつ，
その品詞，活用型，活用形が長単位のものと一致する．
\item[B] 長単位を構成する短単位のうち先頭の要素で，かつ，
その品詞，活用型，活用形のいずれかが長単位のものと一致しない．
\item[I] 長単位を構成する短単位のうち先頭以外の要素で，かつ，
その品詞，活用型，活用形のいずれかが長単位のものと一致しない．
\end{itemize}
これは，長単位を構成する先頭の要素に付与されるラベルは「Ba」か「B」であり，
長単位を構成する先頭以外の要素に付与されるラベルは「Ia」か「I」であることを意味する．
また，「Ba」「Ia」が付与された要素は長単位と同じ品詞，活用型，活用形を持つことを意味する．
したがって，このラベルを利用することにより，長単位境界だけでなく，多くの場合，品詞，活用型，活用形の情報も得られる．

図\ref{fig:suw_luw_org}に「日本型国際貢献が求められています」に対して，ラベルを付与した例を示す．
これらのラベルを正しく推定できれば，
\pagebreak
「Ba」あるいは「Ia」が付与された短単位から品詞，活用型，活用形が得られる．
図\ref{fig:suw_luw_org}は，「ている」以外の長単位については品詞，活用型，活用形も得られることを表わしている．
一方，「ている」については品詞がこれらを構成する短単位「て」「いる」のどちらとも異なるため，
各短単位には「B」あるいは「I」のラベルしか付与されない．
この場合は，ラベルを正しく推定できたとしても品詞などは得られず，単位境界の情報のみが得られることになるため，
次節に述べる後処理により品詞，活用型，活用形を推定する．

チャンキングモデルの素性としては，着目する短単位とその前後2短単位，あわせて5短単位について，以下の情報を利用する．
\begin{itemize}
\item 短単位情報 \\
書字形，語彙素読み，語彙素，品詞，活用型，活用形
\item 付加情報 \\
\ref{additional_annotation}節で述べたCSJにのみ付与されている情報
（フィラー，言いよどみ，アルファベット，外国語，名前，音や言葉のメタ情報を示すタグ）が
それぞれ着目している短単位の直前に付与されているか否かを素性として利用する．
\end{itemize}


\subsubsection{書き換え規則による後処理}
\label{post_process_org}

\ref{chunking_model}節で記したチャンキングモデルにより，ラベルを正しく推定することができれば，
図\ref{fig:suw_luw_org}の「日本型国際貢献」や「求める」などは
「Ba」または「Ia」が付与された短単位から品詞，活用型，活用形が得られる．
一方，「ている」については，品詞が長単位を構成する短単位「て」及び「いる」とは異なるため，
各短単位には「B」あるいは「I」のラベルしか付与されない．
この場合は，ラベルを正しく推定できたとしても品詞は得られず，単位境界の情報のみが得られることになる．
これらの長単位に対しては書き換え規則によって品詞，活用型，活用形を付与する．

単位境界のみが分かっている長単位ごと，つまり，「B」あるいは「I」が付与された短単位のみから構成される
長単位ごとに書き換え規則を獲得，適用することによって品詞，活用型，活用形の情報を得る．
書き換え規則は対象の長単位とその前後の短単位を抽出することによって自動獲得する．
書き換え規則は対象の長単位を構成する短単位，及び，その前後の短単位からなる前件部と，
対象の長単位からなる後件部で構成される．
例えば，図\ref{fig:suw_luw_org}の「ている」については図\ref{fig:rule}のような規則が獲得される．
前件部で同じ規則が複数得られた場合，最も頻度の高いもののみ書き換え規則として獲得する．

図\ref{fig:rule}の規則は，「て」「いる」という短単位にそれぞれ「B」，「I」というラベルが付与され，
前方（文頭側）の短単位が「られ」，後方（文末側）の短単位が「ます」であるとき，
「ている」という助動詞に書き換えられることを意味している．
どの書き換え規則も適用されない場合は，以下の手順で規則を汎化して再適用する．
\begin{itemize}
\item 後方文脈を削除
\item 前方文脈と後方文脈を削除
\item 前方文脈，後方文脈，書字形，語彙素読み，語彙素を削除
\end{itemize}
前方文脈とは対象の長単位より前方（文頭側）の短単位（図\ref{fig:rule}の「られ」），
後方文脈とは対象の長単位より後方（文末側）の短単位（図\ref{fig:rule}の「ます」）を表す．
この手順で再適用し，結果的にどの規則も適用されなかった場合は，短単位の先頭の品詞，活用型，活用形を適用する．

\begin{figure}[t]
\begin{center}
\includegraphics{21-2iaCA13f3.eps}
\end{center}
\caption{書き換え規則の例}
\label{fig:rule}
\end{figure}


\subsection{コーパスの形態論情報における相違点に基づいた長単位解析手法の改善（提案手法）}
\label{apply_corpus_diff}

本節では，\ref{uchimoto_method}節で示した長単位解析手法（従来手法）からの改良点について述べる．

\subsubsection{品詞体系の差異に応じた改善}
\label{apply_pos_tagset}

CSJとBCCWJの品詞体系は\ref{pos_tagset_diff}節で示したように，大きく異なっている．
この問題に対し，以下の点を改善した．

\subsubsection*{汎化素性の利用}

CSJの品詞体系とは異なり，BCCWJの品詞体系では，品詞，活用型，活用形が階層的に定義されている．
しかし，階層化された素性をそのまま利用した場合，各階層の情報が考慮されなくなってしまう．
そこで，階層化された素性に対して上位階層で汎化した素性をチャンキングモデルの素性として追加する．
例えば「名詞-普通名詞-一般」に対しては，「名詞」「名詞-普通名詞」を素性として追加する．


\subsubsection*{カテゴリ推定モデルによる後処理}

CSJでは品詞体系が単純であったため，「B」あるいは「I」のラベルが付与された短単位のみから構成される長単位が少なく，
後処理については書き換え規則である程度対応できていた．
しかし，品詞体系が詳細なBCCWJでは，短単位と長単位の品詞の対応関係が単純ではないため，
書き換え規則で対応するのは困難である．
この問題に対し，次に述べるカテゴリ推定モデルを用いることで解決することを提案する．

カテゴリ推定モデルは，学習データに現れたカテゴリを候補として，
その候補すべてについて尤もらしさを計算するモデルである．
長単位を構成する短単位列を与えると，その長単位に対して最尤のカテゴリを出力する．
推定するカテゴリを品詞，活用型，活用形とした，
品詞推定モデル，及び，活用型推定モデル，活用形推定モデルをそれぞれ学習・適用し，
最も尤もらしい品詞，活用型，活用形を推定する．

推定するカテゴリを品詞とする品詞推定モデルでは，
学習データに現れた品詞のうち，助詞と助動詞を除くすべての品詞候補から最尤の品詞を出力する．
助詞と助動詞については長単位を構成する短単位列が複合辞と一致している場合のみ候補とする．
複合辞と一致しているかどうかは，複合辞辞書との文字列マッチングにより自動判定する．
複合辞辞書はBCCWJで認定された複合辞を予め人手で整理することにより用意した．
素性としては，着目している長単位とその前後の長単位，あわせて3長単位に対して，
先頭から2短単位と末尾から2短単位の計12短単位の情報を用いる．
長単位が1短単位からなる場合は，先頭から2短単位目の情報は与えられなかったもの (NULL) として扱う．
各短単位に対して，書字形，語彙素読み，語彙素，品詞，活用型，活用形，及び，
階層化された素性に対して上位階層で汎化した情報を素性として用いる．

\begin{figure}[b]
\begin{center}
\includegraphics{21-2iaCA13f4.eps}
\end{center}
\caption{品詞推定モデルの適用例}
\label{post_feature}
\end{figure}

図\ref{fig:suw_luw_org}の「てい」に対して品詞推定モデルを適用する例を図\ref{post_feature}に示す．
「てい」では，前後の長単位をあわせた「られ」「てい」「ます」の3長単位に対し，
「られ」「NULL」（「られ」の先頭2短単位），「て」「い」（「てい」の先頭2短単位），
「ます」「NULL」（「ます」の先頭2短単位），及び，「NULL」「られ」（「られ」の末尾2短単位），
「て」「い」（「てい」の末尾2短単位），「NULL」「ます」（「ます」の末尾2短単位）の
各短単位の情報を素性として用いる．
図\ref{post_feature}では，最尤の品詞として助動詞を出力している．

活用型推定モデル，及び，活用形推定モデルは，推定するカテゴリが品詞ではなくそれぞれ活用型，活用形となる点，及び，
動的素性を用いる点を除いて品詞推定モデルと同様である．
動的素性としては，活用型推定モデルでは着目している長単位の品詞
（自動解析時は品詞推定モデルにより自動推定した品詞）を，
活用形推定モデルでは着目している長単位の品詞と活用型
（自動解析時は品詞推定モデル，活用型推定モデルによりそれぞれ自動推定した品詞と活用型）を用いる．


\subsubsection{付加情報の差異に応じた改善}
\label{apply_additional_annotation}

\ref{additional_annotation}節で示したBCCWJにのみ付与されている
以下の情報をチャンキングモデルの素性として利用する．

\begin{itemize}
\item 語種情報 \\
短単位の語種が和語，漢語，外来語，混種語，固有名，記号のいずれであるかを素性として利用する．
\item 囲み情報 \\
BCCWJでは丸付き数字で長単位境界が区切れるため，囲み情報は長単位境界を判定するための大きな手がかりとなる．
そのため，短単位に囲み情報が付与されているか否かを素性として利用する．
\end{itemize}


\subsubsection{ラベル定義の変更}
\label{label_change}

本節では，コーパスの相違点に限らず，既存の手法にも適用できる改良について述べる．


\ref{chunking_model}節で示した4つのラベルを以下のように再定義した．
変更点を下線で示す．
\begin{itemize}
\item[Ba] \underline{単独で}長単位を構成する短単位で，かつ，その品詞，活用型，活用形が長単位のものと一致する．
\item[Ia] \underline{複数短単位で構成される長単位の末尾の短単位で}，
かつ，その品詞，活用型，活用形が長単位のものと一致する．
\item[B] \underline{複数短単位で構成される長単位の先頭の短単位．
もしくは，単独で}長単位を構成する短単位で，かつ，
その品詞，活用型，活用形のいずれかが長単位のものと一致しない．
\item[I] \underline{複数短単位で構成される長単位の先頭でも末尾でもない短単位．
もしくは，複数短単位}\\ \underline{で構成される}長単位の\underline{末尾の短単位}で，かつ，
その品詞，活用型，活用形のいずれかが長単位のものと一致しない．
\end{itemize}
単独の短単位から構成される長単位に対しては，短単位の品詞，活用型，活用形が
長単位のものと一致する場合には「Ba」，一致しない場合には「B」が付与される．
一方，複数短単位から構成される長単位に対しては，
先頭の短単位には「B」，先頭でも末尾でもない短単位には「I」，
末尾の短単位にはその品詞，活用型，活用形が長単位のものと一致する場合には「Ia」，一致しない場合には「I」が付与される．
この定義を利用すると，図\ref{fig:suw_luw_org}の例では，
「Ba」が付与されている「日本」には「B」，
「Ia」が付与されている「国際」には「I」のラベルが付与されることになる．

本改良は，長単位の品詞，活用型，活用形は，長単位を構成する短単位のうち，
末尾の短単位の品詞，活用型，活用形と一致することが多く，
それ以外の位置にある短単位と一致する場合は偶然であることが多いという観察に基づく．
「Ba」，「Ia」のラベルが付与される短単位を長単位を構成する末尾の短単位のみに限定することにより，
チャンキングモデルの精度が向上し，全体の性能が向上することが期待できる．


\section{評価実験}
\label{exp}

本章では，\ref{apply_corpus_diff}節で示した改善策の有効性を確認するため，
\ref{uchimoto_method}節で示した従来手法と各改善策を行った手法の長単位解析精度を比較する．
まず，\ref{pre_exp}節では予備実験として，CSJを用いた実験を行い，従来手法の長単位解析の性能を確認する．
また，BCCWJに適用するために行った改善策がCSJに対しても有効であること示す．
次に，\ref{luw_exp}節でBCCWJを用いた実験を行う．
従来手法と\ref{apply_corpus_diff}節で示した提案手法とを比較し，提案手法の有効性を示す．


\subsection{設定}
\label{exp_setting}

\ref{luw_analysis}章に述べた手法のチャンキングモデルの学習と適用には，
CRF++\footnote{http://crfpp.googlecode.com/svn/trunk/doc/index.html}を用いた．
CRF++はCRFに基づく汎用チャンカーであり，
パラメータはCRF++のデフォルトのパラメータを用いた．
また，改良した後処理に用いる品詞，活用型，活用形推定モデルの学習には
YamCha\footnote{http://chasen.org/{\textasciitilde}taku/software/yamcha/}を用いた．
YamChaはSVMに基づく汎用チャンカーであり，カーネルは多項式カーネル（べき指数3）を採用し，
多クラスへの拡張はone-versus-rest法を用いた．

\begin{table}[b]
\caption{評価データの規模}
\label{data_size}
\input{ca13table03.txt}
\end{table}

実験で用いるデータを表\ref{data_size}に示す．
CSJ，BCCWJともに，コアデータを学習データとテストデータに分け，
学習データはモデルの学習に，テストデータはモデルの評価に用いている．
CSJのデータはUchimotoら\cite{uchimoto2007}の実験設定に合わせて，フィラー，言いよどみを削除して用いた．
チャンキングモデルで用いるフィラーと言いよどみの情報としては，
短単位の直前がフィラーか否か，もしくは，言いよどみか否かの情報を素性として利用している．
また，BCCWJのデータはCSJの結果と比較しやすいように，CSJとデータ規模を同程度にした．
なお，本実験では短単位情報は予め適切な情報が付与されていることを前提とする．

本実験では，正解データの境界（品詞）のうち正しく推定できたものの割合（再現率）と
自動推定した境界（品詞）のうち正しく推定できたものの割合（精度），
下記に示す再現率と精度の調和平均であるF値を評価指標として用いる．
\[
F値= \frac{2 \times 精度 \times 再現率}{精度 + 再現率}
\]


\subsection{予備実験}
\label{pre_exp}

予備実験として，CSJに対する実験を行った．
実験には表\ref{data_size}に示したCSJの学習データとテストデータを用いた．
\ref{uchimoto_method}節で記したCSJを基に構築した長単位解析手法（従来手法）をベースラインとして，
CSJの学習データを用いて学習し，テストデータに適用した．
また，\ref{apply_corpus_diff}節で記した改善のうち，
CSJに対しても適用できる以下のモデルを適用した．
\begin{itemize}
\item {\bf ベースライン+ラベル変更} \\
ベースラインに対して，\ref{label_change}節で示したラベル定義の変更をしたモデル
\item {\bf ベースライン+推定モデル} \\
ベースラインの後処理を書き換え規則から品詞，活用型，活用形推定モデルに変更したモデル
\item {\bf ベースライン+ラベル変更+推定モデル} \\
ベースラインに対して，ラベル定義，及び，後処理を変更したモデル
\end{itemize}

\begin{table}[b]
\caption{CSJで学習したモデルを用いた実験結果}
\label{pre_exp_result}
\input{ca13table04.txt}
\end{table}

結果を表\ref{pre_exp_result}に示す．
CSJに対してベースラインを適用した場合，
境界推定で98.99$\%$，品詞推定で98.93$\%$と高い性能が得られていることがわかる．
ラベル定義を変更したモデルはベースラインに対し，性能が向上しており，
ラベル定義の変更が有効に働くことを示している．
後処理に推定モデルを用いたモデルでは品詞推定の精度が向上した．
性能差はF値で0.4$\%$と小さいため，CSJへ適用する場合には，
書き換え規則でも十分に適用できているといえる．
また，ベースラインに対してラベル定義と後処理を変更した場合が最も性能がよく，
CSJに対しても有効な改良であることがわかった．

次に，CSJで学習したベースラインのモデルをBCCWJのテストデータに適用したところ，
境界推定で74.72$\%$，品詞推定で65.58$\%$と大きく精度が落ちる結果となった．
これは，当然ではあるが，\ref{csj_bccwj_diff}章で記したように，
CSJとBCCWJでは言語単位や品詞体系が異なるためであり，
BCCWJを高精度で解析するには解析器の再構築が必要であることを示唆している．


\subsection{実験}
\label{luw_exp}

表\ref{data_size}に示したBCCWJのデータを用いて実験した．
まず，BCCWJの学習データを用いて構築したベースラインをBCCWJのテストデータに適用した．
その結果を表\ref{exp_result}の3行目に示す．
境界推定において98.74$\%$，品詞推定において97.68$\%$の性能となった．
CSJを用いて学習，テストした表\ref{pre_exp_result}の3行目の結果（ベースライン）と比較すると，
境界推定では0.25$\%$，品詞推定では1.25$\%$，性能が低下した．

\begin{table}[b]
\caption{BCCWJで学習したモデルを用いた実験結果}
\label{exp_result}
\input{ca13table05.txt}
\end{table}

次に，各改善点の有効性を確認するため，以下のモデルを用いて実験した．
\begin{itemize}
\item {\bf ベースライン+汎化素性} \\
ベースラインのチャンキングモデルの素性に\ref{apply_pos_tagset}節で示した汎化素性を追加したモデル
\item {\bf ベースライン+推定モデル} \\
ベースラインの後処理を書き換え規則から\ref{apply_pos_tagset}節で示した品詞，活用型，活用形推定モデルに変更したモデル
\item {\bf ベースライン+語種情報} \\
ベースラインのチャンキングモデルの素性に\ref{apply_additional_annotation}節で示した語種情報を追加したモデル
\item {\bf ベースライン+囲み情報} \\
ベースラインのチャンキングモデルの素性に\ref{apply_additional_annotation}節で示した囲み情報を追加したモデル
\item {\bf ベースライン+ラベル変更} \\
ベースラインに対して，\ref{label_change}節で示したラベル定義の変更をしたモデル
\end{itemize}
結果を表\ref{exp_result}の4行目から8行目に示す．
ベースラインに対して，いずれの改良を加えた場合でもF値が向上した．
境界推定に関しては，汎化素性が性能向上に大きく貢献した．
品詞推定に関しては，後処理を書き換え規則から品詞，活用型，活用形推定モデルにした手法で
大きく性能が向上した．

提案手法としてCSJとBCCWJの形態論情報における相違点に対する改良をすべて行ったモデルを適用した．
結果を表\ref{exp_result}の9行目に示す．
境界推定では98.93$\%$，品詞推定では98.66$\%$の性能が得られ，
ベースラインに対して，境界推定で約0.2$\%$，品詞推定で約1$\%$向上した．
CSJを用いて学習，テストした表\ref{pre_exp_result}の6行目の
結果（ベースライン+ラベル変更+後処理）と比較すると，境界推定では0.13$\%$，
品詞推定では0.39$\%$低いが，この性能低下は主としてCSJに比べBCCWJの方が品詞体系が詳細であるため，
長単位解析自体の問題が難しくなっていることに起因すると考えられる．


また，ベースラインに対して，品詞体系の相違点に対する対処（汎化素性，後処理）を
適用したモデル（{\bf ベースライン+品詞体系対応}）を用いた実験を行った．
結果を表\ref{exp_result}の10行目に示す．
表\ref{exp_result}の9行目の改良手法と同程度の性能が得られており，
主な性能改善は品詞体系の差異に対応することで得られていることがわかる．


\subsection{考察}

\subsubsection{誤り傾向の分析}

CSJに対してベースラインを用いた実験（表\ref{pre_exp_result}），
BCCWJに対してベースライン，及び，提案手法を用いた実験（表\ref{exp_result}）について，
誤り傾向を分析した．

\begin{table}[b]
\caption{境界誤りの例}
\label{boundary_error_ex}
\input{ca13table06.txt}
\end{table}

まず，境界推定誤りの傾向を，それぞれの実験について調査したところ，共通する2つの誤りの傾向が見られた．
1つ目は名詞連続であり，名詞の短単位列に対する長単位境界を誤ることが多かった．
表\ref{boundary_error_ex}の2から4行目に誤りの例を示す．
表の例では，短単位境界を「／」，長単位境界を「$|$」で表している．
例えば，「医学部倫理委員会」では「医学部」と「倫理委員会」の2長単位にすべきところを
「医学部倫理委員会」と誤って1長単位として判定した．
これらを正しく解析するには，「部」や「庁」などの境界になりやすい短単位の辞書を構築し，
境界になりやすい短単位かどうかを素性として追加することで対応できるだろう．
ただし，短単位間の意味的な関係を捉えないと解析が困難な場合もあるため，
意味的な関係を考慮できるようにする必要もある．
2つ目は複合辞に関する誤りであり，複合辞相当の長単位を複合辞として判定できなかったり，
逆に複合辞ではない短単位列を複合辞として判定してしまうことが多かった．
例を表\ref{boundary_error_ex}の5から7行目に誤りの例を示す．
短単位列が複合辞か否かは前後の文脈に大きく依存するため，
正しく解析できるようにするためにはそれらを考慮した素性が必要となるだろう．
表\ref{boundary_error}に全体の誤りに対する名詞連続と複合辞の誤りの割合を示す．
いずれのデータ，モデルにおいても，名詞連続と複合辞については誤り率が高く，
コーパスに関わらず難しい問題であることがわかる．

\begin{table}[b]
\caption{境界推定誤りの傾向}
\label{boundary_error}
\input{ca13table07.txt}
\end{table} 
\begin{table}[b]
\caption{品詞誤り}
\label{pos_error}
\input{ca13table08.txt}
\end{table}
\begin{table}[b]
\caption{品詞誤りの例}
\label{pos_error_exmaple}
\input{ca13table09.txt}
\end{table}

次に品詞誤りの傾向を調査した．
CSJについては品詞誤りがほとんど見られなかったため，BCCWJのみを調査した．
表\ref{pos_error}に主な品詞誤りの原因とその割合を示す．
ベースライン，提案手法のどちらのモデルにおいても，
名詞-普通名詞-一般と名詞-固有名詞-一般などの名詞同士の誤りが多く見られた．
例を表\ref{pos_error_exmaple}の2から4行目に示す．
例えば，「欧州連合」を名詞-固有名詞-一般と判定すべきところを，名詞-普通名詞-一般と誤って判定した．
これに対する改善策としては，固有名詞になりやすい名詞を学習データから取得し，素性として利用することが考えられる．
また，ベースラインを用いた場合は，名詞-普通名詞-一般と副詞，形状詞-一般との誤りが多く見られた．
誤りの例を表\ref{pos_error_exmaple}の5から8行目に示す．
「最近」という長単位を名詞-普通名詞-一般と判定すべきところを，副詞と誤って判定する例などがあった．
一方，これらの誤りは提案手法を用いた場合はほとんど見られなかった．
これはベースラインでは書き換え規則により品詞を付与しているためだと考えられる．
書き換え規則では頻度が高い規則が適用される．
「最近」という長単位の場合，名詞-普通名詞-一般よりも副詞として出現することが多いために
書き換え規則では誤って副詞と判定してしまう．
これに対し，品詞推定モデルでは前後の文脈を考慮し，文脈に沿って品詞を判定するため，
名詞-普通名詞-一般と副詞，及び，名詞-普通名詞-一般と形状詞-一般に関する誤りが大きく減少したと考えられる．


\subsubsection{人が見たときに気になる誤りへの対処}

後処理を書き換え規則から品詞推定モデルにしたことにより，品詞の推定精度が大幅に向上した．
しかし，学習による品詞推定をする場合，人間では誤らないような品詞が付与される可能性がでてくる．
これは，品詞推定の際に，学習データにでてきたすべての品詞の中から最尤の品詞を推定するためである．
もし，BCCWJの品詞体系で認められていない形式で品詞が付与されてしまうと，
少数の誤りであっても人が見たときには目立つ誤りとなる．
特に，研究対象になりやすく，自動解析も強く求められている複合辞などに対しては配慮する必要がある．
例えば，「として」という助詞-格助詞の複合辞に対して，
BCCWJの品詞体系で認められていない助詞-係助詞などの品詞を付与したり，
複合辞とは認められていない短単位列を複合辞として判定してしまうと，
人が見たときに目立つ誤りとなり，解析器の信頼性が大きく低下してしまう．
そのため，\ref{apply_pos_tagset}節で述べた品詞推定モデルでは，
複合辞については予め複合辞辞書を用意し，特定の品詞しか付与しないよう対処している．


\subsubsection{コーパスのアノテーションと自動解析について}

アノテーションの精度を高めることは，コーパスを用いた研究を行う場合，非常に重要であるが，
そのためにはどこに人手をかけるべきかを検討する必要がある．
例えば，言語単位や品詞体系の定義を複雑にすると自動解析が難しくなるため，
自動解析結果の修正に人手をかける必要が生じる．
話し言葉の場合，フィラーや言いよどみなどを適切に自動解析するのは難しいため，
CSJでは書き起こしの段階で人手でタグを付与している．
逆に，コーパスに付加する情報を一部犠牲にして，定義を柔軟にすることで自動解析の精度を向上させることにより，
人手による修正コストを軽減することもできる．
例えば，BCCWJでは同格や並列の関係にある場合に，意味的な関係を考慮し，
連接する短単位それぞれを長単位として適切に自動認定することは困難であるため，1長単位と定義している．
以下の例の「公正」と「妥当」は並列の関係にあるが，1長単位としている．
\begin{quote}
$|$ 公正妥当 $|$ な $|$ 実務慣行 $|$
\end{quote}
CSJやBCCWJにおいては，長単位情報を付与する前段階で上述のような対処をすることにより
長単位解析器としては対処すべき問題が低減されたという面もある．
このように様々な言語現象に対応しつつコーパスに効率良くアノテーションするためには，
自動解析によるアノテーションの前段階で，自動解析が難しそうな問題に対して柔軟に対処することも重要である．


\subsubsection{他の品詞体系への適用についての一考察}

提案手法でBCCWJとは異なる品詞体系に対してどの程度対処できるかについて考察する．
\ref{pre_exp}節と\ref{luw_exp}節での実験により，
提案手法はCSJ，BCCWJのどちらに対しても有効であった．
これは提案手法が従来手法に比べ，以下の問題に対して汎用的に対処できるようになったためであると考える．
\begin{itemize}
\item 長単位の品詞の種類の多さ
\item 階層的な品詞体系
\item 短単位の品詞と長単位の品詞が異なる
\end{itemize}
品詞体系がBCCWJより単純，もしくは，複雑な場合を考える．
まず，品詞体系がBCCWJより単純な場合は，既にCSJに適用した結果からもわかるように，
提案手法により同等以上の性能が得られると考えられる．
次に，品詞体系がBCCWJより複雑な場合であるが，想定される複雑性は以下の通りである．
\begin{itemize}
\item 品詞の種類の増加
\item 品詞体系の階層の複雑化
\item 短単位の品詞と長単位の品詞の関連性の希薄化
\end{itemize}
これらはBCCWJの品詞体系がCSJの品詞体系に対して複雑になった点でもある．
提案手法では，品詞の階層情報（汎化素性）を用いることにより，品詞の推定精度が向上した．
これは，上記の複雑性が増した場合でも，品詞間の関係を別途定義・アノテーションした情報を用いることにより，
解析性能を上げることができることを示している．
ただし，あまりに複雑な品詞体系の場合，必要となる学習データ量が増えるため，
学習データ量が十分ではないことが原因で解析精度が低下することも考えられる．
そのため，品詞体系はバランスを考えて定義することが重要である．

次にCSJやBCCWJとは異なる品詞体系を利用して，コーパスに対して効率よくアノテーションをする方法について考える．
一般に，すべて人手で言語単位や品詞をアノテーションするのはコストが高い．
このコストを軽減するためには，(1) 対象のコーパスを既存の解析器で解析し，言語単位や品詞体系が異なる部分を修正
(2) 修正結果を学習データとして，解析器を再学習し，対象のコーパスを再解析，
というプロセスを繰り返すことができるのが望ましい．
次章では，それを可能とするためのツールについて述べる．
一方，このプロセスにおいて修正箇所が少ない場合には差分に相当する部分のみを学習し，
既存の解析器による解析結果に対して，差分の部分を後処理で書き換えるような方法の方が有効であるということも考えられる．
この方法の有効性の検証は今後の課題である．


\section{長単位解析ツールComainu}
\label{comainu}

提案手法を用いることにより，CSJやBCCWJとは異なる言語単位や品詞体系のコーパスに対しても長単位を付与することが可能である．
より多くの研究者に対して，品詞体系の異なるコーパスや他分野のコーパスへの長単位情報付与を容易にするためには，
長単位解析の学習・解析機能を備えたツールが利用可能になっていることが重要であると考える．

\ref{luw_analysis}章で説明した提案手法を実装することにより，長単位解析ツールComainuを作成した．
モデルの学習にはBCCWJのコアデータ（45,342文，828,133長単位，1,047,069短単位）を用いた．
本ツールは，平文または短単位列を入力すると，長単位を付与した短単位列を出力することができる．
平文が入力された場合，
MeCab\footnote{http://mecab.googlecode.com/svn/trunk/mecab/doc/index.html}とUniDicにより
形態素解析を行った後に長単位解析を行う．
長単位解析のチャンキングモデルにはSVMとCRFのいずれかを用いることができる．
また，平文や短単位列の直接入力だけでなくファイル入力にも対応しており，
解析結果をファイルに保存することも可能である．

\begin{figure}[b]
\begin{center}
\includegraphics{21-2iaCA13f5.eps}
\end{center}
\caption{Comainu による長単位解析の実行例}
\label{comainu_sample}
\end{figure}

図\ref{comainu_sample}にComainuによる長単位解析の解析例を示す．
図\ref{comainu_sample}の例では，平文を入力とし，
CRFによるチャンキングモデルとSVMによる品詞，活用型，活用形推定モデルを用いて
長単位解析を実行し，長単位が付与された短単位列を出力している．
出力の2〜8列目はそれぞれ短単位の書字形，発音系，語彙素読み，語彙素，品詞，活用型，活用形を表し，
出力の9〜14列目はそれぞれ長単位の品詞，活用型，活用形，語彙素読み，語彙素，書字形を表す．

長単位解析ツールComainuはオープンソースとしている\footnote{http://sourceforge.jp/projects/comainu/}．
これにより，長単位のアノテーションが容易になることが期待される．


\section{まとめ}
\label{conclusion}

本論文では，品詞体系の異なるコーパスの解析に必要となる
解析器の改良点を明らかにするためのケーススタディとして，
CSJとBCCWJを用いて，長単位情報を自動でアノテーションした場合に生じる誤りを軽減する方策について述べた．
CSJとBCCWJの形態論情報における相違点に基づき，長単位解析手法を改良し，評価実験により提案手法の有効性を示した．
さらに，提案手法の異なる品詞体系への適用可能性について考察した．
また，本手法を実装した長単位解析システムComainuについて述べた．

本論文では，長単位解析の入力として正しい短単位列を想定したが，
短単位，長単位ともに自動で解析した場合，
短単位解析結果の誤りが伝播して長単位解析の誤りも増える．
また，自動解析結果の誤りを効率よくなくしていくようなコーパスのメンテナンスの枠組みも重要であり，
その枠組みの実現のためには，短単位解析の解析誤りが長単位解析に与える影響の調査，
特に新たな言語単位や品詞体系を用いた場合にどのような影響がでるかを
複数種類のコーパスを対象として比較調査することが今後必要となると考える．


\acknowledgment
本研究は，文部科学省科学研究費補助金特定領域研究「代表性を有する大規模日本語書き言葉コーパスの構築」
（平成18年度〜22年度，領域代表：前川喜久雄）からの助成を得ました．





\bibliographystyle{jnlpbbl_1.5}
\begin{thebibliography}{}

\bibitem[\protect\BCAY{中條\JBA 西垣\JBA 内山\JBA 山崎}{中條 \Jetal
  }{2006}]{nakajo2006}
中條清美\JBA 西垣知佳子\JBA 内山将夫\JBA 山崎淳史 \BBOP 2006\BBCP.
\newblock 初級英語学習者を対象としたコーパス利用学習の試み.\
\newblock \Jem{日本大学生産工学部研究報告 B (文系)}, {\Bbf 39}, \mbox{\BPGS\
  29--50}.

\bibitem[\protect\BCAY{伝\JBA 小木曽\JBA 小椋\JBA 山田\JBA 峯松\JBA 内元\JBA
  小磯}{伝 \Jetal }{2007}]{den2007}
伝康晴\JBA 小木曽智信\JBA 小椋秀樹\JBA 山田篤\JBA 峯松信明\JBA 内元清貴\JBA
  小磯花絵 \BBOP 2007\BBCP.
\newblock コーパス日本語学のための言語資源:
  形態素解析用電子化辞書の開発とその応用.\
\newblock \Jem{日本語科学}, {\Bbf 22}, \mbox{\BPGS\ 101--123}.

\bibitem[\protect\BCAY{Den, Nakamura, Ogiso, \BBA\ Ogura}{Den
  et~al.}{2008}]{den}
Den, Y., Nakamura, J., Ogiso, T., \BBA\ Ogura, H. \BBOP 2008\BBCP.
\newblock \BBOQ A Proper Approach to {Japanese} Morphological Analysis:
  Dictionary, Model, and Evaluation.\BBCQ\
\newblock In {\Bem Proceedings of the 6th International Language Resources and
  Evaluation}, \mbox{\BPGS\ 1019--1024}.

\bibitem[\protect\BCAY{林}{林}{1982}]{hayashi1982}
林{大（監修）} \BBOP 1982\BBCP.
\newblock 図説日本語.\
\newblock \Jem{角川小辞典}, {\Bbf 9}, \mbox{\BPGS\ 582--583}.

\bibitem[\protect\BCAY{Jiang \BBA\ Zhai}{Jiang \BBA\ Zhai}{2007}]{jing2007}
Jiang, J.\BBACOMMA\ \BBA\ Zhai, C. \BBOP 2007\BBCP.
\newblock \BBOQ Instance Weighting for Domain Adaptation in NLP.\BBCQ\
\newblock In {\Bem Proceedings of the 45th Annual Meeting of the Association
  for Computational Linguistics}, \mbox{\BPGS\ 264--271}.

\bibitem[\protect\BCAY{風間\JBA 宮尾\JBA 辻井}{風間 \Jetal }{2004}]{kazama2004}
風間淳一\JBA 宮尾祐介\JBA 辻井潤一 \BBOP 2004\BBCP.
\newblock 教師なし隠れマルコフモデルを利用した最大エントロピータグ付けモデル.\
\newblock \Jem{言語処理学会}, {\Bbf 11}, \mbox{\BPGS\ 3--23}.

\bibitem[\protect\BCAY{小磯\JBA 小木曽\JBA 小椋\JBA 宮内}{小磯 \Jetal
  }{2009}]{koiso2009}
小磯花絵\JBA 小木曽智信\JBA 小椋秀樹\JBA 宮内佐夜香 \BBOP 2009\BBCP.
\newblock コーパスに基づく多様なジャンルの文体比較-短単位情報に着目して.\
\newblock \Jem{言語処理学会第 15 回年次大会予稿集}, \mbox{\BPGS\ 594--597}.

\bibitem[\protect\BCAY{近藤}{近藤}{2012}]{kondo2012}
近藤泰弘 \BBOP 2012\BBCP.
\newblock 日本語通時コーパスの設計について.\
\newblock \Jem{国語研プロジェクトレビュー}, {\Bbf 3}, \mbox{\BPGS\ 84--92}.

\bibitem[\protect\BCAY{Kudo, Yamamoto, \BBA\ Matsumoto}{Kudo
  et~al.}{2004}]{kudo}
Kudo, T., Yamamoto, K., \BBA\ Matsumoto, Y. \BBOP 2004\BBCP.
\newblock \BBOQ Applying Conditional Random Fields to {Japanese} Morphological
  Analysis.\BBCQ\
\newblock In {\Bem Proceedings of the 2004 Conference on Empirical Methods in
  Natural Language Processing}, \mbox{\BPGS\ 230--237}.

\bibitem[\protect\BCAY{前川}{前川}{2004}]{csj}
前川喜久雄 \BBOP 2004\BBCP.
\newblock 『日本語話し言葉コーパス』の概要.\
\newblock \Jem{日本語科学}, {\Bbf 15}, \mbox{\BPGS\ 111--133}.

\bibitem[\protect\BCAY{Maekawa}{Maekawa}{2008}]{bccwj}
Maekawa, K. \BBOP 2008\BBCP.
\newblock \BBOQ Balanced Corpus of Contemporary Written Japanese.\BBCQ\
\newblock In {\Bem Proceedings of the 6th Workshop on Asian Language
  Resources}, \mbox{\BPGS\ 101--102}.

\bibitem[\protect\BCAY{中野}{中野}{1998}]{nakano1998}
中野洋 \BBOP 1998\BBCP.
\newblock 言語情報処理.\
\newblock \Jem{岩波講座 「言語の科学」}, {\Bbf 9}, \mbox{\BPGS\ 149--199}.

\bibitem[\protect\BCAY{Neubig, Nakata, \BBA\ Mori}{Neubig
  et~al.}{2011}]{neubig2011}
Neubig, G., Nakata, Y., \BBA\ Mori, S. \BBOP 2011\BBCP.
\newblock \BBOQ Pointwise Prediction for Robust, Adaptable Japanese
  Morphological Analysis.\BBCQ\
\newblock In {\Bem Proceedings of the 49th Annual Meeting of the Association
  for Computational Linguistics}, \mbox{\BPGS\ 529--533}.

\bibitem[\protect\BCAY{小椋\JBA 山口\JBA 西川\JBA 石塚\JBA 木村}{小椋 \Jetal
  }{2004}]{ogura2004}
小椋秀樹\JBA 山口昌也\JBA 西川賢哉\JBA 石塚京子\JBA 木村睦子 \BBOP 2004\BBCP.
\newblock 『日本語話し言葉コーパス』における単位認定基準について.\
\newblock \Jem{日本語科学}, {\Bbf 16}, \mbox{\BPGS\ 93--113}.

\bibitem[\protect\BCAY{大名}{大名}{2009}]{oishi2009}
大名力 \BBOP 2009\BBCP.
\newblock コーパスから見える文法.\
\newblock \Jem{国際開発研究フォーラム}, {\Bbf 38}, \mbox{\BPGS\ 23--40}.

\bibitem[\protect\BCAY{田野村}{田野村}{2010}]{tanomura2010}
田野村忠温 \BBOP 2010\BBCP.
\newblock 日本語コーパスとコロケーション.\
\newblock \Jem{言語研究}, {\Bbf 138}, \mbox{\BPGS\ 1--23}.

\bibitem[\protect\BCAY{Uchimoto \BBA\ Isahara}{Uchimoto \BBA\
  Isahara}{2007}]{uchimoto2007}
Uchimoto, K.\BBACOMMA\ \BBA\ Isahara, H. \BBOP 2007\BBCP.
\newblock \BBOQ Morphological Annotation of A Large Spontaneous Speech Corpus
  in {Japanese}.\BBCQ\
\newblock In {\Bem Proceedings of the 20th International Joint Conferences on
  Artificial Intelligence}, \mbox{\BPGS\ 1731--1737}.

\end{thebibliography}

\clearpage
\begin{biography}
\bioauthor{小澤　俊介}{
2009年名古屋大学大学院情報科学研究科博士前期課程了．
2012年同大博士後期過程了．博士（情報科学）．
同年より株式会社はてな．
自然言語処理の研究開発に従事．
言語処理学会会員．
}
\bioauthor{内元　清貴}{
1996年京都大学大学院工学研究科修士課程修了．
同年，郵政省通信総合研究所入所．
内閣府出向を経て，
現在，独立行政法人情報通信研究機構研究マネージャー．
博士（情報学）．
自然言語処理の研究，研究成果の社会還元活動に従事．
言語処理学会・情報処理学会・ACL各会員．
}
\bioauthor{伝　　康晴}{
1993年京都大学大学院工学研究科博士後期課程研究指導認定退学．
博士（工学）．
ATR音声翻訳通信研究所研究員，
奈良先端科学技術大学院大学情報科学研究科助教授，
千葉大学文学部助教授・准教授を経て，現在，千葉大学文学部教授．
専門はコーパス言語学・心理言語学・計算言語学．
とくに日常的な会話の分析・モデル化．
社会言語科学会・日本認知科学会・人工知能学会・日本心理学会・
日本認知心理学会各会員．
}
\end{biography}


\biodate



\end{document}
