    \documentclass[japanese]{jnlp_1.4}
\usepackage{jnlpbbl_1.3}
\usepackage[dvips]{graphicx}
\usepackage{amsmath}
\usepackage{hangcaption_jnlp}
\usepackage{udline}
\setulminsep{1.2ex}{0.2ex}
\let\underline

 \usepackage{linguex}
 \usepackage{cgloss4e}
 \usepackage{ascmac}	

\Volume{21}
\Number{3}
\Month{June}
\Year{2014}

\received{2013}{12}{3}
\revised{2014}{2}{7}
\rerevised{2014}{3}{14}
\accepted{2014}{4}{8}

\setcounter{page}{563}

\jtitle{外界照応および著者・読者表現を考慮した日本語ゼロ照応解析}
\jauthor{萩行　正嗣\affiref{Author_1}\affiref{Author_2} \and 河原　大輔\affiref{Author_1} \and 黒橋　禎夫\affiref{Author_1}}
\jabstract{
日本語では用言の項が省略されるゼロ照応と呼ばれる現象が頻出する．ゼロ照応
は照応先が文章中に明示的に出現する文章内ゼロ照応と，明示的に出現しない
外界ゼロ照応に分類でき，従来のゼロ照応解析は主に前者を対象としてきた．
近年，Webが社会基盤となり，Web上でのテキストによる情報伝達がますます重
要性をましている．そこでは，情報の送り手・受け手である著者・読者が重要
な役割をはたすため，Webテキストの言語処理においても著者・読者を正確にと
らえることが必要となる．しかし，文脈中で明確な表現（人称代名詞など）で言及されていない著
者・読者は，従来の文章内ゼロ照応中心のゼロ照応解析では多くの場合対象外
であった．
このような背景から，本論文では，外界ゼロ照応および文章の著者・読者を扱
うゼロ照応解析モデルを提案する．提案手法では外界ゼロ照応を扱うために，
ゼロ代名詞の照応先の候補に外界ゼロ照応に対応する仮想的な談話要素を加え
る．また，語彙統語パターンを利用することで，文章中で著者や読者に言及し
ている表現を自動的に識別する．実験により，我々の提案手法が外界ゼロ照応
解析だけでなく，文章内ゼロ照応解析に対しても有効であることを示す．
}
\jkeywords{ゼロ照応解析，外界ゼロ照応，著者・読者}

\etitle{Japanese Zero Reference Resolution Considering Zero Exophora and Author/Reader Mentions}
\eauthor{Masatsugu Hangyo\affiref{Author_1}\affiref{Author_2} \and Daisuke Kawahara\affiref{Author_1} \and Sadao Kurohashi\affiref{Author_1}} 
\eabstract{
In Japanese, zero references often occur and many of them are categorized into zero exophora, in which a referent is not mentioned in the document. 
However, previous studies have focused   only  on zero  endophora,  in  which a  referent  explicitly  appears.   We  present  a zero  reference  resolution  model  considering zero  exophora and  authors/readers  of  a  document. 
To deal with zero exophora, our model adds  pseudo-entities  corresponding  to  zero exophora to candidate referents of zero pronouns.
In addition, the model automatically detects mentions that refer to the author and reader of a document using lexico-syntactic patterns.
We present particular behavior of authors/readers in a discourse as a feature vector of a machine learning model.
The experimental results demonstrate the effectiveness of our model for not only zero exophora but also zero endophora.
}
\ekeywords{Zero reference resolution, Zero exophora, Author and reader}

\headauthor{萩行，河原，黒橋}
\headtitle{外界照応および著者・読者表現を考慮した日本語ゼロ照応解析}

 \affilabel{Author_1}{京都大学大学院情報学研究科}{Graduate School of Informatics, Kyoto University}
 \affilabel{Author_2}{現在，株式会社ウェザーニューズ}{Presently with Weathernews Inc.}



\begin{document}
\maketitle


\section{はじめに}

ゼロ照応解析は近年，述語項構造解析の一部として盛んに研究されている．
ゼロ照応とは用言の項が省略される現象であり，省略された項（ゼロ代名詞）が他の表現を照応していると解釈できることからゼロ照応と呼ばれている．

\ex. パスタが好きで、毎日($\phi$ガ)($\phi$ヲ)\underline{食べています}。\label{例:ゼロ照応}

例えば，例\ref{例:ゼロ照応}の「食べています」では，ガ格とヲ格の項が省略されている．
ここで，省略されたヲ格の項は前方で言及されている「パスタ」を照応しており，省略されたガ格の項は文章中では明確に言及されていないこの文章の著者を照応している\footnote{以降の例では，ゼロ代名詞の照応先を埋めた形で「パスタが好きで、毎日([著者]ガ)(パスタヲ)食べています。」のように記述する場合がある．ここで「[著者]」は文章内で言及されていない文章の著者を示す．}．
日本語では曖昧性がない場合には積極的に省略が行われる傾向にあるため，ゼロ照応が文章中で頻繁に発生する．
例\ref{例:ゼロ照応}の「パスタ」の省略のようにゼロ代名詞の照応先\footnote{先行詞と呼ばれることもあるが，本論文では照応先と呼ぶ．}が文章中で言及されているゼロ照応は{\bf 文章内ゼロ照応}と呼ばれ，従来はこの文章内ゼロ照応が主な研究対象とされてきた．
一方，例\ref{例:ゼロ照応}の著者の省略のようにゼロ代名詞の照応先が文章中で言及されていないゼロ照応は{\bf 外界ゼロ照応}と呼ばれる．
外界ゼロ照応で照応されるのは例\ref{例:ゼロ照応}のような文章の著者や読者，例\ref{外界:不特定人}のような不特定の人や物などがある\footnote{一般に外界照応と呼ばれる現象には，現場文脈指示と呼ばれる発話現場の物体を指示するものも含まれる．本研究では，このようなテキストの情報のみから照応先を推測できない外界照応は扱わない．また，実験対象としたコーパスにも，画像や表を照応している文書などはテキストのみから内容が推測できないとして含まれていない．}．

\ex. 内湯も窓一面がガラス張りで眺望がよく、快適な湯浴みを([不特定:人]ガ)\underline{楽しめる}。\label{外界:不特定人}

従来，日本語ゼロ照応解析の研究は，ゼロ照応関係を付与した新聞記事コーパス\cite{KTC,iida-EtAl:2007:LAW}を主な対象として行われてきた．
新聞記事は著者から読者に事件の内容などを伝えることが目的であり，社説や投書の除いては著者や読者が談話構造中に登場することはほとんどない．
一方，近年ではWebを通じた情報伝達が盛んに行われており，Webテキストの言語処理が重要となってきている．
Webテキストでは，著者自身のことを述べたり，読者に対して何らかの働きかけをすることも多く，著者・読者が談話構造中に登場することが多い．例えば，Blogや企業の宣伝ページでは著者自身の出来事や企業自身の活動内容を述べることが多く，通販ページなどでは読者に対して商品を買ってくれるような働きかけをする．
このため，著者・読者に関するゼロ照応も必然的に多くなり，その中には外界ゼロ照応も多く含まれる．
\cite{hangyo-kawahara-kurohashi:2012:PACLIC}のWebコーパスではゼロ照応関係の54\%が外界ゼロ照応である．
このため，Webテキストに対するゼロ照応解析では，特に外界ゼロ照応を扱うことが重要となる．

本研究では，ゼロ照応を扱うためにゼロ代名詞の照応先候補として[著者]や[読者]などの文章中に出現しない談話要素を設定することで，外界ゼロ照応を明示的に扱う．
用言のある格が直接係り受け関係にある項を持たない場合
，その格の項は表\ref{文章内照応，外界ゼロ照応，ゼロ照応なしの分類とその例}の3種類に分類される．
1つ目は「(a)文章内ゼロ照応」であり，項としてゼロ代名詞をとり，その照応先は文章中の表現である．2つ目は「(b)外界ゼロ照応」であり，項としてゼロ代名詞をとり，その照応先に対応する表現が文章中にないものである．3つ目は「(c)ゼロ照応なし」であり，項はゼロ代名詞をとらない，すなわちその用言が本質的にその項を必要としない場合である．
外界ゼロ照応を扱うことにより，照応先が文章内にない場合でも，用言のある格がゼロ代名詞を項に持つという現象を扱うことができる．これにより，格フレームなどの用言が項を取る格の知識とゼロ代名詞の出現が一致するようになり，機械学習によるゼロ代名詞検出の精度向上を期待することができる．

\begin{table}[b]
\caption{文章内ゼロ照応，外界ゼロ照応，ゼロ照応なしの分類とその例}
\label{文章内照応，外界ゼロ照応，ゼロ照応なしの分類とその例}
\input{1011table01.txt}
\end{table}

用言が項としてゼロ代名詞を持つ場合，そのゼロ代名詞の照応先の同定を行う．
従来研究ではその手掛かりとして，用言の選択選好\cite{sasano-kawahara-kurohashi:2008:PAPERS,sasano-kurohashi:2011:IJCNLP-2011,imamura-saito-izumi:2009:Short,hayashibe-komachi-matsumoto:2011:IJCNLP-2011}や文脈的な情報\cite{iida-inui-matsumoto:2006:COLACL,iida-inui-matsumoto:2009:ACLIJCNLP}が広く用いられてきた．
本研究では，それらに加えて文章の著者・読者の情報を照応先同定の手掛かりとして用いる．
先に述べたように，従来研究で対象とされてきた新聞記事コーパスでは，著者や読者は談話中にほとんど出現しない．
そのため著者や読者の情報が文脈的な手掛かりとして用いられることはなかった．
しかし，著者や読者は省略されやすいためゼロ代名詞の照応先になりやすい，敬語やモダリティなど著者や読者の省略を推定するための手掛かりが豊富に存在する，などの特徴を持つため，談話中の著者や読者を明示的に扱うことは照応先同定で重要である．
また，著者や読者は前述のような外界ゼロ照応の照応先だけでなく，文章内に言及されることも多い．

\ex. \underline{私}$_{著者}$はもともとアウトドア派ではなかったので，東京にいた頃もキャンプに行ったことはありませんでした。\label{著者表現1}

\ex. \underline{あなた}$_{読者}$は今ある情報か資料を送って，アドバイザーからの質問に答えるだけ。\label{読者表現1}

例\ref{著者表現1}では，文章中に言及されている「私」がこの文章の著者であり，例\ref{読者表現1}では「あなた」が読者である．
本研究ではこのような文章中で言及される著者や読者を{\bf 著者表現}，{\bf 読者表現}と呼び，これらを明示的に扱うことでゼロ照応解析精度を向上させる．
著者や読者は人称代名詞だけでなく固有表現や役職など様々な表現で言及される．
例えば，下記の例\ref{梅辻}では著者自身の名前である「梅辻」によって著者が言及されており，例\ref{管理人}では著者の立場を表す「管理人」によって言及されている．
また，例\ref{お客様}では著者から見た読者の立場である「お客様」という表現によって読者が言及されている．
本研究では人称代名詞に限らず，著者・読者を指す表現を著者・読者表現として扱うこととする．

\ex. こんにちは、企画チームの\underline{梅辻}$_{著者}$です。\label{梅辻}

\ex. このブログは、\underline{管理人}$_{著者}$の気分によって書く内容は変わります。\label{管理人}

\ex. いくつかの質問をお答えいただくだけで、\underline{お客様}$_{読者}$のご要望に近いノートパソコンをお選びいただけます。\label{お客様}

著者・読者表現は様々な表現で言及されるため，表層的な表記のみから，どの表現が著者・読者表現であるかを判断することは困難である．
そこで，本研究では談話要素とその周辺文脈の語彙統語パターンを素性としたランキング学習\cite{herbrich1998learning,joachims2002optimizing}により，文章中の著者・読者表現の同定を行う．


文章中に出現する著者・読者表現が照応先となることを推定する際には通常の文章中の表現に利用する手掛かりと著者・読者特有の手掛かりの両方が利用できる．
\ex. 僕は京都に(僕ガ)\underline{行こう}と思っています。\\
皆さんはどこに行きたいか(皆さんガ)(僕ニ)\underline{教えてください}。\label{著者表現2}

\ref{著者表現2}の1文目では「僕」が文頭で助詞「は」を伴ない，「行こう」を越えて「思っています」に係っていることから「行こう」のガ格の項であると推測される．これは文章中の表現のみが持つゼロ照応解析での手掛かりと言える．
一方，2文目の「教えてください」では，依頼表現であることからガ格の項が読者表現である「皆さん」であり，ニ格の項が著者表現である「僕」であると推測できる．このような依頼や敬語，モダリティに関する手掛かりは著者・読者特有の手掛かりと言える．
また，著者・読者特有の手掛かりは外界ゼロ照応における著者・読者においても同様に利用できる．
そこで，本研究では，ゼロ照応解析において著者・読者表現は文章内ゼロ照応および外界ゼロ照応両方の特徴を持つものとして扱う．


本論文では，文章中の著者・読者表現および外界ゼロ照応を統合的に扱うゼロ照応解析モデルを提案し，自動推定した著者・読者表現を利用することでゼロ照応解析の精度が向上することを示す．
\ref{114736_18Jun13}節で関連研究について説明し，\ref{114801_18Jun13}節で本研究で利用する機械学習手法であるランキング学習について説明する．
\ref{114838_18Jun13}節ではベースラインとなるモデルについて説明し，\ref{130555_9May13}節で実験で利用するコーパスについて述べる．
その後，\ref{135602_6May13}節で著者・読者表現の自動推定について説明し，\ref{115042_18Jun13}節で著者・読者表現と外界照応を考慮したゼロ照応解析モデルを提案する．
\ref{115121_18Jun13}節で実験結果を示し，\ref{115208_18Jun13}節でまとめと今後の課題とする．


\section{関連研究}
\label{114736_18Jun13}

日本語でのゼロ照応解析は文章内ゼロ照応を中心に行われてきた．

ゼロ照応解析の研究では，ゼロ代名詞は既知のものとして照応先の同定のみを行っているものがある．
\cite{iida-inui-matsumoto:2006:COLACL}はゼロ代名詞と照応先候補の統語的位置関係を素性として利用することでゼロ照応解析を行った．この研究では，外界照応を，それに対応するゼロ代名詞に照応性がないと判断する形で扱っている．この研究では，表\ref{文章内照応，外界ゼロ照応，ゼロ照応なしの分類とその例}における(a)文章内ゼロ照応と(b)外界ゼロ照応を区別して扱っているが，(c)ゼロ照応なしについては扱っていないといえる．
\cite{磯崎秀樹:2006-07-15}は，ランキング学習\footnote{当該論文中では優先度学習と呼ばれている．}を利用することで，ゼロ代名詞の照応先同定を行っている．この研究で扱うゼロ代名詞は文章内に照応先があるものに限定しており，表\ref{文章内照応，外界ゼロ照応，ゼロ照応なしの分類とその例}における(a)文章内ゼロ照応の場合のみを扱っているといえる．


ゼロ照応解析は述語項構造解析の一部として解かれることも多い．
述語項構造解析を格ごとに独立して扱っている研究としては\cite{imamura-saito-izumi:2009:Short,hayashibe-komachi-matsumoto:2011:IJCNLP-2011}がある．
\cite{imamura-saito-izumi:2009:Short}は言語モデルの情報などを素性とした最大エントロピーモデルによるゼロ照応解析を含めた述語項構造解析モデルを提案している．このモデルでは各格の照応先の候補として，NULLという特別な照応先を仮定しており，解析器がこのNULLを選択した場合には，「項が存在しない」または「外界ゼロ照応」としており，これらを同一に扱っている．
\cite{hayashibe-komachi-matsumoto:2011:IJCNLP-2011}は述語と項の共起情報などを素性としたトーナメントモデルにより述語項構造解析の一部としてゼロ照応解析を行っている．この研究でも外界ゼロ照応と項が存在しないことを区別して扱っておらず，また解析対象はガ格のみとしている．
用言ごとに全ての格に対して統合的に述語項構造解析を行う研究としては\cite{sasano-kawahara-kurohashi:2008:PAPERS,sasano-kurohashi:2011:IJCNLP-2011}がある．
\cite{sasano-kawahara-kurohashi:2008:PAPERS}はWebから自動的に構築された格フレームを利用し，述語項構造解析の一部としてゼロ照応解析を行う確率的モデルを提案した．
\cite{sasano-kurohashi:2011:IJCNLP-2011}は格フレームから得られた情報や照応先の出現位置などを素性として対数線形モデルを学習することで，識別モデルによるゼロ照応解析を行った．これらの研究では外界ゼロ照応は扱っておらず，外界ゼロ照応の場合にはゼロ代名詞自体が出現しないものとして扱っている．
これらの研究では表\ref{文章内照応，外界ゼロ照応，ゼロ照応なしの分類とその例}における(b)外界ゼロ照応と(c)ゼロ照応なしを区別せず扱っているといえる．

外界ゼロ照応を扱った研究としては\cite{山本和英:1999,平2013}がある．
\cite{山本和英:1999}では対話文に対するゼロ代名詞の照応先の決定木による自動分類を行っている．
この研究では，ゼロ代名詞は既知として与えられており，その照応先を5種類に分類された外界照応，および文章内照応（具体的な照応先の推定までは行わない）の計6種類から選択している．また，話題は旅行対話に限定されている．
この研究では，機能語および用言の語彙情報がゼロ照応における素性として有効であるとしている．機能語，特に待遇表現は著者・読者に関する外界ゼロ照応解析で有効であると考えられ，本研究でも機能語の情報を素性として利用する．一方，用言の語彙情報は文章内ゼロ照応において有効であるとしているが，これは話題を限定しているためであると考えられ，本研究の対象である多様な話題を含むコーパスに対しては有効に働かないと考えられる．本研究では，格フレームにおける頻度情報などとして用言の情報を汎化することで，用言の情報を扱うこととする．また，この研究ではゼロ代名詞を既知としているため，ゼロ代名詞検出において外界ゼロ照応を扱うことの影響については議論されていない．
\cite{平2013}では新聞記事に対する述語項構造解析の一部として外界ゼロ照応も含めたゼロ照応解析を扱っている．新聞記事コーパスでは外界ゼロ照応自体の出現頻度が非常に低いと報告しており，外界ゼロ照応の精度（F値）はガ格で0.31，ヲ格で0.75，ニ格で0.55と非常に低いものとなっている．
また，これらの研究では文章中に出現する著者・読者（本論文における著者・読者表現）と外界の著者・読者との関係については扱っていない．

日本語以外では，中国語，ポルトガル語，スペイン語などでゼロ照応解析の研究が行われている．
中国語においてはゼロ照応解析は独立したタスクとして取り組まれることが多い．
\cite{kong-zhou:2010:EMNLP}ではゼロ代名詞検出，照応性判定，照応先同定の3つのサブタスクにおいて構文木を利用したツリーカーネルによる手法が提案されている．
ポルトガル語，スペイン語では述語項構造解析の一部ではなく，照応解析の一部としてゼロ照応解析に取り組まれることが多い．
これらの言語では主格にあたる語のみが省略されるが，照応解析の前処理として省略された主格を検出し，照応先が文章内にあるかを分類する研究が行われている\cite{poesio2010creating,rello2012elliphant}．


英語においてはゼロ照応解析に近いタスクとして意味役割付与の研究が行われている\cite{gerber-chai:2010:ACL,ruppenhofer-EtAl:2010:SemEval}．
\cite{gerber-chai:2010:ACL}では頻度の高い10種類の動作性名詞に対して，直接係り受けにないものも項として扱い意味役割付与を行ったデータを作成している．また，共起頻度の情報などを利用して自動的に意味役割付与を行っている．
\cite{ruppenhofer-EtAl:2010:SemEval}では意味役割付与タスクの一部として省略された項を扱っている．また，省略された項については，照応先が特定されるDefinite Null Instanceと照応先が不特定なIndefinete Null Instanceを区別して扱っている．


\section{ランキング学習}
\label{114801_18Jun13}

本研究では，ゼロ照応解析および著者・読者表現推定において，ランキング学習と呼ばれる手法を利用する．
ランキング学習は優先度学習とも呼ばれ，
インスタンス間のランキングを学習するための機械学習手法である\cite{herbrich1998learning,joachims2002optimizing}．
ランキング学習では識別関数を$f(\mathbf{x}) = \mathbf{w}\cdot \mathbf{x}$とし以下のように$\mathbf{w}$を学習する．
ここで$\mathbf{x}$は，入力インスタンスの素性表現であり，$\mathbf{w}$は$\mathbf{x}$に対応する，重みベクトルである．
まずランキングに含まれる各インスタンスの組み合わせを生成する．ここでランキング$A > B > C$を考えると，生成される組み合わせは$A > B$，$A > C$，$B > C$となる．そして各組み合わせにおいて識別関数の値がランキング上の順序と同じになるように$\mathbf{w}$を学習する．上述の例で，各インスタンスに対応する素性ベクトルが$\mathbf{x}_A$，$\mathbf{x}_B$，$\mathbf{x}_C$だとすると，$f(\mathbf{x}_A) > f(\mathbf{x}_B)$などとなるように学習する．
なお，学習する順位内に同順位のものがあっても，「それらが同順位である」ということは学習されない．例えば$A > B = C$という順位があった場合には生成される組み合わせは$A > B$，$A > C$だけであり$B = C$という関係が考慮されることはない．

また，同時に複数のランキングを学習することも可能である．例えば，$A_1 > B_1 > C_1$と$A_2 > B_2 > C_2$という二つの独立したランキングがあった場合には$A_1 > B_1$，$A_1 > C_1$，$B_1 > C_1$，$A_2 > B_2$，$A_2 > C_2$，$B_2 > C_2$のようにそれぞれ独立した組み合わせを生成し，これら全てを満たすように識別関数を学習する．

未知のインスタンス集合に対するランキング予測では，各インスタンスに対して学習された$\mathbf{w}$を用いて$f(\mathbf{x})$を計算し，その値の順が出力されるランキングとなる．

ランキング学習は二値分類に適用することが可能であり，正例と負例に対応関係がある場合には通常の二値分類よりも有効であると言われている\cite{磯崎秀樹:2006-07-15}．これは通常の二値分類器では，全ての正例と負例を同一の特徴空間に写像するが，ランキング学習では正例と負例の差を特徴空間に写像するためである．例えば，入力$x_1$に対する出力候補が$(A_1\colon +, B_1\colon -, C_1\colon -)$\footnote{$+$は正例，$-$は負例を示す．}，入力$x_2$に対する出力候補が$(A_2\colon +, B_2\colon +, C_2\colon -)$
となるような学習事例があったとする．この場合，通常の二値分類器では$(A_1\colon +, B_1\colon -, C_1\colon -,A_2\colon +, B_2\colon +, C_2\colon -)$のように事例をひとまとめにして扱うため，本来直接の比較の対象ではない$A_1\colon +$と$C_2\colon -$などが同一の特徴空間上で比較されることとなる．一方，ランキング学習であれば，$A_1 > B_1 = C_1$と$A_2 = B_2 > C_2$のように，ランキングとして表現することで，$A_1\colon +$と$C_2\colon -$などが同一特徴空間上で比較されることはない．このようにして学習された識別関数は二値分類問題における識別関数として利用することができ，二値分類の場合と同様に出力の信頼度としても利用できる．
そこで本研究では，入力毎の出力候補に対して正負の正解がラベル付けされた事例からランキング学習により識別関数を学習し，推定の際には識別関数の出力が最も高くなる（最尤）ものを出力する形でランキング学習を利用する．


\section{ベースラインモデル}
\label{114838_18Jun13}

本節では本研究でのベースラインとなる外界ゼロ照応を考慮しないゼロ照応解析モデルを説明する．
本研究ではゼロ照応解析を用言単位の述語項構造解析の一部として扱う．
用言単位の述語項構造解析では，用言と複数の項の間の関係を扱うことができる．
例えば「(不動産屋ガ)物件を紹介する」のガ格のゼロ照応解析ではヲ格の項が「物件」であることが大きな手掛かりとなる．

各述語項構造は格フレームと，その格フレームの格スロットとその格スロットを埋める項の対応付けとして表現される．
格フレームは用言の用法毎に構築されており，各格フレームはその用言が項を取る表層格（格スロット）とその格スロットの項として取られる語（用例）からなる．
本研究では，Webページから収集された69億文から\cite{kawahara-kurohashi:2006:HLT-NAACL06-Main}の手法で自動構築された格フレームを用いる．
構築された格フレームの例を図\ref{述語項構造解析によるゼロ照応解析の概要}に示す\footnote{{\textless}時間{\textgreater} は「今日」「3時」などの時間表現を汎化したものである．}．

本研究では，ゼロ代名詞の照応先を談話要素という単位で扱う．
談話要素とは文中の表現のうち共参照関係にあるものをひとまとめにしたものである．
例えば図\ref{述語項構造解析によるゼロ照応解析の概要}の例では，「僕」と「自分」や「ラーメン屋$_1$」\footnote{同じ表現を区別するために添字を付与している．}，「その店」，「ラーメン屋$_2$」と「お店」は共参照関係にあるので，それぞれ一つの談話要素として扱う．
そしてゼロ代名詞の照応先はこの談話要素から選択する．例えば，「紹介したい」のガ格では「僕」に対応する(a)を照応先として選択することになる．

述語項構造解析の例を図\ref{述語項構造解析によるゼロ照応解析の概要}に示す\footnote{格に対応付けられるのは談話要素だが，出力される述語項構造を見やすくするために談話要素の代表的な表現を併せて表記している．「×」は格に何も対応付けないことを示す．
}．
なお，本研究ではゼロ照応解析の対象としてはガ，ヲ，ニ，ガ 2 格のみを扱うため，時間格などの他の格については省略することがある．ここでガ 2 格とは京都大学テキストコーパスで定義されている，二重主格構文における主格にあたる格である\footnote{「彼はビールが好きだ」の場合「彼」がガ 2 格にあたる．}．
この例では，「紹介する」に対応する格フレームから「紹介する(1)」を選択し，そのガ格に談話要素(a)，ヲ格に談話要素(c)，時間格に談話要素(d)を対応付け，それ以外の格には談話要素を対応付けない．ゼロ照応解析の出力としては，ガ格の談話要素(a)のみが出力される．

\begin{figure}[t]
\includegraphics{21-3ia1011f1.eps}
\caption{述語項構造解析によるゼロ照応解析の概要}
\label{述語項構造解析によるゼロ照応解析の概要}
\vspace{-6pt}
\end{figure}


ベースラインモデルでは，先行研究\cite{sasano-kurohashi:2011:IJCNLP-2011}と同様に以下の手順で解析を行う．
\begin{enumerate}
 \item 形態素解析，固有表現認識，構文解析を行う．
 \item 共参照解析を行いテキスト中に出現した談話要素を認識する．\label{談話要素設定}
 \item 各用言について以下の手順で述語項構造を決定する．\label{述語項構造}
	   \begin{enumerate}
		\item 以下の手順で解析対象用言がとりえる述語項構造（格フレームと談話要素の対応付け）の組み合わせを列挙する．\label{述語項構造列挙}
			  \begin{enumerate}
			   \item 解析対象用言の格フレームを1つ選ぶ．\label{格フレーム選択}
\pagebreak
			   \item 解析対象用言と係り受け関係にある語と格スロットの対応付けを行う．\label{係り受け対応付け}
			   \item 対応付けられなかったガ格，ヲ格，ニ格，ガ 2 格の格スロットと，対象用言の格スロットとまだ対応付けられていない談話要素の対応付けを行う．\label{ゼロ照応}
			  \end{enumerate}
		\item 学習されたランキングモデルによりもっとも高いスコアが与えられたものを述語項構造として出力する．\label{手順:スコア関数}
	   \end{enumerate}
\end{enumerate}
先行研究と本研究でのベースラインモデルとの違いは，(\ref{手順:スコア関数})でのスコア付けの際の重みの学習方法の違いである．先行研究では対数線形モデルを利用していたが，本研究ではランキング学習を用いた\footnote{対数線形モデル，ランキング学習は共に線形識別器であり，本質的な表現力に差はない．また，事前実験においてベースライン手法においてこれらの間の精度に大きな差がないことを確認している．}．このランキング学習の詳細は\ref{素性の重みの学習}節で説明する．

\begin{figure}[b]
\includegraphics{21-3ia1011f2.eps}
  \caption{述語項構造の候補の例}
\label{述語項構造の候補の例}
\end{figure}

手順(\ref{述語項構造})の述語項構造解析について説明する．
まず，(\ref{述語項構造列挙})の手順で候補となる述語項構造$(cf,a)$を列挙する．
ここで$cf$は選ばれた格フレーム，$a$は格スロットと談話要素の対応付けである．
ただし，同一用言の複数の格に同じ要素が入りにくいという経験則\cite{199787}から，手順(\ref{ゼロ照応})では既に他の格に対応付けられた談話要素は，ゼロ代名詞には対応付けないこととする．
手順(\ref{述語項構造列挙})で列挙される述語項構造の例を図\ref{述語項構造の候補の例}に示す\footnote{【i-j】はi番目の格フレームに対するj番目の対応付けを持つ述語項構造に対して便宜的に割り振ったものである．}．
【1-1】と【2-1】，【1-2】と【2-2】などは，格と談話要素の割り当ては同じであるが格フレームは異なるため別々の述語項構造候補として扱う．
【1-2】と【2-2】のどちらを述語項構造として選んでもゼロ照応解析としての出力は同じになる．
この列挙された述語項構造をそれぞれ\ref{述語項構造を表現する素性}節で説明する手法で素性として表現し，\ref{素性の重みの学習}節で説明する方法で学習された重みを利用してスコア付けを行い，最終的に最もスコアが高かった述語項構造を出力する．







\subsection{述語項構造を表現する素性}
\label{述語項構造を表現する素性}

本節では述語項構造を表現する素性について説明する．
入力テキスト$t$の解析対象用言$p$に格フレーム$cf$を割り当て，その格フレームの格スロットと談話要素の対応付けを$a$とした述語項構造を表現する素性ベクトルを$\phi(\mathit{cf},a,p,t)$とする．
$\phi(\mathit{cf},a,p,t)$は直接係り受けがある述語項構造に関する素性ベクトル$\phi_{\textit{overt-PAS}}(\mathit{cf},a_\mathit{ovet},p,t)$とゼロ照応解析で対象となる各格$c$に談話要素$e$が割り当てられることに関する素性ベクトル$\phi_\mathit{case}(\mathit{cf},c \leftarrow e,p,t)$からなり，具体的には以下のような形とする．ここで，$a_\mathit{overt}$は用言$p$と直接係り受けのある談話要素と格スロットの対応付けである．
\begin{equation}
\begin{split}
 \phi(\mathit{cf},a,p,t) = (& \phi_{\textit{overt-PAS}}(\mathit{cf},a_\mathit{overt},p,t), \\
  & \phi_\mathit{case}(\mathit{cf},ガ \leftarrow e_{ガ},p,t),\phi_\mathit{case}(\mathit{cf},ヲ \leftarrow e_{ヲ},p,t),\\
  & \phi_\mathit{case}(\mathit{cf},ニ \leftarrow e_{ニ},p,t),\phi_\mathit{case}(\mathit{cf},ガ 2 \leftarrow e_{ガ 2},p,t))
\end{split}
\end{equation}

各格に対応する素性ベクトル$\phi_\mathit{case}(\mathit{cf},c \leftarrow e,p,t)$は格$c$に談話要素$e$が対応付けられた場合の素性ベクトル$\phi_{A}(\mathit{cf},c \leftarrow e,p,t)$と何も対応付けられなかった場合の素性ベクトル$\phi_\mathit{NA}(\mathit{cf},c \leftarrow 
\linebreak
×,p,t)$からなる．$\phi_\mathit{case}(\mathit{cf},c \leftarrow e,p,t)$は格$c$がゼロ照応として対応付けられた場合にのみ考慮し，直接係り受け関係にある談話要素に対応付けられた場合には0ベクトルとする．

例えば図\ref{述語項構造の候補の例}の【2-2】を表現する素性ベクトル$\phi(紹介する(2),\{ガ:(a)僕，ヲ:(c)ラーメン屋，ニ:×，ガ 2:×，時間:(d)今日\})$は以下のようになる\footnote{入力テキスト$t$と対象用言$p$は省略した．${\bf 0_\phi}$は$\phi$と同次元を持つ0ベクトルを表す．}．
\begin{gather}
 \small{\phi(紹介する(2),\{ガ:(a)僕，ヲ:(c)ラーメン屋，ニ:×，ガ 2:×，時間:(d)今日\}   )=} \nonumber\\[-2pt]
\begin{split}
 (\phi_{\textit{overt-PAS}}(紹介する(2),\{ガ:×，ヲ:(d)ラーメン屋，ニ:×，\\ガ 2:×，デ格:(c)ブログ\}),\label{素性ベクトル例}
\end{split}\\[-2pt]
\begin{aligned}
 \small&\phi_{A}(紹介する(2),ガ \leftarrow (a)僕),&{\bf 0_{\phi_\mathit{NA}}},\nonumber\\[-2pt]
\small&{\bf 0_{\phi_\mathit{A}}},&{\bf 0_{\phi_\mathit{NA}}},\\[-2pt]
\small&{\bf 0_{\phi_{A}}},&\phi_\mathit{NA}(紹介する(2),ニ \leftarrow ×),\nonumber\\[-2pt]
\small&{\bf 0_{\phi_{A}}},&\phi_\mathit{NA}(紹介する(2),ガ 2 \leftarrow ×))\nonumber
\end{aligned}
\end{gather}

\begin{table}[p]
\caption{$\phi_{A}(\mathit{cf},c \leftarrow e,p,t)$の素性一覧}
\label{割り当ての素性一覧(1)}
\input{1011table02.txt}
\end{table}

述語項構造ベクトルを表現する各要素$\phi_{\textit{overt-PAS}}(\mathit{cf},a_\mathit{overt},p,t)$，$\phi_{A}(\mathit{cf},c \leftarrow e,p,t)$，$\phi_\mathit{NA}(\mathit{cf},c \leftarrow ×,p,t)$の素性について説明する．
まず，$\phi_{\textit{overt-PAS}}(\mathit{cf},a_\mathit{overt},p,t)$には確率的格解析モデル\cite{河原大輔:2007-07-10}から得られる表層の係り受けの確率を用いる．
$\phi_{A}(\mathit{cf},c \leftarrow e,p,t)$に用いる素性の一覧を表\ref{割り当ての素性一覧(1)}に示す\footnote{値の列でlogと書かれたものは，実際にはその対数をとったものを素性として利用している．バイナリと書かれたものは，多値の素性をバイナリで表現したものを素性として利用している．整数値と書かれたものは，その値をそのまま利用している．}．
格フレーム素性は，格フレームから得られる情報である．$e$が複数回言及される場合には，各素性ごとにそれらの値で最も大きいものをその素性の値とする．
例えば，談話要素$e$が格フレーム$\mathit{cf}$の格$c$に対応付く確率の素性を式(\ref{素性ベクトル例})のガ格について考える．上述の例ではガ格に対応付けられた談話要素(a)は「僕」「自分」と2回言及されている．
そこで「僕」「自分」が「紹介する(2)」のガ格に対応付く確率をそれぞれ計算し，最も値が高いものを(a)が「紹介する(2)」のガ格に対応付く確率とする．
用言素性における$p$の持つモダリティなどの情報は，用言の属する基本句に日本語構文・格解析システムKNP ver.~4.0\footnote{http://nlp.ist.i.kyoto-u.ac.jp/index.php?KNP}により付与された情報を利用する．
文脈素性は$e$が前後の文脈でどのような表現で出現するかを扱う素性であり，$e$が複数回言及される場合には，その全てを素性として扱う．
$c$が割り当てられたことの素性は，その格にどの程度ゼロ代名詞が出現するかを調整するための素性となっている．
$\phi_\mathit{NA}(\mathit{cf},c \leftarrow ×,p,t)$に用いる素性を表\ref{割り当てないの素性一覧}に示す．$\phi_\mathit{NA}(\mathit{cf},c \leftarrow ×,p,t)$では対応付けられる要素$e$がないため，格フレームに関する素性のみとなっている．


\begin{table}[t]
\caption{$\phi_\mathit{NA}(\mathit{cf},c \leftarrow ×,p,t)$の素性一覧}
\label{割り当てないの素性一覧}
\input{1011table03.txt}
\end{table}


\subsection{素性の重みの学習}
\label{素性の重みの学習}

前節で入力テキスト$t$，解析対象用言$p$が与えれられたとき，格フレーム$\mathit{cf}$，格スロットと談話要素の対応付け$a$からなる述語項構造を表現する素性を$\phi(\mathit{cf},a,p,t)$としたが，それに対応する素性の重み$\mathbf{w}$をランキング学習により学習する．

ランキング学習の学習データ作成は，対象用言ごとに順位データを作成し，全用言の順位データを集約したものとする．
もし，述語項構造の正解が一意に求められるなら，その述語項構造を上位とし，それ以外の述語項構造を下位とする順位データを作成すればよい．
しかし，実際には以下の2つの問題がある．

1つ目は正解コーパスには1つの格に対して複数の談話要素が対応付けられたものが含まれることである．
例えば図\ref{述語項構造の候補の例(2)}の「焼いている」では正解として\{ガ:×，ヲ:(b)ケーキ+(c)クッキー，ニ:×，ガ 2:×，時間:(d)毎週\}のように，ヲ格に2つの談話要素が対応付けられる．
一方，提案手法では先に述べたように1つの格に対して1つの談話要素しか対応付けない．
そこで，1つの格に対して複数の談話要素が対応付けられている場合には，そのうちどれか1つの談話要素を割り当てていれば正解として扱うこととする．例えば，図\ref{述語項構造の候補の例(2)}では\{ガ:×，ヲ:(b)ケーキ，ニ:×，ガ 2:×，時間:(d)毎週\}と\{ガ:×，ヲ:(c)クッキー，ニ:×，ガ 2:×，時間:(d)毎週\}を正解の談話要素対応付けとする．
また，この正解となる対応付けの集合を$(a^{*}_{1},\cdots,a^{*}_{N})$とする．

2つ目はコーパスには格フレームの正解は付与されていないことである．
先に述べたように述語項構造は格フレームと格スロットと談話要素の対応付けからなる．
格フレームは用言の用法ごとに構築されており，述語項構造候補の格フレームには文脈で使用される用法と全く異なるもの\footnote{慣用的な用法に対応する格フレームなど．}が含まれる．
格スロットと談話要素の対応付けは正しいが，文脈での使用とは異なる用法の格フレームを持つような述語項構造を正解として扱った場合，学習に悪影響を与えると考えられる．
そこで，確率的ゼロ照応解析\cite{sasano-kawahara-kurohashi:2008:PAPERS}を利用することで，各文脈における用法の格フレームを推定する．
確率的ゼロ照応解析では各述語項構造$(\mathit{cf},a)$に対し格フレームの情報などを用いることで$P(\mathit{cf},\mathit{a|p},\mathbf{e})$を推定する．ここで$\mathbf{e}$は文章中に出現する談話要素$e$の集合である\footnote{数式の変数は本論文における表記に統一した．}．

具体的には以下の手順で各対象用言$p$に対して学習データとなるランキングを生成する．
\begin{enumerate}
 \item 用言$p$に対して取り得る述語項構造$(\mathit{cf},a)$を訓練事例として列挙する．
	   \label{190308_16Oct13}
 \item 正解となる対応付け$a^{*}_{1},\cdots,a^{*}_{N}$について
 \begin{enumerate}
  \item 各$(\mathit{cf}, a^*_{i})$の確率的ゼロ照応解析確率を計算し，最も確率が高いものを$(\mathit{cf}^*_{i}, a^*_{i})$とする．
		\label{190424_16Oct13}
  \item $(\mathit{cf}, a^*_{i})$のうち，$(\mathit{cf}^*_{i},a^*_{i})$以外のものを訓練事例から取り除く．
		\label{190439_16Oct13}
 \end{enumerate}
 \item 各$(\mathit{cf}^*_{i},a^*_{i})$が他の$(\mathit{cf},a)$より順位が高くなるようなランキングを用言$p$に対する学習データとする．
	   \label{190449_16Oct13}
\end{enumerate}


図\ref{述語項構造の候補の例(2)}の「焼いている」を例に説明する．
まず「焼いている」の述語項構造解析の候補として【1-1】，$\cdots$，【2-1】，$\cdots$を列挙する（手順(\ref{190308_16Oct13})）．
このうち，格と談話要素の対応付けが正解となるもの$(\mathit{cf}, a^*_{i})$は，\{ガ:×，ヲ:(b)ケーキ，ニ:×，ガ 2:×，時間:(d)毎週\}となっている【1-2】と【2-2】，\{ガ:×，ヲ:(c)クッキー，ニ:×，ガ 2:×，時間:(d)毎週\}となっている【1-3】と【2-3】である．
【1-2】，【2-2】，【1-3】，【2-3】について確率的ゼロ照応解析スコアを計算した結果，【1-2】$>$【2-2】，【1-3】$>$【2-3】となったとする．
この場合【1-2】と【1-3】が$(\mathit{cf}^*_{i}, a^*_{i})$となる（手順(\ref{190424_16Oct13})）．
そこで，訓練事例から【2-2】と【2-3】を取り除く（手順(\ref{190439_16Oct13})）．
そして，【1-2】$=$【1-3】$>$【1-1】$=$【1-4】$= \cdots=$ 【2-1】$=$【2-4】$= \cdots$というランキングを「焼いている」についての学習データとする（手順(\ref{190449_16Oct13})）．
このように各対象用言に対するランキング学習データを生成し，それらを統合したものに対してランキング学習を行うことで$\mathbf{w}$を学習する．

\begin{figure}[t]
\includegraphics{21-3ia1011f3.eps}
\caption{1つの格に複数の談話要素が割り当てられる例}
\label{述語項構造の候補の例(2)}
\vspace{-1\Cvs}
\end{figure} 


\section{コーパス}
\label{130555_9May13}

本研究では，Diverse Document Leads Corpus (DDLC) \cite{hangyo-kawahara-kurohashi:2012:PACLIC}を利用する．
DDLCはWebから収集されたテキストに対して，形態素情報，構文関係，固有表現，共参照関係，述語項構造，著者・読者表現が付与されている．形態素情報，構文関係，固有表現，共参照関係は京都大学テキストコーパス\cite{KTC}と同様の基準で付与されている．
述語項構造も京都大学テキストコーパスと同様の基準で付与されており，文章内ゼロ照応だけでなく外界ゼロ照応も付与されている．外界ゼロ照応の照応先としては表\ref{外界ゼロ照応の照応先と例}の5種類が設定されている．

著者・読者表現は，「=:[著者]」「=:[読者]」というタグ\footnote{「{\it rel}:A」という表記は{\it rel}という関係でAという情報が付与されていることを示す．また，「B $\leftarrow$ {\it rel}:A」という表記はBに対して「{\it rel}:A」という情報が付与されていることを示す．}で基本句単位に付与されており，著者・読者表現が複合語の場合にはその主辞に対して付与されている．
DDLCでは，著者表現，読者表現は1文書中にそれぞれ最大でも1つの談話要素と仮定されており，著者・読者が複数回言及される場合には，そのうち1つに「=:[著者]」「=:[読者]」を付与し，それ以外のものは，著者・読者表現と共参照関係にある，という形で表現される．
下記の例\ref{こま}では，著者は「主婦」や「こま」，「母」など複数の表現で言及されているが，「=:[著者]」は「主婦」に対してだけ付与され，「こま」や「母」には「=:主婦」というタグにより，「主婦」と共参照関係にあるという情報が付与されている．

\begin{table}[t]
\caption{外界ゼロ照応の照応先と例}
\label{外界ゼロ照応の照応先と例}
\input{1011table04.txt}
 \end{table}


\ex .東京都に住む「お気楽\underline{主婦}」\underline{こま}です。\\
\label{こま}
０歳と６歳の男の子の\underline{母}をしてます。\\
\hspace*{4ex}$\left(
\begin{tabular}{@{}l@{}}
主婦$\leftarrow$=:[著者]\\
こま$\leftarrow$=:主婦\\
母$\leftarrow$=:主婦
\end{tabular}
 \right)$

また，組織のウェブページなどの場合にはその組織名や組織を表す表現を著者表現としている．

\ex. ここでは\underline{弊社}の商品及び事業を簡単にご説明します。\\
\hspace*{4ex}(弊社$\leftarrow$=:[著者])

\ex. 神戸徳洲会\underline{病院}では地域の医療機関との連携を大切にしています。\\
\hspace*{4ex}(病院$\leftarrow$=:[著者])

ウェブページでは実際には不特定多数が閲覧できる状態であることが多いが，著者が特定の読者を想定していると考えられる場合には，その特定の読者を表す表現も読者表現として扱っている．
下記の例\ref{読者=人}では，想定している読者が「今後就職を迎える人」だと考えられるので，その主辞の「人」に「=:[読者]」が付与されている．

\ex. 今後就職を迎える\underline{人}に，就職活動をどのように考えれば良いのかをお知らせしてみましょう。
\label{読者=人}\\
\hspace*{4ex}(人$\leftarrow$=:[読者])

一方，想定している読者のうち一部だけを対象とした表現は読者表現として扱っていない．
下記の例\ref{ローソン}では，想定される読者は「オーナーを希望する人」であり，「店舗運営の経験がない方」はそのうちの一部であると考えられるので，読者表現として扱われていない．

\ex. 店舗運営の経験がない\underline{方}でも、ご安心ください。\label{ローソン}
ローソンの研修制度なら、オーナーに必要とされるノウハウを段階的に修得することができます。

 \begin{table}[b]
 \caption{DDLCにおけるゼロ照応の個数}
 \label{ゼロ照応の個数}
\input{1011table05.txt}
\end{table}
 \begin{table}[b]
 \caption{DDLCの文章内ゼロ照応の内訳}
 \label{本コーパスの文章内ゼロ照応の内訳}
\input{1011table06.txt}
\end{table}
 \begin{table}[b]
  \begin{center}
 \caption{DDLCの外界ゼロ照応の内訳}
 \label{本コーパスの外界ゼロ照応の内訳}
\input{1011table07.txt}
\end{table}


表\ref{著者・読者表現の例}にDDLC中の著者・読者表現の例を示す．
DDLC全体1,000文書のうち著者表現が付与された文書は271文書，読者表現が付与された文書は84文書であった．

DDLCにおけるゼロ照応の個数を表\ref{ゼロ照応の個数}に，文章内ゼロ照応，外界ゼロ照応における照応先の内訳を表\ref{本コーパスの文章内ゼロ照応の内訳}と表\ref{本コーパスの外界ゼロ照応の内訳}に示す．
表\ref{本コーパスの文章内ゼロ照応の内訳}において著者・読者とは照応先が著者・読者表現にあたることを示す．
DDLCにおいてはゼロ照応のうち54\%が外界ゼロ照応であること，ゼロ照応はガ格で特に多く起こることが分かる．
また，著者や読者に関するゼロ照応はガ格，ニ格，ガ 2 格で多く出現し，ヲ格ではほとんど出現しないことが分かる．


\begin{table}[t]
\caption{著者・読者表現の例}
\label{著者・読者表現の例}
\input{1011table08.txt}
\end{table}


\section{著者・読者表現推定}
\label{135602_6May13}

日本語では様々な表現で著者や読者が文章中で言及され，\ref{130555_9May13}節で述べたように人称代名詞だけでなく，固有表現，役職など様々な表現で言及される．
一方，表\ref{著者・読者表現の例}に挙げたような表現でも文脈によっては著者・読者表現にならないこともある．
下記の例\ref{お客様not著者}では，「お客様」はこの文章の読者として想定している客とは別の客を指していると考えられるので，読者表現とはならない．

\ex. 先月、お部屋のリフォームをされた\underline{お客様}の例を紹介します。\label{お客様not著者}

このように，表記のみから著者・読者表現を同定することは困難である．
本研究では著者・読者表現候補自体の表現だけでなく，周辺文脈や文章全体に含まれる情報から文章の著者・読者表現を推定することとする．


\ref{130555_9May13}節で述べたように，DDLCでは基本句単位で著者・読者表現がアノテーションされており，共参照関係にある複数の表現が著者・読者表現である場合には，その内の1つに対して著者・読者表現を付与するとしている．
これは\ref{114838_18Jun13}節で述べた談話要素単位に著者・読者表現が付与されていると言え，本研究でも著者・読者表現は談話要素単位で扱う．
著者・読者表現の推定にはランキング学習を利用し，その素性には著者・読者表現自身および周辺文脈の語彙統語パターンを利用する．


\subsection{著者・読者表現推定モデル}

著者・読者表現の推定は著者表現，読者表現それぞれ独立して行う\footnote{以降の具体例では著者表現を例として説明するが，読者表現の場合でも同様である．}．
著者・読者表現の推定にはランキング学習を利用し，著者・読者表現にあたる談話要素が他の談話要素より上位になるように学習する．
例えば図\ref{著者表現が出現する文章例}の著者表現では，談話要素(1)が他の談話要素より上位となる学習データを作成する．
そして学習された識別関数により最上位となった談話要素を著者・読者表現と推定する．
なお，著者・読者表現候補として扱う談話要素は下記の条件のうち最低1つは満たしているもののみとする．
\begin{itemize}
 \item 自立語の形態素のJUMANカテゴリが「人」「組織・団体」「場所」
 \item 固有表現である
 \item 形態素に「方」「人」を含む
\end{itemize}

ここで，学習データ作成時および推定時に考慮しなければならないのは，著者・読者表現が出現しない文書が存在することである．
著者・読者表現が出現しない文書は大きく分けて2つの種類がある．
1つ目は図\ref{談話構造自体に著者が出現しない文章例}のように談話構造自体に著者・読者が出現しない文書である．
2つ目は図\ref{談話構造に著者が出現するが著者表現が出現しない文章例}のように談話構造には著者・読者が出現するが著者・読者表現として明示的に言及されない場合である\footnote{全て省略されており，外界ゼロ照応の照応先としてのみ出現する．}．
これらに対応する仮想的なインスタンスとして，「著者・読者表現なし(談話構造)」と「著者・読者表現なし(省略)」を設定する．

\begin{figure}[b]
\includegraphics{21-3ia1011f4.eps}
\caption{著者表現が出現する文書例}
\label{著者表現が出現する文章例}
\end{figure}
\begin{figure}[b]
\includegraphics{21-3ia1011f5.eps}
\caption{談話構造自体に著者が出現しない文書例}
\label{談話構造自体に著者が出現しない文章例}
\end{figure}

「著者・読者表現なし(談話構造)」は談話構造自体に外界ゼロ照応としても著者・読者が出現しないことに対応するインスタンスであり，文書全体の語彙統語パターンを素性とした文書ベクトルで表現されるインスタンスである．
これは，著者・読者が談話構造自体に出現しない文書では，尊敬や謙譲表現が少ないなど文体的な特徴があると考えられ，文書全体の語彙統語パターンは文体を反映した素性といえるからである．

\begin{figure}[t]
\includegraphics{21-3ia1011f6.eps}
\caption{談話構造に著者が出現するが著者表現が出現しない文書例}
\label{談話構造に著者が出現するが著者表現が出現しない文章例}
\end{figure}

「著者・読者表現なし(省略)」は談話構造には外界ゼロ照応として著者・読者が出現するが，著者・読者表現として明示的に言及されないことに対応するインスタンスであり，ゼロベクトルとして表現される．
識別関数はゼロベクトルについて常に$0$を返すため，このインスタンスより下位に順位付けされた談話要素は二値分類における負例とみなすことができる．
学習された識別関数によるランキングの結果，これらのインスタンスが最上位となった文書については，著者・読者表現が出現しないものとする．



各文書に対する学習データの作成について説明する．以下の手順で著者表現，読者表現に対して文書ごとにランキングデータ作成し，統合したものを最終的な学習データとする．

著者・読者表現が存在する文書については，著者・読者表現にあたる談話要素が他の談話要素および「著者・読者表現なし」より上位になるように学習データを作成する．
例えば，図\ref{著者表現が出現する文章例}の文書における著者表現推定では，
\[
 (1) > (2) = (3) = \cdots  = (6) = 著者表現なし(談話構造) = 著者表現なし(省略)
\]
となるように学習データを作成する．

談話構造自体に著者・読者が出現しない場合には，「著者・読者表現なし(談話構造)」が文書中の談話要素および「著者・読者表現なし(省略)」より上位になるように学習データを作成する．
例えば，図\ref{談話構造自体に著者が出現しない文章例}の文書における著者表現推定では，
\[
著者表現なし(談話構造) > (1) = (2) = \cdots = (6) = 著者表現なし(省略)
\]
となるように学習データを作成する．

談話構造に著者・読者が出現するが著者・読者表現が出現しない場合には，「著者・読者表現なし(省略)」が文書中の談話要素および「著者・読者表現なし(談話構造)」より上位になるように学習データを作成する．
例えば，図\ref{談話構造に著者が出現するが著者表現が出現しない文章例}の文書における著者表現推定では，
\[
著者表現なし(省略) > (1) = 著者表現なし(談話構造)
\]
となるように学習データを作成する． 



学習時の談話構造に著者・読者が出現するかの判定には，コーパスに付与された外界ゼロ照応の情報を利用する．
外界ゼロ照応の照応先として著者・読者が出現する場合には談話構造に著者・読者が出現するとし，それ以外の場合には出現なしとする．
例えば図\ref{談話構造に著者が出現するが著者表現が出現しない文章例}では，「気がつけば」のガ 2 格，「ほっとしました」のガ格などの照応先で著者が出現するので，談話構造に著者が出現していると分かる．
なお，この情報はランキングの学習データを作成する際にのみ利用するので，テストデータに対する著者・読者表現推定時には利用しない．



\subsection{素性として利用する語彙・統語パターン}

談話要素に対しては，談話要素自身と係り先およびこれらの係り受けの語彙統語パターンを素性として扱う．
ここで，語彙統語パターンを扱う単位として，基本句と文節という2つの単位を考える．
談話要素は基本句単位であるが，その基本句が含まれる文節の情報も重要と考えられるからである．

談話要素を表現する語彙統語パターンとしては，談話要素が含まれる基本句・文節，談話要素の係り先の基本句・文節およびこれらの係り受け関係を後述する基準にて汎化したものとなる．
1つの談話要素が複数言及されている場合には，それらを合わせたものをその談話要素の素性として用いる．
また，1文目には自己紹介的な表現が用いられることが多く，以降の文と区別して扱うことが有効であると考えられる．
そこで，1文目で出現した語彙統語パターンは別の素性としても扱うこととする．
例えば，図\ref{著者表現が出現する文章例}の談話要素(1)に対応する素性として利用するものは以下のものを汎化したものとなる．

\begin{screen}
 「基本句:ホテルは」「基本句係り先:ございます。」「基本句係受け:ホテルは $\rightarrow$ ございます。」
「文節:米子タウンホテルは」「文節係り先:ございます。」「文節係り受け:米子タウンホテルは $\rightarrow$ ございます。」
「基本句:ホテルです。」「文節:ホテルです。」 
「1文目基本句:ホテルは」「1文目基本句係り先:ございます。」「1文目基本句係受け:ホテルは $\rightarrow$ ございます。」「1文目文節:米子タウンホテルは」「1文目文節係り先:ございます。」「1文目文節係り受け:米子タウンホテルは $\rightarrow$ ございます。」
\end{screen}

\begin{table}[t]
  \caption{汎化する種類と基準}
  \label{汎化する種類と基準}
\input{1011table09.txt}
\end{table}
\begin{table}[t]
  \caption{一人称代名詞と二人称代名詞}
\label{一人称代名詞}
\input{1011table10.txt}
\end{table}


これらの要素を表\ref{汎化する種類と基準}の基準で汎化することで語彙統語パターンの素性として利用する．
「形態素単位A」の汎化は形態素単位に付与された情報を元に形態素毎に汎化を行う．
なお「品詞+活用」では内容語に対してのみ行い，機能語については汎化しない．

「形態素単位B」でも形態素単位での汎化を行うが，内容語に対する汎化のみを行う．対象となる汎化表現を持たない場合（固有表現による汎化の際に固有表現を持たない場合など）には「品詞+活用」で汎化を行う．

「形態素単位C」では「形態素単位B」と同様に汎化を行うが，複数の形態素をまたいだ汎化を行う場合がある．
分類語彙表による汎化では，分類語彙表に複合語として登録されている場合には，その複合語の分類語彙表の内容を利用する．例えば，「ゴルフ場」は2形態素であるが，分類語彙表には「ゴルフ場:土地利用」として登録されているので，「ゴルフ場」を「土地利用」と汎化する．
固有表現による汎化では，形態素に付与された固有表現は「固有表現名:head」「固有表現名:middle」「固有表現名:tail」「固有表現名:single」のように固有表現中での位置が付与されている．文節による汎化の際には連続したこれらの表現をまとめて「固有表現」という形に汎化する．例えば「ヤフージャパン株式会社」では「ORGANIZATION:head+ORGANIZATION:middle+ORGANIZATION:middle+\linebreak[2]ORGANIZATION:tail」のように固有表現が付与されるが，「ORGANIZATION」として汎化する．
これらの形態素単位での汎化では，形態素ごとに汎化を行い，その後基本句内，文節内で汎化された情報を結合することでその基本句，文節の語彙統語パターンとする．例えば，「基本句:ホテルは」をカテゴリ(CT)の基準で汎化する際には，「ホテル」を「CT-場所-施設」に汎化し，「は」は機能語なのでそのまま「は」とする．そしてこれらを結合した「基本句:CT-場所-施設+は」がこの基本句のカテゴリによる汎化表現となる．

上述の形態素単位の汎化の場合には，基本句，文節内に含まれる個々の汎化表現も素性とする．
例えば，「基本句:ホテルは」のカテゴリによる汎化では上述の「基本句:CT-場所-施設+は」に加えて，「基本句内形態素:CT-場所-施設」も素性とする\footnote{「は」はカテゴリでは汎化されないので，形態素単位の素性としては利用しない．}．

基本句・文節単位での汎化は基本句・文節に付与された情報を元に汎化を行う場合に利用する．
基本句・文節単位での汎化では基本句・文節に付与された情報そのものを基本句，文節の語彙統語パターンとして利用する．
このため，形態素単位による情報は素性としては利用しな
\linebreak
い．



\section{外界照応および著者・読者表現を考慮したゼロ照応解析モデル}
\label{115042_18Jun13}

本節では，外界ゼロ照応および著者・読者表現を考慮したゼロ照応解析モデルについて説明する．
提案モデルでは，ベースラインモデルと同様にゼロ照応解析を述語項構造解析の一部として解く．

提案モデルではゼロ照応解析は以下の手順で行う．
\begin{enumerate}
 \item 形態素解析，固有表現認識，構文解析を行う．
 \item 共参照解析を行いテキスト中に出現した談話要素を認識する．
 \item 著者・読者表現の推定を行い，どの談話要素が著者・読者表現にあたるのかを推定する．\label{手順:著者・読者推定}
 \item 推定された著者・読者表現から仮想的な談話要素を設定する（\ref{節:仮想的談話要素}節で説明）．\label{手順:仮想的談話要素}
 \item 各用言について以下の手順で述語項構造を決定する．
\begin{enumerate}
 		\item 以下の手順で解析対象用言がとりえる述語項構造（格フレームと談話要素の対応付け）の組み合わせを列挙する．
			  \begin{enumerate}
			   \item 解析対象用言の格フレームを1つ選ぶ．
			   \item 解析対象用言と係り受け関係にある語と格スロットの対応付けを行う．
			   \item 対応付けられなかったガ格，ヲ格，ニ格，ガ 2 格の格スロットと，対象用言の格スロットとまだ対応付けられていない談話要素の対応付けを行う．
			  \end{enumerate}
 \item 学習されたランキングモデルによりもっとも高いスコアが与えられたものを述語項構造として出力する．
\end{enumerate}
\end{enumerate}

ベースラインモデルと異なる点は，手順(\ref{手順:著者・読者推定})で文章中の著者・読者表現を推定すること，手順(\ref{手順:仮想的談話要素})で仮想的な談話要素を設定することである．


\subsection{外界ゼロ照応を扱うための仮想的談話要素}
\label{節:仮想的談話要素}

ベースラインモデルではゼロ代名詞の照応先を文章中の談話要素から選択することとした．
提案モデルでは，文章中の談話要素に加えて仮想的な談話要素として[著者]，[読者]，[ 不特定:人]，[ 不特定:その他]を設定し，解析の際の述語項構造の列挙においては，格に対応付ける談話要素の候補としてこれらの仮想的な談話要素も考えることとする\footnote{[不特定:状況]は数が少ないため，[不特定:物]と[不特定:状況]を合わせて[不特定:その他]とした．}．
そして，これらが対応付けられた格は外界ゼロ照応であるとする．
例\ref{仮想的談話要素例}では，「説明します」のガ格が外界ゼロ照応で著者を照応しており，ニ格は外界ゼロ照応で読者を照応しているため，ガ格に[著者]を，ニ格に[読者]を対応付けた述語項構造として表現される．

\ex. 今日はお得なポイントカードについて\underline{説明します}。
\label{仮想的談話要素例}

著者・読者表現が文章中に出現する場合には，[著者]と[読者]の仮想的談話要素の扱いが問題となる．
下記の例\ref{著者・読者表現あり}ではゼロ代名詞$\phi$の照応先は，著者表現である「私」とも[著者]とも考えられる．

\ex. 肩こりや腰痛で来院された患者さんに対し、\underline{私}$_{著者}$は脈を診ることにしています。\\
それは心臓の状態を($\phi$ガ)診ているだけではなく、身体全体のバランスを($\phi$ガ)診たいからです。
\label{著者・読者表現あり}

本研究では，このような曖昧性を取り除くため，照応先としては[著者]，[読者]より著者・読者表現を優先することする．
解析の際，文章中に著者・読者表現が存在する場合には，[著者]，[読者]の仮想的談話要素は照応先として対応付けないこととする．
図\ref{述語項構造解析によるゼロ照応解析の概要}の「紹介します」に対して列挙される述語項構造の例を図\ref{提案手法における述語項構造の候補の例}に示す．
この例では，【1-3】のガ格や【1-4】のガ格などに仮想的な談話要素が対応付けられている．また，「(a)僕」が著者表現にあたるので[著者]はどの格にも対応付けを行わない．

\begin{figure}[t]
\includegraphics{21-3ia1011f7.eps}
  \caption{提案手法における述語項構造の候補の例}
\label{提案手法における述語項構造の候補の例}
\end{figure}

一方，著者・読者表現は外界の[著者]や[読者]と似た振る舞いを取ると考えられる\footnote{例えば著者表現と外界の著者は共に謙譲表現の主体になりやすいなど．}．
そこで，著者・読者表現は他の談話要素と区別し，素性表現の際に[著者]や[読者]の性質を持つように素性を与える．詳細は\ref{素性による述語項構造の表現}節で示す．



\subsection{素性による述語項構造の表現}
\label{素性による述語項構造の表現}

ベースラインモデルと同様に提案モデルでも述語項構造単位を素性で表現し，その構成もベースラインモデルと同様に$\phi(\mathit{cf},a,p,t)$は直接係り受けがある述語項構造に関する素性ベクトル$\phi_{\textit{overt-PAS}}\allowbreak(\mathit{cf},a_\mathit{overt},p,t)$とゼロ照応解析で対象となる格$c$に談話要素$e$が割り当てられることに関する素性ベクトル$\phi_\mathit{case}(\mathit{cf},c \leftarrow e,p,t)$からなる．

ベースラインモデルと提案モデルの差は$\phi_\mathit{case}(\mathit{cf},c \leftarrow e,p,t)$のうち，$c$に談話要素$e$が割り当てられた場合の素性ベクトル$\phi_{A}(\mathit{cf},c \leftarrow e,p,t)$の構成である．
ここでベースラインモデルでの$\phi_{A}(\mathit{cf},c \leftarrow e,p,t)$を$\phi'_{A}(\mathit{cf},c \leftarrow e,p,t)$とおく．
この$\phi'_{A}(\mathit{cf},c \leftarrow e,p,t)$と同内容，同次元の素性ベクトルを文章中に出現した談話要素，外界の[著者]，[読者]などと対応する形で複数並べることで$\phi_{A}(\mathit{cf},c \leftarrow e,p,t)$を構成する．

具体的には以下のような形となる．
\begin{align*}
 \phi_{A}(\mathit{cf},c \leftarrow e,p,t)  = (&\phi_\mathit{mentioned}(\mathit{cf},c \leftarrow e,p,t),\phi_{[著者]}(\mathit{cf},c \leftarrow e,p,t),\\
  & \phi_{[読者]}(\mathit{cf},c \leftarrow e,p,t),\phi_{[不特定:人]}(\mathit{cf},c \leftarrow e,p,t),\\
  & \phi_{[不特定:その他]}(\mathit{cf},c \leftarrow e,p,t),\phi_\mathit{max}(\mathit{cf},c \leftarrow e,p,t) )
\end{align*}

ここで，$\phi_\mathit{mentioned}(\mathit{cf},c \leftarrow e,p,t)$は$e$が文章中に出現した談話要素の場合のみに発火し，内容は$\phi'_{A}(\mathit{cf},c \leftarrow e,p,t)$と同内容とする．
$\phi_{[著者]}(\mathit{cf},c \leftarrow e,p,t)$，$\phi_{[読者]}(\mathit{cf},c \leftarrow e,p,t)$は$e$が外界の[著者]，[読者]の場合または著者・読者表現に対応する談話要素の場合にのみ発火する．$\phi_{[不特定:人]}(\mathit{cf},c \leftarrow e,p,t)$，$\phi_{[不特定:その他]}(\mathit{cf},c \leftarrow e,p,t)$は$e$が外界の[不特定:人]，[不特定:その他]の場合にのみ発火する．
$\phi_{[著者]}(\mathit{cf},c \leftarrow e,p,t)$，$\phi_{[読者]}(\mathit{cf},c \leftarrow e,p,t)$，$\phi_{[不特定:人]}(\mathit{cf},c \leftarrow e,p,t)$，$\phi_{[不特定:その他]}(\mathit{cf},c \leftarrow e,p,t)$は外界の談話要素に対応するため，表記やJUMANカテゴリといった情報を持たない．そこで，格フレーム素性で$e$の表記やJUMANカテゴリの情報を利用する場合には擬似的に表\ref{疑似表記，JUMANカテゴリ}の表記やJUMANカテゴリを利用する．
最後の$\phi_\mathit{max}(\mathit{cf},c \leftarrow e,p,t)$は各素性において$\phi_\mathit{mentioned}(\mathit{cf},c \leftarrow e,p,t)$，$\phi_{[著者]}(\mathit{cf},c \leftarrow e,p,t)$，$\phi_{[読者]}(\mathit{cf},c \leftarrow e,p,t)$，$\phi_{[不特定:人]}(\mathit{cf},c \leftarrow e,p,t)$，$\phi_{[不特定:その他]}(\mathit{cf},c \leftarrow e,p,t)$で対応する素性の最大値を持つ素性ベクトルとする．

\begin{table}[b]
\caption{疑似表記，JUMANカテゴリ}  
\label{疑似表記，JUMANカテゴリ}  
\input{1011table11.txt}
\end{table}

このように素性を表現することで，著者・読者表現に対応する談話要素では$\phi_\mathit{mentioned}$および$\phi_{[著者]}$・$\phi_{[読者]}$が発火することになり，通常の談話要素と著者・読者としての両方の性質を持つこととなる．
また，$\phi_\mathit{max}$は全ての要素に対して発火するため，$\phi_\mathit{max}$に対応する重みでは，ゼロ照応全体に影響する性質が学習されると考えられる．


ここで，図\ref{提案手法における述語項構造の候補の例}の述語項構造候補【1-5】ついて各格の$\phi_{A}(\mathit{cf},c \leftarrow e,p,t)$の例を示す．
\begin{align*}
 \phi_{A}(\mathit{cf},ガ \leftarrow (a)僕,p,t) & = (\phi_\mathit{mentioned}(\mathit{cf},ガ \leftarrow (a)僕,p,t),\phi_{[著者]}(\mathit{cf},ガ \leftarrow (a)僕,p,t),\\
  & \qquad {\bf 0_{\phi_{[読者]}}},{\bf 0_{\phi_{[不特定:人]}}},{\bf 0_{\phi_{[不特定:その他]}}},\\
  & \qquad \mathit{max}(\phi_\mathit{mentioned}(\mathit{cf},ガ \leftarrow (a)僕,p,t),\phi_{[著者]}(\mathit{cf},ガ \leftarrow (a)僕,p,t)) )
\end{align*}
まず，ガ格では「僕」は文章中に言及されているので，$\phi_\mathit{mentioned}(cf,ガ \leftarrow (a)僕,p,t)$が発火し，また著者表現に対応している談話要素なので$\phi_{[著者]}(\mathit{cf},ガ \leftarrow (a)僕,p,t)$も発火する．$\phi_{[読者]}(\mathit{cf},ガ \leftarrow e,p,t)$，$\phi_{[不特定:人]}(\mathit{cf},ガ \leftarrow e,p,t)$，$\phi_{[不特定:その他]}(\mathit{cf},ガ \leftarrow e,p,t)$は発火せず0ベクトルとなる．$\phi_\mathit{max}(\mathit{cf},ガ \leftarrow e,p,t)$は各要素において$\phi_\mathit{mentioned}(\mathit{cf},ガ \leftarrow (a)僕,p,t)$と$\phi_{[著者]}(cf,ガ \leftarrow (a)僕,p,t)$のうち大きい方の値が素性の値となる．
\begin{align*}
 \phi_{A}(cf,ニ \leftarrow [読者],p,t) & = ({\bf 0_{\phi_\mathit{mentioned}}},{\bf 0_{\phi_{[著者]}}},\\
  & \qquad \phi_{[読者]}(\mathit{cf},ニ \leftarrow [読者],p,t),{\bf 0_{\phi_{[不特定:人]}}},\\
  & \qquad {\bf 0_{\phi_{[不特定:その他]}}},\mathit{max}(\phi_{[読者]}(\mathit{cf},ニ \leftarrow [読者],p,t)) )
\end{align*}
ニ格では文章中に言及されていない読者が対応付けられているので，$\phi_{[読者]}(\mathit{cf},ニ \leftarrow [読者],p,t)$のみが発火し，$\phi_\mathit{max}(\mathit{cf},ニ \leftarrow [読者],p,t)$は$\phi_{[読者]}(\mathit{cf},ニ \leftarrow [読者],p,t)$と同じ値となる．
ヲ格は直接係り受けのある「ラーメン屋」と対応付けられているので，ベースラインモデルと同様に素性として考えず，$\phi_{A}(\mathit{cf},ヲ \leftarrow e,p,t)$，$\phi_\mathit{NA}(\mathit{cf},ヲ \leftarrow ×,p,t)$ともに0ベクトルとなる．
ガ 2 格は談話要素に対応付けられていないので，ベースラインモデルと同様に$\phi_\mathit{NA}(\mathit{cf},ガ 2 \leftarrow 
\linebreak
×,p,t)$が発火し，$\phi_{A}(\mathit{cf},ガ 2 \leftarrow e,p,t)$は0ベクトルとなる．


\subsection{使用する素性}

提案モデルでは\ref{述語項構造を表現する素性}節で述べたものに加えて，著者表現，読者表現推定スコアを素性として利用する．
著者表現，読者表現推定スコアは\ref{135602_6May13}節で著者・読者表現を推定した際のランキング学習の識別関数のスコアである．著者表現推定スコアは$e$が著者表現の場合に$\phi_\mathit{mentioned}(\mathit{cf},c \leftarrow e,p,t)$と$\phi_{[著者]}(\mathit{cf},c \leftarrow e,p,t)$に，読者表現推定スコアは$e$が読者表現の場合に$\phi_\mathit{mentioned}(\mathit{cf},c \leftarrow e,p,t)$と$\phi_{[読者]}(\mathit{cf},c \leftarrow e,p,t)$に素性として導入する．



\section{実験}
\label{115121_18Jun13}

\subsection{実験設定}

実験ではDDLCの1,000文書を利用し，5分割交差検定により評価を行った．
述語項構造解析および著者・読者表現推定以外の解析結果が原因となる解析誤りを除くため，形態素情報，係り受け情報，固有表現情報，共参照関係はコーパスに人手で付与された正しい情報を利用する．
著者・読者表現推定および述語項構造のランキング学習には$\mathit{SVM}^\mathit{rank}$\footnote{http://www.cs.cornell.edu/people/tj/svm\_light/svm\_rank.html}を用いた．


\subsection{著者・読者表現推定実験結果と考察}

\begin{table}[b]
\caption{著者・読者表現推定結果}
\label{著者・読者推定結果}
\input{1011table12.txt}
\end{table}

DDLCに対して，著者表現および読者表現を推定した結果を表\ref{著者・読者推定結果}に示す．
ここで，著者表現と読者表現は独立に推定しているため，評価も独立して行った．
著者・読者表現はそれぞれ各文書で最大1つと仮定されているため，著者・読者表現推定は文書ごとの多値分類問題といえる．そのため
評価は文書単位で行い，以下のような数式で求めた．
\pagebreak
\begin{gather*}
 適合率 = \frac{システムが著者・読者表現を正しく推定できた文書数}{システムが著者・読者表現ありと推定した文書数}  \\[0.5zw]
 再現率 = \frac{システムが著者・読者表現を正しく推定できた文書数}{コーパスに著者・読者表現が付与されている文書数} \\[0.5zw]
 F値 = \frac{2 \times 適合率  \times 再現率}{適合率 + 再現率} \\[0.5zw]
 精度 = \frac{システムが著者・読者表現を正しく推定or著者・読者表現なし推定できた文書数}{全文書数} 
\end{gather*}
この結果より，著者・読者表現なしを含めた精度では，著者において0.829，読者において0.936と高い精度を達成できた．
一方，再現率はあまり高くなく，著者表現で0.509，読者表現で0.595であった．これはコーパス全体で著者・読者表現が出現文書より著者・読者表現のない文書の方が多く，学習時に著者・読者表現なしを優先するように学習してしまったためと考えられる．
読者において適合率が低いが，これは読者の一部のみを想定した表現を読者表現と推定してしまうことが多かったためと考えられる．

\begin{figure}[b]
\includegraphics{21-3ia1011f8.eps}
\caption{著者推定誤り例1}
\label{著者推定誤り例1}
\end{figure}

図\ref{著者推定誤り例1}，図\ref{著者推定誤り例2}，図\ref{読者推定誤り例1}に誤り例を示す．
図\ref{著者推定誤り例1}では，コーパスでは著者表現が出現しないが誤って「山形県山上市」を著者表現と推測してしまった．
著者表現では固有表現は大きな手掛かりとなり，特に1文目に出現する固有表現は著者表現となる傾向が強い．
また，文体的に談話構造中に著者が出現するような表現が多用されている．そのため誤って「山形県山上市」を著者表現と推測してしまったと考えられる．

図\ref{著者推定誤り例2}では，コーパスでは「ジュエリー工房」が著者表現だが，著者表現なしと推定してしまった．
この例では「ジュエリー工房」を表現する語彙統語パターンとして利用する部分は，「ジュエリー工房だから」「実現します。」のみであり，手掛かりが少ない．また，「ジュエリー工房」は固有表現でないことも著者表現であると推定することが困難な原因である．

\begin{figure}[t]
\includegraphics{21-3ia1011f9.eps}
\caption{著者推定誤り例2}
\label{著者推定誤り例2}
\end{figure}
\begin{figure}[t]
\includegraphics{21-3ia1011f10.eps}
\caption{読者推定誤り例1}
\label{読者推定誤り例1}
\end{figure}

図\ref{読者推定誤り例1}では，コーパスでは読者表現は出現しないが誤って「お客様」を読者表現と推測してしまった．
この例では，文中の「お客様」は読者ではなく過去に質問をした人を指している．
このような場合でも，「様」や「頂きます」のような敬語表現を用いることが多い．
これらの表現は読者に対しても頻繁に利用されるため，これらの表現が用いられている「お客様」を著者表現と推定してしまったと考えられる．



\subsection{ゼロ照応解析結果と考察}

DDLCに対してゼロ照応解析を行った．
ベースラインは\ref{114838_18Jun13}節で述べた外界ゼロ照応および著者・読者表現を考慮しないモデルである．
提案モデルは\ref{115042_18Jun13}節で述べたモデルである．「提案モデル(推定)」は\ref{135602_6May13}節で述べた手法により自動推定した著者・読者表現を利用したものである．
「提案モデル(正解)」は著者・読者表現についてはコーパスの正解を与えたものである．
評価は各用言の各格ごとに行い，ある格に複数の正解が付与されている場合は，そのうちの1つと一致すれば正解とした．
また，先に述べたように，著者・読者表現がある場合には[著者]・[読者]より著者・読者表現を照応先として優先するとしたが，提案モデル(推定)において，著者・読者表現がある文書に対して著者・読者表現なし，と推定してしまった場合，本来著者・読者表現があるにも関わらず，[著者]・[読者]を割り当てる場合がある．ここでは，ベースラインとの比較のため，このようなものは誤りとして扱った．
表\ref{文章内ゼロ照応解析結果}はベースラインとの比較のために，文章内ゼロ照応のみで評価した結果であり，表\ref{全ゼロ照応解析結果}は外界ゼロ照応を含めた全てのゼロ照応で評価を行った結果である．

\begin{table}[b]
\begin{minipage}{0.49\textwidth}
\caption{文章内ゼロ照応解析結果}
\label{文章内ゼロ照応解析結果}
\input{1011table13.txt}
\end{minipage}
\hfill
\begin{minipage}{0.49\textwidth}
\caption{全ゼロ照応解析結果}
\label{全ゼロ照応解析結果}
\input{1011table14.txt}
\end{minipage}
\end{table}
\begin{figure}[b]
\includegraphics{21-3ia1011f11.eps}
\caption{提案モデルによりゼロ照応解析が改善した例1}
\label{提案モデルによりゼロ照応解析が改善した例}
\end{figure}




この結果から，ゼロ照応解析において外界ゼロ照応および著者・読者表現を考慮することで，外界を含めたゼロ照応全体だけでなく，照応先が文章内に出現する文章内ゼロ照応においても再現率・適合率が向上することが分かる．
文章内ゼロ照応での評価において，提案モデルは推定した著者・読者表現を利用した場合において，ベースラインのモデルよりも適合率，再現率ともに向上していることが分かる．
適合率が向上している原因としては，必須的な格を埋める際に，無理に文章内から選択せず外界の談話要素を選択することができること，敬語などの表現の際に著者表現や読者表現を照応先として選択できることが考えられる．
例えば，図\ref{提案モデルによりゼロ照応解析が改善した例}では，ガ格ではベースラインでは「あなた」を対応付けているが，提案モデルでは[著者]を対応付けることで文章内照応に限っても再現率・適合率が向上している．また，敬語表現に対してニ格を読者表現の「あなた」を正しく対応付けることができるようになった．

再現率が向上する原因としては，ベースラインモデルでは学習の際に外界照応をゼロ代名詞なしと学習してしまうため，必須的な格でも必ずしも対応付ける必要がないと学習してしまうが，提案手法では必須的な格はなるべく埋めるように学習するためである．
例えば図\ref{提案モデルによりゼロ照応解析が改善した例2}では，ベースラインモデルでは必須的な格であるガ格に何も対応付けていない．一方，提案手法ではガ格に対して「神さま」を対応付けることができた．

正解の著者・読者表現を与えた場合には，再現率，適合率ともに大きく向上している．このことから著者・読者表現の推定精度を向上させることで，ゼロ照応解析の再現率・適合率がより向上すると考えられる．

\begin{figure}[b]
\includegraphics{21-3ia1011f12.eps}
\caption{提案モデルによりゼロ照応解析が改善した例2}
\label{提案モデルによりゼロ照応解析が改善した例2}
\end{figure}
\begin{table}[b]
\begin{minipage}{0.49\textwidth}
\caption{文章内ゼロ照応解析結果(ガ格)}
\label{文章内ゼロ照応解析結果(ガ格)}
\input{1011table15.txt}
\end{minipage}
\hfill
\begin{minipage}{0.49\textwidth}
\caption{全ゼロ照応解析結果(ガ格)}
\label{全ゼロ照応解析結果(ガ格)}
\input{1011table16.txt}
\end{minipage}
\end{table}


\begin{table}[t]
\begin{minipage}{0.49\textwidth}
\caption{文章内ゼロ照応解析結果(ヲ格)}
\label{文章内ゼロ照応解析結果(ヲ格)}
\input{1011table17.txt}
\end{minipage}
\hfill
\begin{minipage}{0.49\textwidth}
\caption{全ゼロ照応解析結果(ヲ格)}
\label{全ゼロ照応解析結果(ヲ格)}
\input{1011table18.txt}
\end{minipage}
\end{table}
\begin{table}[t]
\begin{minipage}{0.49\textwidth}
\caption{文章内ゼロ照応解析結果(ニ格)}
\label{文章内ゼロ照応解析結果(ニ格)}
\input{1011table19.txt}
\end{minipage}
\hfill
\begin{minipage}{0.49\textwidth}
\caption{全ゼロ照応解析結果(ニ格)}
\label{全ゼロ照応解析結果(ニ格)}
\input{1011table20.txt}
\end{minipage}
\end{table}
\begin{table}[t]
\begin{minipage}{0.49\textwidth}
\caption{文章内ゼロ照応解析結果(ガ 2 格)}
\label{文章内ゼロ照応解析結果(ガ２格)}
\input{1011table21.txt}
\end{minipage}
\hfill
\begin{minipage}{0.49\textwidth}
\caption{全ゼロ照応解析結果(ガ 2 格)}
\label{全ゼロ照応解析結果(ガ２格)}
\input{1011table22.txt}
\end{minipage}
\end{table}

格ごとに評価を行った結果を表\ref{文章内ゼロ照応解析結果(ガ格)}，表\ref{全ゼロ照応解析結果(ガ格)}，表\ref{文章内ゼロ照応解析結果(ヲ格)}，表\ref{全ゼロ照応解析結果(ヲ格)}，表\ref{文章内ゼロ照応解析結果(ニ格)}，表\ref{全ゼロ照応解析結果(ニ格)}，表\ref{文章内ゼロ照応解析結果(ガ２格)}，表\ref{全ゼロ照応解析結果(ガ２格)}に示す．
表\ref{文章内ゼロ照応解析結果(ガ格)}，表\ref{全ゼロ照応解析結果(ガ格)}から，ガ格において特に提案手法により再現率，適合率が共に向上したことが分かる．これは，ガ格では特に著者に関するゼロ照応が多いこと，外界ゼロ照応を考慮することでほぼ全てのガ格がゼロ代名詞を持つことが原因と考えられる．
表\ref{文章内ゼロ照応解析結果(ニ格)}，表\ref{全ゼロ照応解析結果(ニ格)}から，ニ格においても提案手法により再現率，適合率が共に向上したことが分かる．特に適合率が大きく向上しており，これはニ格は用言の受け手（「紹介します」のニ格など）となることや謙譲語の敬意を示す対象（「お届けします」）となることが多く，読者の情報が照応先推定に大きく寄与したためと考えられる．
一方，表\ref{文章内ゼロ照応解析結果(ヲ格)}，表\ref{全ゼロ照応解析結果(ヲ格)}から，ヲ格では適合率が向上しているものの，再現率が低下していることが分かる．ヲ格は著者・読者に関するゼロ照応が少なく，外界ゼロ照応の割合も少ないことから提案手法が有効に働かなかったものと考えられる．
適合率が向上し，再現率が低下した要因としては格フレームの選択誤りが考えられる．
格フレームの用例選択においては特にヲ格が重要な役割を持っており，ヲ格が省略された場合には正しい格フレームを選択することが困難となる．
特に他の格が外界ゼロ照応となる場合には，ベースライン手法では他の格の割り当てを考慮せずに，照応先候補の中にヲ格として対応するものが存在する格フレームを選択することが多い．この場合，格フレームとしてはヲ格に対応付けられることが妥当であるが，文脈的には正しくないものも含まれるので，再現率は高くなるが適合率が低くなると考えられる．
一方，外界ゼロ照応を考える場合には，ガ格が格フレームに適合するかを考慮する必要があるため，述語項構造全体として適切な格フレームを選択できない場合がある．
その場合には，ヲ格には適切な照応先がないと判断されてしまうので，再現率が低下していると考えられる．
表\ref{文章内ゼロ照応解析結果(ガ２格)}，表\ref{全ゼロ照応解析結果(ガ２格)}からガ 2 格はベースライン，提案手法ともにほぼ推定できていないことが分かる．
これは，ガ 2 格のゼロ照応での出現が非常に少なく学習時にガ 2 格を割り当てる必要がないと判断されることと，ガ 2 格を持つ格フレームが少ないことが原因である．

\begin{figure}[b]
\vspace{-0.5\Cvs}
\includegraphics{21-3ia1011f13.eps}
\caption{提案モデルの誤り例1}
\label{提案モデルの誤り例1}
\end{figure}
\begin{figure}[b]
\includegraphics{21-3ia1011f14.eps}
\caption{提案モデルの誤り例2}
\label{提案モデルの誤り例2}
\vspace{-0.5\Cvs}
\end{figure}

提案手法で誤った例を図\ref{提案モデルの誤り例1}と図\ref{提案モデルの誤り例2}に示す．
図\ref{提案モデルの誤り例1}では，提案手法は「捻出しなければならない」のガ格に[不特定:人]を対応付けてしまった．
\pagebreak
これは，「提出する」の格フレームではガ格に「国」を対応付ける用例が少なかったため，「国」ではなく外界の[不特定:人]を対応付けることを選択してしまったためである．

\begin{table}[t]
\caption{正解の条件を緩めて評価した結果}
\label{緩和評価}
\input{1011table23.txt}
\end{table}

図\ref{提案モデルの誤り例2}は著者表現の推定誤りがゼロ照応解析の誤りの原因である．
この文章では著者表現が「領事館」であるため，「開設しました」のガ格には「領事館」を対応付けなければならない．しかし，提案手法では誤って著者表現なしと推定してしまったため，「領事館」を対応付けることができず，[著者]を対応付けてしまった．
しかし，このような誤りは著者がガ格であるということは推定できているため厳密には誤りとは言えない．
そこで，このような著者・読者表現を対応付けるべき格に対して[著者]，[読者]を対応付けてしまった場合にも正解とみなして評価を行った結果を表\ref{緩和評価}に示す．
表\ref{緩和評価}と表\ref{文章内ゼロ照応解析結果}および表\ref{全ゼロ照応解析結果}を比較すると，正解の条件を緩めた場合に再現率，適合率共に大きく向上している．このことから，提案モデル(推定)の誤りの多くが図\ref{提案モデルの誤り例2}のように著者・読者表現の推定が誤りといえる．一方，正解の条件を緩和させた場合でも提案モデル(コーパス著者・読者表現)よりは低い再現率と適合率となっている．これらの差は，解析時に正しい著者・読者表現を利用できる場合には$\phi_\mathit{mentioned}$および$\phi_{[著者]}$または$\phi_{[読者]}$を素性として利用しているが，解析後に著者・読者表現の情報から[著者]，[読者]を正解とする場合には，解析時には$\phi_{[著者]}$，$\phi_{[読者]}$のみを利用していることである．
このことから，提案手法のように著者・読者表現に対して文章中に言及される談話要素と仮想的な談話要素の両方の性質を与えることが有効といえる．




\subsection{京都大学テキストコーパスでの実験}

DDLCは先頭3文のみで構成されており，文章全体でゼロ照応解析を行った場合の影響が調査できない．
そこで，文章全体での本手法の精度を調査するために京都大学テキストコーパスを利用した実験を行った．
実験では京都大学テキストコーパスのゼロ照応関係が付与された567文書のうち，454文書を訓練に，113文書を評価に利用した．
京都大学テキストコーパスには著者・読者表現が付与されていないため，全ての文書において著者・読者表現が存在しないと仮定して実験を行った．
その結果を表\ref{京都大学テキスココーパスにおける文章内ゼロ照応解析結果}と表\ref{京都大学テキストコーパスにおける全ゼロ照応解析結果}に示す．
この結果から，提案モデルはベースラインモデルに比べ大きくF値が向上したとは言い難い．
これは，京都大学テキストコーパスを構成している新聞記事は，著者・読者が談話に登場することが稀であることが大きな原因である．
提案手法では，特に著者・読者に着目しており，そのために外界ゼロ照応を扱ったが，著者・読者が談話に出現しない場合，外界ゼロ照応を扱うことの寄与度は低いことが分かる．
\linebreak
一方，文章全体におけるF値がベースラインとほぼ同等であり，提案手法を文章全体適用した場合についても，悪影響などはないと言える．
これらの考察から，提案手法を著者・読者が談話に出現する文章の全体に対して適用した場合，先頭3文における実験結果と同様に精度が向上するものと考えられる．
\vspace{-0.3\Cvs}

\begin{table}[t]
\setlength{\captionwidth}{200pt}
\begin{minipage}{200pt}
\hangcaption{京都大学テキストコーパスにおける文章内ゼロ照応解析結果}
\label{京都大学テキスココーパスにおける文章内ゼロ照応解析結果}
\input{1011table24.txt}
\end{minipage}
\hfill
\begin{minipage}{200pt}
\hangcaption{京都大学テキストコーパスにおける全ゼロ照応解析結果}
\label{京都大学テキストコーパスにおける全ゼロ照応解析結果}
\input{1011table25.txt}
\end{minipage}
\end{table}


\section{まとめ}
\label{115208_18Jun13}
\vspace{-0.2\Cvs}

本論文では，外界ゼロ照応および著者・読者表現を考慮した日本語ゼロ照応解析モデルを提案した．
ゼロ照応解析の前処理として文章中の著者・読者表現を語彙統語パターンを用いて自動的に判別し，ゼロ照応解析において他の談話要素と区別して扱った．
DDLCを用いた実験の結果，外界ゼロ照応だけでなく，文章内ゼロ照応においても提案手法を用いることで，高い精度が得られた．

今後の課題としては，著者・読者表現の精度向上が挙げられる．本研究では，テキストの内容のみから著者・読者表現推定を行ったが，実際の解析ではWeb特有の情報（URLやHTMLタグ）が利用できる．これらの情報は，URLのドメインに含まれる表現は著者表現である可能性が高い，.comドメインでは客が読者表現になりやすい，など，著者・読者表現推定において有用であると考えれられる．

ゼロ照応解析の精度向上では，事態間関係知識の利用などが考えられる．例えば，「Aを食べた $\leftarrow$ Aが美味しかった」といった関係が分かっていれば，「チョコを食べたが(チョコガ)美味しかった」の「美味しかった」のゼロ照応解析で手掛かりになると考えられる．
\vspace{-0.5\Cvs}


\bibliographystyle{jnlpbbl_1.5}
\begin{thebibliography}{}

\bibitem[\protect\BCAY{Gerber \BBA\ Chai}{Gerber \BBA\
  Chai}{2010}]{gerber-chai:2010:ACL}
Gerber, M.\BBACOMMA\ \BBA\ Chai, J. \BBOP 2010\BBCP.
\newblock \BBOQ Beyond NomBank: A Study of Implicit Arguments for Nominal
  Predicates.\BBCQ\
\newblock In {\Bem Proceedings of the 48th Annual Meeting of the Association
  for Computational Linguistics}, \mbox{\BPGS\ 1583--1592}, Uppsala, Sweden.
  Association for Computational Linguistics.

\bibitem[\protect\BCAY{Hangyo, Kawahara, \BBA\ Kurohashi}{Hangyo
  et~al.}{2012}]{hangyo-kawahara-kurohashi:2012:PACLIC}
Hangyo, M., Kawahara, D., \BBA\ Kurohashi, S. \BBOP 2012\BBCP.
\newblock \BBOQ Building a Diverse Document Leads Corpus Annotated with
  Semantic Relations.\BBCQ\
\newblock In {\Bem Proceedings of the 26th Pacific Asia Conference on Language,
  Information, and Computation}, \mbox{\BPGS\ 535--544}, Bali,Indonesia.
  Faculty of Computer Science, Universitas Indonesia.

\bibitem[\protect\BCAY{Hayashibe, Komachi, \BBA\ Matsumoto}{Hayashibe
  et~al.}{2011}]{hayashibe-komachi-matsumoto:2011:IJCNLP-2011}
Hayashibe, Y., Komachi, M., \BBA\ Matsumoto, Y. \BBOP 2011\BBCP.
\newblock \BBOQ Japanese Predicate Argument Structure Analysis Exploiting
  Argument Position and Type.\BBCQ\
\newblock In {\Bem Proceedings of 5th International Joint Conference on Natural
  Language Processing}, \mbox{\BPGS\ 201--209}, Chiang Mai, Thailand. Asian
  Federation of Natural Language Processing.

\bibitem[\protect\BCAY{Herbrich, Graepel, Bollmann-Sdorra, \BBA\
  Obermayer}{Herbrich et~al.}{1998}]{herbrich1998learning}
Herbrich, R., Graepel, T., Bollmann-Sdorra, P., \BBA\ Obermayer, K. \BBOP
  1998\BBCP.
\newblock \BBOQ Learning Preference Relations for Information Retrieval.\BBCQ\
\newblock In {\Bem ICML-98 Workshop: Text Categorization and Machine Learning},
  \mbox{\BPGS\ 80--84}.

\bibitem[\protect\BCAY{Iida, Inui, \BBA\ Matsumoto}{Iida
  et~al.}{2006}]{iida-inui-matsumoto:2006:COLACL}
Iida, R., Inui, K., \BBA\ Matsumoto, Y. \BBOP 2006\BBCP.
\newblock \BBOQ Exploiting Syntactic Patterns as Clues in Zero-Anaphora
  Resolution.\BBCQ\
\newblock In {\Bem Proceedings of the 21st International Conference on
  Computational Linguistics and 44th Annual Meeting of the Association for
  Computational Linguistics}, \mbox{\BPGS\ 625--632}, Sydney, Australia.
  Association for Computational Linguistics.

\bibitem[\protect\BCAY{Iida, Inui, \BBA\ Matsumoto}{Iida
  et~al.}{2009}]{iida-inui-matsumoto:2009:ACLIJCNLP}
Iida, R., Inui, K., \BBA\ Matsumoto, Y. \BBOP 2009\BBCP.
\newblock \BBOQ Capturing Salience with a Trainable Cache Model for
  Zero-anaphora Resolution.\BBCQ\
\newblock In {\Bem Proceedings of the Joint Conference of the 47th Annual
  Meeting of the ACL and the 4th International Joint Conference on Natural
  Language Processing of the AFNLP}, \mbox{\BPGS\ 647--655}, Suntec, Singapore.
  Association for Computational Linguistics.

\bibitem[\protect\BCAY{Iida, Komachi, Inui, \BBA\ Matsumoto}{Iida
  et~al.}{2007}]{iida-EtAl:2007:LAW}
Iida, R., Komachi, M., Inui, K., \BBA\ Matsumoto, Y. \BBOP 2007\BBCP.
\newblock \BBOQ Annotating a Japanese Text Corpus with Predicate-Argument and
  Coreference Relations.\BBCQ\
\newblock In {\Bem Proceedings of the Linguistic Annotation Workshop},
  \mbox{\BPGS\ 132--139}, Prague, Czech Republic. Association for Computational
  Linguistics.

\bibitem[\protect\BCAY{Imamura, Saito, \BBA\ Izumi}{Imamura
  et~al.}{2009}]{imamura-saito-izumi:2009:Short}
Imamura, K., Saito, K., \BBA\ Izumi, T. \BBOP 2009\BBCP.
\newblock \BBOQ Discriminative Approach to Predicate-Argument Structure
  Analysis with Zero-Anaphora Resolution.\BBCQ\
\newblock In {\Bem Proceedings of the ACL-IJCNLP 2009 Conference Short Papers},
  \mbox{\BPGS\ 85--88}, Suntec, Singapore. Association for Computational
  Linguistics.

\bibitem[\protect\BCAY{磯崎\JBA 賀沢\JBA 平尾}{磯崎 \Jetal
  }{2006}]{磯崎秀樹:2006-07-15}
磯崎秀樹\JBA 賀沢秀人\JBA 平尾努 \BBOP 2006\BBCP.
\newblock 辞書式順序を持つペナルティによるゼロ代名詞解消.\
\newblock \Jem{情報処理学会論文誌}, {\Bbf 47}  (7), \mbox{\BPGS\ 2279--2294}.

\bibitem[\protect\BCAY{Joachims}{Joachims}{2002}]{joachims2002optimizing}
Joachims, T. \BBOP 2002\BBCP.
\newblock \BBOQ Optimizing Search Engines using Clickthrough Data.\BBCQ\
\newblock In {\Bem Proceedings of the 8th ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}, \mbox{\BPGS\ 133--142}. ACM.

\bibitem[\protect\BCAY{河原\JBA 黒橋}{河原\JBA
  黒橋}{2007}]{河原大輔:2007-07-10}
河原大輔\JBA 黒橋禎夫 \BBOP 2007\BBCP.
\newblock 自動構築した大規模格フレームに基づく構文・格解析の統合的確率モデル.\
\newblock \Jem{自然言語処理}, {\Bbf 14}  (4), \mbox{\BPGS\ 67--81}.

\bibitem[\protect\BCAY{Kawahara \BBA\ Kurohashi}{Kawahara \BBA\
  Kurohashi}{2006}]{kawahara-kurohashi:2006:HLT-NAACL06-Main}
Kawahara, D.\BBACOMMA\ \BBA\ Kurohashi, S. \BBOP 2006\BBCP.
\newblock \BBOQ A Fully-Lexicalized Probabilistic Model for Japanese Syntactic
  and Case Structure Analysis.\BBCQ\
\newblock In {\Bem Proceedings of the Human Language Technology Conference of
  the NAACL, Main Conference}, \mbox{\BPGS\ 176--183}, New York City, USA.
  Association for Computational Linguistics.

\bibitem[\protect\BCAY{Kawahara, Kurohashi, \BBA\ Hasida}{Kawahara
  et~al.}{2002}]{KTC}
Kawahara, D., Kurohashi, S., \BBA\ Hasida, K. \BBOP 2002\BBCP.
\newblock \BBOQ Construction of a Japanese Relevance-tagged Corpus.\BBCQ\
\newblock In {\Bem Proceedings of The 3rd International Conference on Language
  Resources Evaluation}.

\bibitem[\protect\BCAY{Kong \BBA\ Zhou}{Kong \BBA\
  Zhou}{2010}]{kong-zhou:2010:EMNLP}
Kong, F.\BBACOMMA\ \BBA\ Zhou, G. \BBOP 2010\BBCP.
\newblock \BBOQ A Tree Kernel-Based Unified Framework for Chinese Zero Anaphora
  Resolution.\BBCQ\
\newblock In {\Bem Proceedings of the 2010 Conference on Empirical Methods in
  Natural Language Processing}, \mbox{\BPGS\ 882--891}, Cambridge, MA.
  Association for Computational Linguistics.

\bibitem[\protect\BCAY{村田\JBA 長尾}{村田\JBA 長尾}{1997}]{199787}
村田真樹\JBA 長尾真 \BBOP 1997\BBCP.
\newblock
  用例や表層表現を用いた日本語文章中の指示詞・代名詞・ゼロ代名詞の指示対象の推定.\
\newblock \Jem{自然言語処理}, {\Bbf 4}  (1), \mbox{\BPGS\ 87--109}.

\bibitem[\protect\BCAY{Poesio, Uryupina, \BBA\ Versley}{Poesio
  et~al.}{2010}]{poesio2010creating}
Poesio, M., Uryupina, O., \BBA\ Versley, Y. \BBOP 2010\BBCP.
\newblock \BBOQ Creating a Coreference Resolution System for Italian.\BBCQ\
\newblock In {\Bem Proceedings of the 7th Conference on International Language
  Resources and Evaluation (LREC'10)}, Valletta, Malta. European Language
  Resources Association (ELRA).

\bibitem[\protect\BCAY{Rello, Baeza-Yates, \BBA\ Mitkov}{Rello
  et~al.}{2012}]{rello2012elliphant}
Rello, L., Baeza-Yates, R., \BBA\ Mitkov, R. \BBOP 2012\BBCP.
\newblock \BBOQ Elliphant: Improved Automatic Detection of Zero Subjects and
  Impersonal Constructions in Spanish.\BBCQ\
\newblock In {\Bem Proceedings of the 13th Conference of the European Chapter
  of the Association for Computational Linguistics}, \mbox{\BPGS\ 706--715}.
  Association for Computational Linguistics.

\bibitem[\protect\BCAY{Ruppenhofer, Sporleder, Morante, Baker, \BBA\
  Palmer}{Ruppenhofer et~al.}{2010}]{ruppenhofer-EtAl:2010:SemEval}
Ruppenhofer, J., Sporleder, C., Morante, R., Baker, C., \BBA\ Palmer, M. \BBOP
  2010\BBCP.
\newblock \BBOQ SemEval-2010 Task 10: Linking Events and Their Participants in
  Discourse.\BBCQ\
\newblock In {\Bem Proceedings of the 5th International Workshop on Semantic
  Evaluation}, \mbox{\BPGS\ 45--50}, Uppsala, Sweden. Association for
  Computational Linguistics.

\bibitem[\protect\BCAY{Sasano, Kawahara, \BBA\ Kurohashi}{Sasano
  et~al.}{2008}]{sasano-kawahara-kurohashi:2008:PAPERS}
Sasano, R., Kawahara, D., \BBA\ Kurohashi, S. \BBOP 2008\BBCP.
\newblock \BBOQ A Fully-Lexicalized Probabilistic Model for Japanese Zero
  Anaphora Resolution.\BBCQ\
\newblock In {\Bem Proceedings of the 22nd International Conference on
  Computational Linguistics (Coling 2008)}, \mbox{\BPGS\ 769--776}, Manchester,
  UK. Coling 2008 Organizing Committee.

\bibitem[\protect\BCAY{Sasano \BBA\ Kurohashi}{Sasano \BBA\
  Kurohashi}{2011}]{sasano-kurohashi:2011:IJCNLP-2011}
Sasano, R.\BBACOMMA\ \BBA\ Kurohashi, S. \BBOP 2011\BBCP.
\newblock \BBOQ A Discriminative Approach to Japanese Zero Anaphora Resolution
  with Large-scale Lexicalized Case Frames.\BBCQ\
\newblock In {\Bem Proceedings of 5th International Joint Conference on Natural
  Language Processing}, \mbox{\BPGS\ 758--766}, Chiang Mai, Thailand. Asian
  Federation of Natural Language Processing.

\bibitem[\protect\BCAY{平\JBA 永田}{平\JBA 永田}{2013}]{平2013}
平博順\JBA 永田昌明 \BBOP 2013\BBCP.
\newblock 述語項構造解析を伴った日本語省略解析の検討.\
\newblock \Jem{言語処理学会第19回年次大会発表論文集}, \mbox{\BPGS\ 106--109}.

\bibitem[\protect\BCAY{山本\JBA 隅田}{山本\JBA 隅田}{1999}]{山本和英:1999}
山本和英\JBA 隅田英一郎 \BBOP 1999\BBCP.
\newblock 決定木学習による日本語対話文の格要素省略補完.\
\newblock \Jem{自然言語処理}, {\Bbf 6}  (1), \mbox{\BPGS\ 3--28}.

\end{thebibliography}


\begin{biography}
\bioauthor{萩行　正嗣}{
2008年京都大学工学部電気電子工学科卒業．2010年同大学大学院情報学研究科修士課程修了．2014年同大学院博士後期課程修了．博士（情報学）．現在，株式会社ウェザーニューズ所属．
}
\bioauthor{河原　大輔}{
1997年京都大学工学部電気工学第二学科卒業．1999年同大学院修士課程修了．
2002年同大学院博士課程単位取得認定退学．東京大学大学院情報理工学系研究
科学術研究支援員，独立行政法人情報通信研究機構研究員，同主任研究員を経
て，2010年より京都大学大学院情報学研究科准教授．自然言語処理，知識処理
の研究に従事．博士（情報学）．言語処理学会，情報処理学会，人工知能学会，
電子情報通信学会，ACL，各会員．
}
\bioauthor{黒橋　禎夫}{
1994年京都大学大学院工学研究科電気工学第二専攻博士課程修了．博士（工学）．2006年4月より京都大学大学院情報学研究科教授．自然言語処理，知識情報処理の研究に従事．言語処理学会10周年記念論文賞等を受賞．
}

\end{biography}


\biodate


\end{document}
