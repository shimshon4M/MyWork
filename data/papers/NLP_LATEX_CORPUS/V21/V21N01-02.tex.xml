<?xml version="1.0" ?>
<root>
  <section title="Introduction">Thedetectionofuncertaintyinlanguage,alsoknownashedging,hasreceivedaconsiderableamountofinterestfromtheNLPcommunity.Itconsistsofevaluatingsentenceuncertaintybyidentifyingthepresenceofatleastonecueword,whicharewordsthatadduncertaintymeaningtothebaseproposition(ex.:word``may''in``hemaybethere'').Hedginghasthustwomainconcerns:disambiguatingwords(ex.:``may''doesnotexpressuncertaintyin``youmayenter''),anddeterminingwhetherawordisacueornot.ItwasnotuntilrecentlythathedginghasbeeninvestigatedinNLP.Earlierworksonhedgedetectiontaskincludeusingahand-craftedlistofcues,andmedlock07andszarvas08classifyingsentencesbasedonsemi-automaticallycollectedtraininginstances.State-of-the-artmethods,suchasCoNLL-2010SharedTaskparticipants,usefeature-richsupervisedlearning.Fortrainingdata,bothsentence-levelhedging(i.e.,binaryvalueindicatingwhetherasentenceisuncertainornot)aswellascuewordsareannotated.Twomainapproachesareobserved:atwo-stepapproach,firstdisambiguatingandidentifyingcuesusingaclassifier,thenevaluatingsentenceuncertaintybasedonselectedcues;andaone-stepapproach,directlyclassifyinghedgingatsentence-level.Thetwo-stepapproachthususescuesastheclassifiertarget,whereastheone-stepusesthemasfeatures.Thisworkpresentsasimpleyeteffectiveapproachtohedgeidentificationwhichdoesnotrequirecueannotation.Itdecouplesdisambiguationandselectionintoathree-stepapproach:(i)disambiguation;(ii)cueselection;and(iii)sentence-leveluncertaintyevaluation.Onemotivationforthisapproachisthatbynotrequiringcueannotation,othermodalcategoriesasidefromuncertainty(ex.:volition,obligation,etc.)mightbebenefitted,sincetheyaremodeledinasimilarfashion.Furthermore,63%ofthecuetypesinauncertaintyWikipediacorpuspresenteduniqueoccurrences,whichtranslatesintounreliableexamplesfortheclassifierinthetwo-stepapproach,andintosparsityandfeatureinconsistencyintheone-stepapproach.Cueselectionaddressesthisbyactivelydiscardingnon-cuesandirrelevantcues.Finally,bydecouplingdisambiguationandselection,itisalsopossibleto:useanyWSDsystem,whetherstate-of-the-artorless-resourcefulmethodsbasedonlyonstem/lemma/POS;tunecueselection,minimizingsentence-levelhedgingerror;andcompareeffectsofdifferentWSDmethods.</section>
  <section title="Datasets and baselines for the hedging detection task">IntheCoNLL-2010ST,uncertaintydetectionwasaddressedintwodomains:BiologicalandWikipediaarticles.Forbiologicalarticles,whichconsistofarticlesfromtheBioScopecorpus,FlyBasemedlock07,BMCBioinformaticsandPubMedCentral,uncertaintyisexpressedbyusinghedgecuewords.Typicalcuesareauxiliaryverbs(may,might,etc.),verbswithspeculativecontent(suggest,indicate,etc.),adjectivesoradverbs(probable,likely,etc.),conjunctions(or,either,etc.)andothers.Anexampleisasfollows:ForWikipediaarticles,reliableknowledgeispreferredoverstatementsreflectingauthoropinions.Itisthereforecrucialtodetectweasels,whicharewordsthatvaguelyspecifythesourceofinformation.Typicalweaselsareelementsdenotinguncertainty(probable,likely,etc.),generalization(widely,traditionally,etc.),obviousness(clearly,arguably,etc.),passiveformswithdummysubjects(itisclaimed,etc.),numericallyvagueexpressions(certain,numerous,etc.),amongothers.Anexampleisgivenbelow.Itshouldbenotedthatthementionedexpressionsarenotalwaysweasels,dependingonthecontext.Fortask1ofthesharedtask,trainingsentencesweremanuallymarkedasuncertainiftheycontainedatleastonecueword;hedgeandweaselcuewordswerealsoannotated.Evaluationwasdonebasedonprecision,recallandF_1measureofsentencehedging.Tableshowsabenchmarkresultingfromana&quot;ivealgorithmthatsimplyclassifiesasuncertainanysentencethatcontainsatleastoneofthehedgecueexpressionsfoundintraining,andTableshowsparticipatingsystemsandprovidesasummaryonmethodsandresults.</section>
  <section title="Process overview"/>
  <subsection title="Word sense disambiguation">Disambiguation,thefirststepofthesentencehedgedetectionprocess,isresponsiblefordifferentiatingwordsensesregardinguncertainty,outputtingadifferentstringrepresentationforeachwordsense.Inthiswork,thefollowingsevendifferentsetsoflinguisticinformationareused:	A:Sentencesplitter,tokenizerandstemmer(minimumrequirementforprocessing)	B:Sentencesplitter,tokenizer,lemmatizerandPOStagger	C:DisambiguationusingWordNet::SenseRelate,whichmeasuressemanticrelatednessbetweenawordanditsneighborsusingWordNetpedersen09.Foreverycontentwordofasentence,similarityandrelatednessmeasures(ex.:WordNetpathlengthsaugmentedwithinformationcontent)arecalculatedforallsurroundingwordsenses,andthesensewiththehighestsumofscoresischosen	D:DisambiguationusingsemanticgraphconnectivityonthebasisoftheunderlyingknowledgebaseofferedbyBabelNetnavigli10,navigli12	E:UnionofBandD,addingdifferentlevelsofgranularity,sincesomedisambiguatedwordsensesmaybetoogranularforthetask	F:UnionofA,BandD,addingdifferentlevelsofgranularity	G:SubsetofF.WordswithintrainingcueexpressionsareencodedusingA,BandD;theyarethencombined,formingthesamecueexpressionsasintrainingdata,butwithdifferentencodingsforeachwordUnigrams,bigramsandtrigramsareusedinordertoaccountforlocalcontextforeachoftheprevioussetsasillustratedinTable,exceptforG,whichusesannotatedcueexpressions.EncodingsystemsareidentifiedbyXNherein,whereXistheone-charactercodeforeachsetandNisthenumberofn-grams.Forexample,A2standsforbigramsusingstemsonly.</subsection>
  <subsection title="Cue selection">Theresultingstringsfromthedisambiguationsteparethenusedasinputofthecueselectionstepinabag-of-wordsapproach.Sincedifferentmethodscanhavedifferentencodingschema(ex.:``believe''maybeencodedas``believe/VB/v1''or``believe/VB-bn:83369v''),selectioniscompatiblewithanyWSDtool.Inaddition,lessresource-intensivedescriptionschemacanalsobeused(ex.:usingonlylemmaandPOS,asin``believe/VB'').CueselectionisthenadecisionproblemwhichretrievesthesetofwordswhosesentencehedgingF_1valueintrainingdataisoptimal.Asadecisionproblem,itisnecessarytotraversetheinfeasibleO(2^m)combinatorialsearchspaceofmwords.Inadditiontothisconcern,othertask-specificconsiderationsregardingnoise,cue-boundedhedgingandselectionequivalencearemade.Firstly,noiseregardsnon-cuewordsinthecueselectiontask.Giventhebelowclassificationofcuesandnon-cuesbasedonfrequencyandambiguitywithqualitativeevaluationofprecisionandrecall,itisunderstoodthatnoisemaypresenthigherF_1valuethansomecuewords.Itisthusdifficulttodifferentiatecasefromcase,aswellascasefrombothand.	Frequentunambiguouscues:HighP,mediumR(ex.:suggest)	Frequentambiguouscues:LowP,mediumR(ex.:consider)	Infrequentunambiguouscues:HighP,lowR(ex.:speculate)	Infrequentambiguouscues:LowP,lowR(ex.:assume)	Frequentnon-cuesco-occurringwithcues:MediumP,highR(ex.:that)	Infrequentnon-cuesco-occurringwithcues:VariableP,lowR	Non-cuesnotco-occurringwithcues:ZeroP,zeroRSecondly,somedifficultiesarisefromcue-boundedhedging,whichisdefinedasthefactthatifawordfromagivensentencesisalreadyselected,anyotherselectionwillnotfurtheraffecttheuncertaintystatusofs.Becauseofthisproperty,ifnon-cuewordsarenotproperlyfiltered,thenumberoffalsepositiveswillincreasedramatically.Additionally,consideringawordwandasetofwordsW=w_1,w_2,,iftheselectionofwhashigherF_1thaneachw_1,w_2,individuallybutlowerthanW(truewhenfalsepositivesforw_1,w_2,areoverlapping),itisnottheoreticallypossibletodeterminetheglobaloptimuminatimelymanner.Finally,definingselectionequivalenceastwodifferentsetsofwordsW_1andW_2whichpertaintothesamesentences,thesesetsobtainexactlythesameselectionandconsequentlysameF_1value.Standardselectionaloneisnotabletoaddressthisequivalence,sincethechoicebetweenW_1andW_2hastobebasedonempiricalobservationsondata.Consideringtheseconcerns,thecueselectionprocessisthendesignedasiterativeselectionsofwords.Eachiterationisdividedintosearchingacandidatecuewordbasedonsomeheuristics,whichaddressesthecombinatorialsearchproblem,andselecting/deselectingisbasedonimprovementofF_1value(theobjectivefunction).Theprocessishaltedwhentherearenowordswhoseselection/deselectioncanfurtherincreaseF_1.Fortheheuristics,F_1isalsothemostobviouschoice.However,F_1forcaseishigherthancasesand,causingnoisetobeselectedearlier;moreover,becauseofselectionequivalence,thealgorithmmaychooseasetofwordswhichleadstoworseresultsinsentencehedging,despitehavingdefinedthedeselectionoperation.AbetterapproachisthentoguaranteeasmanycorrectselectionsaspossibleinearlystepsbyusingF_asprimaryheuristic(=0.25determinedarbitrarily)andF_1asfallback.Withalargerweighttoprecision,frequentcueshavehigherconfidenceandarethusselectedearlier.Thisalsoguaranteesfasterconvergence.FurtherdetailsonthealgorithmisgiveninSection,andempiricalobservationsonitsbehaviorinSection.Itshouldbeobservedthatcueselection(CS)issimilartofeatureselection(FS)methodsdash97,yu03,fleuret04,especiallyheuristicwrappers.However,thetwodomainsdifferconsiderably.WhileFSisconcernedwithfeatureredundancy,optionallyremovingthelessinformativeofcorrelatedfeaturesets,CSisconcernedwiththetask-specificselectionequivalence,inwhichitiscrucialtoremoveallnon-cuewordsbecauseofcue-boundedhedging.Inaddition,theonlywaytofilternon-cuesistoidentifythecorrectcuewithinthesamesentence,favoringaconfidence-basedapproachoverastrictlycorrelation-basedone.HeuristicCSisalsosimilartodecisionlistlearning,whichdeterminestheorderthatrulesofarule-basedsystemareapplied.ThemaindifferencebetweenthetwoisthatheuristicCSisconcernednotonlywiththeorderofselection,butalsowithdeterminingwhichwordsarenotselected.</subsection>
  <subsection title="Sentence-level hedge detection">Inthefinalstep,selectedcuesareusedforevaluatingsentencehedgingintestingdata.Theclassificationisasimplebinarytest,inwhichasentencethatcontainsanyoftheidentifiedcuewordsisconsideredtobeuncertain.</subsection>
  <section title="Proposed method for cue selection">Letatrainingcorpusbecomposedofsentencess_jS,1jn,eachofwhichhasabinarymodalityvaluemod(s_j)0,1.Forthehedgingdetectionproblem,mod(s_j)isequalto1ifsisuncertain,or0otherwise.Eachsentenceisdescribedasabag-of-words.Eachwordw_i,1im=|W|,canthenbedescribedbyasparsebinaryvectoru_i,whereeachu_ijindicateswhetherw_iexistsinsentences_j.LetSbethecurrentstateofselection,andS'bethesetofwordsthatarenotcurrentlyselected.Letalso:W0,1^nbeafunctionthatgivenasetofwordsW,returnsabinaryvectorasfollows:Thevectoru=(S)isthendefinedasasparsebinaryvectorwhoseelementsindicatethesentencesthatareselectedgivencurrentstateS.Letu^+beavectorindicatingsentencesthatareannotatedasuncertainintrainingdata,letvbethevectorwhosebinaryelementsareequaltothenegationofthoseinv,andlet|v|beafunctionthatcountstheamountof1'sinabinaryvectorv.Foranygivenvectorv,itispossibletocalculateprecision,recallandF_measure,asindicatedbelow:tp(v)=|u^+v|[2](v)=|u^+v|[2](v)=|u^+v|[2](v)=tp(v)tp(v)+fp(v)[2](v)=tp(v)tp(v)+fn(v))[2]_(v)=(1+^2)P(v)R(v)^2P(v)+R(v)gather*Algorithmprovidestheproposedcueselectionmethod.ItstartswithanemptyselectedsetSandanunselectedsetS'containingallwords(linesto).Ineachstep,vectorscorrespondingtonextstatesarecalculatedforeveryword(linesto).Thesevectorsareusedinordertodeterminewhichwordsshouldbeusedforupdatingthecurrentstate.[!bht]		0.8	[1]		SelectCuesW=w_i,u_i,u^+			Filter(W)			u(0,0,,0)			S			S'W			true				i_1,i_11				i2tom					w_iSu_i'(S-w_i)					u_i'(S+w_i)					F_(u_i')&gt;F_(u_i_')i_i					F_1(u_i')&gt;F_1(u_i_1')i_1i								F_1(u_i_)&gt;F_1(u)					w_i_SSelect(w_i_)Unselect(w_i_)				F_1(u_i_1)&gt;F_1(u)					w_i_1SSelect(w_i_1)Unselect(w_i_1)								returnS				u(S)						algorithmicalgorithmGivencandidatewordschosenbyheuristicsF_andF_1(linesto),oneofthemisselected/deselectedifandonlyiftheoperationgeneratesagainintheglobalobjectiveF_1(linesto).Asaresult,thealgorithmselectsthecandidatewordifitimprovesF_1,deselectswhenapreviousselectionwasharmfultoF_1,andstopswhennooperationcanimproveF_1,returningthecurrentstate(line).Inputwordsarefilteredinthebeginning(line).Wordswhosenumberoftruepositivesaresmallerthanathresholdareexcludedfromtheinput,sincetheratioofnoisetocuewordsoccurrenceistoolargeforwordswithsmallnumberoftruepositives;thisisobtainedempiricallyfromthetrainingdata.Somepropertiesobtainedfromsuchapproachisasfollows.First,theproposedselectiondoesnotrequirecueannotationandiscompletelylanguageindependent;itisalsoflexibleastowhatcanbeencodedasinputwords.Consequently,itisindependentofNLPtoolsandworkswithsentencesplitter,tokenizerandstemmerattheveryminimum.Inaddition,thetimecomplexityofthealgorithmisgivenbyO(kmn),wherekisthenumberofselectedwords|S|plustwicethenumberofdeselectionoperations.Sincetheratioofcuesinthevocabularyissmall(|S|m)anddeselectionsarerare,complexityissubquadraticinarealsetting.ThespacecomplexityisO(mn),butthiscanbedecreasedwithsparsevectorimplementations.Finally,thealgorithmisalsoeasilyparallelizable,aseachrunoftheinnerloop(linesto)isindependent.DifferentdistributednodescanthereforecalculateF_andF_1individually,andthecontrollercanidentifythecandidatewordsforeachiterationandupdatethestate.</section>
  <section title="Experiments"/>
  <subsection title="Analysis of cue selection">Thissectionanalyzestheoutputofcueselectionbeforeusingitforsentence-levelhedgeevaluation,giventhetwopreviouslymentionedcorpora.Tableprovidessomeoftheresultingselectedcuewordsfordifferentsettings,fromwhichtheinternalmechanismofthealgorithmcanbebetterunderstood.ByusingF_=0.25asprimaryheuristicsforselection,thepriorityisgiventowordswithhigherprecision,eliminatingfrequentnon-cuesfromearlyselectionsandthusavoidingselectionequivalence.Aftersomeselections,thefallbackheuristicisthenused,sincethelargerweighttoprecisiongeneratescandidateswithpoorerF_1.Inaddition,thresholdvaluecanbeempiricallyobtainedfromtrainingdataatthisstagebytestingdifferentvaluesandanalyzingtheresultingselection.Itisobservedthatalthoughsatisfactoryresultswereobtainedbyfiltering,itisnotpossibletocompletelyisolatenoisefromcuewordsbyusingsuchsimplecriteria.</subsection>
  <subsection title="Analysis of sentence-level hedge detection">Thecuewordsobtainedfromtheprevioussteparethenusedfordetectinguncertainsentencesintestingdata,withexperimentalresultsindicatedinTable.ThesedetectedsentencesarecomparedagainstCoNLL-2010STbaselinesinTable,noticingthatexceptforG3,cueannotationisnotused.Firstandforemost,itisobservedthatB2andF3obtainedpromisingresultsinbothdatasets:B2useslittleresources(onlylemmatizerandPOStagger),whereasF3usesWSDandcombineslinguisticfeatures,whichincreasespreprocessingandtrainingtime.WhencomparingtoCoNLL-2010STparticipantsinTable,theproposedmethodoutperformsallsystemsfortheWikipediaweaseldataset(withB2obtaining+1.77percentagepointswhencomparedagainstthebestbaselineandG3obtaining+3.42~pp)andranks8thfortheBiologicaldataset(B2-1.52~pp,F3-1.31~pp).Itisalsothebestoverallsystem,ifF_1valuesforthetwodatasetsweretobeaveraged(B2+2.68~pp).Inaddition,bigramsandtrigramsprovidemuchbetterresultsthanunigramsforthebiologicalcorpus,althoughsuchtrendisnotobservedfortheWikipediadataset.SpecificallyfortheWikipediacorpus,itispossibletoverifythattheproposedlesssupervisedmethodobtainshighervaluesforannotationwhichislessreliablefromamachinelearnerpoint-of-view.AsobservedbythelowerF-valuesofthebaseline,cuesaremoreambiguousinthisdataset,whereasfeaturesarenotabletoproperlydisambiguatewordssuchas``some'',``probably''and``many''(Tableb).Thisalsoexplainswhyincreasingthelocalcontextwindowwithbigramsortrigramsdoesnotpresentalargerdifferenceinperformance.Aninterestingfactisthatonlythisworkandgeorgescul10wereabletooutperformtheWikipediana&quot;ivebaselineof59.01%fromTable.Thisislikelybecausebothsystemsusesomesortofcueselectioninordertoeliminatespuriouscues,whichiscarriedinparametertuningfortheothersystem.AnotherobservationisthatG3greatlyimprovesdetectionfortheWikipediacorpus(+1.65~ppcomparedtoB2)byrestrictingconsideredwordstoannotatedcuesintrainingdata,providinganextralayerfornoisefiltering.Thissuggeststhatmoreefficientfiltersmaybeabletofurtherimproveresults.Asforexecutiontimes,asidefromthecomputationallyexpensiveWSDandfromF3,whichhasalargemduetona&quot;ivelycombiningsetsoflinguisticinformation,processingisveryfastbecauseoftheO(kmn)complexity.Runningtimesarethencomparedwithothersystems.georgescul10usedaPentium4,3.2~GHz,3~GBRAMenvironmentandtook4hoursforparametertuning,19.5~sfortrainingand2.6~sfortestinginthebiologicalcorpus,and13~hparametertuning,49.1~strainingand21.5~stestingintheWikipediacorpus.morante10usedaMacOSX,2.2~GHz,2~GBRAM,obtaining22.5~hwhentrainingontheWikipediacorpus,andusedanIntelXeon,2.8~GHz,8~GBRAM,obtaining10.44~swhentestingonthebiologicalcorpus,inacrossdomainapproach.usedanIntelXeon,2.0~GHz,6~GBRAMandobtainedtrainingandtestinginunder8minutesinthebiologicalcorpus.Finally,itcouldbeobservedthatF_1obtainedintrainingmaynotbeusedforestimatingqualityofhedgedetection.TheredoesnotseemtobeanycorrelationbetweentrainingandtestingF_1.Itisneverthelessanindicatorthatoptimizationalone,withoutempirically-drivenheuristics,maynotproducethebestresults.</subsection>
  <subsection title="Error analysis">Therearetwomainsourcesofsentencehedgingerror,oneofwhichisdisambiguation.SetsCandDobtainedresultspoorerthanexpectedforvariousreasons.AsidefromerrorsfromWSDsystems,somecuesarenotcoveredbyusualWSDmethods,suchasmodalverbs.Furthermore,unlikeCoNLL-2010STsystems,disambiguationmethodsusedarenotdesignedspecificallyforuncertainty;thereisatrade-offbetweendisambiguationaccuracyandcueannotationrequirement.Sensedistributionisalsoanissue:ifacuesenseproducesanumberofpositiveinstancesthatissmallerthan,thenthenumberoftruepositivestendstodecreaseaswell.Theothersourceoferrorisfiltering.Althoughaverysimplisticfilteringcriteriawasused,thealgorithmwouldbenefitfromamoreefficientapproachinordertobetterhandledifferentiationofinfrequentambiguouscuesandnoise,asobservedinG3.Needlesstomention,animprovementinfilteringshouldaddressthesensedistributionproblem.Finally,itisobservedmosterrorsarecausedbyWSD,sincefilteringaffectsonlyasmallnumberofinstances.Inaddition,precisionisaffectedexclusivelybyWSD,asfilteringdoesnotcausefalsepositives.</subsection>
  <section title="Conclusion">Thisworkproposedasimplecueselectionalgorithmwhichminimizeshedgingerroranddoesnotrequirecueannotation.Experimentalresultsshowedthatcarefulselectionobtainshighperformanceregardlessofdisambiguationefforts,evenwhenusingonlylemmasandPOStags.ItoutperformedbaselinesystemsontheWikipediacorpus,whichislessreliablefromamachinelearnerpoint-of-view,andobtainedthehighestaverageF_1metricoverall,indicatingagoodmethodforthegeneralcase.Allofthesesuggestanapproachwithhighapplicability,suitedforotherlanguagesormodalcategories.document</section>
</root>
