<?xml version="1.0" ?>
<root>
  <jtitle>語順の相関に基づく機械翻訳の自動評価法</jtitle>
  <jauthor>平尾努磯崎秀樹須藤克仁DuhKevin塚田元永田昌明</jauthor>
  <jabstract>効率的に機械翻訳システムを開発していくためには，質の高い自動評価法が必要となる．これまでに様々な自動評価法が提案されてきたが，参照翻訳とシステム翻訳との間で一致するNグラムの割合に基づきスコアを決定するBLEUや最大共通部分単語列の割合に基づきスコアを決定するROUGE-Lなどがよく用いられてきた．しかし，こうした方法にはいつくかの問題がある．ルールベース翻訳(RBMT)の訳を人間は高く評価するが，従来の自動評価法は低く評価する．これは，RBMTが参照翻訳と違う訳語を使うことが多いのが原因である．これら従来の自動評価法は単語が一致しないと大きくスコアが下がるが，人間はそうとは限らない．一方，統計的機械翻訳(SMT)で英日，日英翻訳を行うと，「AなのでB」と訳すべきところを「BなのでA」と訳されがちである．この訳には低いスコアが与えられるべきであるが，Nグラムの一致割合に着目するとあまりスコアは下がらない．こうした問題を解決するため，本稿では，訳語の違いに寛大で，かつ，大局的な語順を考慮した自動評価法を提案する．大局的な語順は順位相関係数で測定し，訳語の違いは，単語適合率で測定するがパラメタでその重みを調整できるようにする．NTCIR-7，NTCIR-9の特許翻訳タスクにおける英日，日英翻訳のデータを用いてメタ評価を行ったところ，提案手法が従来の自動評価法よりも優れていることを確認した．</jabstract>
  <jkeywords>機械翻訳，自動評価法，順位相関</jkeywords>
  <section title="はじめに">機械翻訳システムの開発過程では，システムの評価と改良を幾度も繰り返さねばならない．信頼性の高い評価を行うためには，人間による評価を採用することが理想ではあるが，時間的な制約を考えるとこれは困難である．よって，人間と同程度の質を持つ自動評価法，つまり，人間の評価と高い相関を持つ自動評価法を利用して人間の評価を代替することが実用上求められる．こうした背景のもと，様々な自動評価法が提案されてきた．BLEU,NIST,METEOR，WordErrorRate(WER)などが広く利用されているが，そのなかでもBLEUは，数多くの論文でシステム評価の指標として採用されているだけでなく，評価型ワークショップにおける公式指標としても用いられており，自動評価のデファクトスタンダードとなっている．その理由は，人間による評価との相関が高いと言われていること，計算法がシステム翻訳と参照翻訳（正解翻訳）との間で一致するNグラム（一般的にN=4が用いられる）を数えあげるだけで実装も簡単なことにある．しかし，BLEUのようにNグラムという短い単語列にのみに着目してスコアを決定すると，システム翻訳が参照翻訳のNグラムを局所的に保持しているだけで，その意味が参照翻訳の意味と大きく乖離していようとも高いスコアを与えてしまう．局所的なNグラムは一致しつつも参照翻訳とは異なるような意味を持つ翻訳をシステムが生成するという現象は，翻訳時に大きな語順の入れ替えを必要としない言語間，つまり，構文が似ている言語間の翻訳ではほとんど起こらない．例えば，構文が似ている言語対である英語，仏語の間の翻訳では大きな語順の入れ替えは必要なく，BLEUと人間の評価結果との間の相関も高い．一方，日本語と英語のように翻訳時に大きな語順の入れ替えが必要となる言語対を対象とすると，先に示した問題が深刻となる．例えば，Echizen-yaらは日英翻訳において，BLEU，その変種であるNISTと人間の評価との間の相関が低いことを報告している．文全体の大局的な語順を考慮する自動評価法としては，ROUGE-L，IMPACTがある．これらの手法は参照翻訳とシステム翻訳との間で一致する最長共通部分単語列(LongestCommonSubsequence:LCS)に基づき評価スコアを決定する．LCSという文全体での大局的な語順を考慮していることから，英日，日英翻訳システムの評価において，Nグラム一致率に基づく自動評価法よりもより良い評価ができるだろう．しかし，Nグラム一致率に基づく自動評価法と同様，訳語の違いに敏感すぎるという問題がある．後に述べるが，NTCIR-9での特許翻訳タスクにおいては，人間が高い評価を与えるルールベースの翻訳システムに高スコアを与えることができないという問題がある．本稿では日英，英日という翻訳時に大きな語順の入れ替えを必要とする言語対を対象とした翻訳システムの自動評価法を提案する．提案手法の特徴は，Nグラムという文中の局所的な単語の並びに着目するのではなく，文全体における大局的な語順に着目する点と，参照翻訳とシステム翻訳との間で一致しない単語を採点から外し，別途，ペナルティとしてそれをどの程度重要視するかを調整できるようにすることで訳語の違いに対して寛大な評価を行う点にある．より具体的には，システム翻訳と参照翻訳との間の語順の近さを測るため，両者に一致して出現する単語を同定した後，それらの出現順序の近さを順位相関係数を用いて計算し，これに重み付き単語正解率と短い翻訳に対するペナルティを乗じたものを最終的なスコアとする．近年，提案手法と同じく語順の相関に基づいた自動評価法であるLRscoreがBirchらによって独立に提案されている．LRscoreは，参照翻訳とシステム翻訳との間で一致する単語の語順の近さをKendall距離で表し，それをさらに低レンジでのスコアを下げるために非線形変換した後，短い翻訳に対するペナルティを乗じ，さらにBLEUスコアとの線形補間で評価スコアを決定する．提案手法とLRscoreは特殊な状況下では同一の定式化となるが，研究対象としてきた言語対が異なることから，相関係数と語彙の一致に対する考え方が大きく異なる．提案手法がどの程度人間の評価に近いかを調べるため，NTCIR-7，NTCIR-9の日英，英日，特許翻訳タスクのデータを用いて検証したところ，翻訳システムの評価という観点から，従来の自動評価法よりも人間の評価に近いことを確認した．以下，2章ではBLEUを例として，Nグラムという局所的な語順に着目してシステムを評価することの問題点，3章ではLCSを用いてシステムを評価することの問題点を指摘する．そして，4章でそれら問題点の解決法として，訳語の違いに寛大，かつ，大局的な語順の相関に基づく自動評価法を提案する．5章で実験の設定を詳述し，6章では実験結果を考察する．最後に7章でまとめ，今後の課題について述べる．</section>
  <section title="Nグラム一致率に基づく自動評価法の問題点">Nグラム一致率を用いてシステム翻訳を評価する際の問題点を以下に定義するBLEUを例として説明する．システム翻訳文集合をH，それに対応する参照翻訳文集合をRとする．システム翻訳文h_iHには，対応する参照翻訳文の集合R_iRが割り当てられており，R_iのj番目の参照翻訳文をr_jとする．なお，S=|H|=|R|とする．ここで，BLEUは，以下の式で定義される．NはNグラムの長さパラメタであり，一般的にはN=4である．P_nは，Nグラム適合率であり，以下の式で定義される．count(h_i,t_n)は，任意のNグラム(t_n)のシステム翻訳文h_iにおける出現頻度，max_count(R_i,t_n)は，t_nの参照翻訳文集合R_iにおける出現頻度の最大値，_r_jR_icount(r_j,t_n)である．BP(BrevityPenalty)は，短いシステム翻訳に対するペナルティであり，以下の式で定義される．closest_len(R)は，各h_iHに対し，最も近い単語数の参照翻訳文r_jR_iを決定した後，それらの単語数を全てのiで合計したもの，len(H)は，h_i単語数を全てのiで合計したものを表す．いま，原文(s)，参照翻訳(r)，システム翻訳(h_1,h_2)が以下の通り与えられたとしよう．rは原文の直訳であり，h_1はほぼそれと等しい訳であるが，h_2は「風邪をひいたので，彼は雨に濡れた」という意味であり，原文が表す因果関係が逆転している．h_1とh_2を比較すると，翻訳としての流暢さ(fluency)，いわゆる言語モデル的な確からしさは同程度であるが，内容の適切性(adequacy)は，h_1がh_2よりも高くならねばならない．ここで，この2つのシステム翻訳を先に示したBLEUで評価してみよう．h_1，h_2ともrよりも長いため，ともにBPは1となる．h_1のP_1〜P_4はそれぞれ，9/12，7/11，5/10，3/9なので，BLEUスコアは0.53となる．一方，h_2のP_1〜P_4はそれぞれ，11/11，9/10，6/9，4/8なので，BLEUスコアは0.74となる．この結果は，我々の直感に反しており，BLEUを最大化するようにシステムを最適化することが，良い翻訳システムの開発に結びつくかどうかは疑問である．こうした問題が起こる原因はNグラムという局所的な語の並びにのみに着目してスコアを計算することにある．短い単語列のみを評価対象とすると，先の例のように，参照翻訳の節中のNグラムを保持していれば，節の順番が入れ替わったとしても十分高いスコアを獲得する．もちろん，h_2のような翻訳をシステムが出力するようなことはほとんどあり得ないのではないかという疑問もあろう．確かに語順が似た言語対を対象とする場合や翻訳システムがルールベースで構築されている場合には起こりにくい問題であるが，語順が大きく異なる言語対を対象とした統計翻訳(StatisticalMachineTranslation:SMT)システムでは十分起こり得る問題である．以下にWeb上のSMTによる翻訳サービスの出力例を示す．SMT出力をみると，訳語という観点では参照翻訳と良く合致しており，バイグラム，トライグラムでもある程度の数が一致している．しかし，原文の「店に行く」の主体が「ボブ」であるという構造を捉えることができず，その主体が「メアリ」となってしまっている．SMTシステムでは，大きな語順の入れ替えを許すと探索空間は膨大になる．よって，現実的な時間で翻訳文を生成するため，語順の入れ替えにある程度の制限を設けざるを得ない．その結果，Nグラムでは参照翻訳と良く合致するものの原文の意味とはかけ離れた翻訳を出力することがある．このような状況のもと，BLEUスコアで翻訳システムを比較すると，正しい評価ができない可能性が高い．なお，この問題はBLEUに限ったことではなく，その変種であるNISTスコア，METEORなどNグラム一致率を利用した自動評価法すべてに当てはまる問題である．</section>
  <section title="LCSに基づく自動評価法の問題点">ROUGE-L，IMPACTは，参照翻訳とシステム翻訳との間の最長共通部分単語列(LCS)に基づき評価スコアを決定する．先に挙げた例で説明する．rとh_1との間のLCSは，``Hecaughtacoldbecauseheintherain''であり，その長さ（単語数）は9である．rの長さは11，h_1の長さは12であることから，LCSの適合率は9/12，再現率は9/11となる．一方，rとh_2との間のLCSは，``hegotsoakedintherain''であり，その長さは6である．h_2の長さは11なので，LCSの適合率は6/11，再現率は6/11となる．ROUGE-LスコアはLCS適合率と再現率の調和平均，F値なのでBLEUとは違い，h_1をh_2より高く評価することができる．IMPACTはROUGE-Lを改良したものであり，上述のLCSを一度見つけただけでやめるのではなく，見つかったLCSを削除した単語列に対し，再度LCSを探すということを繰り返す．つまり，h_1の例では，rとh_1から，``Hecaughtacoldbecauseheintherain''を削除し，から，h_2の例では，``hegotsoakedintherain''を削除し，から，再度LCSを探し出すという手順を繰り返す．これらの手法の問題点は，参照翻訳とシステム翻訳との間のLCS適合率，再現率を計算するため，それらの間で一致しなかった単語を評価の対象に含めている点にある．例えば，以下のシステム翻訳h_3を考えると，rとh_3との間のLCSは，``hecaughtacoldtherain''となるので，LCS適合率，再現率はそれぞれ，6/13，6/11となり，適合率がh_2の場合より低い値をとってしまい，ROUGE-Lスコアはh_2の場合よりも低くなる．このように適合率，再現率といった参照翻訳とシステム翻訳との間で一致しない単語を評価に含めてしまう尺度を用いると訳語の違いに敏感になり過ぎ，システムを過小評価することがある．</section>
  <section title="語順の相関に基づく自動評価法">本稿では，Nグラム一致率に基づく自動評価法の問題点を解決するため，文内の局所的な語の並びに着目するのではなく，大局的な語の並びに着目する．つまり，参照翻訳とシステム翻訳との間で一致して出現する単語の出現順の近さに基づき評価する．さらに，訳語の違いに寛大な評価をするため，システム翻訳の単語適合率の重みを調整できるようにして別途ペナルティとして用いる．</section>
  <subsection title="単語アラインメント">参照翻訳とシステム翻訳の語順との間の相関を計算するため，双方の翻訳に一致して出現する単語を同定しなければならない．これは，参照翻訳とシステム翻訳との間の単語アラインメントを決定する問題となる．本稿では，単語の表層での一致に基づくアラインメント法を採用した．Algorithmにその疑似コードを示す．[b][1]Readhypothesissentenceh=w_1^h,w_2^h,,w_m^hReadreferencesentencer=w_1^r,w_2^r,,w_n^rInitializeworderwithanemptylist.eachwordw_i^hinhw_i^happearsonlyonceeachinhandrappendjs.t.w_i^h=w_j^rtoworder=2tom-iw_i^h,,w_i+(-1)^happearsonlyonceeachinhandrappendjs.t.w_i^h,,w_i+(-1)^h=w_j^r,,w_j+(-1)^rtoworderbreaktheloop=2toiw_i-(-1)^h,,w_i^happearsonlyonceeachinhandrappendjs.t.w_i-(-1)^h,,w_i^h=w_j-(-1)^r,,w_j^rtoworderbreaktheloopReturnworderalgorithmicalgorithmシステム翻訳を長さm，参照翻訳を長さnの単語リストして読み込み，アラインメントを格納する配列worderを初期化する（1〜3行目）．システム翻訳の単語リストの先頭から順に単語w_i^hを取り出し，その単語がシステム翻訳，参照翻訳の双方にただ1度のみ出現している場合，iと単語w_i^hの参照翻訳における出現位置jを対応づける（5,6行目）．それ以外の場合，w_i^hを基準として右側にNグラムを伸長させ，システム翻訳と参照翻訳の双方における出現頻度が1となった時点でiとjを対応づける（8〜13行目）．それでも対応がつかない場合，w_i^hを基準として左側にNグラムを伸長させ，システム翻訳と参照翻訳の双方における出現頻度が1となった時点でiとjを対応づける（15〜20行目）．これでも曖昧性が残る（システム翻訳と参照翻訳での頻度が1にならない）場合，あるいは対応先が見つからない場合は単語対応付けを行わない．図に2章の例文に対する単語アラインメントを示す．上段の例から，worderの1番目の要素，つまり，h_1の1単語目がrの1番目の要素（単語）に対応することがわかる．下段の例から，h_2の1単語目がrの6番目の単語と対応していることがわかる．</subsection>
  <subsection title="単語出現順の相関">1対1の単語アラインメントを決定することができれば，参照翻訳とシステム翻訳から単語出現位置IDを要素とするリストを得ることができる．図の例では，r:[1,2,3,4,5,6,9,10,11]，h_1:[1,2,3,4,5,6,9,10,11]およびr:[1,2,3,4,5,6,7,8,9,10,11]，h_2:[6,7,8,9,10,11,5,1,2,3,4]という2つのリストペアを得る．こうした順序列間の順位相関係数を計算することで参照翻訳とシステム翻訳との間で一致して出現する単語の出現順の近さを測ることができる．本稿では以下に示すKendallの順位相関係数()を採用した．順位相関係数としては，Spearmanの順位相関係数()もよく知られている．しかし，と比べては，順位の小さな入れ替わりには寛容すぎ，大きな入れ替わりには厳しすぎる．予備実験の結果では，人間の評価との間の相関がよりも低い傾向を示したため，本稿ではを採用した．T_iは，アラインメント手続きを用いてシステム翻訳から得た単語出現位置のIDリスト(worder)について，i番目の要素の値よりも大きな要素がi+1番目からn番目の要素までの間に出現する数，U_iはその逆に，i番目の要素の値よりも小さな要素がi+1番目からn番目の要素までの間に出現する数を表す．表に図のh_2から得たとT_i，U_iをそれぞれ示す．この表より，rとh_2との間の語順の相関をKendallので計算すると，(r,h_2)=(21-34)/((1110)/2)=-0.236となる．同様に図のh_1から得たworderを用いて(r,h_1)を計算すると(r,h_1)=(36-0)/((98)/2)=1となる．は参照翻訳とシステム翻訳との語順が完全一致する場合に1，逆順の場合に-1をとる．BLEUでは，h_2がh_1よりも高いスコアを獲得したが，文全体での語順に着目し，システム翻訳と参照翻訳との間の語順の順位相関を計算すると，h_1がh_2よりも高いスコアを獲得でき，我々の直感に合致した結果を得ることができた．ただし，は負の値をとり得るため，従来の自動評価法が出力するスコアレンジと同様[0,1]の値をとるよう以下の式で正規化する．</subsection>
  <subsection title="ペナルティ">参照翻訳とシステム翻訳との間の語順の相関を計算するためには，単語アラインメントを決定し，双方に一致して出現する単語のみを評価の対象としなければならない．しかし，参照翻訳とシステム翻訳との間で一致する単語のみを評価対象とすることには以下の2つ問題がある．システム翻訳の単語数に対し，参照翻訳との間で一致する単語の割合が少ない場合，過剰に高いスコアを与える可能性がある．システム翻訳の単語数が少ない場合，過剰に高いスコアを与える可能性がある，(1)に関して，以下の例を考えよう．hは5単語からなる訳であり，そのうち``John''，``a''，``yesterday''のみしか参照翻訳と一致していない．しかし，その出現順が参照翻訳と一致していることからNKTは1となる．つまり，システムが出力した単語数に関係なく順位相関だけをみていると不当に高いスコアを獲得する可能性がある．次に(2)に関して，以下の例を考えよう．システム翻訳は2単語しかない意味の無い訳であるにもかかわらず，単語正解率は1であり，2単語の出現順序も参照翻訳と一致していることから，NKTも1となる．つまり，単語数が少ない場合，順位相関と単語正解率だけでは不当に高いスコアを獲得する可能性がある．このように，順位相関係数を用いると，システム翻訳の2単語のみが参照翻訳と出現順まで一致すると，不当に高いスコアを獲得する可能性がある．よって，本稿では，前者に対して単語正解率(P)，後者に対してはBLEUのBPをペナルティとして導入する．それぞれの定義を以下に示す．P(h_i,r_i)=len(worder)len(h_i)BP_s(h_i,r_i)=(1,(1-len(r_i)len(h_i)))gather単語正解率は，システム翻訳の単語のうちアラインメントをとることができた単語数(len(worder))の割合であり，len(r)は，参照翻訳の単語数，len(h)はシステム翻訳の単語数である．BLEUのBPは文集合全体で計算していたが，ここでは，文単位で計算することに注意されたい．これらを用いて最終的な自動評価スコアを以下の式()で定義する．なお，この手法をRIBES(Rank-basedIntuitiveBilingualEvaluationScore)と名付け，\urlhttp://www.kecl.ntt.co.jp/icl/lirg/ribes/にてオープンソースソフトウェアとして公開している．(0)は単語適合率の重みであり，が大きいほど訳語の違いに敏感になる．参照翻訳が1つしかない場合，参照翻訳にはない訳語をシステムが出力する可能性が高いため，は小さめに設定した方がよいだろう．参照翻訳が複数の場合，参照翻訳のいずれかに出現する単語をシステムが出力する可能性が高くなる．そこで，不適切な訳語を厳しく採点するためは高めに設定した方がよいだろう．(0)はBPの重みであり，が大きいほど訳文の長さに敏感になる．参照翻訳が1つしかない場合，それよりも短い翻訳があり得る可能性が高いので，は小さめに設定してよいだろう．参照翻訳が複数ある場合，一番短い翻訳を基準にして考えれば，を高めに設定してよいだろう．</subsection>
  <section title="実験の設定"/>
  <subsection title="実験データ">RIBESの有効性を示すため，NTCIR-7，NTCIR-9の特許翻訳タスク(PATMT)のデータを用いて評価実験（評価指標の評価なので，以降メタ評価と呼ぶ）を行った．言語対は英日(EJ)，日英(JE)とした．それぞれのデータセットの文数，1文あたりの参照翻訳の数，評価者の数，参加システム数を表に示す．なお，カッコ内の数字はルールベースシステムの数を示す．NTCIRワークショップの事務局から公開されているデータには，EJ，JEタスクとも1つの参照翻訳しか含まれていない．そこで，NTCIR-7のデータに対してのみ，特許翻訳の専門家に依頼し，参照翻訳を独自に追加した．また，NTCIR-7のEJタスクに関しては，5システムだけにしか人間の主観評価の結果が与えられていなかったため，特許に精通した被験者5名で再度JEタスクと同様，5段階評価で主観評価を行った．さらに，評価対象とする翻訳システムに著者のグループの英日翻訳システムを追加し，計14システムで実験を行った．全てのデータに対し，メタ評価の対象は翻訳の内容としての適切性(adequacy)のみとした．これは，翻訳の流暢さよりも内容の適切性を自動評価できた方がより良い翻訳システムの開発に貢献できると考えたからである．なお，各システム翻訳文に対し複数の人間の評価スコアが与えられている場合には，その平均値を文に対する評価スコアとした．このように各システム翻訳文に対して評価値を決定し，これを文集合全体での平均したものを人間がシステムに与えた評価スコアとした．</subsection>
  <subsection title="比較した自動評価手法">比較評価には，Nグラム一致率に基づく評価手法として先に説明したBLEU，大局的な単語列を考慮した評価法として同じく先に説明したROUGE-L，その改良版であるIMPACTを用いた．IMPACTには，LCSの長さに応じた重みパラメタ，語順の入れ替えに応じた重みパラメタがある．詳細については文献を参照されたい．なお，ROUGE-L，IMPACTとも参照翻訳が複数ある場合には個々の参照翻訳を用いて求めたスコアの最大値を評価スコアとして採用した．BLEUの計算にはmteval-v13a，ROUGE-Lには，ROUGE-1.5.5，IMPACTにはIMPACTversion4を利用した．また，LRscoreも比較評価の対象とした．LRscoreは，参照翻訳とシステム翻訳との間の語順の近さを表すスコアとBLEUスコアとの間の線形補間で評価スコアを決定する．語順の近さを表す尺度としては，ハミング距離d_h(h,r)を利用するものとKendallのに基づくd_k(h,r)を利用するものがあるが，以降では，本稿との関連が深い後者について述べる．LRscoreの定義を以下に示す．R(H,R)は以下の式で定義される．d_k(h,r)は，文献に従うとd_k(h,r)=1-1-(h,r)で定義されるが，それ以前の文献では，d_k(h,r)=(h,r)も用いられている．以降，前者をd_k_1，後者をd_k_2とよぶ．RIBESで=0,=1と設定したときと，LRscoreにd_k_2を採用，=1と設定したとき，これら2つの手法は一致する．しかし，LRscoreは日本語，英語のような大きな語順の入れ替えがある言語対を対象として考案された手法ではなく，ヨーロッパ言語間，中英翻訳という比較的語順が似た言語を対象として考案されたため，最終的にはd_k_1を採用することで順位相関の低レンジスコアの感度を下げ，さらに語順の近い言語対を対象としたときに実績のあるBLEUの恩恵を受けるため，それとの間の線形補間という定式化に至ったのであろう．後述するが，英日，日英翻訳の評価ではBLEUを利用するメリットは期待できない．さらに，NKTをd_k_1によって非線形変換することで低レンジスコアの感度をさらに下げるメリットも元々高いNKTを得ることが難しい英日，日英翻訳タスクでは期待できない．以上より，LRscoreは確かにRIBESと良く似た手法といえるが，BLEUを補うために派生した評価指標と捉えた方が自然であり，RIBESとはその根底にある研究の動機に大きな違いがある．なお，LRscoreには，参照翻訳とシステム翻訳との間の単語アラインメントを決定する手段が提供されないため，以降の実験では本稿での単語アラインメントを利用した．</subsection>
  <subsection title="メタ評価の指標">本稿では，メタ評価の指標として広く用いられているPearsonの積率相関係数，Spearmanの順位相関係数，Kendallの順位相関係数を用いた．Pearsonの積率相関係数は人間の評価と自動評価の結果がどの程度線形の関係にあるかを評価し，Spearman，Kendallの相関係数は人間の評価と自動評価の結果の順位がどの程度近いかを評価する．SpearmanとKendallの違いは，先にも説明したように順位の差に対して重みをどのように与えるかという点にある．</subsection>
  <subsection title="実験の手順">RIBESに対してはシステム翻訳の長さに対する重みパラメタと単語正解率に対する重みパラメタ，IMPACTに対してはLCSに対する重みパラメタと語順の違いに対する重みパラメタ，LRscoreには順位相関係数とBLEUスコアの重みを調整するパラメタがある．これらの手法に対しては，以下の手順でパラメタの最適化を行い，メタ評価を行った．文のIDをランダムに10個選択する．選択したIDによる10文の集合を用いて，文集合全体での人間の評価スコアと自動評価スコアとの間のSpearmanの順位相関係数が最大となるようパラメタを決定する．(2)で決定したパラメタを用いて(1)の残りの文集合全体を用いてメタ評価を行い，相関係数を記録する．(1)から(3)を100回繰り返し，相関係数の平均値を求める．なお，パラメタが存在しないBLEUとROUGE-Lに対しては，(2)をスキップし，同様の手順でメタ評価を行った．</subsection>
  <section title="実験結果と考察"/>
  <subsection title="NTCIR-7データ">表にオーガナイザから配布された参照翻訳のみを用いた時の相関係数の平均値，表に複数参照翻訳を用いた時の相関係数の平均値を示す．平均値の差の検定には，ペアワイズの比較にWilcoxonの符号順位検定を採用し，Holm法による多重比較を用いた．表，より，どの手法に対してもSpearmanの順位相関係数の方がKendallの順位相関係数よりも高い．Kendallの順位相関係数は，2つの順序列の間で一致する半順序関係の数に基づき決定されるため，細かな順位の間違いに敏感である．一方，Spearmanの順位相関係数は順序列の間の順位の差に基づき決定されるため，細かな順位の間違いには鈍感である．本稿で用いたデータでは，自動評価法にとって，明らかに良いシステムと悪いシステムの区別が容易であったため，大きな差での順位の不一致が減少し，Spearmanの順位相関係数がKendallの順位相関係数よりも相対的に高い値を記録したのであろう．ただし，全体の傾向としては両者の間に大きな違いはない．Pearsonの積率相関係数は順位相関係数より高い値を示しており，BLEUではその差が特に大きい．たとえば，表の英日タスクでは，Pearsonが0.9以上であることに対し，Spearmanは0.7程度でしかない．これは，人間の評価との順位付けはやや強い程度相関でしか示していないにも関わらず，線形の相関は非常に強いことを意味する．Pearsonの積率相関係数は外れ値がある場合，その値が過剰に高く見積もられるということが知られているが，その影響が強く出ているのではないかと考える．よって，以降では主に順位相関に焦点をあてて議論する．表より，日英翻訳に関しては，RIBESが他のすべての手法に対して統計的有意に優れている．英日翻訳に関しては，RIBESとLRscore(d_k_1)が同程度で優れており，十分強い相関である．ROUGE-L，IMPACTが，RIBESほどではないものの比較的よい相関を得ていることに対し，BLEUは双方の言語対において，相関係数の平均値が他のほぼ全ての手法に対し統計的に有意な差で劣っている．さらに，順位相関係数は弱い相関程度でしかない．この結果は，英日，日英翻訳という大きな語順の入れ替えを必要とする言語対を対象とした場合，Nグラム一致率で自動評価を行うことが不適切であることを示唆している．表より，参照翻訳の数が増えると相関係数の平均値は上昇する傾向にある．BLEUの相関係数が他の手法よりも有意に劣っていることは単一参照翻訳の場合と同様であるが，順位相関係数はやや強い相関程度にまで上昇している．ROUGE-L，IMPACTの相関係数も上昇しており，ROUGE-Lは日英翻訳に関しては，他のすべての手法に対して，英日翻訳に関しては，RIBES以外の手法に対して統計的有意に優れている．RIBESは，日英翻訳ではROUGE-Lに次いでIMPACTと同程度，英日翻訳ではROUGE-Lと同程度であるが，十分強い相関を示している．BLEU，ROUGE-L，IMPACTの相関係数が複数参照翻訳が与えられた場合に顕著に改善される理由は，語彙のバリエーションが増えたことであろう．これらの手法は，Nグラム一致率，LCS適合率，再現率を利用しているため，参照翻訳とシステム翻訳との間で一致しない単語も評価対象となる．よって，語彙の一致判定を単に文字列としての一致だけで判定すると，意味的には一致するはずのものが一致せずに不当に低いスコアを得るという問題が起こる．しかし，複数参照翻訳の場合には，語彙のバリエーションが増えるためこうした問題は軽減されるのであろう．一方，RIBESでもシステム翻訳と参照翻訳との間の単語一致率は利用するため，一致しない単語を評価対象として用いているが，パラメタによりその影響を小さく抑えることができる．よって，単一参照翻訳でも複数参照翻訳でも安定して高い相関を示すことができた．</subsection>
  <subsection title="NTCIR-9データ">表に相関係数の平均値を示す．NTCIR-7のデータとは異なり，どの手法も相関係数の平均値は大幅に低下している．特にROUGE-L，IMPACT，BLEUは，非常に弱い相関，あるいは無相関と言えるほどである．この原因は先の実験結果と同様，参照翻訳の数が1つであることに加え，評価対象となる翻訳システムの中でのルールベースの翻訳システム（SMTとのハイブリッドも含む）の占める割合が増したことにある．NTCIR-7と比較すると，日英翻訳に関してはルールベースシステムは2システムから6システムへ，英日翻訳に関しては1システムから5システムへと増えた．図にユニグラム適合率と人間のスコアの関係を示す．図中四角のマーカ(RBMT-1〜6，JAPIO，TORI，EIWA)がルールベースの翻訳システムである．図から明らかなように，ルールベースの翻訳システムはユニグラム適合率が低いにも関わらず人間の評価では高いスコアを獲得している．つまり，これらのシステムの翻訳は，参照翻訳と一致する単語の割合が少ないにも関わらず人間の評価では高いスコアを獲得している．ルールベースの翻訳システムは，訓練データから訳語を推定するSMTシステムほど語彙の統制がとれていない．よって，翻訳対象のドメインに合致した語彙，すなわち，特許特有の語彙を用いて翻訳できるとは限らない．しかし，SMTシステムにとって大きな問題となる語順の入れ替えに関しては，記述されたルールに当てはまる限りは問題となり得ない．よって，特許の訳語として多少おかしくとも文全体で意味が通る翻訳となり，その結果，人間が高いスコアを与えたのであろう．先にも説明した通り，BLEUは，Nグラムの適合率，ROUGE-LとIMPACTはLCSの適合率・再現率に基づきスコアを決定する．つまり，参照翻訳と一致する単語の割合が大きいシステム翻訳にしか高いスコアを与えることができない．よって，ルールベースシステムに高いスコアを与えることはできず，それらの性能を低く見積もってしまったことにより，相関が著しく低下したと考える．一方，RIBESとLRscoreはこれらの手法とは異なり，単語正解率，BLEUスコアをパラメタで軽減することで単語一致率の低いシステムであっても高いスコアを与えることができるという特徴がある．実際，日英，英日の双方においてROUGE-L，IMPACT，BLEUといった従来の自動評価法に対し，統計的有意に高い相関係数を獲得していることがそれの有効性を示唆している．ユニグラム適合率が低いところでの自動評価法の性能をより詳細に調べるため，ルールベースの翻訳システムのみを取り出し，同様の実験を行い相関係数の平均値を求めたところ，表を得た．翻訳システム数は日英翻訳タスクで6，英日翻訳タスクで5である．サンプル数が少ないため，相関係数の値に対する信頼性がこれまでの実験よりも劣ることに注意されたい．表より，RIBESは日英翻訳タスクでROUGE-L，IMPACTに劣るものの全体を通してみれば他の手法より良い相関を得ている．参照翻訳が1つしかないという影響もあるが，英日翻訳タスクではROUGE-L，IMPACT，BLEUは負の相関でしかない．さらに，先に示したとおり，表においてRIBESがルールベースシステム，SMTシステム双方を含む場合でも良い相関を得たこともふまえると，参照翻訳とシステム翻訳との間で一致する単語が少ない場合でもRIBESは有効であると考える．以上より，BLEU，ROUGE-L，IMPACTといった単語の一致に強く依存する従来の自動評価法は，単一参照翻訳時，評価対象としてルールベースシステムが多く混在する場合には著しく信頼性が低下することを示した．特に，参照翻訳は常に複数与えられるとは限らないため，自動評価法としては単一参照翻訳でも人間の評価結果との間の相関が高いことが望ましい．RIBESは，参照翻訳数，ルールベースシステムの数が変化した場合でも安定して高い相関であることから，従来の自動評価法よりも優れていると考える．ただし，RIBESには単語正解率と短い翻訳に対するペナルティを調整するための重みパラメタがある．これらパラメタを最適化するためには，いわゆる教師データが必要となることから，それを必要としないBLEU，ROUGE-Lよりもコストのかかる手法とも言える．しかし，実験結果より，各システムに対し10文を人間が評価した結果を教師データとしてパラメタを最適化できることを示した．よって，十分低いコストでパラメタの最適が可能である．</subsection>
  <subsection title="獲得されたパラメタに関する考察">最後にRIBESとLRscoreについて獲得されたパラメタ，,,の違いから考察する．図に,の分布，図にの分布を示す．図より，パラメタ最適化のための訓練事例が10文と少ないことも影響してか，獲得されたパラメタにばらつきがあるが，単一参照翻訳の場合には小さなが選択されている割合が多く，4.3節の仮説と一致する．また，NTCIR-9のように単一参照翻訳かつルールベースシステムの数が多い場合には，=0の場合が非常に多い．一方，複数参照翻訳がある場合，比較的大きなが選択されている．に関しては，複数参照翻訳時には高い値が選ばれている割合が高く，4.3節の仮説と一致するが，単一参照翻訳時には必ずしも低い値が選ばれておらず先の仮説と一致しない．しかし，人間の評価との間の相関をみる限り，RIBESは従来法よりも比較的高い相関を得ていることから，これは大きな問題ではないと考える．図より，LRscoreのNTCIR-9では，=1付近が多く選択されており，語順の相関に対する重みを上げ，語彙の一致（BLEUスコア）に対する重みを下げるようにパラメタを選択しており，RIBESと同様の傾向を示している．しかし，NTCIR-7では，単一参照翻訳，複数参照翻訳に関わらず0.3から0.6までの値が多く選ばれており，NTCIR-9の場合ほどBLEUスコアに対する重みを下げるようなパラメタが選択されていない．NTCIR-7ではRIBESの相関が概ねLRscoreの相関を上回っていたが，その原因はこうした語彙の一致に対する重みの違いによると考える．さらにRIBESは2つのパラメタ,があることに対し，LRscoreは1つのパラメタしかない．よって，RIBESはLRscoreよりもより柔軟にデータにフィットできる点が双方の手法のパラメタ選択，相関係数の差に影響を与えたとも考える．5.2節でも述べたが，=0,=1，=1とした場合，RIBESもLRscoreも語彙の一致スコアを考慮せず順位相関と短い翻訳に対するペナルティだけを考慮することになり，両者はほぼ一致する．図より，LRscoreでは=1が試行の半数近くで選択されているが，図のNTCIR-9でそうした例がみられるものの全体に占める割合は決して多くはない．また，LRscoreの語順相関の計算法としてd_k_1とd_k_2の双方を試したが，一貫して，d_k_1の方がよい相関を示した．こうしたことから，基本的にはRIBESとLRscoreは違うものと捉えて差し支えないだろう．</subsection>
  <section title="まとめと今後の課題">本稿では，翻訳時に大きな語順の入れ替えが必要となる英日，日英翻訳システムを対象として，文全体での大局的な語順の相関をKendallの順位相関係数に基づき決定し，これと単語適合率，短い翻訳に対するペナルティを重み付きで乗じた自動評価法であるRIBES(Rank-basedIntuitiveBilingualEvaluationScore)を提案した．NTCIR-7，NTCIR-9の特許翻訳タスクのデータを用いてメタ評価を行ったところ，BLEU，ROUGE-L，IMPACTといった従来の自動評価法は，参照翻訳の数が少ない場合，評価対象システムにおけるルールベースシステムの割合が大きい場合に相関が低下することに対し，RIBESは，こうした状況でも安定して高い相関を示すことを確認した．また，RIBESと同じくKendallの順位相関係数に基づく自動評価手法であるLRscoreと比較してもRIBESが少なくとも同等以上の性能であることを確認した．評価実験では，英日，日英翻訳システムを対象としたが，これ以外にも翻訳時に大きな語順の入れ替えを必要とする言語対を対象とした翻訳システムの評価時には有効であると考える．本稿では，100文規模程度のコーパスを用いて翻訳システム間の優劣を人間と同様に自動評価すること，つまり，システム単位での人間の評価結果に対して相関が高い自動評価法を実現することを目的としたが，こうした粗い評価だけではなく，翻訳システムの特徴をより詳細に分析するため，個々の文に対して人間が与えたスコアと自動評価法が与えたスコアとの間の相関を向上させることを目的とした研究もある．機械翻訳システムが発展していくにつれ，文単位で細かな評価をしたいという要求はより増していくと考えられるので，こうした着眼点は極めて重要である．ROUGE-L，IMPACTと同様，RIBESは文単位でも自動評価スコアを計算できるので，これらの手法の文単位での自動評価スコアと人間の評価スコアとの間のSpearmanの順位相関係数を計算した．その結果を表に示す．NTCIR-7データでの相関がNTCIR-9データでの相関よりも高いという傾向はシステム単位での実験結果と同様であるが，相関係数の値は大きく下がっている．この原因は，個々の文に対する人間の評価にはゆれがあることだろう．3手法とも，NTCIR-7の日英翻訳タスクではやや弱い相関，英日翻訳タスクではやや強い相関である．RIBESはROUGE-Lとほぼ同程度で，日英翻訳タスクでIMPACTに4ポイント程度劣るものの英日翻訳タスクではIMPACTに9ポイント程度勝っている．一方，NTCIR-9データの場合，もともとシステム単位での相関もNTCIR-7データほど高くはなかったが，文単位での相関は，どの手法でもほぼ無相関という結果であった．この原因には，評価のゆれに加え，各翻訳文に対する評価者が1名であることから，人間の評価の信頼性が低いこと，スコアが5段階のカテゴリカルデータになってしまったこと，参照翻訳数が1つしかないことが考えられる．今後，より良い自動評価法を開発するため，こうした文単位での相関をシステム単位での相関程度まで向上させることは自動評価法の大きな課題だと考える．</section>
</root>
