<?xml version="1.0" ?>
<root>
  <jtitle>共変量シフトの問題としての語義曖昧性解消の領域適応</jtitle>
  <jauthor>新納浩幸佐々木稔</jauthor>
  <jabstract>本稿では語義曖昧性解消(WordSenseDisambiguation,WSD)の領域適応が共変量シフトの問題と見なせることを示し，共変量シフトの解法である確率密度比を重みにしたパラメータ学習により，WSDの領域適応の解決を図る．共変量シフトの解法では確率密度比の算出が鍵となるが，ここではNaiveBayesで利用されるモデルを利用した簡易な算出法を試みた．そして素性空間拡張法により拡張されたデータに対して，共変量シフトの解法を行う．この手法を本稿の提案手法とする．BCCWJコーパスの3つ領域OC（Yahoo!知恵袋），PB（書籍）及びPN（新聞）を選び，SemEval-2の日本語WSDタスクのデータを利用して，多義語16種類を対象に，WSDの領域適応の実験を行った．実験の結果，提案手法はDaum'e</jabstract>
  <jkeywords>語義曖昧性解消，領域適応，共変量シフト，Daum'e</jkeywords>
  <section title="はじめに">本稿では語義曖昧性解消(WordSenseDisambiguation,WSD)をタスクとした領域適応の問題が共変量シフトの問題と見なせることを示す．そして共変量シフトの解法である確率密度比を重みにしたパラメータ学習により，WSDの領域適応の解決を図る．共変量シフトの解法では確率密度比の算出が鍵となるが，ここではNaiveBayesで利用されるモデルを利用した簡易な算出法を試みた．そして素性空間拡張法により拡張されたデータに対して，共変量シフトの解法を行う．この手法を本稿の提案手法とする．自然言語処理の多くのタスクにおいて帰納学習手法が利用される．そこではコーパス(S)からタスクに応じた訓練データを作成し，その訓練データから分類器を学習する．そしてこの分類器を利用することで当初のタスクを解決する．このとき実際のタスクとなるデータはコーパス(S)とは領域が異なるコーパス(T)のものであることがしばしば起こる．この場合，コーパス(S)（ソース領域）から学習された分類器では，コーパス(T)（ターゲット領域）のデータを精度良く解析することができない問題が生じる．これが領域適応の問題であり，近年活発に研究が行われている．WSDは文(x)内の多義語(w)の語義(cC)を識別する問題である．(P(c|x))を文(x)内の単語(w)の語義が(c)である確率とすると，確率統計的には(_cCP(c|x))を解く問題といえる．例えば単語(w=)「ボタン」には少なくとも(c_1:)服のボタン，(c_2:)スイッチのボタン，(c_3:)花のボタン（牡丹），の3つの語義がある．そして文(x=)「シャツのボタンが取れた」が与えられたときに，文中の「ボタン」が(C=c_1,c_2,c_3)内のどれかを識別する．直接的には教師付き学習手法を用いて(P(c|x))を推定して解くことになる．WSDの領域適応の問題は，前述したように，教師付き学習手法を利用する際に学習もとのソース領域のコーパス(S)と，分類器の適用先であるターゲット領域のコーパス(T)が異なる問題である．領域適応ではソース領域(S)から(S)上の条件付き分布(P_S(c|x))は学習できるという設定なので，(P_S(c|x))やその他の情報を利用して，ターゲット領域(T)上の条件付き分布(P_T(c|x))を推定できれば良い．ここで「シャツのボタンが取れた」という文中の「ボタン」の語義は，この文がどのような領域のコーパスに現れても変化するとは考えづらい．つまり(P_T(c|x))は領域に依存していないため，(P_S(c|x)=P_T(c|x))が成立していると考えられる．今(P_S(c|x))は推定できるので，(P_S(c|x)=P_T(c|x))が成立していれば，(P_T(c|x))を推定する必要はないように見える．ただしソース領域だけを使って推定した(P_S(c|x))では，実際の識別精度は低い場合が多い．それは(P_S(x)P_T(x))から生じている．(P_S(c|x)=P_T(c|x))だが(P_S(x)P_T(x))という仮定の下で，(P_T(c|x))を推定する問題は共変量シフトの問題である．本稿ではWSDの領域適応の問題を共変量シフトの問題として捉え，共変量シフトの解法を利用してWSDの領域適応を解決することを試みる．訓練データを(D=(x_i,c_i)_i=1^N)とする．共変量シフトの標準的な解法では(P_T(c|x))に確率モデル(P(c|x;))を設定し，次に確率密度比(r(x_i)=P_T(x_i)/P_S(x_i))を重みにした以下の対数尤度を最大にする()を求めることで，(P_T(c|x))を構築する．[_i=1^Nr(x_i)P(c_i|x_i;)]また領域適応に対してはDaum'eの手法が非常に簡易でありながら，効果が高い手法として知られている．Daum'eの手法は，データの表現を領域適応に効果が出るように拡張し，拡張されたデータを用いてSVM等の学習手法を利用する手法である．ここでは拡張する手法を「素性空間拡張法(FeatureAugmentation)」と呼び，拡張されたデータを用いてSVMなどで識別までを行う手法を「Daum'eの手法」と呼ぶことにする．拡張されたデータに対しては任意の学習手法が利用できる．つまり素性空間拡張法により拡張されたデータに対して，共変量シフトによる解法を利用することも可能である．本稿ではこの手法を提案手法とする．実験では現代日本語書き言葉均衡コーパス（BCCWJコーパス）における3つの領域OC（Yahoo!知恵袋），PB（書籍）及びPN（新聞）を利用する．SemEval-2の日本語WSDタスクではこれらのコーパスの一部に語義タグを付けたデータを公開しており，そのデータを利用する．すべての領域である程度の頻度が存在する多義語16単語を対象にして，WSDの領域適応の実験を行う．領域適応としてはOC→PB，PB→PN，PN→OC，OC→PN，PN→PB，PB→OCの計6通りが存在する．結果(166=96)通りのWSDの領域適応の問題に対して実験を行った．その結果，提案手法はDaum'eの手法と同等以上の正解率を出した．本稿で用いた簡易な確率密度比の算出法であっても共変量シフトの解法を利用する効果が高いことが示された．より正確な確率密度比の推定法を利用したり，SVMを利用するなどの工夫で更なる改善が可能である．また教師なし領域適応へも応用可能である．WSDの領域適応に共変量シフトの解法を利用することは有望であると考えられる．</section>
  <section title="関連研究">自然言語処理における領域適応は，帰納学習手法を利用する全てのタスクで生じる問題であるために，その研究は多岐にわたる．利用手法をおおまかに分類すると，ターゲット領域のラベル付きデータを利用するかしないかで分類できる．利用する場合を教師付き領域適応手法，利用しない場合を教師なし領域適応手法と呼ぶ．本稿における手法は教師付き領域適応手法の範疇に入るので，ここでは提案手法に関連する教師付き領域適応手法の従来研究を述べる．教師付き領域適応手法においては，一般に，ターゲット領域の知識は使えるだけ使えばよいはずなので，ポイントはソース領域の知識の利用方法にある．ソース領域とターゲット領域間の距離が離れすぎている場合，ソース領域の知識を使いすぎると分類器の精度が悪化する現象がおこる．これは負の転移と呼ばれている．負の転移を避けるには，本質的に，ソース領域とターゲット領域間の距離を測り，その距離を利用してソース領域の知識の利用を制御する形となる．Aschは品詞タグ付けをタスクとして領域間の類似性を測り，その類似度から領域適応を行った際に精度がどの程度悪くなるかを予測できることを示した．張本は構文解析をタスクとしてターゲット領域を変化させたときの精度低下の要因を調査し，そこから新たな領域間の類似性の尺度を提案している．Plankは構文解析をタスクとして領域間の類似性を測ることで，ターゲット領域を解析するのに最も適したソース領域を選んでいる．PonomarevaやRemusは感情極性分類をタスクとして領域間の類似度を学習中のパラメータに利用した．これらの研究はタスク毎に類似性を測るが，WSDがタスクの場合，領域間の類似性はWSDの対象単語に依存していると考えられる．古宮は対象単語毎に領域間の距離を含めた性質によって適用する学習手法を変化させている．上記した古宮の一連の研究は広い意味でアンサンブル学習の一種である．そこでアンサンブルされる各要素となる学習手法をみるとソース領域のデータとターゲット領域のデータへの各重みが異なるだけである．つまり領域適応においてはソース領域のデータとターゲット領域のデータへの各重みを調整して，学習手法を適用するというアプローチが有力である．Jiangは(P_S(c|x))と(P_T(c|x))との差が極端に大きいデータを``misleading''データとして訓練データから取り除いて学習することを試みた．これは``misleading''データの重みを0にした学習と見なせるため，この手法も重み付けの手法と見なせる．本稿で利用する共変量シフト下での学習もこの範疇の手法といえる．素性空間拡張法も重み付け手法である．ただしデータではなくデータ中の素性に重みをつける．そこではソース領域の訓練データのベクトル(x_s)を((x_s,x_s,0))と連結した3倍の長さのベクトルに直し，ターゲット領域の訓練データのベクトル(x_t)を((0,x_t,x_t))と連結した3倍の長さのベクトルに直す．ここで(0)は(x_s)や(x_t)と同じ次元数であり，しかもすべての次元の値が0であるようなベクトルである．この3倍にしたベクトルを用いて，通常の分類問題として解く．この手法は非常に簡易でありながら，効果が高い手法として知られている．この拡張手法はソース領域とターゲット領域に共通している特徴が重なることで，結果として共通している特徴の重みがつくことで領域適応に効果が出ると考えられる．また領域適応の問題を共変量シフト下の学習を用いて解決する研究としては，Jiangの研究と齋木の研究がある．Jiangは確率密度比を手動で調整し，モデルにはロジステック回帰を用いている．また齋木は(P(x))をunigramでモデル化することで確率密度比を推定し，モデルには最大エントロピー法のモデルを用いている．ただしどちらの研究もタスクはWSDではない．また共変量シフト下では(P_S(c|x)=P_T(c|x))を仮定するが，(P_S(x|c)=P_T(x|c))を仮定するアプローチもある．この場合，ベイズの定理から_cCP_T(c|x)&amp;=_cCP_T(c)P_T(x|c)&amp;=_cCP_T(c)P_S(x|c)align*となるので領域適応の問題は(P_T(c))の推定に帰着できる．実際，Chanらは(P_S(x|c))と(P_T(x|c))の違いの影響は非常に小さいと考え，(P_S(x|c)=P_T(x|c))を仮定し，(P_T(c))をEMアルゴリズムで推定することでWSDの領域適応を行っている．更に新納らは(P_S(x|c)=P_T(x|c))の仮定があったとしても，コーパスのスパース性から単純に(P_T(x|c))を(P_S(x|c))で置き換えることはできないと考え，(P_T(c))の推定の問題と(P_T(x|c))の推定の問題を個別に対処することを提案している．</section>
  <section title="期待損失最小化からみた共変量シフト">対象単語(w)の語義の集合を(C)，また(w)の用例(x)内の(w)の語義を(c)と識別したときの損失関数を(l(x,c,d))で表す．(d)は(w)の語義を識別する分類器である．(P_T(x,c))をターゲット領域上の分布とすれば，領域適応の問題における期待損失(L_0)は以下で表せる．[L_0=_x,cl(x,c,d)P_T(x,c)]また(P_S(x,c))をソース領域上の分布とすると以下が成立する．[L_0=_x,cl(x,c)P_T(x,c)P_S(x,c)P_S(x,c)]ここで共変量シフトの仮定から[P_T(x,c)P_S(x,c)=P_T(x)P_T(c|x)P_S(x)P_S(c|x)=P_T(x)P_S(x)]となり，(r(x)=P_T(x)/P_S(x))とおくと以下が成立する．[L_0=_x,cr(x)l(x,c,d)P_S(x,c)]訓練データを(D=(x_i,c_i)_i=1^N)とし，(P_S(x,c))を経験分布で近似すれば，[L_01N_i=1^Nr(x_i)l(x_i,c_i,d)]となるので，期待損失最小化の観点から考えると，共変量シフトの問題は以下の式(L_1)を最小にする(d)を求めればよいことがわかる．</section>
  <section title="重み付き対数尤度の最大化">分類器(d)として以下の事後確率最大化推定に基づく識別を考える．[d(x)=_cP_T(c|x)]また損失関数として対数損失(-P_T(c|x))を用いれば，は以下となる．[L_1=-_i=1^Nr(x_i)P_T(c|x_i)]つまり，分類問題の解決に(P_T(c|x,))のモデルを導入するアプローチを取る場合，共変量シフト下での学習では，確率密度比を重みとした以下に示す重み付き対数尤度(L())を最大化するパラメータ()を求める形となる．ここではモデルとして以下の式で示される最大エントロピー法を用いる．(x=(x_1,x_2,,x_M))が入力で(c)がクラスである．関数(f_j(x,c))は素性関数であり，実質(x)の真のクラスが(c)のときに(x_j)を返し，そうでないとき0を返す関数に設定される．(Z(x,))は正規化項であり，以下で表せる．そして(=(_1,_2,,_M))が素性に対応する重みパラメータとなる．共変量シフト下ではない通常のケースでは，重みパラメータは最尤法から求める．つまり，訓練データ(D=(x_i,c_i)_i=1^N)とすると，以下の式(F())を最大にする()を求める．[F()=_i=1^NP(c_i|x_i)]これを各(_j)で偏微分し極値問題に直すと以下が成立する．[F()_j=_i=1^Nf_j(x_i,c_i)-_i=1^N_cCP_T(c|x_i,)f_j(x_i,c)=0]これを勾配法などで解くことにより()が求まる．共変量シフト下の学習ではの(L())を最大にする()を求める．上記と全く同じ手順で，[L()_j=_i=1^Nr(x_i)f_j(x_i,c_i)-_i=1^N_cCP(c|x_i,)r(x_i)f_j(x_i,c)=0]が得られる．これを勾配法などで解くことにより()が求まる．今，事例(x_i)の頻度を(h_i)とすると，尤度は以下となる．[_i=1^NP(c_i|x_i)^h_i]対数を取れば以下が得られる．[_i=1^Nh_iP(c_i|x_i)]この式は重み付き対数尤度のと同じ形なので，実際に()を求めるためには，事例(x_i)の頻度(h_i)を(r(x_i))と考えて，最大エントロピー法のツールなどを用いればよい．</section>
  <section title="確率密度比の算出">共変量シフト下の学習では確率密度比の算出が鍵である．直接的には(P_S(x))と(P_T(x))を推定し，その比を取ればよいが，(P_S(x))や(P_T(x))を正確に推定することは困難であり，その比をとれば更に誤差が大きくなると予想できる．そのため確率密度比を直接モデル化して求める手法が活発に研究されている．ただし本稿では簡易な手法を利用して確率密度比を算出することにした．本稿の目的はこのような簡易な手法による確率密度比の算出法であっても，WSDの領域適応の有力な解法になることを示すことである．対象単語(w)の用例(x)の素性リストを(f_1,f_2,,f_n)とする．求めるのは領域(RS,T)上の(x)の分布(P_R(x))である．ここではNaiveBayesで使われるモデルを用いて算出する．NaiveBayesのモデルでは以下を仮定する．[P_R(x)=_i=1^nP_R(f_i)]領域(R)のコーパス内の(w)の全ての用例について素性リストを作成しておく．ここで用例の数を(N(R))とおく．また(N(R))個の用例の中で，素性(f)が現れた用例数を(n(R,f))とおく．MAP推定でスムージングを行い，(P_R(f))を以下で定義する．[P_R(f)=n(R,f)+1N(R)+2]以上より，ソース領域(S)の用例(x)に対して，確率密度比(r(x)=P_T(x)P_S(x))が計算できる．ターゲット領域(T)の用例(x)に対しては(r(x)=1)とする．また(r_x&lt;0.01)となる用例(x)は訓練データから削除した．</section>
  <section title="提案手法">「関連手法」の節で素性空間拡張法を紹介した．素性空間拡張法はデータの表現を領域適応で効果が出るように拡張する手法である．そして拡張されたデータに対しては任意の学習手法が利用できる．つまり拡張されたデータに対して，共変量シフト下の学習も可能である．本稿では，素性空間拡張法により拡張されたデータに対して，4章で説明した共変量シフト下の学習を行うことを提案手法する．具体的に示す．素性空間拡張法により，ソース領域の訓練データ(x_s)は(u_s=(x_s,x_s,0))という3倍の長さのベクトルに拡張され，ターゲット領域の訓練データ(x_t)は(u_t=(0,x_t,x_t))という3倍の長さのベクトルに拡張される．ここで(u_s)に対しては確率密度比(r(x_s)=P_T(x_s)/P_S(x_s))の重みをつけ，(u_t)に対しては重み1をつける．また(P_T(c|u))のモデルに最大エントロピー法を用い，重み付き対数尤度を最大化するパラメータを求めることで，(P(c|u))を推定する．上記の重み付き対数尤度の式（目的関数）を示しておく．今，ソース領域の訓練データを(D_s=(x_s^(i),c_s^(i))_i=1^n)，ターゲット領域の訓練データを(D_t=(x_t^(i),c_t^(i))_i=1^m)とおく．また(x_s^(i))と(x_t^(i))を素性空間拡張法により拡張したデータをそれぞれ(u_s^(i))と(u_t^(i))とおく．ここで(x_s^(i))と(x_t^(i))は(M)次元，(u_s^(i))と(u_t^(i))は(3M)次元のベクトルであることに注意する．提案手法の重み付き対数尤度の式は以下となる．L()=_i=1^nr(x_s^(i))P(c_s^(i)|u_s^(i),)+_i=1^mP(c_t^(i)|u_t^(i),)(c|u,)=1Z(u,)(_j=1^3M_jf_j(u,c))(u,)=_cC(_j=1^3M_jf_j(u,c))gather*</section>
  <section title="実験">BCCWJコーパスのPB（書籍），OC（Yahoo!知恵袋）及びPN（新聞）を異なった領域として実験を行う．SemEval-2の日本語WSDタスクではこれら領域のコーパスの一部に語義タグを付けたデータを公開しており，そのデータを利用する．この3つの領域からある程度頻度のある多義語16単語をWSDの対象単語とする．これら単語と辞書上での語義数及び各コーパスでの頻度と語義数をに示す．領域適応の方向としてはOC→PB，PB→PN，PN→OC，OC→PN，PN→PB，PB→OCの計6通りの方向が存在する．本稿で利用した素性は以下の8種類である．(e0)(w)の表記，(e1)(w)の品詞，(e2)(w_-1)の表記，(e3)(w_-1)の品詞，(e4)(w_1)の表記，(e5)(w_1)の品詞，(e6)(w)の前後3単語までの自立語の表記，(e7)e6の分類語彙表の番号の4桁と5桁．なお対象単語の直前の単語を(w_-1)，直後の単語を(w_1)としている．単語(w_i)についてソース領域(S)からターゲット領域(T)への領域適応の実験について説明する．まずターゲット領域(T)のラベル付きデータをランダムに15個取り出し，残りを評価データとする．つまり利用できる訓練データはソース領域(S)のラベル付きデータとターゲット領域(T)からランダムに取り出した15個のラベル付きデータとなる．この訓練データを用いて手法Aにより分類器を作成し，先の評価データの語義識別の正解率(P_i,k)を測る．この実験を5回行い(P_i,1,P_i,2,,P_i,5)を得る．それらの平均(P_i)を「単語(w_i)の(S)から(T)への領域適応における手法Aの平均正解率」とする．上記の単語(w_i)を16種類の各対象単語(w_1,w_2,,w_16)に変えることで，16個の平均正解率(P_1,P_2,,P_16)が得られる．それらの平均(P)を「(S)から(T)への領域適応における手法Aの平均正解率」とする（参照）．上記の手法Aとしては，以下の6種類を試す．(1)ソース領域のラベル付きデータのみを用いる手法（ターゲット領域の15個のラベル付きデータの重みを0とする手法）(S-Only)，(2)ターゲット領域からランダムに取り出した15個のラベル付きデータのみを用いる手法（ソース領域のラベル付きデータの重みを0とする手法）(T-Only)，(3)ソース領域のラベル付きデータとターゲット領域の15個のラベル付きデータを用いる手法(S+T)，(4)Daum'eの手法(Daum'e)，(5)本稿で示した簡易手法により算出した確率密度比を用いた共変量シフトによる手法(Cov-Shift)，(6)素性空間拡張法から得られた訓練データに対して，本稿で示した簡易手法により算出した確率密度比を用いた共変量シフトによる手法（提案手法）の計6種類である．またすべての手法において学習アルゴリズムとしては最大エントロピー法を用いた．またその実行にはツールのClassiasを用いた．(S)から(T)への領域適応における各手法の平均正解率をに示す．Daum'eとCov-Shiftを比較するとDaum'eの方がわずかに高い正解率を示している．この点は考察で議論する．ただし提案手法はDaum'eよりも高い正解率であり，共変量シフトによる解法の効果が確認できる．</section>
  <section title="考察"/>
  <subsection title="負の転移の有無">WSDの領域適応では，対象単語毎に領域適応の問題が生じている．実験では領域の組み合わせで6通り，対象単語が16単語あるので，合計96(=616)通りの領域適応の問題を扱ったことになる．ここでは各領域適応の問題に対して負の転移が生じているかどうかを調べ，それぞれのケースに分けて，各手法の正解率を調べた．まず負の転移が生じているかどうかの判定には，先の実験でより得られた|T-Only|，|S-Only|及び|S+T|の正解率を利用する．もしも正解率で以下の関係が成立しているなら，負の転移が生じていないと考えられる．結果をに示す．チェックがつけられた箇所が負の転移が生じていない領域適応の問題である．96種類の領域適応の問題の中で44種類において負の転移が生じていない．次に負の転移が生じているかいないかのケースに分けて，各手法の平均正解率を調べた．結果をに示す．において領域適応に対処する3手法（Daum'e，Cov-Shift，提案手法）を見ると，提案手法は負の転移の有無に関わらずCov-Shiftよりも高い正解率であり，提案手法はCov-Shiftの改良になっていることがわかる．更に負の転移が生じていないケースではCov-ShiftはDaum'eよりも正解率が高く，このケースでは素性に重みをつけるよりも事例に重みをつける方が効果があることがわかる．ただし負の転移が生じるケースでは，提案手法はDaum'eよりも正解率が若干低い．つまり提案手法をDaum'eの手法の改良と見た場合，負の転移が生じるケースでは正解率の低下を抑え，その代わりに負の転移が生じないケースで正解率を高めることで，全体的な正解率を改善する手法と見なせる．また領域適応に対処しない3手法(S-Only,T-Only,S+T)も含めて比較すると，負の転移が生じるケースでは領域適応に対処する3手法（Daum'e，Cov-Shift，提案手法）の正解率はかなり悪い．つまりWSDの領域適応では負の転移を検出することで大きな改善が期待できる．共変量シフト下の学習では，負の転移が生じているケースに対しては，ソース領域のデータに0に近い重みを与えられればよいはずである．より正確な確率密度比の推定法を利用することで，このような重み付けが可能だと考える．この点は今後の課題である．</subsection>
  <subsection title="確率密度比の調整">確率密度比を精度良く推定することは困難な問題である．そのために求まった確率密度比を調整することも行われている．杉山は確率密度比(r)に(p)((0&lt;p&lt;1))乗した(r^p)を重みにすることを提案している．またYamadaはrelativedensityratioとして確率密度比を以下の形で求めることを提案してる．[P_T(x)P_T(x)+(1-)P_S(x)]ここでは(r^0.5)の重みと(=0.5)のrelativedensityratioを試した．結果をに示す．における提案手法とCov-Shiftはにおける提案手法とCov-Shiftと同じものである．(r^0.5)がの重み(r)を0.5乗したものであり，RDRが(=0.5)のrelativedensityratioである．をみると，(r^0.5)やrelativedensityratioの調整は一部有効な問題もあったが，全体として見ると，効果はあまりない．これも本来の確率密度値(P_S(x))や(P_T(x))の推定が簡易すぎるために生じていると考える．確率密度比を確率統計的により精緻に求めていくことは重要である．ただし確率密度比は事例の重み，つまり事例の重要度を意味している．事例の重要度という自然言語処理的な観点からWSDの領域適応に特化した重みの設定も可能である．</subsection>
  <subsection title=" SVM の利用">本稿では学習アルゴリズムとして最大エントロピー法を用いた．共変量シフトの解法として，重み付き対数尤度を最大化する形では，(P_T(c|x))をモデル化するアプローチに限られる．しかし共変量シフト下の学習では確率密度比を重みにして期待損失を最小化すれば良いので，損失関数ベースの学習手法が利用できる．例えばヒンジ損失関数に密度比で重みづけすることで共変量シフト下の学習にSVMを利用できる．ただしSVM自体の実装が容易ではないために簡単に試すことはできない．ここでは共変量シフト下の学習にSVMを用いるのではなく，素性空間拡張法により拡張されたデータに対して，SVMを利用してみる．実行にはツールのlibsvmcjlin/libsvm/を用いた．またそこで利用したカーネルは線形カーネルである．実験結果をに示す．提案手法が本稿での提案手法での平均正解率であり，D3-MEが素性空間拡張法と最大エントロピー法を利用した場合の平均正解率である．つまり提案手法とD3-MEは，での提案手法とDaum'eに対応する．そしてD3-SVMが素性空間拡張法とSVMを利用した場合の平均正解率である．提案手法はD3-SVMよりもわずかに高い正解率となっているが，その差は小さく識別能力については，提案手法とD3-SVMは同程度と言える．またD3-SVMはD3-MEよりも正解率が高い．つまり最大エントロピー法ではなく，SVMを利用する方が正解率が高くなると予想できる．このことから共変量シフト下の学習にSVMを利用すれば，改善が可能であると考えられる．これは今後の課題である．</subsection>
  <subsection title="教師なし手法への適用">共変量シフト下での学習では訓練データの中にターゲット領域のデータが含まれる必要はない．ターゲット領域の訓練データを含めなければ，教師なし領域適応手法となるはずである．この点を確認した実験を行った．実験結果をに示す．表のS-Onlyの列はソース領域の訓練データだけで学習した結果である．これはのS-Onlyに対応する．W-S-Onlyはソース領域の訓練データのみを使った共変量シフト下での学習手法である．また参考までに提案手法の結果も記している．確率密度比を用いるW-S-Onlyではソース領域のデータへの重みが小さくなりがちである．ここでの実験では重みが0.01未満の場合はそのデータを省いて学習させている．そのためにW-S-Onlyでは極端にラベル付きデータが減少するケースがあった．結果として精度が低くなってしまったと考えられる．また多くの単語で正解率の低下が起こっていた．この原因としては，重みのあるデータの欠如だと考える．例えば，語義(c_1)のデータ(x_1)の重みが(0.01)，語義(c_2)のデータ(x_2)の重みが(0.02)である場合，どちらの重みも「小さく」，その差はほぼ等しいと見なして(P(c_1)=P(c_2)=0.5)と考えるのが妥当であるが，「小さい」という点を考えないと(P(c_1)=1/3),(P(c_2)=2/3)となってしまう．「小さい」という点を考えるためには比較となるある程度「大きな」データが必要である．例えば，上記の設定の上で語義(c_1)のデータ(x_3)の重みが1などというデータが存在すれば，(P(c_1)=101/103),(P(c_2)=2/103)となり，これは妥当である．つまり重みが低いデータが多数を占めるような場合，信頼性のある推定が行えない．ある程度，重みのあるデータが必要だと思われる．このため共変量シフト下での学習を教師なしの枠組みに単純に利用することは難しい．教師なしの枠組みへの利用方法の検討は今後の課題である．</subsection>
  <section title="おわりに">本稿ではWSDの領域適応の問題が共変量シフトの問題と見なせることを示した．そして，共変量シフトの標準的な解法である確率密度比を重みにしたパラメータ学習により，WSDの領域適応の解決が図れることを示した．また素性空間拡張法により拡張されたデータに対して，共変量シフトの解法を行う手法を提案した．BCCWJコーパスの3つ領域OC（Yahoo!知恵袋），PB（書籍）及びPN（新聞）を選び，SemEval-2の日本語WSDタスクのデータを利用して，上記領域にある程度の頻度がある多義語16単語を対象に，WSDの領域適応の実験を行った．実験の結果，提案手法はDaum'eの手法と同等以上の正解率を出した．共変量シフトの解法では確率密度比の算出が鍵となるが，ここではNaiveBayesで利用されるモデルを利用した簡易な算出法を試みた．このような簡易な算出法であってもWSDの領域適応に共変量シフトの解法を利用する効果が高いことが示された．より正確な確率密度比の推定法を利用したり，最大エントロピー法に代えてSVMを利用するなどの工夫で更なる改善が可能である．また教師なし領域適応へも応用可能である．WSDの領域適応に共変量シフトの解法を利用することは有望であると考えられる．の作者である岡崎直観氏に，Classiasの事例の重み付け方法について教えていただきました．また本稿の査読者殿には有益なコメントいただきました．感謝いたします．document</section>
</root>
