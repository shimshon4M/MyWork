<?xml version="1.0" ?>
<root>
  <section title="Introduction">CasemarkersinasentencearerepresentedbypostpositionalparticlesinJapanese.Incorrectusageofparticlescausesseriouscommunicationerrorsbecausereaderscannotunderstandthecontentofsentencesthroughthecasemarkers.Forexample,itisunclearastowhatmustbedeletedinthefollowingsentence.[t]minipageIftheaccusativeparticleoisreplacedbyanominativeonega,itbecomesclearthatthewriterwantsthee-mailtobedeleted(``Whenthee-mailhasarrived,pleasedeleteit.'').SuchparticleerrorsfrequentlyoccurinsentenceswrittenbyJapaneselearners(c.f.,Section).AutomaticerrorcorrectionoftextswrittenbyJapaneselearnersisessentialineducationaswellasbusiness.Forexample,someJapanesecorporationsoutsourcethecomputersoftwaretoforeigncompanies,especiallyChinaandIndia,usingJapaneseorEnglishasthecommunicationlanguage.However,Japaneseisaforeignlanguageformostdevelopmentcompanies,andtextssuchase-mailscancontainmanyerrorsthatinhibiteffectivecommunication.Thus,automaticerrorcorrectionisregardedasakeysolution.ThispaperpresentsamethodthatcanautomaticallycorrectJapaneseparticleerrors.ThistaskcorrespondstoprepositionerrorcorrectioninEnglish.ForEnglisherrorcorrection,manystudiesemployclassifiers,whichselecttheappropriateprepositions/articlesbyrestrictingtheerrortypestoarticlesandfrequentprepositions.Onthecontrary,proposedmachine-translation-basederrorcorrection.Thisapproachcanhandleallerrortypesbyconvertinglearners'sentencesintothecorrectversion.Althoughthetargetofthispaperisparticleerror,weemployasimilarapproachbasedonsequenceconversionsinceitoffersexcellentscalability.However,themachine-translation-basedapproachrequiresasufficientnumberofparallelsentences,whichconsistoflearners'erroneoussentencesandtheirmanuallycorrectedones,tolearnthemodels.However,collectingalargenumberofsuchpairsisdifficultbecauseofhighcosts.Toavoidthisproblem,weproposethefollowingtwomethods.UsingLargeRawTextCorpora(CombinationofLanguageModelProbabilitiesandBinaryFeatures)BecauseonehalfoftheparallelsentencesarecorrectJapanese,theycanbeeasilyobtainedfromrawtextcorpora.Thus,weregardalargetextcorpusasasetofcorrectedsentencesandincorporateitintotheerrorcorrectormodels.Thetextcorpusisusedforcomputinglanguagemodelprobabilities.Theerrorcorrectormodelsarejointlyoptimizedwithbinaryfeaturesacquiredfromtheparallelsentencesbyusingdiscriminativelearning.Weexpectthattheerrorcorrectioncoveragewillbeimprovedbecausethedegreeofsentencecorrectnessismeasuredbylanguagemodelprobabilities,eveniftheerrorsrarelyappearintheparallelsentences.ExpansionoftheParallelSentencesUsingPseudo-ErrorSentences(andApplicationofDomainAdaptation)Sincecollectinglearners'erroneoussentencesisnoteasy,weautomaticallygeneratepseudo-errorsentences,whichmimiclearners'realerrors,byapplyingerrorpatternstocorrectsentencesrozovskaya-roth:2010:NAACLHLT.Anadditionaltrainingcorpusconsistsofthepairsofpseudo-errorsentencesandtheirsourcesentences.Notethatthepseudo-errorsentencesdonotcompletelyreflecttheerrordistributionoflearners'sentences.Therefore,wefurtherapplyadomainadaptationtechniquethatregardsthepseudo-errorsandrealerrorsasthesourceandtargetdomains,respectively.Weexpectstableimprovementiftheerrordistributionsarepartiallydifferent.Theremainderofthispaperisorganizedasfollows.InSection,weanalyzeerrorsinthesentenceswrittenbyJapaneselearners.Sectiondescribesourerrorcorrectionmethodanduseofrawtextcorpora.Sectiondescribestheexpansionofparallelsentencesusingpseudo-errorsentencesanddomainadaptation.InSection,weconductexperimentstoconfirmerrorcorrectionaccuracy.Sectionintroducesrelatedstudies,andSectionconcludesthisstudy.</section>
  <section title="Errors in Sentences Written by Japanese Learners">Inordertoanalyzeerrortypes,wefirstcollectederroneousexampleswrittenbyChinesenativespeakerswhoarelearningJapanese.Thirtysevensubjects,wholearntJapanesewhiletheyare/wereatengineeringuniversitiesinJapan,participatedintheexperiment.TheperiodsspentlivinginJapanrangedfromsixmonthstosixyears.Weprovided80EnglishsentencesobtainedfromLinuxmanualsand24figurestoeachsubject(104tasksintotal),andtheyre-wrotethesentencesinJapanese(Hereafter``learners'sentences'').Asaresult,2,770learners'sentenceswerecollected.EachsentencewasrevisedbyJapanesenativespeakers(calledthecorrectsentences).Tomaketherevisions,abareminimumoferrorcorrectionwasappliedtothepointatwhichthesentencesbecomegrammatically/pragmaticallycorrect,whileretainingthemeaning.Inotherwords,onlyfatalerrorswerecorrectedfromtheviewpointofJapanesegrammar.</section>
  <subsection title="Error Categorization and Distribution">Wecategorizedtheerrorsintothreemajortypes,grammatical,vocabulary,andsurfaceformerrors,andsetsub-categories,asshowninTable.Therewritersrevised2,171of2,770sentences.Theunchangedsentencesincluded559sentenceswithouterrorsand40sentencefragments.Thefollowinganalyseswereappliedtothe2,171revisedsentences.Thenumberofcorrectederrorswas4,916words/phrases(2.26persentence).Fromtheperspectiveofdistributionbycategories,mostweregrammaticalerrors(54%),followedbyerrors(28%)andsurfaceformerrors(16%).Therestwerecompounderrors.Fromtheperspectiveofsub-categories,themostfrequenterrorswerelocatedinparticlesorauxiliaryverbs(33%),followedbytransliteration(11%)andsynonyms(10%).</subsection>
  <subsection title="Analyses and Discussion">Withregardtothemostfrequenterrors,particleerrorsappearwidelyintextswrittenbynon-nativeJapanesespeakersregardlessoftheirfirstlanguage.ThisisbecauseparticlesarespecialgrammaticalunitsinJapanese,andhavevariousfunctions.Forexample,caseparticlesspecifythecaseofnounphrasessuchasnominative,accusative,andpossessive.Topicparticlesworkastopicmarkers,andcancombinewithcaseparticles.Thereareothertypessuchasconjunctiveandsentence-endparticles.Therefore,appropriateusageisdifficultformostnon-nativespeakers,andeffectiveteachingshouldincludecorrectionofparticleerrors.Sincefallibilitydependsontheparticle,wecalculatedthefrequencyofparticleerrors.Table~showsthetop10particleerrors.CorrectiontypeinTabledenotestheeditoperationneededtocorrecttheerrors:insertion(INS),deletion(DEL),orsubstitution(SUB).Fromtheviewpointofeditoperation,substitutionswerethemostfrequent(74%),particleomissions(INS)were17%,andexcessparticles(DEL)were9%.Thefollowingareexamplesofsubstitution,insertion,anddeletion.Rank1inTableimpliesthatthetopicparticlewashouldappearonlyonceinasentence.Ifitappearstwice,oneofthemshouldbereplacedbyacaseparticlega(mondai-*wa/gaaru-toki-wa...``ifyouhaveproblem...'').Rank3showsanexampleofinsertion.Lackofthepossessiveparticlenobetweenacardinalnumberandanounwasfrequent(futatsu*/-nofile``twofiles'').Rank8showsanexampleofdeletion.Manylearnersappendedthepossessiveparticlenoafterverbsoradjectives,whichmodifyanoun(chiisai*-no/e``smallpicture'').TheaboveanalysesconfirmedthatparticleerrorcorrectioniseffectiveforJapaneselearners.Theseerrorsarecorrectedbyapplyingtheeditoperationsofsubstitution,insertion,anddeletion.</subsection>
  <section title="Error Correction by Discriminative Sequence Conversion">Thissectiondescribeserrorcorrectionusingdiscriminativesequenceconversion.Ourerrorcorrectionmethodconvertslearners'wordsequencesintocorrectsequences(sentencesaresegmentedintowordsinadvancebyamorphologicalanalyzer).Themethodissimilartophrase-basedstatisticalmachinetranslation(PBSMT),buthasthefollowingthreedifferences:1)itadoptsconditionalrandomfields,2)itallowsinsertionanddeletion,and3)combinesn-grambinaryfeaturesandlanguagemodelprobabilities.Unliketheclassificationapproach(e.g.,suzuki-toutanova:2006:COLACL),theconversionapproachcancorrectmultipleerrorsofalltypesinasentence.</section>
  <subsection title="Basic Procedure">WeapplyamorphemeconversionapproachthatconvertstheresultsofaspeechrecognizerintowordsequencesforlanguageanalyzerprocessingImamura:MorphTrans2011.Itcorrectsparticleerrorsintheinputsentencesasfollows.First,allmodificationcandidatesareobtainedbyreferringtoaphrasetable.Thistable,calledtheconfusionsetrozovskaya-roth:2010:EMNLPintheerrorcorrectiontask,storespairsofincorrectandcorrectparticles(Table).Thecandidatesarepackedintoalatticestructure,calledthephraselattice(Figure).Todealwithunchangedwords,italsocopiestheinputwordsandinsertsthemintothephraselattice.Next,thebestphrasesequenceinthephraselatticeisidentifiedonthebasisoftheconditionalrandomfields(CRFs).TheViterbialgorithmisusedfordecodingbecauseerrorcorrectiondoesnotchangethewordorder.Althoughthephraselatticeincludesobviouslyungrammaticalsequences(e.g.,asequenceinwhichidenticalparticlesoareadjacent),wedonotprunethem,andsearchforthebestsequenceaccordingtothemodel.Whiletraining,wordalignmentiscarriedoutbydynamicprogrammingmatching.Fromthealignmentresults,thephrasetableisconstructedbyacquiringparticleerrors,andtheCRFmodelsaretrainedusingthealignmentresultsassuperviseddata.</subsection>
  <subsection title="Insertion/Deletion">Theerrorcorrectioninthispaperusesinsertionanddeletion,whereasphrase-basedSMTsgenerallytranslateasentenceusingonlysubstitution.Sinceaninsertioncanberegardedasreplacinganemptywordwithanactualwordanddeletionisthereplacementofanactualwordwithanemptyone,wetreattheseoperationsassubstitutionwithoutdistinctionwhilelearning/applyingtheCRFmodels.However,insertionisahighcostoperationbecauseitmayoccuratanylocationandcancausethelatticesizetoexplode.Toavoidthisproblem,wepermitoneonlywordinsertionandonlyimmediatelyafternouns.Thisrestrictionmakesafewerrorsimpossibletocorrect(e.g.,itbecomesimpossibletoinsertthetopicparticlewaimmediatelyafterthedativeparticleni).Notethatsubstitutioncanberepresentedasasequenceofinsertionanddeletion.Inthispaper,thesuperviseddatafortheCRFmodelsarecreatedwhileconsecutiveinsertionanddeletionarebundledassubstitution.Duringerrorcorrection,multiplecandidatesconsistingofthesamesurfaceformsexistinthephraselattice;oneisrepresentedbysubstitutionandtheotherisasequenceofinsertionandsubstitution.However,substitutionisselectedinalmostallcasesbythelearntmodels.</subsection>
  <subsection title="Features">Inthispaper,weusemappingfeaturesandlinkfeatures.Theformermeasurethecorrespondencebetweeninputandoutputwords(similartothetranslationmodelsofPBSMT),whilethelattermeasurethefluencyoftheoutputwordsequence(similartolanguagemodels).Figureillustratesthefeatures,andTableshowsthefeaturetemplates.Manynaturallanguageprocessing(NLP)tasksthatemploydiscriminativemodels(e.g.,namedentityrecognition)usenotonlythetargetedwordbutalsoitsneighboringwordsasfeatures.Weemploythesameapproach.Thefocusedphraseanditstwoneighboringwordsintheinputareregardedasthewindow.Themappingfeaturesaredefinedasthepairsoftheoutputphraseanduni-,bi-,andtri-gramsinthewindow.Allmappingfeaturesarebinary.Linkfeaturesaredetailedinthenextsection.</subsection>
  <subsection title="Using Raw Text Corpora and Incorporation into Link Features">Thelinkfeaturesareimportantfortheerrorcorrectiontaskbecausethesystemhastojudgeoutputaccuracy.Thecorrectsentencescanbeeasilyobtainedfromrawtextcorpora.Usingthesecorpora,wemixthetwotypesoffeatures,asfollows,andoptimizetheirweightsintheCRFframework.Weleverageacharacteristicthatdiscriminativemodelscanhandlefeaturesthatdependoneachother.N-grambinaryfeaturesN-gramsoftheoutputwords,from1to3,areusedasbinaryfeatures.Theseareobtainedfromthecorrectsideofatrainingcorpus(parallelsentences).Sincetheweightsofindividualfeaturesareoptimizedconsideringallfeatures(includingthemappingfeatures),fine-tuningcanbeachieved.Theaccuracybecomesalmostperfectonthetrainingcorpus.Inotherwords,wecancertainlycorrecterrorsinnewtextsifthesameerrorpatternsarepresentinthetrainingcorpus.LanguagemodelprobabilityThisisalogarithmicvalue(realvalue)ofthen-gramprobabilityoftheoutputwordsequence.Onefeatureweightisassigned.Then-gramlanguagemodelcanbeconstructedfromlargetextcorpora,i.e.,itdoesnotrequireparallelsentences.Thelanguagemodelprobabilitiesprovidegrammaticalcorrectnessscores,regardlessofwhethertheerrorsinthenewtextsappearedinthetrainingcorpusornot.Incorporatingbinaryandrealfeaturesyieldsaroughapproximationofgenerativemodelsinsemi-supervisedconditionalmodels.Itcanappropriatelycorrectnewsentenceswhilemaintaininghighaccuracyonthetrainingcorpus.</subsection>
  <section title="Expansion of Parallel Sentences Using Pseudo-error Sentences">TheerrorcorrectordescribedinSectionrequiresparallelsentences,whichcorrespondtobilingualsentencesinmachinetranslation.However,itisdifficulttocollectasufficientlylargesetofsuchsentences.Weresolvethisproblembyusingpseudo-errorsentencestoexpandthem.Inthissection,wedescribethegenerationofpseudo-errorsentencesandtheirapplicationusingdomainadaptation.</section>
  <subsection title="Pseudo-Error Generation">Aswedescribedbefore,correctsentences,whicharehalvesoftheparallelsentences,canbeeasilyacquiredfromrawtextcorpora.Ifwecangenerateerrorsthatmimiclearners'sentences,wecanobtainnewparallelsentences.Weutilizethemethodofrozovskaya-roth:2010:NAACLHLT.Concretely,whenparticlesappearinthecorrectsentence,theyarereplacedbyincorrectonesinaprobabilisticmannerbyapplyingthephrasetable(whichstorestheerrorpatterns)intheoppositedirection.Theerrorgenerationprobabilitiesarerelativefrequenciesonthetrainingcorpus(i.e.,therealerrorcorpus).Namely,whereP_error(f|e)denotestheerrorgenerationprobability,C(f,e)denotestheco-occurrencefrequencyofthecorrectparticleeanditserrorparticlefintherealerrorcorpus,andC(e)denotesthefrequencyofcorrectparticleeinthecorpus.Themodelsarelearntusingbothrealerrorcorpusandpseudo-errorcorpus.</subsection>
  <subsection title="Domain Adaptation by Feature Augmentation">Althoughtheerrorgenerationprobabilitiesarecomputedfromtherealerrorcorpus,theerrordistributionisnotexactlythesame.Tofitthepseudo-errorstotherealerrorsbetter,weapplyadomainadaptationtechnique.Namely,weregardthepseudo-errorcorpusasthesourcedomainandtherealerrorcorpusasthetargetdomain,andmodelsthatfitthetargetdomainarelearnt.Inthispaper,weusedaumeiii:2007:ACLMain'sfeatureaugmentationmethodfordomainadaptation.Thismethodlearnsthetargetdomainmodelsbyexpandingitsfeaturespace,andhasthesameeffect;thatis,themodelsforthesourcedomainareregardedaspriordistributionforthetargetdomain.Inaddition,iteliminatestheneedtochangethelearningalgorithm.Webrieflyreviewthefeatureaugmentation.Thefeaturespaceissegmentedintothreeparts:common,source,andtarget.Thefeaturesextractedfromthesourcedomaindata(D_s)aredeployedtothecommonandsourcespaces,andthosefromthetargetdomaindata(D_t)aredeployedtothecommonandtargetspaces.Namely,thefeaturespaceistripled(Figure).Parameterestimationiscarriedoutintheusualwayontheabovefeaturespace.Consequently,theweightsofthecommonfeaturesareemphasizedifthefeaturesareconsistentbetweenthesourceandtarget.Withregardtodomain-dependentfeatures(i.e.,inconsistentbetweenthesourceandtarget),theweightsinthesourceorthetargetspaceareemphasized.Withrespecttothefeaturesthatonlyappearinthesourceortargetdomain,theweightsinthecommonanddomain-dependentspacesareemphasized.Figureshowsanexampleoffeatureaugmentation.Here,wesimplifythetasktojustaskingwhetherthecaseparticlegashouldbereplacedwithoorleftunchanged.Weassumethatthefollowingthreefeaturesareacquiredfromthesourceandtargetdomaindata(weassumetemplateNo.11inTable).``Kinougariyou''appearsinbothsourceandtargetdomains,andgaisreplacedwitho.``Datagahenkou''appearsinbothsourceandtargetdomains.However,itisunchangedinthesourcedomainandisreplacedwithointhetargetdomaindata.``Kansuugajikkou''onlyappearsinthesourcedomaindata.Whenweestimateparametersontheabovefeaturespace,theweightsofA)inthecommonspaceareemphasizedbecauseitisconsistentbetweenthedomains.Onthecontrary,theweightsofB)inthecommonspacebecomezero,andthoseinthesourceortargetspaceareemphasized,dependingonthecorrection,becausethedomainsareinconsistent.Furthermore,theweightsofC)inthecommonandsourcespacesareemphasized.Errorcorrectionusesonlyfeaturesinthecommonandtargetspaces.Theerrordistributionapproachestherealerrordistributionbecausetheweightsofthefeaturesareoptimizedtothetargetdomain.Inaddition,itbecomesrobustagainstnewsentencesbecausethecommonfeaturesacquiredfromthesourcedomaincanbeusedevenwhentheydonotappearinthetargetdomain.FromtheexamplesinFigure,thefeaturesofC),whichonlyappearedinthesourcedomaindata,canbeusedtoenhanceerrorcorrection.</subsection>
  <section title="Experiments"/>
  <subsection title="Experimental Settings">TargetParticles:Thetargetparticlesinthispaperarewordswhosepart-of-speech(POS)tagisParticle,accordingtotheipadic-2.7.0morphemedictionary.Thedictionaryincludesnotonlycaseparticlesbutalsotopic(focus),adverbial,conjunctive,sentence-ending,andparallelparticles(236particlesintotal).Thirty-eightparticlesinthedictionary,whichappearinlearners'corpusandwereincorrectlyusedbythelearnersatleastonce,aretargetedintheexperiments.'sstudy,whichhandled10caseparticlesandtopicparticlewa,weexcludedativeparticleeandincludecombinedparticleni-yoriinsteadofyorialone.Inaddition,wehandletopicparticlewaasanindividualword,whereassuzuki-toutanova:2006:COLACL'sstudyregardedasequenceofcaseandtopicparticlesasoneword.Learners'Corpus(RealErrorCorpus):Thelearners'corpususedintheexperimentsconsistsof2,770parallelsentences(104tasks)collectedinSection.Fromthesesentences,onlyparticleerrorswereretained;theothererrorswerecorrectedbycopyingthecorrespondingpartsofthecorrectsentence.Therefore,theparallelsentencesfortheexperimentscontainonlyparticleerrors.IfjustthePOStagsofwordsweredifferent(i.e.,surfaceformswereidentical)betweenthepairs,wedidnotregardthemaserrors,andPOStagsofthecorrectsentencewerecopiedtothelearner'ssentence.Thenumberofincorrectparticleswas1,087(8.0%)of13,534.Notethatmostparticlesdidnotneedtoberevised.Thenumberofpairtypesofincorrectparticlesandtheircorrectoneswas132(SUB:95,INS:14,DEL:23).AllsentencesintheexperimentsweresegmentedandtaggedbytheJapanesemorphologicalanalyzerMeCab,usingtheipadic-2.7.0morphemedictionary.ThewordinformationconsistedofthesurfaceformanditsPOStag.LanguageModel:ThiswasconstructedfromJapaneseWikipediaarticlesoncomputersandJapaneseLinuxmanuals,527,151sentencesintotal.SRILMwasusedtotrainatrigrammodel.Duringmodelconstruction,themodifiedKneser-Neydiscountingandinterpolationwereusedforbackoffsmoothing,andunknownunigramswereretainedaspseudo-word&lt;unk&gt;.Pseudo-errorCorpus:Thepseudo-errorsweregeneratedusing10,000sentencesrandomlyselectedfromthecorpusforthelanguagemodel.Wechangedthescalingfactoroferrorgenerationprobabilitiesfrom0.0(i.e.,noerrors)to2.0,whiletherelativefrequencyintherealerrorcorpuswastakenas1.0.EvaluationMetrics:Five-foldcross-validationontherealerrorcorpuswasused.Weusedtwometricsasfollows.Precisionandrecallrates,andF-measuresoferrorcorrectionbythesystems.Wecomparedonlysurfaceformsofthewordsforscorecomputation.Mostparticlesdidnotneedtoberevisedinthistask.Thesystemmayexcessivelyreviseparticlesthatdonotneedtobecorrected(over-correction).Therefore,weemployanothermetriccalledrelativeimprovement,whichrepresentsthedifferencebetweenthenumberofaccuratelycorrectederrorparticlesandthatofover-correctedparticles(i.e.,thosethatdidnotneedtobecorrected).Thisisapracticalmetricbecauseitdenotesthenumberofparticlesthathumanrewritersdonotneedtoreviseaftersystemcorrection.Therelativeimprovementbecomeszeroifthesystemdoesnotchangeanyparticle.</subsection>
  <subsection title="Experiment 1: Using Raw Text Corpora">First,weevaluatetheeffectsoflanguagemodelprobabilityusingrawtextcorporainerrorcorrection.Thefollowingthreemethodswerecompared.ProposedMethod:Incorporatingthelanguagemodelprobabilityasalinkfeaturewithn-grambinaryfeatures.N-gramBinaryFeatures:Onlyn-grambinaryfeaturesareusedaslinkfeatures(i.e.,thelanguagemodelprobabilityisnotused).LanguageModelProbability:Onlythelanguagemodelprobabilityisusedasthelinkfeature(i.e.,n-grambinaryfeaturesarenotused).Tableshowstheresults.Inthetable,denotesasignificantdifferencebetweentheproposedmethodandthen-grambinaryfeaturemethod,anddenotesasignificantdifferencebetweentheproposedmethodandthelanguagemodelprobabilitymethod(p&lt;0.05).Focusingonprecisionrates,theproposedandn-grambinaryfeaturemethodsyieldedthesameaccuracies,whilethelanguagemodelprobabilitymethodyieldedloweraccuracythantheothermethods.Focusingonrecallrates,theproposedmethodhadsignificantlybetteraccuracythantheothertwomethods(from9.9%and11.2%to18.9%).ThisindicatedthattheproposedmethodhadthehighestF-measure.Comparingwiththen-grambinaryfeaturesandthelanguagemodelprobabilitymethods,therecallrateofthelanguagemodelprobabilitymethodwasslightlyhigher.Consequently,F-measurescores,indecreasingorder,weretheproposedmethod,thelanguagemodelprobabilitymethod,andthen-grambinaryfeaturemethod.However,focusingonrelativeimprovements,theimprovementofthelanguagemodelprobabilitymethodwasdegraded(i.e.,over-correctionoccurred)eventhoughtherecallratewashigherthanthatofthen-grambinaryfeaturemethod.Thisisbecauseofthecharacteristicofthistask;about92%oftheparticlesdidnotneedmodification.Improvingtherecallratesometimescausesover-correction.Theproposedmethodhadbetterrelativeimprovementthantheothertwomethods.However,thedifferencebetweentheproposedandn-grambinaryfeaturemethodswasnotsignificant.Wesupposethatthen-grambinaryfeaturesareespeciallyeffectiveincorrectingcertainerrors,andhaveanadvantagefromtheperspectiveofrelativeimprovement.Theproposedmethodimprovedtherecallratewhilemaintainingtheprecisionrate,andthusincreasedrelativeimprovement.Weconcludethatcombiningthelanguagemodelprobabilitywiththen-grambinaryfeaturesiseffectiveinimprovingerrorcorrectionaccuracy.</subsection>
  <subsection title="Experiment 2: Expansion of Parallel Sentences UsingPseudo-error Sentences">Next,weevaluatetheeffectofintroducingpseudo-errorsentences.Theexperimentswerecarriedoutbychangingtheusageofpseudo-errorsentences;thelinkfeaturesarethoseproposedinSection.Figureplotstheprecision/recallcurvesforthefollowingfourcombinationsoftrainingcorporaandmethod.NotethateachprecisionrateinFigurewas,atthespecifiedrecallrate,achievedbyselectingcorrectedparticlesfromhighscorecorrections.TRG:Themodelsweretrainedusingonlytherealerrorcorpus(baseline).SRC:Themodelsweretrainedusingonlythepseudo-errorcorpus.ALL:Themodelsweretrainedusingtherealerrorandpseudo-errorcorporabysimplyaddingthem.AUG:Theproposedmethod.Featureaugmentationwasrealizedbyregardingthepseudo-errorsasthesourcedomainandtherealerrorsasthetargetdomain.TheSRCcase,whichusesonlythepseudo-errorsentences,didnotmatchtheprecisionofTRG.TheALLcasematchedtheprecisionofTRGathighrecallrates.AUG,theproposedmethod,achievedhigherprecisionthanTRGathighrecallrates.Attherecallrateof18%,theprecisionrateofAUGwas55.4%,whilethatofTRGwas50.5%(thesignificancewasp=0.16).NotethattheprecisionrateofSRCwas35.6%attherecallrateof18%,whichisbetterthanrandomcorrection.Figureshowstherelativeimprovementofeachmethodaccordingtoerrorgenerationprobabilities.Inthisexperiment,ALLachievedhigherimprovementthanTRGatscalingfactorsoferrorgenerationprobabilitiesrangingfrom0.0(noerrors)to0.6.Althoughtheimprovementswerehigh,wehavetocontroltheerrorgenerationprobabilitybecausetheimprovementsintheSRCcasefellasthescalingfactorwasraised.Ontheotherhand,AUGachievedstableimprovementregardlessoftheerrorgenerationprobability.Therelativeimprovementatascalingfactorof1.0wassignificantlyimprovedbecausetheimprovementofTRGwas+28andthatofAUGwas+59(p&lt;0.05).Thus,wecanconcludethatdomainadaptationtopseudo-errorsentencesisthepreferredapproach.</subsection>
  <subsection title="Examples of Error Correction">Inthesecondexperiment(c.f.,Section),theprecisionandrecallratesoftheproposedmethod(AUG)were54.8%(210/383)and19.3%(210/1,087),respectively,whenthescalingfactoroferrorgenerationprobabilitieswas1.0.Theprecisionrateof55%wasnothighenoughbecause45%ofsystemmodificationsshouldbecorrectedagain.However,someusageswereclearlyincorrectfromthegrammaticalperspective,whileotherswereacceptablebecausealternativeparticlescanbeaccepteddependingonthecontext.Accordingly,wecarriedoutasubjectiveevaluation.Onehundredandseventy-threeparticles,whichthesystemmodifiedwithoutmatchingtheanswer,wereevaluatedbyoneevaluator.Notethat151ofthemwereover-corrections,wheretheanswerswerenotmodifiedfromlearners'sentences.Weaskedtheevaluatorwhetherthesystemmodificationwouldbeacceptable,i.e.,grammaticallyandsemanticallyidenticaltotheanswer.Asaresult,103ofthe173particleswereacceptable.Therefore,theprecisionrateincludingacceptablemodificationbecame81.7%((210+103)/383).Tableshowsexamplesofsystemcorrections.Successfulcorrectionswererealizedbysubstitution,insertion,anddeletion.Theacceptableexamplesincludedsubstitutionfromtopicparticlewatocaseparticlega(No.4),anddividingacompoundnounbyinsertingthecorrectparticle(No.5).Unacceptableexamplesincludedthefollowing:idiomaticexpressionwasover-corrected(No.7),thecaseparticleforthepassivevoicewasreplacedwiththatfortheactivevoice(No.8),correctionisimpossiblewithoutknowledgeofLinux'sfreecommand(No.10).No.9issimilartoNo.4.However,itwasjudgedunacceptablebecause``watashi-tachi(we)''and``anata(you)''areconcord.Therefore,thesameparticlesshouldbeused.Featuresusedinthispaperonlyreflectlocalcontextofparticles.Uncorrectableparticleerrorswillpersistunlessglobalcontextfeaturesareintroduced.</subsection>
  <section title="Related Studies">ParticleerrorcorrectionforJapaneselearnershasbeenresearchedforalongtime.Recently,suzuki-toutanova:2006:COLACLusedmaximumentropy(ME)classifierstorestoreparticles(mainlycasemarkers)omittedfromsentences.Theyinsertedtheappropriateparticleatthegivenpositionoftheparsedandtaggedsentences.Ohki:ParticleError2011jdetectedincorrectusageofparticlesinalearner'ssentence.Theytaggedandparsedtheerroneoussentenceanddetectedtheincorrectusage(includinglackofparticles)bythesupportvectormachines(SVMs),whichusedthefeaturesofneighboringwordsanddependencystructure.Onlydetectionwascarriedout.InEnglishpreposition/articlecorrection,HAN10.821correctedprepositionerrorsusingMEclassifiers,wherethefeatureswereneighboringwordsandheadwordsacquiredbyparsers.Similarly,gamon:2010:NAACLHLTproposedamethodtodetectandcorrectprepositionandarticleerrors.ThedetectionwasbasedonMEclassifiers,andthecorrectionwasbasedondecisiontrees.rozovskaya-roth:2010:EMNLPcorrectedprepositionerrorsusingclassifiersbasedonaveragedperceptronmodels.Thesestudiescorrectederrorsbyclassifiersbyrestrictingerrortypestothoseofparticles,prepositions,andarticles.However,mizumoto-EtAl:2011:IJCNLP-2011proposedanerrorcorrectionmethodthatdidnotrestricttheerrortype.Theycollectednumerouspairsoflearners'sentencesandtheirnativecorrectionfromasocialnetworkservice,andcorrectederrorsbyusingaphrase-basedSMT.Notethattheydidnotsegmentlearners'sentencesintowordsbecausetheyincludederrors.Theerroneoussentencesweresegmentedintocharacters.Comparedtoourstudy,thesequenceconversionusedinthispaperissimilartothatusedinphrase-basedSMT.Therefore,ourmethodcanhandleallerrortypes,i.e.,itdoesnotneedtorestricterrorstoparticles.However,ourmethodassumesthattheinputsentencesaresegmentedintowordsinadvance.Theaccuraciesoftaggingandparsingmightdecreasewhenerroneoussentencesareanalyzed.Weneedtoinvestigatewordsegmentationmethodsforerroneoussentences.Fromtheperspectiveofcorrectsentencemodels,suzuki-toutanova:2006:COLACL,Ohki:ParticleError2011j,HAN10.821,rozovskaya-roth:2010:EMNLPmodeledcorrectsentencesusingn-grambinaryfeatures.gamon:2010:NAACLHLT,mizumoto-EtAl:2011:IJCNLP-2011modeledthemaslanguagemodelprobabilities.Inthispaper,wecombinedbothfeaturesandoptimizedthemtogetherwithmappingfeatures.Asaconsequence,therecallrateswereimproved.Fromtheperspectiveoflearners'sentenceusage,allstudiesreportedthattheerrorcorrectionaccuraciesimprovedbyincorporatingmodelsthatrepresenttrendsinlearners'errorswithmodelsofnativespeakers'texts.Themethodinthispaperencompassestheabovefindingsbyusingmappingfeaturesthatrepresentlearners'errortendencies.rozovskaya-roth:2010:NAACLHLThaveproposedtheuseofpseudo-errorsentences,whichmimiclearners'sentences.Theyreportedthaterrorcorrectionaccuracywasimprovedbyaddingpseudo-errorsentences,whichhavethesamedistributionaslearners'sentences.However,thebesterrorgenerationmethoddependedonthedata(concretely,thelearner'snativelanguage),anderrorgenerationshouldbemanuallycontrolled.Inthispaper,weadjustthedifferencebetweenpseudo-errorsandrealerrorsusingthedomainadaptationtechnique;hence,stableimprovementisachieved.dahlmeier-ng:2012:EMNLP-CoNLLproposedadecoderthatcancorrectmultipleerrorsinasentence.Thedecoderhandlesnotonlypreposition/articleerrorsbutalsotypos,punctuationerrors,andnumberagreementerrors.Thecorrectionhypothesesaregeneratedusingclassifiersandrules,whichdependontheerrortype,andaninputsentenceisrewrittenbyhillclimbing.Thedecodermaintainsmultiplehypotheseswhiledecodinginordertoreducetheriskoffallingintolocaloptima.OurmethodmaintainsallhypothesesinthephraselatticeandusestheViterbialgorithmtosearchforthebestcombination.Theresultistheoptimalsolutionfromtheviewpointofthemodels.Wedescribedhowmostparticlesdonotneedtobecorrected,andthatimprovingtherecallratedoesnotdirectlyaffecttherelativeimprovementinthistask.Thisphenomenon,calledtheimbalanceddataproblem,isamajorproblemwhenapplyingmachinelearningtechniquestopracticaltasks(c.f.,thesurveypaperHe:Imbalanced2009).Manysolutionshavebeenproposedtoaddressthisproblem.Forexample,samplingmethodsaimtobalancedatabydecreasingmajoritydataorincreasingminoritydata.TheminimumBayesriskmethodslearnmodelswithdifferentcosts,dependingonthemajorityandminorityclassificationerrors(inthistask,correctionerrors).Weneedtoinvestigatewhatmethodscanbeappliedtoourtask.Notethatthepseudo-errorsentencesproposedinthispaperaredifferentfromtheover-samplingmethodbecauseourpurposeistoincreasethetrainingdatawithoutchangingerrordistribution.</section>
  <section title="Conclusion">Inthispaper,weproposeamethodtocorrectparticleerrorsinthesentencesofJapaneselearners.Intheerrorcorrectiontask,itisdifficulttocollectsufficientnumberofpairsoflearners'andcorrectsentences.Toavoidthisproblem,wefirstcombinethebinaryfeaturesacquiredfromparallelsentences(smallscale)andthelanguagemodelprobabilityderivedfromlarge,rawtextcorpora.Byoptimizingtheabovefeaturesviadiscriminativelearning,weimprovetherecallrateoferrorcorrection.Inaddition,wegeneratepseudo-errorsentences,whichmimiclearners'sentencesandaddthemtoparallelsentences.Byincorporatingdomainadaptation,stableimprovementisachieved.Ourdiscriminativesequenceconversioncanhandlenotonlyparticleerrorsbutalsoallothererrortypes.Inthefuture,weaimtoapplythismethodtoasmanyothererrortypesaspossible.partofthisstudywaspresentedatthe50thAnnualMeetingoftheAssociationforComputationalLinguistics.TheauthorsaregratefultoDr.~TomokoIzumi,whoprovidedadviceontheEnglishtranslationofJapanesegrammar.Theauthorsappreciatehelpfulcommentsgivenbyanonymousreviewers.document</section>
</root>
