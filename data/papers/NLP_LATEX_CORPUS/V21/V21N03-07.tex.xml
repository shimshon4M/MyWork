<?xml version="1.0" ?>
<root>
  <jtitle>外界照応および著者・読者表現を考慮した日本語ゼロ照応解析</jtitle>
  <jauthor>萩行正嗣河原大輔黒橋禎夫</jauthor>
  <jabstract>日本語では用言の項が省略されるゼロ照応と呼ばれる現象が頻出する．ゼロ照応は照応先が文章中に明示的に出現する文章内ゼロ照応と，明示的に出現しない外界ゼロ照応に分類でき，従来のゼロ照応解析は主に前者を対象としてきた．近年，Webが社会基盤となり，Web上でのテキストによる情報伝達がますます重要性をましている．そこでは，情報の送り手・受け手である著者・読者が重要な役割をはたすため，Webテキストの言語処理においても著者・読者を正確にとらえることが必要となる．しかし，文脈中で明確な表現（人称代名詞など）で言及されていない著者・読者は，従来の文章内ゼロ照応中心のゼロ照応解析では多くの場合対象外であった．このような背景から，本論文では，外界ゼロ照応および文章の著者・読者を扱うゼロ照応解析モデルを提案する．提案手法では外界ゼロ照応を扱うために，ゼロ代名詞の照応先の候補に外界ゼロ照応に対応する仮想的な談話要素を加える．また，語彙統語パターンを利用することで，文章中で著者や読者に言及している表現を自動的に識別する．実験により，我々の提案手法が外界ゼロ照応解析だけでなく，文章内ゼロ照応解析に対しても有効であることを示す．</jabstract>
  <jkeywords>ゼロ照応解析，外界ゼロ照応，著者・読者</jkeywords>
  <section title="はじめに">ゼロ照応解析は近年，述語項構造解析の一部として盛んに研究されている．ゼロ照応とは用言の項が省略される現象であり，省略された項（ゼロ代名詞）が他の表現を照応していると解釈できることからゼロ照応と呼ばれている．.パスタが好きで、毎日(ガ)(ヲ)食べています。例えば，例の「食べています」では，ガ格とヲ格の項が省略されている．ここで，省略されたヲ格の項は前方で言及されている「パスタ」を照応しており，省略されたガ格の項は文章中では明確に言及されていないこの文章の著者を照応している．日本語では曖昧性がない場合には積極的に省略が行われる傾向にあるため，ゼロ照応が文章中で頻繁に発生する．例の「パスタ」の省略のようにゼロ代名詞の照応先が文章中で言及されているゼロ照応は文章内ゼロ照応と呼ばれ，従来はこの文章内ゼロ照応が主な研究対象とされてきた．一方，例の著者の省略のようにゼロ代名詞の照応先が文章中で言及されていないゼロ照応は外界ゼロ照応と呼ばれる．外界ゼロ照応で照応されるのは例のような文章の著者や読者，例のような不特定の人や物などがある．.内湯も窓一面がガラス張りで眺望がよく、快適な湯浴みを([不特定:人]ガ)楽しめる。従来，日本語ゼロ照応解析の研究は，ゼロ照応関係を付与した新聞記事コーパスを主な対象として行われてきた．新聞記事は著者から読者に事件の内容などを伝えることが目的であり，社説や投書の除いては著者や読者が談話構造中に登場することはほとんどない．一方，近年ではWebを通じた情報伝達が盛んに行われており，Webテキストの言語処理が重要となってきている．Webテキストでは，著者自身のことを述べたり，読者に対して何らかの働きかけをすることも多く，著者・読者が談話構造中に登場することが多い．例えば，Blogや企業の宣伝ページでは著者自身の出来事や企業自身の活動内容を述べることが多く，通販ページなどでは読者に対して商品を買ってくれるような働きかけをする．このため，著者・読者に関するゼロ照応も必然的に多くなり，その中には外界ゼロ照応も多く含まれる．のWebコーパスではゼロ照応関係の54%が外界ゼロ照応である．このため，Webテキストに対するゼロ照応解析では，特に外界ゼロ照応を扱うことが重要となる．本研究では，ゼロ照応を扱うためにゼロ代名詞の照応先候補として[著者]や[読者]などの文章中に出現しない談話要素を設定することで，外界ゼロ照応を明示的に扱う．用言のある格が直接係り受け関係にある項を持たない場合，その格の項は表の3種類に分類される．1つ目は「(a)文章内ゼロ照応」であり，項としてゼロ代名詞をとり，その照応先は文章中の表現である．2つ目は「(b)外界ゼロ照応」であり，項としてゼロ代名詞をとり，その照応先に対応する表現が文章中にないものである．3つ目は「(c)ゼロ照応なし」であり，項はゼロ代名詞をとらない，すなわちその用言が本質的にその項を必要としない場合である．外界ゼロ照応を扱うことにより，照応先が文章内にない場合でも，用言のある格がゼロ代名詞を項に持つという現象を扱うことができる．これにより，格フレームなどの用言が項を取る格の知識とゼロ代名詞の出現が一致するようになり，機械学習によるゼロ代名詞検出の精度向上を期待することができる．用言が項としてゼロ代名詞を持つ場合，そのゼロ代名詞の照応先の同定を行う．従来研究ではその手掛かりとして，用言の選択選好や文脈的な情報が広く用いられてきた．本研究では，それらに加えて文章の著者・読者の情報を照応先同定の手掛かりとして用いる．先に述べたように，従来研究で対象とされてきた新聞記事コーパスでは，著者や読者は談話中にほとんど出現しない．そのため著者や読者の情報が文脈的な手掛かりとして用いられることはなかった．しかし，著者や読者は省略されやすいためゼロ代名詞の照応先になりやすい，敬語やモダリティなど著者や読者の省略を推定するための手掛かりが豊富に存在する，などの特徴を持つため，談話中の著者や読者を明示的に扱うことは照応先同定で重要である．また，著者や読者は前述のような外界ゼロ照応の照応先だけでなく，文章内に言及されることも多い．.私_著者はもともとアウトドア派ではなかったので，東京にいた頃もキャンプに行ったことはありませんでした。.あなた_読者は今ある情報か資料を送って，アドバイザーからの質問に答えるだけ。例では，文章中に言及されている「私」がこの文章の著者であり，例では「あなた」が読者である．本研究ではこのような文章中で言及される著者や読者を著者表現，読者表現と呼び，これらを明示的に扱うことでゼロ照応解析精度を向上させる．著者や読者は人称代名詞だけでなく固有表現や役職など様々な表現で言及される．例えば，下記の例では著者自身の名前である「梅辻」によって著者が言及されており，例では著者の立場を表す「管理人」によって言及されている．また，例では著者から見た読者の立場である「お客様」という表現によって読者が言及されている．本研究では人称代名詞に限らず，著者・読者を指す表現を著者・読者表現として扱うこととする．.こんにちは、企画チームの梅辻_著者です。.このブログは、管理人_著者の気分によって書く内容は変わります。.いくつかの質問をお答えいただくだけで、お客様_読者のご要望に近いノートパソコンをお選びいただけます。著者・読者表現は様々な表現で言及されるため，表層的な表記のみから，どの表現が著者・読者表現であるかを判断することは困難である．そこで，本研究では談話要素とその周辺文脈の語彙統語パターンを素性としたランキング学習により，文章中の著者・読者表現の同定を行う．文章中に出現する著者・読者表現が照応先となることを推定する際には通常の文章中の表現に利用する手掛かりと著者・読者特有の手掛かりの両方が利用できる．.僕は京都に(僕ガ)行こうと思っています。皆さんはどこに行きたいか(皆さんガ)(僕ニ)教えてください。の1文目では「僕」が文頭で助詞「は」を伴ない，「行こう」を越えて「思っています」に係っていることから「行こう」のガ格の項であると推測される．これは文章中の表現のみが持つゼロ照応解析での手掛かりと言える．一方，2文目の「教えてください」では，依頼表現であることからガ格の項が読者表現である「皆さん」であり，ニ格の項が著者表現である「僕」であると推測できる．このような依頼や敬語，モダリティに関する手掛かりは著者・読者特有の手掛かりと言える．また，著者・読者特有の手掛かりは外界ゼロ照応における著者・読者においても同様に利用できる．そこで，本研究では，ゼロ照応解析において著者・読者表現は文章内ゼロ照応および外界ゼロ照応両方の特徴を持つものとして扱う．本論文では，文章中の著者・読者表現および外界ゼロ照応を統合的に扱うゼロ照応解析モデルを提案し，自動推定した著者・読者表現を利用することでゼロ照応解析の精度が向上することを示す．節で関連研究について説明し，節で本研究で利用する機械学習手法であるランキング学習について説明する．節ではベースラインとなるモデルについて説明し，節で実験で利用するコーパスについて述べる．その後，節で著者・読者表現の自動推定について説明し，節で著者・読者表現と外界照応を考慮したゼロ照応解析モデルを提案する．節で実験結果を示し，節でまとめと今後の課題とする</section>
  <section title="関連研究">日本語でのゼロ照応解析は文章内ゼロ照応を中心に行われてきた．ゼロ照応解析の研究では，ゼロ代名詞は既知のものとして照応先の同定のみを行っているものがある．はゼロ代名詞と照応先候補の統語的位置関係を素性として利用することでゼロ照応解析を行った．この研究では，外界照応を，それに対応するゼロ代名詞に照応性がないと判断する形で扱っている．この研究では，表における(a)文章内ゼロ照応と(b)外界ゼロ照応を区別して扱っているが，(c)ゼロ照応なしについては扱っていないといえる．は，ランキング学習を利用することで，ゼロ代名詞の照応先同定を行っている．この研究で扱うゼロ代名詞は文章内に照応先があるものに限定しており，表における(a)文章内ゼロ照応の場合のみを扱っているといえる．ゼロ照応解析は述語項構造解析の一部として解かれることも多い．述語項構造解析を格ごとに独立して扱っている研究としてはがある．は言語モデルの情報などを素性とした最大エントロピーモデルによるゼロ照応解析を含めた述語項構造解析モデルを提案している．このモデルでは各格の照応先の候補として，NULLという特別な照応先を仮定しており，解析器がこのNULLを選択した場合には，「項が存在しない」または「外界ゼロ照応」としており，これらを同一に扱っている．は述語と項の共起情報などを素性としたトーナメントモデルにより述語項構造解析の一部としてゼロ照応解析を行っている．この研究でも外界ゼロ照応と項が存在しないことを区別して扱っておらず，また解析対象はガ格のみとしている．用言ごとに全ての格に対して統合的に述語項構造解析を行う研究としてはがある．はWebから自動的に構築された格フレームを利用し，述語項構造解析の一部としてゼロ照応解析を行う確率的モデルを提案した．は格フレームから得られた情報や照応先の出現位置などを素性として対数線形モデルを学習することで，識別モデルによるゼロ照応解析を行った．これらの研究では外界ゼロ照応は扱っておらず，外界ゼロ照応の場合にはゼロ代名詞自体が出現しないものとして扱っている．これらの研究では表における(b)外界ゼロ照応と(c)ゼロ照応なしを区別せず扱っているといえる．外界ゼロ照応を扱った研究としてはがある．では対話文に対するゼロ代名詞の照応先の決定木による自動分類を行っている．この研究では，ゼロ代名詞は既知として与えられており，その照応先を5種類に分類された外界照応，および文章内照応（具体的な照応先の推定までは行わない）の計6種類から選択している．また，話題は旅行対話に限定されている．この研究では，機能語および用言の語彙情報がゼロ照応における素性として有効であるとしている．機能語，特に待遇表現は著者・読者に関する外界ゼロ照応解析で有効であると考えられ，本研究でも機能語の情報を素性として利用する．一方，用言の語彙情報は文章内ゼロ照応において有効であるとしているが，これは話題を限定しているためであると考えられ，本研究の対象である多様な話題を含むコーパスに対しては有効に働かないと考えられる．本研究では，格フレームにおける頻度情報などとして用言の情報を汎化することで，用言の情報を扱うこととする．また，この研究ではゼロ代名詞を既知としているため，ゼロ代名詞検出において外界ゼロ照応を扱うことの影響については議論されていない．では新聞記事に対する述語項構造解析の一部として外界ゼロ照応も含めたゼロ照応解析を扱っている．新聞記事コーパスでは外界ゼロ照応自体の出現頻度が非常に低いと報告しており，外界ゼロ照応の精度（F値）はガ格で0.31，ヲ格で0.75，ニ格で0.55と非常に低いものとなっている．また，これらの研究では文章中に出現する著者・読者（本論文における著者・読者表現）と外界の著者・読者との関係については扱っていない．日本語以外では，中国語，ポルトガル語，スペイン語などでゼロ照応解析の研究が行われている．中国語においてはゼロ照応解析は独立したタスクとして取り組まれることが多い．ではゼロ代名詞検出，照応性判定，照応先同定の3つのサブタスクにおいて構文木を利用したツリーカーネルによる手法が提案されている．ポルトガル語，スペイン語では述語項構造解析の一部ではなく，照応解析の一部としてゼロ照応解析に取り組まれることが多い．これらの言語では主格にあたる語のみが省略されるが，照応解析の前処理として省略された主格を検出し，照応先が文章内にあるかを分類する研究が行われている．英語においてはゼロ照応解析に近いタスクとして意味役割付与の研究が行われている．では頻度の高い10種類の動作性名詞に対して，直接係り受けにないものも項として扱い意味役割付与を行ったデータを作成している．また，共起頻度の情報などを利用して自動的に意味役割付与を行っている．では意味役割付与タスクの一部として省略された項を扱っている．また，省略された項については，照応先が特定されるDefiniteNullInstanceと照応先が不特定なIndefineteNullInstanceを区別して扱っている</section>
  <section title="ランキング学習">本研究では，ゼロ照応解析および著者・読者表現推定において，ランキング学習と呼ばれる手法を利用する．ランキング学習は優先度学習とも呼ばれ，インスタンス間のランキングを学習するための機械学習手法である．ランキング学習では識別関数をf(x)=wxとし以下のようにwを学習する．ここでxは，入力インスタンスの素性表現であり，wはxに対応する，重みベクトルである．まずランキングに含まれる各インスタンスの組み合わせを生成する．ここでランキングA&gt;B&gt;Cを考えると，生成される組み合わせはA&gt;B，A&gt;C，B&gt;Cとなる．そして各組み合わせにおいて識別関数の値がランキング上の順序と同じになるようにwを学習する．上述の例で，各インスタンスに対応する素性ベクトルがx_A，x_B，x_Cだとすると，f(x_A)&gt;f(x_B)などとなるように学習する．なお，学習する順位内に同順位のものがあっても，「それらが同順位である」ということは学習されない．例えばA&gt;B=Cという順位があった場合には生成される組み合わせはA&gt;B，A&gt;CだけでありB=Cという関係が考慮されることはない．また，同時に複数のランキングを学習することも可能である．例えば，A_1&gt;B_1&gt;C_1とA_2&gt;B_2&gt;C_2という二つの独立したランキングがあった場合にはA_1&gt;B_1，A_1&gt;C_1，B_1&gt;C_1，A_2&gt;B_2，A_2&gt;C_2，B_2&gt;C_2のようにそれぞれ独立した組み合わせを生成し，これら全てを満たすように識別関数を学習する．未知のインスタンス集合に対するランキング予測では，各インスタンスに対して学習されたwを用いてf(x)を計算し，その値の順が出力されるランキングとなる．ランキング学習は二値分類に適用することが可能であり，正例と負例に対応関係がある場合には通常の二値分類よりも有効であると言われている．これは通常の二値分類器では，全ての正例と負例を同一の特徴空間に写像するが，ランキング学習では正例と負例の差を特徴空間に写像するためである．例えば，入力x_1に対する出力候補が(A_1+,B_1-,C_1-)，入力x_2に対する出力候補が(A_2+,B_2+,C_2-)となるような学習事例があったとする．この場合，通常の二値分類器では(A_1+,B_1-,C_1-,A_2+,B_2+,C_2-)のように事例をひとまとめにして扱うため，本来直接の比較の対象ではないA_1+とC_2-などが同一の特徴空間上で比較されることとなる．一方，ランキング学習であれば，A_1&gt;B_1=C_1とA_2=B_2&gt;C_2のように，ランキングとして表現することで，A_1+とC_2-などが同一特徴空間上で比較されることはない．このようにして学習された識別関数は二値分類問題における識別関数として利用することができ，二値分類の場合と同様に出力の信頼度としても利用できる．そこで本研究では，入力毎の出力候補に対して正負の正解がラベル付けされた事例からランキング学習により識別関数を学習し，推定の際には識別関数の出力が最も高くなる（最尤）ものを出力する形でランキング学習を利用する</section>
  <section title="ベースラインモデル">本節では本研究でのベースラインとなる外界ゼロ照応を考慮しないゼロ照応解析モデルを説明する．本研究ではゼロ照応解析を用言単位の述語項構造解析の一部として扱う．用言単位の述語項構造解析では，用言と複数の項の間の関係を扱うことができる．例えば「(不動産屋ガ)物件を紹介する」のガ格のゼロ照応解析ではヲ格の項が「物件」であることが大きな手掛かりとなる．各述語項構造は格フレームと，その格フレームの格スロットとその格スロットを埋める項の対応付けとして表現される．格フレームは用言の用法毎に構築されており，各格フレームはその用言が項を取る表層格（格スロット）とその格スロットの項として取られる語（用例）からなる．本研究では，Webページから収集された69億文からの手法で自動構築された格フレームを用いる．構築された格フレームの例を図に示す時間は「今日」「3時」などの時間表現を汎化したものである．．本研究では，ゼロ代名詞の照応先を談話要素という単位で扱う．談話要素とは文中の表現のうち共参照関係にあるものをひとまとめにしたものである．例えば図の例では，「僕」と「自分」や「ラーメン屋_1」，「その店」，「ラーメン屋_2」と「お店」は共参照関係にあるので，それぞれ一つの談話要素として扱う．そしてゼロ代名詞の照応先はこの談話要素から選択する．例えば，「紹介したい」のガ格では「僕」に対応する(a)を照応先として選択することになる．述語項構造解析の例を図に示す．なお，本研究ではゼロ照応解析の対象としてはガ，ヲ，ニ，ガ2格のみを扱うため，時間格などの他の格については省略することがある．ここでガ2格とは京都大学テキストコーパスで定義されている，二重主格構文における主格にあたる格である．この例では，「紹介する」に対応する格フレームから「紹介する(1)」を選択し，そのガ格に談話要素(a)，ヲ格に談話要素(c)，時間格に談話要素(d)を対応付け，それ以外の格には談話要素を対応付けない．ゼロ照応解析の出力としては，ガ格の談話要素(a)のみが出力される．ベースラインモデルでは，先行研究と同様に以下の手順で解析を行う．形態素解析，固有表現認識，構文解析を行う．共参照解析を行いテキスト中に出現した談話要素を認識する．各用言について以下の手順で述語項構造を決定する．			以下の手順で解析対象用言がとりえる述語項構造（格フレームと談話要素の対応付け）の組み合わせを列挙する．						解析対象用言の格フレームを1つ選ぶ．			解析対象用言と係り受け関係にある語と格スロットの対応付けを行う．			対応付けられなかったガ格，ヲ格，ニ格，ガ2格の格スロットと，対象用言の格スロットとまだ対応付けられていない談話要素の対応付けを行う．					学習されたランキングモデルによりもっとも高いスコアが与えられたものを述語項構造として出力する．	先行研究と本研究でのベースラインモデルとの違いは，()でのスコア付けの際の重みの学習方法の違いである．先行研究では対数線形モデルを利用していたが，本研究ではランキング学習を用いた．このランキング学習の詳細は節で説明する．手順()の述語項構造解析について説明する．まず，()の手順で候補となる述語項構造(cf,a)を列挙する．ここでcfは選ばれた格フレーム，aは格スロットと談話要素の対応付けである．ただし，同一用言の複数の格に同じ要素が入りにくいという経験則から，手順()では既に他の格に対応付けられた談話要素は，ゼロ代名詞には対応付けないこととする．手順()で列挙される述語項構造の例を図に示す．【1-1】と【2-1】，【1-2】と【2-2】などは，格と談話要素の割り当ては同じであるが格フレームは異なるため別々の述語項構造候補として扱う．【1-2】と【2-2】のどちらを述語項構造として選んでもゼロ照応解析としての出力は同じになる．この列挙された述語項構造をそれぞれ節で説明する手法で素性として表現し，節で説明する方法で学習された重みを利用してスコア付けを行い，最終的に最もスコアが高かった述語項構造を出力する</section>
  <subsection title="述語項構造を表現する素性">本節では述語項構造を表現する素性について説明する．入力テキストtの解析対象用言pに格フレームcfを割り当て，その格フレームの格スロットと談話要素の対応付けをaとした述語項構造を表現する素性ベクトルを(cf,a,p,t)とする．(cf,a,p,t)は直接係り受けがある述語項構造に関する素性ベクトル_overt-PAS(cf,a_ovet,p,t)とゼロ照応解析で対象となる各格cに談話要素eが割り当てられることに関する素性ベクトル_case(cf,ce,p,t)からなり，具体的には以下のような形とする．ここで，a_overtは用言pと直接係り受けのある談話要素と格スロットの対応付けである．各格に対応する素性ベクトル_case(cf,ce,p,t)は格cに談話要素eが対応付けられた場合の素性ベクトル_A(cf,ce,p,t)と何も対応付けられなかった場合の素性ベクトル_NA(cf,c×,p,t)からなる．_case(cf,ce,p,t)は格cがゼロ照応として対応付けられた場合にのみ考慮し，直接係り受け関係にある談話要素に対応付けられた場合には0ベクトルとする．例えば図の【2-2】を表現する素性ベクトル(紹介する(2),ガ:(a)僕，ヲ:(c)ラーメン屋，ニ:×，ガ2:×，時間:(d)今日)は以下のようになる．(紹介する(2),ガ:(a)僕，ヲ:(c)ラーメン屋，ニ:×，ガ2:×，時間:(d)今日)=[-2pt](_overt-PAS(紹介する(2),ガ:×，ヲ:(d)ラーメン屋，ニ:×，ガ2:×，デ格:(c)ブログ),split[-2pt]&amp;_A(紹介する(2),ガ(a)僕),&amp;0_\phi_\mathitNA,[-2pt]&amp;0_\phi_\mathitA,&amp;0_\phi_\mathitNA,[-2pt]&amp;0_\phi_A,&amp;_NA(紹介する(2),ニ×),[-2pt]&amp;0_\phi_A,&amp;_NA(紹介する(2),ガ2×))alignedgather述語項構造ベクトルを表現する各要素_overt-PAS(cf,a_overt,p,t)，_A(cf,ce,p,t)，_NA(cf,c×,p,t)の素性について説明する．まず，_overt-PAS(cf,a_overt,p,t)には確率的格解析モデルから得られる表層の係り受けの確率を用いる．_A(cf,ce,p,t)に用いる素性の一覧を表に示す．格フレーム素性は，格フレームから得られる情報である．eが複数回言及される場合には，各素性ごとにそれらの値で最も大きいものをその素性の値とする．例えば，談話要素eが格フレームcfの格cに対応付く確率の素性を式()のガ格について考える．上述の例ではガ格に対応付けられた談話要素(a)は「僕」「自分」と2回言及されている．そこで「僕」「自分」が「紹介する(2)」のガ格に対応付く確率をそれぞれ計算し，最も値が高いものを(a)が「紹介する(2)」のガ格に対応付く確率とする．用言素性におけるpの持つモダリティなどの情報は，用言の属する基本句に日本語構文・格解析システムKNPver.~4.0により付与された情報を利用する．文脈素性はeが前後の文脈でどのような表現で出現するかを扱う素性であり，eが複数回言及される場合には，その全てを素性として扱う．cが割り当てられたことの素性は，その格にどの程度ゼロ代名詞が出現するかを調整するための素性となっている．_NA(cf,c×,p,t)に用いる素性を表に示す．_NA(cf,c×,p,t)では対応付けられる要素eがないため，格フレームに関する素性のみとなっている</subsection>
  <subsection title="素性の重みの学習">前節で入力テキストt，解析対象用言pが与えれられたとき，格フレームcf，格スロットと談話要素の対応付けaからなる述語項構造を表現する素性を(cf,a,p,t)としたが，それに対応する素性の重みwをランキング学習により学習する．ランキング学習の学習データ作成は，対象用言ごとに順位データを作成し，全用言の順位データを集約したものとする．もし，述語項構造の正解が一意に求められるなら，その述語項構造を上位とし，それ以外の述語項構造を下位とする順位データを作成すればよい．しかし，実際には以下の2つの問題がある．1つ目は正解コーパスには1つの格に対して複数の談話要素が対応付けられたものが含まれることである．例えば図の「焼いている」では正解としてガ:×，ヲ:(b)ケーキ+(c)クッキー，ニ:×，ガ2:×，時間:(d)毎週のように，ヲ格に2つの談話要素が対応付けられる．一方，提案手法では先に述べたように1つの格に対して1つの談話要素しか対応付けない．そこで，1つの格に対して複数の談話要素が対応付けられている場合には，そのうちどれか1つの談話要素を割り当てていれば正解として扱うこととする．例えば，図ではガ:×，ヲ:(b)ケーキ，ニ:×，ガ2:×，時間:(d)毎週とガ:×，ヲ:(c)クッキー，ニ:×，ガ2:×，時間:(d)毎週を正解の談話要素対応付けとする．また，この正解となる対応付けの集合を(a^*_1,,a^*_N)とする．2つ目はコーパスには格フレームの正解は付与されていないことである．先に述べたように述語項構造は格フレームと格スロットと談話要素の対応付けからなる．格フレームは用言の用法ごとに構築されており，述語項構造候補の格フレームには文脈で使用される用法と全く異なるものが含まれる．格スロットと談話要素の対応付けは正しいが，文脈での使用とは異なる用法の格フレームを持つような述語項構造を正解として扱った場合，学習に悪影響を与えると考えられる．そこで，確率的ゼロ照応解析を利用することで，各文脈における用法の格フレームを推定する．確率的ゼロ照応解析では各述語項構造(cf,a)に対し格フレームの情報などを用いることでP(cf,a|p,e)を推定する．ここでeは文章中に出現する談話要素eの集合である．具体的には以下の手順で各対象用言pに対して学習データとなるランキングを生成する．用言pに対して取り得る述語項構造(cf,a)を訓練事例として列挙する．	正解となる対応付けa^*_1,,a^*_Nについて各(cf,a^*_i)の確率的ゼロ照応解析確率を計算し，最も確率が高いものを(cf^*_i,a^*_i)とする．		(cf,a^*_i)のうち，(cf^*_i,a^*_i)以外のものを訓練事例から取り除く．		各(cf^*_i,a^*_i)が他の(cf,a)より順位が高くなるようなランキングを用言pに対する学習データとする．	図の「焼いている」を例に説明する．まず「焼いている」の述語項構造解析の候補として【1-1】，，【2-1】，を列挙する（手順()）．このうち，格と談話要素の対応付けが正解となるもの(cf,a^*_i)は，ガ:×，ヲ:(b)ケーキ，ニ:×，ガ2:×，時間:(d)毎週となっている【1-2】と【2-2】，ガ:×，ヲ:(c)クッキー，ニ:×，ガ2:×，時間:(d)毎週となっている【1-3】と【2-3】である．【1-2】，【2-2】，【1-3】，【2-3】について確率的ゼロ照応解析スコアを計算した結果，【1-2】&gt;【2-2】，【1-3】&gt;【2-3】となったとする．この場合【1-2】と【1-3】が(cf^*_i,a^*_i)となる（手順()）．そこで，訓練事例から【2-2】と【2-3】を取り除く（手順()）．そして，【1-2】=【1-3】&gt;【1-1】=【1-4】==【2-1】=【2-4】=というランキングを「焼いている」についての学習データとする（手順()）．このように各対象用言に対するランキング学習データを生成し，それらを統合したものに対してランキング学習を行うことでwを学習する．</subsection>
  <section title="コーパス">本研究では，DiverseDocumentLeadsCorpus(DDLC)を利用する．DDLCはWebから収集されたテキストに対して，形態素情報，構文関係，固有表現，共参照関係，述語項構造，著者・読者表現が付与されている．形態素情報，構文関係，固有表現，共参照関係は京都大学テキストコーパスと同様の基準で付与されている．述語項構造も京都大学テキストコーパスと同様の基準で付与されており，文章内ゼロ照応だけでなく外界ゼロ照応も付与されている．外界ゼロ照応の照応先としては表の5種類が設定されている．著者・読者表現は，「=:[著者]」「=:[読者]」というタグ:A」という表記はrelという関係でAという情報が付与されていることを示す．また，「Brel:A」という表記はBに対して「rel:A」という情報が付与されていることを示す．で基本句単位に付与されており，著者・読者表現が複合語の場合にはその主辞に対して付与されている．DDLCでは，著者表現，読者表現は1文書中にそれぞれ最大でも1つの談話要素と仮定されており，著者・読者が複数回言及される場合には，そのうち1つに「=:[著者]」「=:[読者]」を付与し，それ以外のものは，著者・読者表現と共参照関係にある，という形で表現される．下記の例では，著者は「主婦」や「こま」，「母」など複数の表現で言及されているが，「=:[著者]」は「主婦」に対してだけ付与され，「こま」や「母」には「=:主婦」というタグにより，「主婦」と共参照関係にあるという情報が付与されている．.東京都に住む「お気楽主婦」こまです。０歳と６歳の男の子の母をしてます。*4ex()また，組織のウェブページなどの場合にはその組織名や組織を表す表現を著者表現としている．.ここでは弊社の商品及び事業を簡単にご説明します。*4ex(弊社=:[著者]).神戸徳洲会病院では地域の医療機関との連携を大切にしています。*4ex(病院=:[著者])ウェブページでは実際には不特定多数が閲覧できる状態であることが多いが，著者が特定の読者を想定していると考えられる場合には，その特定の読者を表す表現も読者表現として扱っている．下記の例では，想定している読者が「今後就職を迎える人」だと考えられるので，その主辞の「人」に「=:[読者]」が付与されている．.今後就職を迎える人に，就職活動をどのように考えれば良いのかをお知らせしてみましょう。*4ex(人=:[読者])一方，想定している読者のうち一部だけを対象とした表現は読者表現として扱っていない．下記の例では，想定される読者は「オーナーを希望する人」であり，「店舗運営の経験がない方」はそのうちの一部であると考えられるので，読者表現として扱われていない．.店舗運営の経験がない方でも、ご安心ください。ローソンの研修制度なら、オーナーに必要とされるノウハウを段階的に修得することができます。</section>
</root>
