<?xml version="1.0" ?>
<root>
  <title>後方文脈を考慮した係り受けモデル</title>
  <author>内元清貴村田真樹関根聡井佐原均</author>
  <jabstract>係り受け解析は日本語解析の重要な基本技術の一つとして認識されている．依存文法に基づく日本語係り受け解析では，文を文節に分割した後，それぞれの文節がどの文節に係りやすいかを表す係り受け行列を作成し，一文全体が最適な係り受け関係になるようにそれぞれの係り受けを決定する．本論文ではそのうち，係り受け行列の各要素の値を計算するためのモデルについて述べる．アプローチとしては，主にルールベースによる方法と統計的手法の二つのものがあるが，我々は利用可能なコーパスが増加してきたこと，規則の変更に伴うコストなどを考慮して，統計的手法をとっている．統計的手法では行列の各要素の値は確率値として計算される．これまでよく用いられていたモデル(旧モデル)では，その確率値を計算する際に，着目している二つの文節が係るか係らないかということのみを考慮していた．本論文では，着目している二つの文節(前文節と後文節)だけを考慮するのではなく，前文節と前文節より文末側のすべての文節との関係(後方文脈)を考慮するモデルを提案する．このモデルをME(最大エントロピー)に基づくモデルとして実装した場合，旧モデルを同じくMEに基づくモデルとして実装した場合に比べて，京大コーパスに対する実験で，全く同じ素性を用いているにもかかわらず係り受け単位で1%程度高い精度(88%)が得られた．</jabstract>
  <jkeywords>係り受け解析，構文解析，最大エントロピー(ME)</jkeywords>
  <section title="はじめに">係り受け解析は日本語解析の重要な基本技術の一つとして認識されている．係り受け解析には，日本語が語順の自由度が高く省略の多い言語であることを考慮して依存文法(dependencygrammar)を仮定するのが有効である．依存文法に基づく日本語係り受け解析では，文を文節に分割した後，それぞれの文節がどの文節に係りやすいかを表す係り受け行列を作成し，一文全体が最適な係り受け関係になるようにそれぞれの係り受けを決定する．依存文法による解析には，主にルールベースによる方法と統計的手法の二つのアプローチがある．ルールベースによる方法では，二文節間の係りやすさを決める規則を人間が作成する．一方，統計的手法では，コーパスから統計的に学習したモデルをもとに二文節間の係りやすさを数値化して表す．我々は，ルールベースによる方法ではメンテナンスのコストが大きいこと，また統計的手法で利用可能なコーパスが増加してきたことなどを考慮し，係り受け解析に統計的手法を採用することにした．統計的手法では二文節間の係りやすさを確率値として計算する．その確率のことを係り受け確率と呼ぶ．これまでよく用いられていたモデル(旧モデル)では，係り受け確率を計算する際に，着目している二つの文節が係るか係らないかということのみを考慮していた．本論文では，着目している二つの文節(前文節と後文節)だけを考慮するのではなく，前文節と前文節より文末側のすべての文節との関係(後方文脈)を考慮するモデルを提案する．このモデルは以下の二つの特徴を持つ．二つの文節(前文節と後文節)間の関係を，「間」(前文節が二文節の間の文節に係る)か「係る」(前文節が後文節に係る)か「越える」(前文節が後文節を越えてより文末側の文節に係る)かの三カテゴリとして学習する．(旧モデルでは二文節が「係る」か「係らないか」の二カテゴリとして学習していた．)着目している二つの文節の係り受け確率を求める際に，その二文節に対しては「係る」確率，二文節の間の文節に対しては前文節がその文節を越えて後文節に係る確率(「越える」の確率)，後文節より文末側の文節に対しては前文節がその文節との間にある後文節に係る確率(「間」の確率)をそれぞれ計算し，それらをすべて掛け合わせた確率値を用いて係り受け確率を求める．(旧モデルでは，着目している二文節が係る確率を計算し，係り受け確率としていた．)このモデルをME(最大エントロピー)に基づくモデルとして実装した場合，旧モデルを同じくMEに基づくモデルとして実装した場合に比べて，京大コーパスに対する実験で，全く同じ素性を用いているにもかかわらず係り受け単位で1%程度高い精度(88%)が得られた．</section>
  <section title="係り受け確率モデル">統計的日本語係り受け解析では，二文節間の係りやすさは確率値で表される．この確率値は係り受け確率モデルから計算される．</section>
  <subsection title="係り受け確率モデル(旧モデル)">この節ではこれまでに依存文法に基づく係り受け解析によく用いられているモデルについて説明する．入力文Sが与えられると，Sはn個の文節b_1,,b_nに一意に分割されると仮定し，Sをそれらの順序付き集合B=b_1,,b_nで表す．そして，文全体の係り受け関係Dはそれぞれの文節b_i(i=1,,n-1)を係り元の文節とする係り受け関係D_iの順序付き集合D=D_1,,D_n-1で表されると仮定する．さらに文節の集合Bが決まると，それぞれ文節b_i(1in-1)と文節b_m(m&gt;i,2mn)に関して観測される素性F_i,mが一意に決まると仮定し，文節の集合Bを素性の集合FF&amp;=&amp;F_1,2,F_1,3,,F_i,m,,F_n-1,neqnarrayで表す．統計的係り受け解析とは，Sが与えられたときに文全体の係り受けがDとなる確率P(D|S)が最も高くなるものを全体の係り受け関係とする処理のことである．つまり，D_best&amp;=&amp;argmax_DP(D|S)&amp;=&amp;argmax_DP(D|B)&amp;=&amp;argmax_DP(D|F)eqnarrayとなるようなD_bestを求めることに相当する．日本語の係り受けには，主に以下の特徴があるとされている．係り受け関係は交差しない．(非交差条件)係り要素は受け要素を一つだけ持つ．以降では，これらの特徴を満たすようなD_bestを求めることを考える．まず，式()のP(D|F)は以下のように変形できる．P(D|F)&amp;=&amp;P(D_1,,D_n-1|F)&amp;=&amp;P(D_n-1|F)P(D_n-2|D_n-1,F)P(D_n-3|D_n-2,D_n-1,F)&amp;&amp;P(D_1|D_2,,D_n-1,F)eqnarrayこの式で各々の係り受けつまりD_1,,D_n-1が独立であると仮定すると，P(D|F)は以下のようにそれぞれの文節に対する係り受けの確率の積で表せる．P(D|F)&amp;=&amp;_i=1^n-1P(D_i|F)eqnarrayここで，D_i,i+jを後で定義するように文節b_iと文節b_i+jの間の関係を表すフラグとし，D_iを文節b_iと文節b_i+j(1jn-i)との間の関係の集合として，以下のように表す．ここでは，上述の(i)の特徴を仮定している．D_i&amp;=&amp;D_i,i+1,D_i,i+2,,D_i,neqnarrayすると，式()から以下の式が導ける．P(D|F)&amp;=&amp;_i=1^n-1P(D_i,i+1,,D_i,n|F)eqnarray旧モデルでは，D_i,i+jとして文節b_iが文節b_i+jに係るか否かの1，0の二値をとると仮定していた．文節b_iと係り受けの関係にある文節が文節b_iの次から数えてdep(i)(1dep(i)n-i)番目の係り先の候補であるとき，上述の(iii)の特徴，つまり係り要素は受け要素を一つだけ持つということを仮定すると，D_i,i+l&amp;=&amp;.eqnarray*となる．よって，式()は以下のように変形できる．P(D|F)&amp;=&amp;_i=1^n-1P(D_i,i+dep(i)=1|F)P(D_i,i+1=0|D_i,i+dep(i)=1,F)&amp;&amp;P(D_i,i+2=0|D_i,i+1=0,D_i,i+dep(i)=1,F)&amp;&amp;P(D_i,n=0|D_i,n-1=0,,D_i,i+1=0,D_i,i+dep(i)=1,F)eqnarrayD_i,i+dep(i)=1のとき必ずD_i,i+j=0(j=dep(i))となるので，P(D_i,i+1=0|D_i,i+dep(i)=1,F)&amp;=&amp;1P(D_i,i+2=0|D_i,i+1=0,D_i,i+dep(i)=1,F)&amp;=&amp;1P(D_i,n=0|D_i,n-1=0,,D_i,i+1=0,D_i,i+dep(i)=1,F)&amp;=&amp;1eqnarray*となり，式()は次のように表せる．P(D|F)&amp;=&amp;_i=1^n-1P(D_i,i+dep(i)=1|F)eqnarrayさらに，F_i,mはそれぞれ独立で，文節b_iと文節b_i+dep(i)との関係D_i,i+dep(i)はF_i,i+dep(i)のみによって決まり，他のF_i,i+j(j=dep(i))とは独立であると仮定する．すると，式()は，以下のように変形できる．P(D|F)&amp;=&amp;_i=1^n-1P(D_i,i+dep(i)=1|F_i,i+dep(i))eqnarray式()と式()とからD_bestが導かれる．</subsection>
  <subsection title="後方文脈を考慮した係り受け確率モデル">この節では，我々が提案するモデルについて説明する．旧モデルでは，二つの文節の関係を「係る」か「係らない」かの二カテゴリとして学習し，それらの二文節が係る確率を計算して係り受け確率としていた．我々のモデルでは，(A)二つの文節(前文節と後文節)間の関係を，「間」か「係る」か「越える」かの三カテゴリとして学習し，(B)着目している二つの文節の係り受け確率を求める際に，その二文節に対しては「係る」確率，二文節の間の文節に対しては前文節がその文節を越えて後文節に係る確率(「越える」の確率)，後文節より文末側の文節に対しては前文節がその文節との間にある後文節に係る確率(「間」の確率)をそれぞれ計算し，それらをすべて掛け合わせた確率値を用いて係り受け確率を求める．このモデルでは，D_i,i+jの仮定が旧モデルにおけるものと異なる．D_i,i+jとしては，「越える」，「係る」，「間」を表す0，1，2の三値をとると仮定する．この点がこのモデルの特徴の一つである．文節b_iと係り受けの関係にある文節がdep(i)(1dep(i)n-i)番目の係り先の候補であるとき，節の(iii)の特徴，つまり係り要素は受け要素を一つだけ持つということを仮定すると，D_i,i+l&amp;=&amp;.eqnarray*となる．よって，式()は以下のように変形できる．P(D|F)&amp;=&amp;_i=1^n-1(_j=1^dep(i)-1P(D_i,i+j=0|D_i,i+k=0|1k&lt;j,F).&amp;&amp;_j=dep(i)+1^n-iP(D_i,i+j=2|D_i,i+k=0|1k&lt;dep(i),&amp;&amp;D_i,i+k=2|j&lt;kn-i,F)&amp;&amp;.P(D_i,i+dep(i)=1|D_i,i+k|1k&lt;dep(i),dep(i)&lt;kn-i,F))eqnarrayD_i,i+lの値が決まるのは，l&lt;mを満たすようなmに対しD_i,i+m=0になる場合か，l&gt;mを満たすようなmに対し，D_i,i+m=2になる場合か，D_i,i+dep(i)=1となるdep(i)が決まる場合かのいずれかである．これらの条件を満たさないように両端から順に係り受け関係D_i,i+lを確率式の前件部に移していけば，式()の最後の項を除くそれぞれの項の確率値は一意には決まらない．式()の最後の項はD_i,i+lの値がすべて決まるとdep(i)が決まるので確率値は1になる．式()のその他の項については，D_i,i+k=0|1k&lt;jの間の独立性，およびD_i,i+k=0|1k&lt;dep(i),D_i,i+k=2|j&lt;kn-iの間の独立性を仮定すると，式()は次のように表せる．P(D|F)&amp;=&amp;_i=1^n-1(_j=1^dep(i)-1P(D_i,i+j=0|F)_j=dep(i)+1^n-iP(D_i,i+j=2|F))eqnarray節の旧モデルにおける仮定と同様にF_i,m(m=1,,n)はそれぞれ独立で，D_i,i+jつまり文節b_iと文節b_i+jとの関係は，F_i,i+jのみによって決まると仮定する．すると，式()は，以下のように変形できる．P(D|F)&amp;=&amp;_i=1^n-1(_j=1^dep(i)-1P(D_i,i+j=0|F_i,i+j)_j=dep(i)+1^n-iP(D_i,i+j=2|F_i,i+j))eqnarrayこの式のP(D|F)をP_new_model(D|F)とし，旧モデルの式()におけるP(D|F)をP_old_model(D|F)とする．P_old_model(D|F)では素性F_i,i+dep(i)が用いられているが，P_new_model(D|F)では用いられておらず，P_new_model(D|F)では素性F_i,i+dep(i)以外の素性が用いられているが，P_old_model(D|F)では用いられていない．したがって，P_old_model(D|F)とP_new_model(D|F)は相補的な関係にあるため，この二つを組み合わせる．すると以下の式が得られる．P(D|F)^2&amp;=&amp;P_old_model(D|F)P_new_model(D|F)&amp;=&amp;_i=1^n-1(_j=1^dep(i)-1P(D_i,i+j=0|F_i,i+j).&amp;&amp;P(D_i,i+dep(i)=1|F_i,i+dep(i))&amp;&amp;._j=dep(i)+1^n-iP(D_i,i+j=2|F_i,i+j))eqnarray本節のモデルはこの式の平方根をとることによって係り受け確率P(D|F)を求めるものである．このP(D|F)と式()とからD_bestが導かれる．実際にこのモデルから係り受け確率がどのように求まるかを図を用いて説明する．図はある文節b_iの係り先の候補が5個あったときにそれぞれの候補に係るとしたときの係り受け確率を計算している様子を表している．このとき，文節b_iとそれぞれの候補との関係がそれぞれ「越える」，「係る」，「間」となる確率として表のような値が得られたと仮定している．[htbp]table*例えば，候補3を係り先だと仮定したとき，候補1，候補2は越えて，文節b_iと候補4，候補5の間に係る確率は，式()の文節b_iに関する項を用いてP(D_i|F)^2&amp;=&amp;_j=1^2P(D_i,i+j=0|F_i,i+j)P(D_i,i+3=1|F_i,i+3)_j=4^5P(D_i,i+j=2|F_i,i+j)&amp;=&amp;P(D_i,i+1=0|F_i,i+1)P(D_i,i+2=0|F_i,i+2)P(D_i,i+3=1|F_i,i+3)&amp;&amp;P(D_i,i+4=2|F_i,i+4)P(D_i,i+5=2|F_i,i+5)&amp;=&amp;0.60.60.50.40.6=0.0432eqnarray*つまり，P(D_i|F)&amp;=&amp;0.0432=0.208eqnarray*のように計算され，これが最も高い．各々の係る確率だけを考えた場合にはそれぞれのP(D_i,i+j=1|F_i,i+j)を比較することになり，候補3と候補4の確率がどちらも0.5のため決まらないが，この方法によると，候補3を優先的に係り先とすることになる．一文全体の確率はそれぞれの文節について求めた係り受け確率の積で表され，その積の値が最も高くなるようにそれぞれの係り受けを決めることになる．我々は，式()においてP(D|F)を最大にする係り受け関係の集合D_bestを求めるために，文末から文頭に向けて解析することにより，効率良く組み合わせの数を減らしながら一文全体の係り受けを決定する方法を提案している．この方法では解の探索をビームサーチにより行う．この方法によると決定的に解析を行ってもビーム幅を広くしたときとほとんど同じ精度が得られることが実験により分かっている．そこで，D_bestを求めるためにこの方法を採用する．このとき，上述のモデルに節で述べた(ii)の特徴，つまり非交差条件を仮定すると，文節b_iの係り先の候補はすでに解析の終わった文節b_i+1から文節b_nまでの係り受け関係D_i+1,,D_nに依存して制限されることになる．つまり，非交差条件のために文節b_iの係り先の候補とはなり得ない文節b_jに対しては，係る確率は0になり，「越える」か「間」かについてはb_jの前後の文節のうち非交差条件を満たす文節と文節b_iとの関係が決まれば一意に決まり確率は1になる．文節b_iの係り受け確率P(D_i|F)は文節b_iのすべての係り先の候補につい確率値を足すと1になるように正規化する．実際にこのモデルから係り受け確率がどのように求まるかを図を用いて説明する．図は図において非交差条件を考慮した場合の係り受け確率の計算の仕方を表している．ここで，ある文節b_iより後方の文節について，破線の矢印で表されるような係り受け関係が決まったものと仮定している．このとき，候補3と候補4は非交差条件を満たさないために文節b_iの係り先の候補とはなり得ない．また，文節b_iとそれぞれの候補との関係としては，図と同様に表の値が得られたと仮定している．例えば，候補5を係り先だと仮定したとき，図の一番下の例のように，候補1，候補2に対しては越える確率，候補5に対しては係る確率を用いてそれぞれ掛け合わせ，その平方根をとることにより係り受け確率が得られる．一文全体の確率はそれぞれの文節について求めた係り受け確率の積で表され，その積の値が最も高くなるように各々の係り受けを決めることになる．</subsection>
  <section title="実験結果">この節では，新モデル(後方文脈を考慮したモデル)と旧モデルとの比較実験を行う．実験に用いたコーパスは，京大コーパス(Version2)の一般文の部分で，基本的に学習には1月1日と1月3日から8日までの7日分(7,958文)，試験には1月9日の1日分(1,246文)を用いた．節に述べた旧モデルと節に述べた新モデル(後方文脈を考慮したモデル)のそれぞれを文献と同様にMEモデルとして実装し，テストコーパスに対する係り受け解析の精度を調べた．係り受け解析の実験に用いた素性は，文献のものと同じものとした．これは表の基本素性と呼ばれるものとそれらの組み合わせである．このうち，学習コーパス中に4回以上現れた素性約38,000個を用いている．解析結果を表に示す．ここで，係り受けの正解率というのは文末の一文節を除く残りすべての文節に対して，係り先を正しく推定していた文節の割合を求めたものである．また，文正解率というのは文全体の解析が正しいものの割合を意味する．表の第1行および第2行はそれぞれ新モデル，旧モデルを用いて京大コーパス1月9日の文を解析した結果である．いずれも，コーパスの形態素情報，文節区切情報を入力として，文節間係り受けの解析を決定的に(ビーム幅k=1)行なった．ビーム幅を大きくしても精度にほとんど違いはなかったため，決定的に解析した結果のみを示した．ベースラインとしては各文節がすべて隣に係るとしたときの精度をあげた．新モデルとしては節に述べた後方文脈を考慮したモデルの精度をあげた．</section>
  <subsection title="旧モデルとの比較">本節では，節に述べた旧モデルと節に述べた新モデル(後方文脈を考慮したモデル)をそれぞれ理論と実験の観点，学習の観点から比較する．[理論と実験の観点から]式()は式()を包含するものであり，式()に比べるとより多くの文節との関係(素性F_i,i+jで表される)が考慮されている．ただし，式()から式()を導くときに用いている独立性の仮定は，実際の現象そのままではなく近似になっているので，旧モデルに比べると近似の部分が多い．しかしながら，同じ素性を用いた実験(表)で，新モデルは旧モデルに比べて1%程度良い結果を得ている．これは多少近似があっても実際に係り受け確率の計算に多くの情報を考慮している新モデルの方が良いということを示している．次に，図~に文節長と解析精度の関係をあげる．この図から，どの文節数に対しても新モデルの精度は旧モデルの精度とほぼ同等以上であることが分かる．[学習の観点から]学習には学習コーパス中で非交差条件を満たす任意の二文節を用いる．旧モデルでは各二文節に対し「係る」と「係らない」の二つのカテゴリを学習しているのに対し，新モデルでは「越える」と「係る」と「間」の三つのカテゴリを学習している．一般に学習するカテゴリを多くするとデータスパースネスになりやすいが，新モデルでは三つのカテゴリに分けてもデータスパースネスの問題は生じない．これは新モデルで「越える」と「間」の二つのカテゴリに分けた，旧モデルの「係らない」というカテゴリにはもともと十分な学習データがあったためである．例えば，ある文節の係り先の候補が10個あるときには，そのうち1個だけが「係る」に対するデータであり，残りの9個は「係らない」に対するデータである．ここで「係らない」を「越える」と「間」の二つに分けても，「係る」に比べるとそれぞれ十分な量の学習データがある．次に，新モデルが旧モデルに比べて優れていることを定量的に示すデータを図~にあげる．これはそれぞれのモデルに対し，学習コーパスの量と解析精度の関係をプロットしたものである．学習コーパスの量にかかわらず，新モデルの方が旧モデルに比べて常に1%程度精度がよいことが分かる．</subsection>
  <subsection title="その他のモデルとの比較">統計的な手法では，ルールベースに比べて並列構造や従属節間の係り受け関係に対する解析誤りが多い．西岡山らは，この後者の問題を取り上げ，二つの文節の関係が係るか係らずに越えるかを学習するモデルを提案した．このモデルを用いることにより，二つの文節だけでなくその二文節とそれらの間にある文節との関係も扱えるようになる．本論文で我々が提案したモデル(後方文脈を考慮したモデル)はさらにその二文節とそれらよりも文末に近い側の文節との関係も扱うため，彼らのモデルよりも多くの情報を考慮していることになる．表~の一行目に西岡山らのモデルを用いたときの実験結果を示す．実験に用いた素性，コーパスは章の最初に説明したものと同じである．後方文脈を考慮したモデルを用いた実験のときと異なるのは，「係る」と「越える」の二つのカテゴリを学習するモデルを用いている部分のみである．表~より定量的にも，後方文脈を考慮したモデルのように「間」というカテゴリも考慮した方がよいことが分かる．次に，後方文脈を考慮したモデルにおいて三カテゴリを学習する必要があることを示す．後方文脈を考慮したモデルでは特徴(1)としてあげたように二文節間の関係を「間」か「係る」か「越える」かの三カテゴリとして学習する．この三カテゴリのうち二つのカテゴリ「間」と「越える」を，旧モデルの二カテゴリのうち「係らない」によって代用させたモデルを考える．このモデルは，係り受け確率を求める際に，着目している二つの文節(前文節と後文節)だけを考慮するのではなく，前文節と前文節より文末側のすべての文節との関係(後方文脈)を考慮している点が旧モデルとは異なる．表~の二行目にこのモデルを用いたときの実験結果を示す．この表より，「間」と「越える」の違いは区別して学習するべきであることが分かる．</subsection>
  <section title="おわりに">係り受け解析は日本語解析の重要な基本技術の一つとして認識されている．依存文法に基づく解析には，主にルールベースによる方法と統計的手法の二つのアプローチがあるが，我々は利用可能なコーパスが増加してきたこと，規則の変更に伴うコストなどを考慮して，統計的手法をとっている．統計モデルとしてこれまでよく用いられていたもの(旧モデル)では，係り受け確率を計算する際に，着目している二つの文節が係るか係らないかということのみを考慮していた．本論文では，着目している二つの文節(前文節と後文節)だけを考慮するのではなく，前文節と前文節より文末側のすべての文節との関係(後方文脈)を考慮するモデルを提案した．このモデルをME(最大エントロピー)に基づくモデルとして実装した場合，旧モデルを同じくMEに基づくモデルとして実装した場合に比べて，京大コーパスに対する実験で，全く同じ素性を用いているにもかかわらず係り受け単位で1%程度高い精度(88%)が得られた．また，後方文脈を考慮したモデルの精度は旧モデルに比べて，どの文長に対してもほぼ常に良く，学習コーパスの量によらず常に1%程度良かった．plain++document</section>
</root>
