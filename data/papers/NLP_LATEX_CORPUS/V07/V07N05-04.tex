




\documentstyle[epsf,jnlpbbl,eepic,epic]{jnlp_j}

\setcounter{page}{71}
\setcounter{巻数}{7}
\setcounter{号数}{5}
\setcounter{年}{2000}
\setcounter{月}{11}
\受付{2000}{1}{6}
\再受付{2000}{3}{28}
\採録{2000}{4}{13}

\setcounter{secnumdepth}{2}


\newcommand{\undercirc}{}
\newcommand{\undercross}{}

\title{３つ以下の候補から係り先を選択する係り受け解析モデル}
\author{金山 博\affiref{u-tokyo} \and 鳥澤 健太郎\affiref{u-tokyo}\affiref{PRESTO}
	\and 光石 豊\affiref{u-tokyo} \and 辻井 潤一\affiref{u-tokyo}\affiref{UMIST}}


\headauthor{金山, 鳥澤, 光石, 辻井}
\headtitle{３つ以下の候補から係り先を選択する係り受け解析モデル}

\affilabel{u-tokyo}{東京大学大学院理学系研究科 情報科学専攻}
	{Department of Information Science, Graduate School of Science, University of Tokyo}
\affilabel{PRESTO}{科学技術事業団 さきがけ研究21}
	{PRESTO, Japan Science and Technology Corporation}
\affilabel{UMIST}{マンチェスター工科大 計算言語学センター}
	{University of Manchester Institute of Science and Technology}

\jabstract{
本稿では、日本語係り受け解析のための統計的手法について述べる。
この手法は、統計値の計算方法が従来の手法と異なる。
従来の手法では、２つの文節間が依存関係にある確率をそれぞれの文節の組に対して計算するが、
本研究で提案する「３つ組／４つ組モデル」は、
係り元の文節と係り先の文節の候補となる全ての文節に関する情報を確率の条件部として、
ある文節が係り先として選択される確率を求める。
なお、係り先の候補は、HPSGに基づいた文法及びヒューリスティクスによって高々３つに絞られる。
確率の推定には最大エントロピー法を用いており、
我々の構文解析器はEDRコーパスに対して文節正解率88.6$\%$という
高い解析精度を達成した。
}

\jkeywords{係り受け解析, 統計的手法, 文法, ＭＥ法}

\etitle{A Statistical Japanese Dependency Analysis Model \\
with Choice Restricted to at Most Three \\ Modification Candidates}
\eauthor{KANAYAMA Hiroshi\affiref{u-tokyo} 
	\and TORISAWA Kentaro\affiref{u-tokyo}\affiref{PRESTO}
	\and MITSUISHI Yutaka\affiref{u-tokyo}
	\and TSUJII Jun'ichi\affiref{u-tokyo}\affiref{UMIST}
}

\eabstract{
This paper describes a statistical method for Japanese dependency analysis.
The method differs from conventional statistical models in the way of 
calculating statistical values.
The conventional models calculate the probability of a correct dependency 
between two {\it bunsetsu}s (phrasal units of Japanese) for each
pair of {\it bunsetsu}s.
On the other hand, we propose the {\sl triplet/quadruplet model},
in which the conditional part of the probability 
consists of information on a modifier {\it bunsetsu} and
all its modification candidates,
and the probability that a candidate is chosen as the modifiee is calculated. 
The number of candidates is restricted to three or less by an HPSG-based grammar and heuristics.
With a maximum entropy estimation, our parser achieves high accuracy:
88.6$\%$ for the analysis of the EDR annotated corpus.
}

\ekeywords{Dependency analysis, Statistical method, Grammar, Maximum entropy method}

\begin{document}
\maketitle

\newpage

\newcounter{enums}


\section{はじめに}\label{sec:introduction}

本稿では、人手で記述された文法及び統計情報を用いて
日本語の係り受け関係を求める手法について述べる。
特に、文法とヒューリスティクスにより文節の係り先の候補を絞った時に
構成することができる新しいモデルを提案し、
それにより高い係り受けの精度（文節正解率88.6$\%$）が得られることを示す。

我々のグループでは、
何らかの意味表現を構成できるような高機能な構文解析器を実現することを最終目標とし、
HPSG\cite{PollardSag94}の枠組みに基づいた文法を作成している。
現状では意味表現の構成こそできていないが、
新聞や雑誌などの実世界の文章の殆どに対して構文木を出力できる、
被覆率の高い日本語文法SLUNG\cite{Mitsuishi98}を開発した。
しかしながら、文法的に可能な構造を列挙するだけでは、曖昧性が大きいため、実用に耐えない。
また、今後の課題である意味構造の自動学習のためにも、曖昧性の解消が要求される。

本研究では、
文法を用いた構文解析の結果の曖昧性解消を目的として、文節単位の係り受け解析によって、
最も可能性の高い統語構造を選択できるようにする。
また、係り受け解析を行う際に文法を用いることが精度の向上に寄与している。
係り受け解析は以下のような手順でなされる。

\begin{itemize}
\item
まず、文法SLUNGで構文解析し、各文節の係り先の候補を、文法が許す文節に絞る。
\item
文法により絞った係り先候補が４つ以上存在する場合、それを
係り元から見て(1)最も近い文節、(2)二番目に近い文節、(3)最も遠い文節の３つに制限する。
これは、上記の三文節のいずれかが正解となる場合が98.6$\%$を占めるという観察に基づいている。
この制限により、以下で考える統計モデルにおいて、係り先の候補は常に３つ以下であるとみなせる
\footnote{候補が１つの場合は、係り先をその文節に決定できるため、
候補が２つまたは３つの時にのみ確率を計算する。}。
\item
係り元文節がそれぞれの候補に係る確率を、{\bf ３つ組／４つ組モデル}を用いて求める。
このモデルは、
係り元の文節と、２つまたは３つの係り先候補の全てを同時に考慮するという特徴があり、
最大エントロピー法\cite{Berger96}を用いて推定される。
\item
文法が出力するそれぞれの部分木（文節間の係り受けに相当する）に上記の統計値を割り当てて、
最も高い優先度が割り当てられた文全体の構文木が選択される。
\end{itemize}

本研究で用いるモデルと他の研究でのモデルの違いについてであるが、
従来の統計モデル\cite{Uchimoto99}\cite{Haruno98}\cite{Fujio99}では、
係り元文節$i$・係り先文節$j$に対して、
係り元文節の属性$\Phi_i$及び係り先文節の属性$\Psi_{i,j}$
（係り元と係り先の文節間の属性を含む）を前件として、
係り受けが成立する（Tが出力される）条件付き確率
\begin{equation}
P(i \rightarrow j) = P(\mbox{T} \mid \Phi_i, \Psi_{i,j})
\label{equ:naive0}
\end{equation} \refstepcounter{enums}
を求めていた。
これに対し、本研究で用いる３つ組／４つ組モデルでは、係り元文節$i$の候補$t_n$に関して、
$i$の属性を$\Phi_i$、
$t_k$及び$i$と$t_k$の文節間の属性を$\Psi_{i,t_k}$とするとき、
$\Phi_i$と全ての$t_k$に対する$\Psi_{i,t_k}$を前件として、
$n$番目の候補が選ばれる条件付き確率
\begin{eqnarray}
P(i \rightarrow t_n) = & P(n \mid \Phi_i, \Psi_{i,t_1}, \Psi_{i,t_2}) &（候補が２つのとき : n=1,2）
\label{equ:triplet0}
 \refstepcounter{enums}
\\
P(i \rightarrow t_n) = & P(n \mid \Phi_i, \Psi_{i,t_1}, \Psi_{i,t_2}, \Psi_{i,t_3}) & 
（候補が３つのとき : n=1,2,3）
\label{equ:quadruplet0}
\end{eqnarray} \refstepcounter{enums}
を求める。上記の(\ref{equ:triplet0}),~(\ref{equ:quadruplet0})
式をそれぞれ３つ組モデル・４つ組モデルと呼ぶ。
なお、ここでの$n$番目の候補とは、表層文中で係り元から数えて$n$番目の文節ではなく、
文法的に許される係り先のうち２つまたは３つに絞ったものの中で、
係り元から$n$番目に近い文節である。

\ref{sec:related}~節では、従来の統計方式の日本語係り受け解析に関する関連研究、
本研究で用いる日本語文法、及び最大エントロピー法を紹介する。
\ref{sec:ourmodel}~節では、上記で概観した我々の手法を順に詳しく述べる。
\ref{sec:result}~節の実験結果で、対照実験の結果とともに
３つ組／４つ組モデルの有効性を示す。
そして、\ref{sec:observations}~節で、具体的なパラメータの観察や
他研究との比較を行う。


\section{関連研究}\label{sec:related}

本節では、これまでに提案されてきた日本語構文解析のための統計的アプローチと、
本研究で構文解析に用いる日本語文法SLUNG、及び
確率モデルの推定に用いる最大エントロピー法を紹介する。

\subsection{従来の統計的構文解析手法}\label{subsec:conventional}

日本語の係り受け解析のための統計的手法として、様々なモデルが考案されており、
次の２つに大別される。

\begin{description}
\item[生起確率を計算するモデル]
文$s$が与えられた時に、ある構文木$T$が生起する条件付き確率を求める方法である。
すなわち、次のような構文木$T$を選択する。
\begin{equation}
\mathop{\rm argmax}_T P(T|s)
\end{equation} \refstepcounter{enums}
\item[文中の係り受け確率の積をとるモデル]
文節$i$と文節$j$が係り受け関係にある確率$P(i \rightarrow j)$を考え、
式(\ref{equ:product})に示すような、
文中にある全ての係り受けの積を最大化する係り受け関数 $dep(i)$ を求める方法である。
\begin{equation}
\mathop{\rm argmax}_{dep}
\displaystyle{\prod_i} P(i \rightarrow dep(i))
\label{equ:product}
\end{equation} \refstepcounter{enums}
\end{description}

前者に属するものとして、確率文脈自由文法を用いたもの\cite{Mori98}や、
確率一般化LR法を用いたもの\cite{Shirai98}などがある。
これらは、数学的に妥当な確率を用いることができ、
形態素解析など様々なレベルとの統合が容易であるという利点があるものの、
現状では係り受け解析の精度は最高でも白井らの85〜86$\%$にとどまっている。

一方、後者の手法は、比較的学習が容易なため、
高い解析精度が得られる手法が多数提案されている。
実際、最大87.9$\%$と、生起確率に基づくものよりも高い精度が報告されている\cite{Uchimoto99b}。
本研究の手法もこのアプローチに基づいており、
以下でいくつかの研究を紹介する。
これらの手法及び本稿で提案する手法は、上記の$P(i \rightarrow j)$の求め方に違いがある。


決定木を用いたモデル\cite{Haruno98}、最大エントロピー法を用いたモデル\cite{Uchimoto99}、
距離確率と語彙確率を用いたモデル\cite{Fujio99}では、
係り元文節$i$の品詞や語彙や読点の有無など、係り先文節$j$の品詞や語彙、
そして二文節間の距離・読点や副助詞「は」の数などを属性として、
ある属性を持った二文節が存在する時に
それが係り受け関係にある確率を二文節$i, j$間の係り受けのしやすさとしている。
英語の統計的構文解析において
二語間の距離が係り受けを決定する重要な要素となる\cite{Collins96}のと同様に、
日本語の解析においても二文節間の距離が重要であるとされ、
上記のモデルではいずれも文節間にある文節数を属性として用いている。
これらのモデルでは、文節$i$と$j$以外の文節の情報は、
文節間の距離などの属性を除いては反映されない。


係り元・係り先とそのまわりの文節を考慮するモデル\cite{Uchimoto99b}では、
係り元文節$i$の係り先文節$j$への係りやすさの計算に、
$i$より右側にある全ての文節の情報を用いている。
そのために、二文節間の関係を、「係る」「係らない」の二値ではなく、
「越えて、遠くの文節に係る」「係る」「手前の文節に係る」の三値を出力するものとして学習する。
そして、$i$が$j$に係る確率を、$i$が$i, j$間の文節を「越える」確率と
$i$が$j$より右側の文節より「手前に係る」確率の積で補正する。
これにより、ある種の文脈情報が取り扱えることになり、
解析精度が\cite{Uchimoto99}より約$1\%$向上したことが報告されている。
但し、このモデルでは、係り元文節がそれより右側にあるそれぞれの文節に
「係る」か「越える」か「手前に係る」かを
互いに独立であると仮定しなければならない。


本研究で用いる３つ組／４つ組モデル\cite{Kanayama99}では、
２つまたは３つの係り先候補の属性を
同時に考慮できるため、文脈情報が扱えるうえ、
さまざまな望ましい点がある。
これに関しては本論文の\ref{sec:ourmodel}~節以降で詳しく解説する。


\subsection{日本語文法SLUNG}\label{subsec:slung}

本論文で提案する手法では、人手で書かれた文法で候補を絞ることが必須である。
我々が用いるSLUNG\cite{Mitsuishi98}は、
HPSG\cite{PollardSag94}の枠組みで記述された日本語文法であり、
8つのスキーマと、48個の語彙項目テンプレート\footnote{主に自立語に対して、品詞毎に振る舞いを記述したもの。
具体的な語彙は区別していない。}、105個の語彙項目\footnote{助詞や接尾辞などに対しては、
それぞれの語に対して文法的性質が記述されている。}からなる。
EDR日本語コーパス\cite{EDR}の文に対して98.4$\%$と、
非常に高い被覆率（構文木を一つでも返した文の割合）を示している。

文法自体は曖昧性解消の機構を持っていないため、SLUNGを用いて構文解析した場合、
文法的に許される全ての構文木が出力される。
本研究では、文節係り受けの統計モデルを用いることにより、
出力された構文木から最も優先度の高いものを選び出すことができるようになる。


\subsection{最大エントロピー法}\label{subsec:me}

統計モデルの推定に、最大エントロピー法(ME法)\cite{Berger96}を用いる。
ME法では、
「学習コーパス中の履歴の特定の条件を満たし、かつ特定の出力値を得る場合」
（素性）の頻度を得て、
様々な素性に対するパラメータを、
出力値の確率分布が最も一様分布に近づくように調整して求める。
別の素性に対し、それぞれ満たす集合に重なりがあってもよく、
抽象度の高い素性と低い素性を任意に混ぜることができるため、
統計モデルを構築する際のデータスパースネスの問題を軽減できる。
日本語係り受け解析でもME法は非常に有用で\cite{Uchimoto99}、
品詞の情報だけでなく、頻度の高い単語に対しては語彙的情報も加えるといった
柔軟な素性の追加が容易である。

本稿での実験における精度は、単純な相対頻度で推定した３つ組／４つ組モデル\cite{Kanayama99}
よりも約1.9$\%$向上しているが、
その要因として、ME法を用いることで以前よりも多くの素性を追加できたことが挙げられる。


\section{本研究の手法}\label{sec:ourmodel}

本節では、「３つ組／４つ組モデル」を用いて係り受け解析をする手順を解説する。
係り受け解析の全体の流れは図\ref{fig:flow}~のようになっている。

３つ組／４つ組モデルの準備として、
\ref{subsec:restrict}~節で述べる手法により、
各文節の係り先候補を３つ以下に制限する。
まず、文法を用いて、
各文節の係り先として文法的に正しいものを列挙する。
その中で係り元から一番近い文節・二番目に近い文節・
最も遠い文節を選び出し、他を無視する。
そして、係り先の候補の集合の中で、ある要素が係り先として選択される確率を、
係り元文節と全ての係り先の候補の属性を同時に考慮するモデル（３つ組／４つ組モデル）で推定する。
\ref{subsec:tripquad}~節では、モデルの特徴及び利点について述べる。
最後に、上記のモデルを用いて文全体の最適な係り受けを選択する方法を、
\ref{subsec:sentence}~節で解説する。


\begin{figure}[t]
	\begin{center}
	\small
	\setlength{\unitlength}{.35mm}
	\begin{picture}(360,180)
	
	\put(0,160){\framebox(100,10){文}}
	\put(50,160){\vector(0,-1){20}}
	\put(0,130){\framebox(100,10){可能な全ての構文木}}
	\put(100,135){\vector(1,0){40}}
	\put(140,130){\framebox(100,10){各文節の係り先候補}}
	\put(160,130){\vector(0,-1){50}}
	\put(140,70){\framebox(100,10){各係り受けの確率}}
	\put(160,70){\vector(-4,-1){80}}
	\put(50,130){\vector(0,-1){80}}
	\put(0,40){\framebox(100,10){統計値付き構文木}}
	\put(50,40){\vector(0,-1){20}}
	\put(0,10){\framebox(100,10){最適な構文木}}
	\put(100,15){\vector(1,0){40}}
	\put(140,10){\framebox(100,10){最適な文節係り受け}}

	\put(50,145){\makebox(30,10){\shortstack{文法}}}
	\put(160,115){\makebox(70,10){\shortstack{最大３つの候補}}}
	\put(160,85){\makebox(60,20){\shortstack{\bf ３つ組・\\ \bf ４つ組モデル}}}

	\put(260,120){\framebox(100,10){学習コーパス}}
	\put(310,100){\line(0,1){20}}
	\put(220,100){\line(1,0){90}}
	\put(220,100){\vector(0,-1){20}}
	\put(220,85){\makebox(80,10){\shortstack{ＭＥ法にて推定}}}

	\put(100,125){\makebox(40,10){\shortstack{変換}}}
	\put(100,5){\makebox(40,10){\shortstack{変換}}}

	\end{picture}
	\caption{係り受け解析の流れ}
	\label{fig:flow}
	\end{center}
\end{figure}


\subsection{準備：係り受け候補の制限}\label{subsec:restrict}

\subsubsection{文法の利用}

本システムでは、文を入力とし、JUMAN\cite{JUMAN}で形態素解析をした後、
文法SLUNG\cite{Mitsuishi98}で構文解析する。
SLUNGは、JUMANの形態素を解析の単位として、
文法的に正しい全ての構文木を出力する。
これを係り先候補の制限に使うために、
それぞれの構文木中の部分木を、図~\ref{fig:transform}のようにして、
文節単位の係り受け構造に帰着させる。
部分木{\sf M}の左部分木{\sf L}、右部分木{\sf R}の最も右側にある語を
それぞれ$l$, $r$とし、それらが属する文節を$b(l)$, $b(r)$とするとき、
$b(l)$は$b(r)$に係ることになる。

一つの構文木は一つの係り受け構造に対応するが、可能な構文木が複数あるため、
一つの係り元文節に対して、係り先候補となる文節が複数求まる。
以下では、その候補の中から正しいものを選び出すことを考える。

\begin{figure}[t]
	\begin{center}
	\small
	\setlength{\unitlength}{.25mm}
	\begin{picture}(180,55)
	\put(0,10){\line(2,1){80}}
	\put(30,25){\line(2,-1){30}}
	\put(80,10){\line(2,1){40}}
	\put(80,50){\line(2,-1){80}}
	\put(0,10){\line(1,0){60}}
	\put(80,10){\line(1,0){80}}
	\put(45,-5){\makebox(10,10){\small\shortstack{$l$}}}
	\put(145,-5){\makebox(10,10){\small\shortstack{$r$}}}
	\put(70,55){\makebox(20,10){\shortstack{\sf M}}}
	\put(15,28){\makebox(10,10){\shortstack{\sf L}}}
	\put(125,33){\makebox(10,10){\shortstack{\sf R}}}
	\end{picture}
	\begin{picture}(40,55)
	\put(0,20){\line(1,0){5}}
	\put(0,20){\line(0,1){5}}
	\put(5,15){\line(0,1){5}}
	\put(5,15){\line(2,3){5}}
	\put(0,25){\line(1,0){5}}
	\put(5,25){\line(0,1){5}}
	\put(5,30){\line(2,-3){5}}
	\end{picture}
	\begin{picture}(120,55)
	\put(0,10){\framebox(40,20){$b(l)$}}
	\put(80,10){\framebox(40,20){$b(r)$}}
	\put(20,30){\line(0,1){20}}
	\put(20,50){\line(1,0){80}}
	\put(100,50){\vector(0,-1){20}}	
	\end{picture}
	\caption
	{部分木から文節係り受けへの変換}
	\label{fig:transform}
	\end{center}
\end{figure}


人手で記述する文法を用いることには、
\ref{sec:introduction}~節で述べたような我々の最終目標に達するための要件である他に、
決してありえない構造を排除することができるという利点がある。
文法の制約が過剰でないことは、\ref{sec:related}~節で述べたように
SLUNGの被覆率が高いことが保証している。

\subsubsection{係り先候補の３つ以下への制限}

日本語の文節の係り先の傾向として、
(1)近くから遠くになるに従って割合が減少すること、
(2)最も遠い文節に係る場合だけは比較的多いことが知られている。
この傾向は例えば\cite{Maruyama92}で分析されている。

SLUNGにより係り先候補を絞った場合にもこの傾向はやはり顕著である。
EDRコーパスの文をSLUNGで解析した際の、
係り先候補の数、及び正しい係り先の位置の関係の分布を表\ref{tab:position}~に示す。
表中の「第一」「第二」…は、
文法で制限された係り先候補のうち、
係り元文節から近い順に何番目が正しい係り先であるかを意味する。
「最遠」は係り元から最も遠い候補である。
このデータより、
係り元文節から(1)最も近い文節・(2)二番目に近い文節・(3)最も遠い文節のいずれかに係る場合だけで
98.6$\%$を占める\footnote{これは、文法で候補を制限しているためである。
文法を用いずに３つの文節に絞っても、92.6$\%$程度しかカバーできない。}ことがわかる。

この性質を利用して、係り先の候補が４つ以上存在する場合にも上記の３文節だけを考え、
その他の文節を無視することにする。
この制限によって、係り受け精度の上限は98.6$\%$となるが、
わずか1.4$\%$の犠牲により
問題を大幅に単純化することができ、
次節で述べる３つ組／４つ組モデルの構成が可能になる。

\begin{table}[tb]
\begin{center}
\small
\begin{tabular}{|c|c||c|c|c|c|c|c||c|}
\hline 候補の数  & 比率 & {\bf 第一} & {\bf 第二} & 
第三 & 第四 & $..$ & {\bf 最遠} & 第一,第二,最遠 \\
\hline \hline
1     & 32.7 & 100  & $-$    & $-$    & $-$    & $-$      & \footnotesize (100) & 100  \\ \hline
2     & 28.1 & 74.3 & 26.7 & $-$    & $-$    & $-$      & \footnotesize (26.7) & 100  \\ \hline
3     & 17.5 & 70.6 & 12.6 &\footnotesize(16.8) &  $-$  & $-$ &  16.8 & 100  \\ \hline
4     & 9.9  & 70.4 & 11.1 &  4.7 &\footnotesize(13.8) & $-$ &  13.8   & 95.3 \\ \hline
5     & 5.4  & 70.1 & 11.6 &  4.2 & 2.5  &$..$& 11.5   & 93.2 \\ \hline
6以上 & 6.4  & 70.3 & 10.8 &  3.9 & 2.4  & $..$&  9.6   & 90.7 \\ \hline 
\hline
合計  & 100  & $-$    & $-$    &  $-$   & $-$ & $..$& $-$    & {\bf 98.6} \\ \hline
\end{tabular}
\caption{\small 係り先の候補の数に対する、正しい係り先の分布（単位は$\%$）\\
{\footnotesize 「比率」は、候補の数の分布を示す。括弧付きの値は他の項との重複を表す。}}
\label{tab:position}
\end{center}
\end{table}

\subsection{３つ組／４つ組モデル}\label{subsec:tripquad}

３つ組／４つ組モデルは、文節$i$が文節$t_n$に係る確率$P(i \rightarrow t_n)$を
式(\ref{equ:triplet2}),~式(\ref{equ:quadruplet2})で計算する。
但し、$t_n$は文節$i$の係り先の（３つ以下に限定された）候補、
$\Phi_i$は文節$i$の属性、$\Psi_{i,t_n}$は$t_n$及び二文節$i, t_n$間の属性を表す。
\begin{eqnarray}
P(i \rightarrow t_n) = & P(n \mid \Phi_i, \Psi_{i,t_1}, \Psi_{i,t_2}) & （係り先の候補が２つのとき : n=1,2）
\label{equ:triplet2}
 \refstepcounter{enums}
\\
P(i \rightarrow t_n) = & P(n \mid \Phi_i, \Psi_{i,t_1}, \Psi_{i,t_2}, \Psi_{i,t_3}) & （係り先の候補が３つのとき : n=1,2,3）
\label{equ:quadruplet2}
\end{eqnarray} \refstepcounter{enums}

このモデルの特徴は、上記の式から推測される通り、
「{\bf 係り元文節と、係り先の候補となる全ての文節の属性を同時に考慮}すること」、そして
「それぞれの係り先の候補の係りやすさを求めるのではなく、
{\bf 各候補が選ばれる確率}を求める」ことである。
これらの意義は次の３点にある。

\vskip 2mm
\begin{description}
\item[意義1]
文節間の距離でなく、係り先の{\bf 候補の中での相対的位置}を用いて係り先を選べること
\item[意義2]
着目している候補だけでなく、{\bf 文脈}、すなわち他の候補の属性を考慮できること
\item[意義3]
ある係り元に対する全ての候補への係りやすさを、{\bf 同じ条件の下で計算}できること
\end{description}
\vskip 2mm
以下で、これらの意義について順に述べる。

\subsubsection{意義1~:~候補の中での相対的位置}

文節間の距離は、係り受け解析における重要な要素として考えられているが、
係り先の候補の中の位置の方が重要な場合がある。
例として、(\ref{sent:karega})の各文における「彼が」の係り先を推定する時を考える。
両者とも、「走るのを」が正しい係り先と考えられる。

\vskip 2mm
\begin{tabular}{lll}
\refstepcounter{enums}\label{sent:karega}
 (\theenums) & a. & {\bf 彼が} \underline{走るのを} 見た ことが ありますか。\\
 & b. & {\bf 彼が} ゆっくり \underline{走るのを} 見た ことが ありますか。
\end{tabular}
\refstepcounter{equation}
\vskip 2mm

文法を用いずに文節数を距離とするモデルでは、
「彼が」と「走るのを」の文節間距離は
aでは1、bでは2と異なっている反面、
aでの「彼が → 見た」
\footnote{「→」は、二文節間の係り受けを表す。}
とbでの「彼が → 走るのを」が、
係り元からの距離が2である動詞であるという点で、
似た事象であると見なされる。

一方、文法で係り先を絞った場合、
a,~bとも「彼が」の係り先の候補は「走るのを」と「見た」の２つとなる。
このように、係り先の候補のみに着目すれば、両者を同じ事象として扱えるので、
より効率のよい学習が行えるようになる。


\subsubsection{意義2~:~文脈の考慮}

(\ref{sent:watashino})において、「私の」の係り先を考える。
正解は、それぞれ「娘に」「友人の」である。

\vskip 2mm
\begin{tabular}{lll}
\refstepcounter{enums} 
\label{sent:watashino}
(\theenums) & a. & {\bf 私の} かわいい \underline{娘に} 道で ばったり 会った。\\
            & b. & {\bf 私の} \underline{友人の} 娘に 道で ばったり 会った。
\end{tabular}
\refstepcounter{equation}
\vskip 2mm

係り元文節と係り先文節、及び文節間距離を考えるモデルでは、
a, bにおける「私の → 娘に」は区別されることなく、全く同じ係り受け確率が付与される。
しかしながら、この確率は非常に低くなる。なぜなら、
実際にEDRコーパスの一部を観察したところ、aの「${\sf N_1}$の ${\sf A}$ ${\sf N_2}$」
\footnote{{\sf N}と{\sf A}はそれぞれ名詞、形容詞を表す。}
という構文に対し、
bのような「${\sf N_1}$の ${\sf N_2}$ ${\sf N_3}$」の構文の頻度が４倍程度あり、
後者の構文では、${\sf N_1}$は近くの${\sf N_2}$を修飾する場合が約75％と、
圧倒的に多いからである。
従って、aにおいて、「私の → 娘に」に比べて
「私の → かわいい」の確率のほうが高くなり、解析誤りを引き起こす
\footnote{なお、「顔のかわいい女の子」のような構文では、
「顔の」は「かわいい」に係るのが正しいが、この構文が現れる頻度は低い。
このような場合は、語彙情報を加えることによって解決できよう。}。

係り元と係り先の３つの候補全てを同時に考慮すると、この誤りを防ぐことができる。
aにおいて「私の」と、その係り先候補である「かわいい」「娘に」「会った。」を同時に考えて、
三者のそれぞれが選ばれる確率を計算した場合、
第二候補であっても、第一候補の形容詞連体形よりも高い確率が割り当てられ、
正しく係り先を求めることができる。
このような現象は、
第一候補である形容詞や副詞を飛び越えて第二候補に係るケースなどで
一般的に数多く見受けられる。


\subsubsection{意義3~:~同じ条件下での係りやすさの計算}

これは意義2~とも関連するが、
ある一つの係り元に対する係り受けの確率を、
共通の前件を持った条件付き確率で計算できるという利点である。

(\ref{sent:watashino}a)の「私の」の係り先を考える際には、
従来の手法は式(\ref{equ:pairmodel})、我々の手法は式(\ref{equ:tqmodel})を求めることになる
\footnote{$\Psi$は文節間の属性も含むため、係り元文節にも依存するが、以下の式・図では省略している。}。
(\ref{equ:pairmodel})ではそれぞれの条件付き確率の前件が異なるため、５つの値の和は1にならないのに対し、
式(\ref{equ:tqmodel})では３つの和が1になる。
従って、３つ組／４つ組モデルにおいて
推定する条件付き確率は、
係り元とその係り先候補がある文脈において、
それぞれの係り先候補が選ばれる確率に一致することになる。
なお、考慮する条件を図示すると、それぞれ図\ref{fig:oldmodel}~、図\ref{fig:abcd}~のようになる。

\begin{figure}[t]
	\begin{center}
	\small
	\setlength{\unitlength}{.35mm}
	\begin{picture}(340,65)
	\put(0,15){\thicklines\framebox(40,10){\small 私の}}
	\put(60,15){\thicklines\framebox(40,10){\small かわいい}}
	\put(120,15){\thicklines\framebox(40,10){\small 娘に}}
	\put(180,15){\thicklines\framebox(40,10){\small 道で}}
	\put(240,15){\thicklines\framebox(40,10){\small ばったり}}
	\put(292,15){\thicklines\framebox(56,10){\small 会った。}}
	\put(26,25){\line(0,1){14}}
	\put(23,25){\line(0,1){17}}
	\put(20,25){\line(0,1){20}}
	\put(17,25){\line(0,1){23}}
	\put(14,25){\line(0,1){26}}
	\put(26,39){\line(1,0){54}}
	\put(23,42){\line(1,0){117}}
	\put(20,45){\line(1,0){180}}
	\put(17,48){\line(1,0){243}}
	\put(14,51){\line(1,0){306}}
	\put(80,39){\vector(0,-1){14}}
	\put(140,42){\vector(0,-1){17}}
	\put(200,45){\vector(0,-1){20}}
	\put(260,48){\vector(0,-1){23}}
	\put(320,51){\vector(0,-1){26}}
	\put(0, 0){\makebox(40,10){\shortstack{$\Phi_{私の}$}}}
	\put(60, 0){\makebox(40,10){\shortstack{$\Psi_{かわいい}$}}}
	\put(120, 0){\makebox(40,10){\shortstack{$\Psi_{娘に}$}}}
	\put(180, 0){\makebox(40,10){\shortstack{$\Psi_{道で}$}}}
	\put(240, 0){\makebox(40,10){\shortstack{$\Psi_{ばったり}$}}}
	\put(300, 0){\makebox(40,10){\shortstack{$\Psi_{会った。}$}}}
	\put(60, 39){\line(1,1){23}}
	\put(120,42){\line(1,1){20}}
	\put(180,45){\line(1,1){17}}
	\put(240,48){\line(1,1){14}}
	\put(300,51){\line(1,1){11}}
	\put(60,39){\circle*{2}}
	\put(120,42){\circle*{2}}
	\put(180,45){\circle*{2}}
	\put(240,48){\circle*{2}}
	\put(300,51){\circle*{2}}
	\put(80,59){\makebox(40,10){\shortstack{\tiny $P(私の \rightarrow かわいい)$}}}
	\put(140,59){\makebox(40,10){\shortstack{\tiny $P(私の \rightarrow 娘に)$}}}
	\put(200,59){\makebox(40,10){\shortstack{\tiny $P(私の \rightarrow 道で)$}}}
	\put(260,59){\makebox(40,10){\shortstack{\tiny $P(私の \rightarrow ばったり)$}}}
	\put(320,59){\makebox(40,10){\shortstack{\tiny $P(私の \rightarrow 会った。)$}}}
	\end{picture}
	\caption{従来のモデルで考慮する条件 \\
	{\footnotesize 各係り受けの確率が、それぞれの二文節の属性を用いて
	(\ref{equ:pairmodel})式で計算される。}}
	\label{fig:oldmodel}
	\end{center}
\end{figure}

\begin{figure}[t]
	\begin{center}
	\small
	\setlength{\unitlength}{.35mm}
	\begin{picture}(340,45)
	\put(0,15){\thicklines\framebox(40,10){\small 私の}}
	\put(60,15){\thicklines\framebox(40,10){\small かわいい}}
	\put(120,15){\thicklines\framebox(40,10){\small 娘に}}
	\put(180,15){\makebox(40,10){\shortstack{道で}}}
	\put(240,15){\makebox(40,10){\shortstack{ばったり}}}
	\put(292,15){\thicklines\framebox(56,10){\small 会った。}}
	\put(20,25){\line(0,1){10}}
	\put(30,35){\oval(20,20)[tl]}
	\put(30,45){\line(1,0){280}}
	\put(70,35){\oval(20,20)[tr]}
	\put(130,35){\oval(20,20)[tr]}
	\put(310,35){\oval(20,20)[tr]}
	\put(80,35){\vector(0,-1){10}}
	\put(140,35){\vector(0,-1){10}}
	\put(320,35){\vector(0,-1){10}}
	\put(80, 35){\line(1,1){15}}
	\put(140,35){\line(1,1){15}}
	\put(320,35){\line(1,1){15}}
	\put(80,35){\circle*{2}}
	\put(140,35){\circle*{2}}
	\put(320,35){\circle*{2}}
	\put(84,48){\makebox(40,10){\shortstack{\tiny $P(私の \rightarrow かわいい)$}}}
	\put(144,48){\makebox(40,10){\shortstack{\tiny $P(私の \rightarrow 娘に)$}}}
	\put(324,48){\makebox(40,10){\shortstack{\tiny $P(私の \rightarrow 会った。)$}}}
	\put(0, 0){\makebox(40,10){\shortstack{$\Phi_{私の}$}}}
	\put(60, 0){\makebox(40,10){\shortstack{$\Psi_{かわいい}$}}}
	\put(120, 0){\makebox(40,10){\shortstack{$\Psi_{娘に}$}}}
	\put(300, 0){\makebox(40,10){\shortstack{$\Psi_{会った。}$}}}
	\end{picture}
	\caption{３つ組／４つ組モデルで考慮する条件 \\
		{\footnotesize 係り元と３つに絞られた係り先候補の属性を用いて、
		それぞれの候補に係る確率を(\ref{equ:tqmodel})式で求める。}}
	\label{fig:abcd}
	\end{center}
\end{figure}

\begin{equation}
\left. \begin{array}{ll}
P(私の \rightarrow かわいい)  & = P(\mbox{T} \mid \Phi_{私の}, \Psi_{かわいい}) \\
P(私の \rightarrow 娘に)  & = P(\mbox{T} \mid \Phi_{私の}, \Psi_{娘に}) \\
P(私の \rightarrow 道で)  & = P(\mbox{T} \mid \Phi_{私の}, \Psi_{道で}) \\
P(私の \rightarrow ばったり)  & = P(\mbox{T} \mid \Phi_{私の}, \Psi_{ばったり}) \\
P(私の \rightarrow 会った。) \qquad  & = P(\mbox{T} \mid \Phi_{私の}, \Psi_{会った。})
\end{array}
\right\}
\nonumber \\
\qquad \qquad \qquad \quad \quad 
\label{equ:pairmodel}
\end{equation}\refstepcounter{enums}
\begin{equation}
\left. \begin{array}{ll}
P(私の \rightarrow かわいい) & = P(1 \mid \Phi_{私の}, \Psi_{かわいい}, \Psi_{娘に}, \Psi_{会った。}) \\
P(私の \rightarrow 娘に) & = P(2 \mid \Phi_{私の}, \Psi_{かわいい}, \Psi_{娘に}, \Psi_{会った。}) \\
P(私の \rightarrow 会った。)\qquad  & = P(3 \mid \Phi_{私の}, \Psi_{かわいい}, \Psi_{娘に}, \Psi_{会った。})
\end{array}
\right\}
\nonumber \\
\qquad
\label{equ:tqmodel}
\end{equation}\refstepcounter{enums}


\subsection{最適な係り受けの選択}\label{subsec:sentence}

各文節間の係りやすさ$P(i \rightarrow j)$を求めるにあたって、
係り元文節に対する係り先文節の候補の数に依って、次のようなモデルを用いることにする。

\begin{itemize}
\item
係り先候補が１つの場合：その係り先に確定するため、$P(i \rightarrow j) = 1.0$となる。
\item
係り先候補が２つの場合：係り元と２つの係り先の文節の情報を考慮する「３つ組モデル」を用いる。
\item
係り先候補が３つ以上の場合：係り先の候補のうち、係り元に最も近い文節、二番目に近い文節、
最も遠い文節の３つだけを考え、係り元とその３つの文節の情報を考慮する「４つ組モデル」を用いる。
\end{itemize}

こうして求まった値を用いて、SLUNGの出力した全ての部分木M
に対して、統計値$Q(\mbox{M})$を以下のようなアルゴリズムで割り振る。
なお、SLUNGの出力する構文木の終端記号は、
文節単位でなく、単語（JUMANの出力する形態素）を単位としている。

\begin{itemize}
\item
部分木Mがただ一つの単語からなる場合、$Q(\mbox{M})$ = 1.0
\item
そうでない場合、図\ref{fig:partialtree}~の部分木において、
左部分木Lの最も右側の単語を$l$、右部分木Rの最も右側の単語を$r$として、
$l$、$r$の属する文節をそれぞれ$b(l)$、$b(r)$とする。
このとき、
\begin{equation}
Q(\mbox{M}) = Q(\mbox{L}) \times Q(\mbox{R}) \times P(b(l) \rightarrow b(r))
\end{equation}\refstepcounter{enums}
\end{itemize}

文全体に対応する構文木で、この統計値が最大になるようなものを探索し、
その構文木を再び文節の係り受け関係に変換して出力する。
こうして得られた文の係り受けは、必ず文法的に正しい構文木に対応しており、
係り受け同士が交差することはない。


\begin{figure}[t]
	\begin{center}
	\small
	\setlength{\unitlength}{.35mm}
	\begin{picture}(140,55)
	\put(0,10){\line(2,1){70}}
	\put(30,25){\line(2,-1){30}}
	\put(80,10){\line(2,1){30}}
	\put(70,45){\line(2,-1){70}}
	\put(0,10){\line(1,0){60}}
	\put(80,10){\line(1,0){60}}
	\put(50,0){\makebox(10,10){\shortstack{$l$}}}
	\put(130,0){\makebox(10,10){\shortstack{$r$}}}
	\put(60,45){\makebox(20,10){\shortstack{M}}}
	\put(20,25){\makebox(10,10){\shortstack{L}}}
	\put(110,25){\makebox(10,10){\shortstack{R}}}
	\end{picture}
	\caption{SLUNGの出力する部分木M}
	\label{fig:partialtree}
	\end{center}
\end{figure}

\section{実験結果}\label{sec:result}

３つ組／４つ組モデルを用いた係り受け解析の実験環境と用いた素性、
及び実験結果を示す。
さらに、学習コーパスの量を変えた実験や、
３つ組／４つ組モデルを導入したことの効用を確かめるための対照実験の結果を載せる。

\subsection{実験環境}\label{subsec:env}

EDR日本語コーパス\cite{EDR}の208,157文
\footnote{このうち、括弧付けの順番が逆転している5,263文は除外した。}のうち、
192,778文を学習、3,372文をテストに用いた。
\ref{subsec:restrict}節で述べたような観察や、
次節で述べる考察などにはその他の6,744文を用いている。
これは、テストコーパスの解析結果を人が見てモデルを修正することによる
コーパスへの特化を防ぐためである。

前節で述べた通り、係り先の候補が２つの場合のための「３つ組モデル」と
候補が３つ以上の場合のための「４つ組モデル」の二つのモデルを別個に作る。
学習コーパス中の文をSLUNGで構文解析して、
係り先候補が２つである文節に対して、
係り元文節と２つの係り先候補の属性の組を履歴として「３つ組モデル」を構成する。
そして、係り先候補が３つ以上である文節に対しては、
\ref{subsec:restrict}節で述べた方法で候補を３つに制限し、
係り元文節と３つの係り先候補の属性の組を履歴として「４つ組モデル」を構成する。
これらは最大エントロピー法のツール 
ChoiceMaker Maximum Entropy Estimator\cite{Borthwich99}を使って推定される。

推定の際に用いた素性を表\ref{tab:features}~に示す。
素性の値は\cite{Haruno98}~\cite{Uchimoto99}に倣っており、
品詞の分類などにはJUMANの出力結果を用いている。但し、
京大コーパスを用いた実験と違って、
形態素解析の正解は与えられておらず、誤りを含む場合がある。

以下で各素性について解説する。なお、
{\bf 主辞}とは、
品詞大分類が「特殊」「助動詞」「助詞」「接尾辞」「判定詞」の
いずれかであるものを除いて、文節内で最も右側にある語、
{\bf 語形}とは、
品詞大分類が「特殊」であるものを除いて、文節内で最も右側にある語である。


\begin{description}
\item[品詞] 語形・主辞ともに、JUMANの品詞細分類が用いられる。
\item[助詞・副詞] 頻度の高い26種の助詞と69種の副詞。
\item[主辞語彙] 品詞に依らず、主辞として現れる語のうち頻度の高い294種の語彙。
\item[語形語彙] 品詞が「助動詞」「接尾辞」のうち、頻度の高い70種の語彙。
\item[活用形] JUMANの出力する活用形を、
「基本形」「連用形」「連体形」「テ形」「タ形」「その他」の６種に分類したもの。
\item[文節間読点の数・「は」の数]
係り元と係り先の文節間にある読点の数を、「0」「1」「2」「3以上」の４値で表す。
同様に、副助詞「は」の数を 「0」「1」「2以上」の３値で表す。
\end{description}

表\ref{tab:features}~中の「異なり数」とは各素性の取りうる値の総数であり、
素性番号19〜27の組み合わせ素性に関しては、それぞれの要素の積を記してある。
実際には、履歴の数と出力値の数（2または3）の積だけの素性が用いられる。
また、係り先に関する素性（素性番号8〜27）は、それぞれの係り先候補（３つ組モデルでは２つ、
４つ組モデルでは３つ）に対して素性が割り振られる
\footnote{例として、14の係り先読点の素性は、
３つ組モデルに対しては、２つの候補それぞれに対して
読点の有無を考え、さらに「第一候補に係る場合」「第二候補に係る場合」の
二つの出力値があるため、$2 \times 2 \times 2 = 8$、
同様に４つ組モデルに対しては $2 \times 3 \times 3 = 18$の素性がある。}。
このうち、コーパス中で３回以上出現したものが有効素性となる。

\begin{table}
	\begin{center}
	\begin{tabular}{|c|l|r|r|r|}
	\hline 
	\multicolumn{1}{|c|}{素性} &
	\smash{\lower2.0ex\hbox{素性の種類}} & 
	\smash{\lower2.0ex\hbox{異なり数}} &
	\multicolumn{2}{c|}{有効素性数} \\
	\cline{4-5} 番号 & & & ３つ組 & ４つ組 \\
	\hline \hline
	1 & 係り元主辞品詞 & 24 & 42 & 64 \\ \hline
	2 & 係り元語形品詞 & 34 & 66 & 99 \\ \hline
	3 & 係り元助詞 & 27 & 47 & 73 \\ \hline
	4 & 係り元副詞 & 70 & 131 & 193 \\ \hline
	5 & 係り元語形語彙 & 71 & 110 & 225 \\ \hline
	6 & 係り元活用形 & 6 & 12 & 18 \\ \hline
	7 & 係り元読点の有無 & 2 & 4 & 6 \\ \hline \hline
	8 & 係り先主辞品詞 & 24 & 70 & 158 \\ \hline
	9 & 係り先語形品詞 & 34 & 96 & 231 \\ \hline
	10 & 係り先主辞語彙 & 295 & 1164 & 2597 \\ \hline
	11 & 係り先助詞 & 27 & 92 & 204 \\ \hline
	12 & 係り先語形語彙 & 71 & 216 & 454 \\	\hline
	13 & 係り先活用形 & 6 & 24 & 53 \\ \hline
	14 & 係り先読点の有無 & 2 & 8 & 18 \\ \hline
	15 & 係り先「は」の有無 & 2 & 8 & 18 \\ \hline
	16 & 係り先引用「と」の有無 & 2 & 6 & 17 \\ \hline
	17 & 文節間読点の数 & 4 & 16 & 36 \\ \hline
	18 & 文節間「は」の数 & 3 & 12 & 27 \\ \hline \hline
	19 & 係り元語形品詞×係り先主辞品詞 & 816 & 1187 & 2727 \\ \hline 
	20 & 係り元語形品詞×係り元読点×係り先読点 & 136 & 380 & 870 \\ \hline 
	21 & 係り元助詞×係り先主辞語彙 & 7965 & 6465 & 13463 \\ \hline
	22 & 係り元語形品詞×係り先語形品詞 & 1156 & 1213 & 3108 \\ \hline
	23 & 係り元助詞×係り先助詞 & 729 & 618 & 1637 \\ \hline
	24 & 係り元語形品詞×係り先助詞 & 918 & 1025 & 2494 \\ \hline
	25 & 係り元語形品詞×係り先語形語彙 & 2414 & 1483 & 3514 \\ \hline
	26 & 係り元語形品詞×助詞×読点の有無×係り先主辞品詞 & 132192 & 1331 & 3058 \\ \hline
	27 & 係り元主辞品詞×語形品詞×活用形×係り先主辞品詞×活用形 & 705024 & 6605 & 14700 \\ \hline
	\hline & 合計 & - & 22433 & 50063 \\ \hline
	\end{tabular}
	\caption{用いた素性 \\
	{\footnotesize 8番以降の素性は、係り先に関する素性なので、
	２つまたは３つの全ての候補に対して考える。}} \label{tab:features}
	\end{center}
\end{table}

\subsection{実験結果}\label{subsec:result}

\ref{subsec:env}~に記したコーパスに対する、次の２つの精度を
測定した結果を表\ref{tab:result}~に示す。

\begin{description}
\item[文節正解率] 
	文中の最後の文節を除く全ての文節に対して、
	その係り先が正解と一致する割合。
	表\ref{tab:result}~においてのみ、
	後ろから二番目の文節（可能な係り先が最後の文節のみであるので、
	必ず正解する）を除外した値を参考のために載せてある。
\item[文正解率]
	一文中の係り受けが全て正解する文の割合。
	なお、テストコーパスの平均文節数は8.82である。
\end{description}

なお、「解析成功文」とは、
テストコーパスのうち構文解析が成功した文、
即ちSLUNGが少なくとも一つの構文木を返した3,326文
（全体の98.63$\%$にあたる）に対する正解率を測ったものである。
また、参考のためにコーパス中の「すべての文」に対しての精度も測っている。
SLUNGでの構文解析が失敗した文に関しては、
各係り元文節に対して最も高い確率が割り振られた候補を決定的に係り先と判定し、
どの候補にも係り得ないとされた文節は隣の文節を修飾すると仮定して正解率を測った。

表\ref{tab:result}~は学習コーパスの約19万文を全て用いた時の値である。
学習コーパスの量を変えた時の解析成功文に対する文節正解率を図\ref{fig:graph}~に示す。

\begin{table}
	\begin{center}
	\begin{tabular}{|l|l|rc|}
	\hline
	\smash{\lower3.0ex\hbox{解析成功文}} & 
	文節正解率 & {\bf 88.55$\%$} & (23078/26062) \\
	& 　　　（後ろから二番目の文節を除く場合） & 86.88$\%$ & (19752/22736) \\
	\cline{2-4}
	& 文正解率   & 46.90$\%$ & (1560/3326) \\
	\hline
	\smash{\lower3.0ex\hbox{すべての文}} & 
	文節正解率 & 88.33$\%$ & (23350/26436) \\
	& 　　　（後ろから二番目の文節を除く場合） & $86.62\%$ & (19978/23064) \\
	\cline{2-4}
	& 文正解率   & 46.35$\%$ & (1563/3372) \\
	\hline
	\end{tabular}
	\caption{学習に19万文を用いたときの解析精度}
	\label{tab:result}
	\end{center}
\end{table}

\begin{figure}[t]
	\begin{center}
	\small
	\setlength{\unitlength}{.15mm}
	\begin{picture}(600,270)
	\put(30,30){\vector(1,0){600}}
	\put(30,30){\vector(0,1){260}}
	\put(60,49){\circle*{7}}
	\put(90,91){\circle*{7}}
	\put(120,120){\circle*{7}}
	\put(150,142){\circle*{7}}
	\put(180,146){\circle*{7}}
	\put(210,166){\circle*{7}}
	\put(240,177){\circle*{7}}
	\put(270,185){\circle*{7}}
	\put(300,184){\circle*{7}}
	\put(330,185){\circle*{7}}
	\put(360,186){\circle*{7}}
	\put(390,182){\circle*{7}}
	\put(420,181){\circle*{7}}
	\put(450,188){\circle*{7}}
	\put(480,198){\circle*{7}}
	\put(510,195){\circle*{7}}
	\put(540,203){\circle*{7}}
	\put(570,209){\circle*{7}}
	\put(600,205){\circle*{7}}
	\path(60,49)(90,91)(120,120)(150,142)(180,146)(210,166)
		(240,177)(270,185)(300,184)(330,185)(360,186)
		(390,182)(420,181)(450,188)(480,198)(510,195)
		(540,203)(570,209)(600,205)
 	\multiput(30,50)(10,0){59}{\line(1,0){3}}
 	\multiput(30,150)(10,0){59}{\line(1,0){3}}
 	\multiput(30,250)(10,0){59}{\line(1,0){3}}
	\put(-10,45){\makebox(30,10){87.0}}
	\put(-10,145){\makebox(30,10){88.0}}
	\put(-10,245){\makebox(30,10){89.0}}
	\put(-10,265){\makebox(30,10){($\%$)}}
	\put(160,10){\makebox(40,10){5}}
	\put(310,10){\makebox(40,10){10}}
	\put(460,10){\makebox(40,10){15}}
	\put(600,10){\makebox(40,10){（万文）}}
	\end{picture}
	\caption{学習コーパスの量と文節正解率の関係}
	\label{fig:graph}
	\end{center}
\end{figure}

\subsection{対照実験}\label{subsec:control_exp}

\ref{sec:ourmodel}~節で述べた３つ組／４つ組モデルの有効性を示すために、
以下のような対照実験を行った。
これらのモデルでは、
他の統計的係り受け解析モデル\cite{Fujio99}~\cite{Haruno98}~\cite{Uchimoto99}と同様に、
二つの文節及び文節間の属性から、二文節間の係りやすさを独立に計算する。
また、係り先候補の中での位置を出力とする代わりに、
係り元と係り先の文節間の距離（「１」「２から５」「６以上」の３値）を導入している。
ME法による推定において\ref{subsec:env}~節に示した素性と同じ素性を使っており、
その全てに対して上記の距離の属性を組み合わせている。

\begin{description}
\item[文法なしモデル]
文法を用いて候補を絞ることをせず、係り元文節より右側の全ての文節に対して統計値を求める。
係り元・係り先文節の属性と文節間距離などを用いて、
二文節があった時にそれが係り受け関係にある確率を計算する。
これは概ね、他の研究と同様のモデルである。
\item[候補限定なしモデル]
構文解析の結果文法が許した係り先に対してのみ、
文法なしモデルと同様、係り元・係り先属性と文節間距離から係る確率を求める。
\item[２つ組モデル]
文法が許す係り先候補を、
\ref{subsec:restrict}~節で述べた方法で３つに絞って、その３つに対してのみ統計値を求める。
上記のモデルと同様、係り元・係り先属性と文節間距離から、係る確率を求める。
なお、考慮する係り先候補は３つ組／４つ組モデルの時と同じになる。
\end{description}

対照実験の結果は表\ref{tab:control_exp}~の通りである。
「３つ組／４つ組モデル」は「２つ組モデル」と比べて
精度が0.9$\%$ほど向上している。
このデータから、３つ組／４つ組モデルが有効であることを
次節にて論じる。

\begin{table}[t]
	\begin{center}
	\begin{tabular}{|l|cccc|rc|rc|}
	\hline
	 & G & H & T & D &
	   \multicolumn{2}{c|}{解析成功文} & 
	   \multicolumn{2}{c|}{すべての文} \\
	\hline \hline
	文法なしモデル & $-$ & $-$ & $-$ & $+$ & 86.70$\%$ & (22594/26062) & 86.61$\%$ & (22895/26436) \\
	\hline
	候補限定なしモデル & $+$ & $-$ & $-$ & $+$ & 87.37$\%$ & (22770/26062) & $87.18\%$ & (23046/26436) \\
	\hline
	２つ組モデル & $+$ & $+$ & $-$ & $+$ & 87.67$\%$ & (22849/26062) & $87.49\%$ & (23128/26436) \\ 
	\hline
	３つ組／４つ組モデル & + & + & + & $-$ & 88.55$\%$ & (23078/26062) 
		& 88.33$\%$ & (23350/26436) \\
	\hline
	\end{tabular}
	\caption{対照実験の結果（文節正解率）\\
	{\footnotesize G, H, T, D はそれぞれ「文法の利用」「候補を３つに限定」「３つ組／４つ組モデル」} \\
	{\footnotesize 「文節間距離属性の利用」の有無を表す。}}
	\label{tab:control_exp}
	\end{center}
\end{table}


\section{考察}\label{sec:observations}

ここでは、本稿で提案する手法がどのように精度向上に寄与しているかの観察、
及び他研究との比較を行う。

\subsection{「３つ組／４つ組モデル」の効用}

表\ref{tab:control_exp}~にある対照実験の結果は、
以下の理由から３つ組／４つ組モデルの有効性を示しているといえる。

\begin{itemize}
\item
「３つ組／４つ組モデル」の精度は「２つ組モデル」の精度よりも
約0.9$\%$上回っている。
両者とも、文法とヒューリスティクスにより係り先候補を３つ以下に限定しているが、
それらの係り先候補を同時に考慮するモデルを用いた方が精度が上がることが確認された。
\item
「２つ組モデル」は、「文法なしモデル」より1.0$\%$、「候補限定なしモデル」よりも
0.3$\%$高い精度を出している。
従って、文法を用いることや係り先候補を３つに限定することは
妥当な措置であり、「２つ組モデル」は「３つ組／４つ組モデル」の比較対象として適当である。
\end{itemize}

次に、両者のモデルで実際に解析を行う時の、
具体的なMEのパラメータを観察してみる。
例として、文(\ref{sent:kodomo})
の「子供たちの」の各候補への係りやすさを計算する。
「子供たちの」の係り先候補は、「甲高い」「声で」「騒然となる。」の３文節で、
正解は「声で」である。

\vskip 2mm
\refstepcounter{enums}\label{sent:kodomo}
(\theenums)  そんなとき、 {\bf 子供たちの} 甲高い \underline{声で} 騒然となる。
\refstepcounter{equation}
\vskip 2mm

各候補への係りやすさを２つ組モデル・４つ組モデル
\footnote{候補数が３なので、４つ組モデルが使われる。}で推定する際の
ME法のパラメータ$\alpha_k$のうち主な（$| \log\alpha_k |$が大きい）ものを、
それぞれ表\ref{tab:pair_me}~,表\ref{tab:quad_me}~に示す。
パラメータ$\alpha_k$のうち、履歴$a$、出力値$b$に対応する素性のものを掛け合わせるので、
$\alpha_k$の値が1.0より大きいものは出力値を$b$にすることを助長するパラメータ、
1.0より小さいものは$b$にすることを抑制するパラメータである。
「$\alpha_k$の積」の項は、
表に載せていないものも含め、
対応する出力値に関する全てのパラメータの積である。

\subsubsection{２つ組モデルの場合}

このモデルでは、係り先ごとに別々の条件で係りやすさを計算する。
各係り先への係りやすさ$P(i \rightarrow j)$は、
出力値Ｔに対する$\alpha_k$の積を、
出力値Ｔ,~Ｆに対する$\alpha_k$の積の和で割ったものである。
例えば、$P(子供たちの \rightarrow 甲高い)$は、
$0.93/(0.93+0.81)=0.53$となる。

「声で」に係る場合のパラメータに注目すると、
係り元助詞「の」は隣の文節に係る傾向が強いことから、
文節間距離が「２から５」に対するパラメータが小さくなっている。
そのため、「甲高い」に係る確率の方が高くなってしまう。

\subsubsection{４つ組モデルの場合}

全ての係り先への係りやすさを共通の確率分布を用いて計算する。
出力値$b$は$1,2,3$の３値をとり、
第一候補への係りやすさ$P(i \rightarrow t_1)$は
出力値１に対する$\alpha_k$の積を、
３つの出力値に対する$\alpha_k$の積の和で割ったものであり、
表\ref{tab:quad_me}~の例では
$0.682/(0.682+2.39+0.106)=0.215$となる。

出力値が2となる場合のパラメータに着目する。
係り元が「の」で、第一候補が「形容詞」であること、
第二候補が「名詞」であること、第三候補が「形容詞」であることの全てが
第二候補に係るパラメータを高めており、第二候補に係る確率が
第一候補に係る確率を上回っている。
特に、出力値$b$と異なる候補（この場合、第一・第三候補）に関係する素性
も強い影響を及ぼしていることが興味深い。

\begin{table}[t]
	\scriptsize
	\begin{center}
	\begin{tabular}{|p{.8cm}||c|p{5cm}|p{.7cm}|p{1.15cm}|r|r|}
	\hline
	係り先$j$ & 
	\smash{\lower1.6ex\hbox{素性番号}} & 
	\smash{\lower1.6ex\hbox{履歴$a$}} & 
	出力値$b$ & パラメータ$\alpha_k$ & 
	\smash{\lower1.6ex\hbox{$\alpha_k$の積}} & 
	\smash{\lower1.6ex\hbox{$P(i \rightarrow j)$}} \\
	\hline \hline
	甲高い & 26 & 係り元語形「接続助詞」「の」・読点「無」・係り先主辞「形容詞」・距離「１」 
	& T & 0.83 & 0.93 & 0.53 \\
	\cline{2-6}
	& 6 & 係り先活用形「基本形」・距離「１」 & F & 0.69 & 0.81 & \\
	\cline{2-5}
	& 26 & 係り元語形「接続助詞」「の」・読点「無」・係り先主辞「形容詞」・距離「１」 & F & 1.19 & & \\
	\cline{2-5}
	& 27 & 係り元主辞「普通名詞」・語形「接続助詞」・係り先主辞「形容詞」「基本形」・距離「１」 & F & 0.81 & & \\
	\hline \hline
	声で & 3 & 係り元助詞「の」・距離「２〜５」 & T & 0.78 & 0.57 & 0.31 \\
	\cline{2-5}
	& 10 & 係り先主辞「声」・距離「２〜５」 & T & 0.79 & & \\
	\cline{2-5}
	& 23 & 係り元助詞「の」・係り先助詞「で」・距離「２〜５」 & T & 1.82 & & \\
	\cline{2-5}
	& 26 & 係り元語形「接続助詞」・係り先品詞「名詞」・距離「２〜５」 & T & 0.84 & & \\
	\cline{2-5}
	& 27 & 係り元主辞「普通名詞」・係り元語形「接続助詞」「の」・係り先主辞「普通名詞」・距離「２〜５」 & T & 0.81 & & \\
	\cline{2-6}
	& 27 & 係り元主辞「普通名詞」・係り元語形「接続助詞」「の」・係り先主辞「普通名詞」・距離「２〜５」 & F & 1.06 & 1.26 & \\
	\hline \hline
	騒然となる & 3 & 係り元助詞「の」・距離「２〜５」 & T & 0.78 & 0.11 & 0.10 \\
	\cline{2-5}
	& 8 & 係り先主辞「形容詞」・距離「２〜５」 & T & 0.86 & & \\
	\cline{2-5}
	& 26 & 係り先語形「接続助詞」「の」・読点「無」・係り先主辞「形容詞」・距離「２〜５」 & T & 0.48 & & \\
	\cline{2-6}
	& 26 & 係り元語形「接続助詞」「の」・読点「無」・係り先主辞「形容詞」・距離「２〜５」 & F & 1.08 & 1.03 & \\
	\hline
	\end{tabular}
	\caption{(\ref{sent:kodomo})の「子供たちの」の係り先推定の際の、２つ組モデルにおけるＭＥのパラメータ \\
	{\footnotesize それぞれの係り先に対して、別個に「係る（Ｔ）」「係らない（Ｆ）」を出力値とする
		パラメータが計算される。}}
	\label{tab:pair_me}
	\end{center}
\end{table}

\begin{table}[t]
	\scriptsize
	\begin{center}
	\begin{tabular}{|c|p{6cm}|p{.7cm}|p{1.15cm}|r|r|}
	\hline
	\smash{\lower1.6ex\hbox{素性番号}} & 
	\smash{\lower1.6ex\hbox{履歴$a$}} & 
	出力値$b$ & 
	パラメータ$\alpha_k$ & 
	\smash{\lower1.6ex\hbox{$\alpha_k$ の積}} & 
	\smash{\lower1.6ex\hbox{$P(i \rightarrow t_b)$}}\\
	\hline \hline
	10-2 & 第二候補主辞語彙「声」 & 1 & 0.83 & 0.682 & 0.215 \\
	\cline{1-4}
	21-1 & 係り元助詞「の」・第一候補主辞語彙「その他」 & 1 & 0.78 & & \\
	\cline{1-4}
	26-1 & 係り元語形「接続助詞」「の」・読点「無」・第一候補主辞「形容詞」 & 1 & 0.84 & & \\
	\hline
	11-2 & 第二候補助詞「で」 & 2 & 1.29 & 2.39 & 0.752 \\
	\cline{1-4}
	13-1 & 第一候補活用形「基本形」 & 2 & 0.77 &  &  \\
	\cline{1-4}
	26-1 & 係り元語形「接続助詞」「の」・読点「無」・第一候補主辞「形容詞」 & 2 & 1.23 & & \\
	\cline{1-4}
	26-2 & 係り元語形「接続助詞」「の」・読点「無」・第二候補主辞「名詞」 & 2 & 1.25 & & \\
	\cline{1-4}
	26-3 & 係り元語形「接続助詞」「の」・読点「無」・第三候補主辞「形容詞」 & 2 & 1.24 & & \\
	\cline{1-4}
	27-1 & 係り元主辞「普通名詞」・語形「接続助詞」・第一候補主辞「形容詞」「基本形」 & 2 & 0.84 & & \\
        \hline
	3 & 係り元助詞「の」 & 3 & 0.59 & 0.106 & 0.034 \\
	\cline{1-4}
	7 & 係り元読点「無」 & 3 & 0.84 & & \\
	\cline{1-4}
	10-2 & 第二候補主辞語彙「声」 & 3 & 2.15 & & \\
	\cline{1-4}
	11-2 & 第二候補助詞「で」 & 3 & 0.46 & & \\
	\cline{1-4}
	17-3 & 係り元-第三候補文節間読点「無」 & 3 & 1.40 & & \\
	\cline{1-4}
	20-3 & 係り元語形品詞「形容詞」・第三候補読点「無」 & 3 & 0.80 & & \\
	\cline{1-4}
	27-2 & 係り元主辞「普通名詞」・語形「接続助詞」・第二候補主辞「名詞」 & 3 & 0.79 & & \\
	\cline{1-4}
	27-3 & 係り元主辞「普通名詞」・語形「接続助詞」・第三候補主辞「形容詞」「基本形」 & 3 & 0.70 & & \\
	\hline
	\end{tabular}
	\caption{(\ref{sent:kodomo})の「子供たちの」の係り先推定の際の、４つ組モデルにおけるＭＥのパラメータ \\
	{\footnotesize 各候補が選ばれる（出力値が1,2,3）場合のパラメータを一つの確率分布で求めている。}}
	\label{tab:quad_me}
	\end{center}
\end{table}


\subsection{他研究との比較}
\subsubsection{EDRコーパスでの精度の比較}

係り受けの精度判定にEDRコーパスを用いている他研究と比較してみる。
決定木を用いた手法\cite{Haruno98}での精度は84〜85$\%$、
語の共起確率を用いた手法\cite{Fujio99}では、86.8$\%$となっている。
我々の手法はこれらを上回っており、
EDRコーパスに対してテストした中では
最も高い水準といえよう。

また、\cite{Kanayama99}では、３つ組／４つ組モデルを
単純な相対頻度を用いて構成している。
そこでの精度は86.7$\%$であり、ME法の利用によって
約1.9$\%$精度が向上したことになる。
精度向上の要因は、
ME法によってデータスパースネスの問題が軽減でき、
従来は入れられなかった語彙や活用に関する素性を追加できたことであると思われる。


\subsubsection{京大コーパスでの精度の比較}


いくつかの研究では、京大コーパス\cite{kc}を用いて精度を測っている。
構文的・語彙的情報を統合して構文木の生起確率を求めている手法\cite{Shirai98}
での精度は85〜86$\%$である。

本研究と同様に、ME法を用いた研究~\cite{Sekine99},~\cite{Uchimoto99b}
では、京大コーパスの1月9日分の1,246文を用いている。
比較のために、同じコーパスでテストした結果は、
表\ref{tab:accuracy_kc}~のようになった。

\begin{table}
	\begin{center}
	\begin{tabular}{|l|l|rc|}
	\hline
	\smash{\lower2.0ex\hbox{解析成功文}} & 
		文節正解率 & 87.08$\%$ & (8299/9530) \\
			\cline{2-4}

&		文正解率   & 44.70$\%$ & (493/1103) \\
	\hline
	\end{tabular}
	\caption{EDRコーパスで学習し、京大コーパスでテストした際の解析結果}
	\label{tab:accuracy_kc}
	\end{center}
\end{table}

文末から決定的に係り先を決定するモデル\cite{Sekine99}の精度は87.14$\%$で我々と同程度、
後方文脈を考慮するモデル\cite{Uchimoto99b}は87.93$\%$で我々の精度よりも高くなっている。
その原因として、以下のことが考えられる。

\begin{itemize}
\item
我々は、学習データとしてEDRコーパスを用いている。
\cite{Uchimoto99b}などと比べて約24倍の学習データがあるとはいえ、
括弧付けの方針の違いなどから、
京大コーパスでの解析の誤りを引き起こすことが多い。
\item
関根ら、内元らは京大コーパス中にある形態素解析・文節区切りの結果を用いているのに対し、
我々はJUMANで解析したものを用いているため、形態素解析の誤りを含み、
解析誤りの原因となっている。
\item
文法SLUNGがEDRコーパスの括弧付けの方針に従って作られており、
京大コーパスにあるような係り方を許さない場合がある。
\end{itemize}

現在のところ、京大コーパスの解析には被覆率・精度ともに充分でないが、
文法やシステムの改変により対処した上で
本論文で提案する手法を有効に適用できるようにすれば、
より高い精度が得られると考えている。

\subsubsection{学習量の比較}

図\ref{fig:graph}~より、最高値に近い精度を得るためには、
10〜15万文の学習コーパスを要している。
この学習量は、EDRコーパスを用いている研究\cite{Fujio99}と同程度であり、
\cite{Uchimoto99}などの京大コーパスを用いた場合より、
20倍程度の学習量になっている。

一般に、ME法を用いることにより学習量を減らすことができると考えられているが、
３つ組／４つ組モデルでは、複数の係り先に関する属性を同時に捉える
条件付き確率を用いているため、
区別される事象の数が大きくなり、多くの学習量が必要になっている。

我々のモデルは、EDRコーパスのような多くの学習データを有効に
利用できるモデルであるといえる反面、京大コーパスのように
学習データ量が限られている時には、
より効率のよい素性選択などが要求されるであろう。



\subsubsection{解析速度の比較}

本研究での係り受け解析は、あくまで詳細な構文構造を得るという目標の
前段階であるため、速度に焦点を当ててはいないが、参考のために比較しておく。

文末から決定的に係り先を決定するモデル\cite{Sekine99}では、
一文当たり平均0.03秒（Sun Ultra10, 300MHz）で解析できるのに対し、
一方、我々のシステムではEDRコーパスの文に対して平均約0.5秒
（Pentium III, 500MHz : 経験的に、上記の計算機の約３倍の速度）を要する。
両者には大きな差があるものの、我々の速度も非実用的なものではない。

また、そのほとんどはHPSGパーザによる部分木の生成の時間である。
単に係り受け構造を求めるだけなら速度を向上する余地は多分にあるうえ、
HPSGパーザ自体の高速化も研究されており\cite{Nishida99,Torisawa00}、
速度の問題は深刻であるとは考えていない。



\section{まとめ}\label{sec:conclusion}

本稿では、文法を用いて係り受け解析をする際に望ましい統計モデルについて論じた。
係り先の候補を文法が許すものに制限した後、
係り元から最も近い文節・二番目に近い文節・最も遠い文節のみに絞る。
これにより、係り元と全ての係り先候補の属性を同時に考慮する
「３つ組／４つ組モデル」を用いることができるようになり、
88.6$\%$という高い係り受け精度を達成した。
また、このモデルが精度向上に確かに寄与していることを示した。



\bibliographystyle{jnlpbbl}
\bibliography{nlp-j}


\begin{biography}
\biotitle{略歴}
\bioauthor{金山 博}{
1998年東京大学理学部情報科学科卒業。
2000年同大学院理学系研究科修士課程修了。
同年日本アイ・ビー・エム（株）入社、
現在に至る。
構文解析・機械翻訳等に関する研究に従事。
}

\bioauthor{鳥澤 健太郎}{
1992年東京大学理学部情報科学科卒業。
1995年同大学大学院理学系研究科情報科学専攻博士課程退学、
同年より同大学大学院理学系研究科情報科学専攻助手。
1998年より科学技術振興事業団さきがけ研究21研究員兼任。
計算言語学の研究に従事。理学博士。言語処理学会会員。
}

\bioauthor{光石 豊}{
1996年東京大学理学部情報科学科卒業。
1998年同大学院理学系研究科修士課程修了。
現在、同博士課程在学中。
HPSGの枠組による日本語文法に関する研究に従事。
ACM SIGMODJ学生会員。
}

\bioauthor{辻井 潤一}{
1971年京都大学工学部電気工学科卒業。1973年同大学大学院工学研究科修士課程修了。
京都大学工学部電気第2工学科助手、
助教授を経て、1988年英国マンチェスタ大学科学技術研究所(UMIST)教授、
1995年より、東京大学大学院理学系研究科教授（情報科学専攻）、現在に至る。
1981〜82年フランス・CNRS（グルノーブル）、招聘研究員。 
自然言語処理、機械翻訳の研究に従事。工学博士。
国際計算言語学委員会(ICCL)メンバ、情報処理学会、人工知能学会など、会員。
2000年6月より言語処理学会会長。
}

\bioreceived{受付}
\biorevised{再受付}
\bioaccepted{採録}

\end{biography}

\newpage
\thispagestyle{plain}

\verb+ +

\end{document}

