



\documentstyle[epsbox,jnlpbbl]{jnlp_j_b5}

\setcounter{page}{163}
\setcounter{巻数}{7}
\setcounter{号数}{4}
\setcounter{年}{2000}
\setcounter{月}{10}
\受付{2000}{2}{23}
\再受付{2000}{4}{20}
\採録{2000}{6}{30}

\setcounter{secnumdepth}{2}

\title{コーパスからの語順の学習}
\author{内元 清貴\affiref{CRL} \and 村田 真樹\affiref{CRL} 
  \and 馬 青\affiref{CRL} \and 関根 聡\affiref{NYU} 
  \and 井佐原 均\affiref{CRL}}

\headauthor{内元，村田，馬，関根，井佐原}
\headtitle{コーパスからの語順の学習}

\affilabel{CRL}{郵政省通信総合研究所}
{Communications Research Laboratory, Ministry of Posts and Telecommunications}
\affilabel{NYU}{ニューヨーク大学 コンピュータサイエンス学科}
{Computer Science Department, New York University}

\jabstract{
  本論文では，日本語の語順の傾向をコーパスから学習する手法を提案する．
  ここで語順とは係り相互間の語順，
  つまり同じ文節に係っていく文節の順序関係を意味するものとする．
  我々が提案する手法では，文節内外に含まれるさまざまな情報から
  語順の傾向を自動学習するモデルを用いる．
  このモデルによって，それぞれの情報が語順の決定にどの程度寄与するか，
  また，どのような情報の組み合わせのときにどのような傾向の語順になるか
  を推測することができる．
  個々の情報が語順の決定に寄与する度合は
  最大エントロピー(ME)法によって効率良く学習される．
  学習されたモデルの性能は，そのモデルを用いて語順を決めるテストを行ない，
  元の文における語順とどの程度一致するかを調べることによって
  定量的に評価することができる．
  正しい語順の情報はテキスト上に保存されているため，
  学習コーパスは必ずしもタグ付きである必要はなく，
  生コーパスを既存の解析システムで解析した結果を用いてもよい．
  本論文ではこのことを実験によって示す．
}

\jkeywords{語順，コーパス，学習，最大エントロピーモデル，生成}

\etitle{Word Order Acquisition from Corpora}
\eauthor{Kiyotaka Uchimoto\affiref{CRL} \and Masaki Murata\affiref{CRL} 
  \and Qing Ma\affiref{CRL} \and Satoshi Sekine\affiref{NYU} 
  \and Hitoshi Isahara\affiref{CRL}} 

\eabstract{
  In this paper we propose a method for acquiring word order from corpora. 
  We define word order as the order of modifiers or 
  the order of bunsetsus which depend on the same modifiee. 
  The method uses a model which automatically discovers what the tendency of 
  the word order in Japanese is by using various kinds of information in 
  and around the target bunsetsus. 
  It shows us to what extent each piece of information contributes to 
  deciding the word order and which word order tends to be selected 
  when several kinds of information conflict. 
  The contribution rate of each piece of information in deciding word order 
  is efficiently learned by a model within a maximum entropy (ME) framework. 
  The performance of the trained model can be evaluated 
  by checking how many instances of word order selected by the model 
  agree with those in the original text. 
  A raw corpus instead of a tagged corpus can be used to train the model, 
  if it is first analyzed by a parser. This is possible because text in the 
  corpus is in the correct word order. In this paper, we show that this is
  indeed possible.
}

\ekeywords{word order, corpora, learning, maximum entorpy model, generation}

\def\q{}
\def\p{}

\begin{document}
\maketitle


\section{はじめに}
\label{sec:introduction}

日本語は語順が自由であると言われている．
しかし，これまでの言語学的な調査によると実際には，
時間を表す副詞の方が主語より前に来やすい，長い修飾句を持つ文節は前に来やすい
といった何らかの傾向がある．
もしこの傾向をうまく整理することができれば，
それは文を解析あるいは生成する際に有効な情報となる．

本論文では語順とは，係り相互間の語順，
つまり同じ文節に係っていく文節の順序関係を意味するものとする．
語順を決定する要因にはさまざまなものがある．
それらの要因は語順を支配する基本的条件として文献\cite{Saeki:98}に
まとめられており，それを我々の定義する語順について解釈しなおすと
次のようになる．
\begin{itemize}
\item 成分的条件
  \begin{itemize}
  \item 深く係っていく文節は浅く係っていく文節より前に来やすい．

    深く係っていく文節とは係り文節と受け文節の距離が長い文節のことを言う．
    例えば，係り文節と受け文節の呼応を見ると，基本的語順は，
    感動詞などを含む文節，時間を表す副詞を含む文節，
    主語を含む文節，目的語を含む文節の順になり，
    このとき，時間を表す副詞を含む文節は主語を含む文節より深く
    係っていく文節であると言う．
    このように係り文節と受け文節の距離を表す概念を係りの深さという．
  \item 広く係っていく文節は狭く係っていく文節より前に来やすい．

    広く係っていく文節とは受け文節を厳しく限定しない文節のことである．
    例えば，「東京へ」のような文節は「行く」のように何らかの移動を表す
    動詞が受け文節に来ることが多いが，「私が」のような文節は受け文節を
    それほど限定しない．
    このとき，「私が」は「東京へ」より広く係っていく文節であると言う．
    このように係り文節がどの程度受け文節を限定するかという概念を
    係りの広さと言う．
  \end{itemize}
\item 構文的条件
  \begin{itemize}
  \item 長い文節は短い文節より前に来やすい．
    
    長い文節とは修飾句の長い文節のことを言う．
  \item 文脈指示語を含む文節は前に来やすい．
  \item 承前反復語を含む文節は前に来やすい．
    
    承前反復語とは前文の語を承けて使われている語のことを言う．
    例えば，
    「あるところにおじいさんとおばあさんがおりました．
    おじいさんは山へ柴刈におばあさんは川へ洗濯に行きました．」
    という文では，2文目の「おじいさん」や「おばあさん」が承前反復語である．
  \item 提題助詞「は」を伴う文節は前に来やすい．
  \end{itemize}
\end{itemize}
以上のような要素と語順の関係を整理する試みの一つとして，
特に係りの広さに着目し，
辞書の情報を用いて語順を推定するモデルが提案された\cite{Tokunaga91b}．
しかし，動詞の格要素の語順に限定しており必須格しか扱えない，
文脈情報が扱えないなどの問題点が指摘されている\cite{Saeki:98}．
語順を推定するモデルとしては他にN-gramモデルを用いたもの\cite{Maruyama:94}
があるが，これは一文内の形態素の並びを推定するモデルであり，
我々とは問題設定が異なる．
また，上に箇条書きとしてあげたような要素は特に考慮していない．
英語については，
語順を名詞の修飾語の順序関係に限定し
統計的に推定するモデルが提案された\cite{Shaw:99}が，
語順を決定する要因として多くの要素を同時に考慮することはできないため，
日本語の語順に対して適用するのは難しい．

本論文では，上に箇条書きとしてあげたような要素と語順の傾向との関係を
コーパスから学習する手法を提案する．
この手法では，語順の決定にはどの要素がどの程度寄与するかだけでなく，
どのような要素の組み合わせのときにどのような傾向の語順になるかということも
コーパスから自動学習することができる．
個々の要素の寄与の度合は最大エントロピー(ME)モデルを用いて効率良く学習する．
学習されたモデルの性能は，そのモデルを用いて語順を決めるテストを行ない，
元の文における語順とどの程度一致するかを調べることによって
定量的に評価することができる．
正しい語順の情報はテキスト上に保存されているため，
学習コーパスは必ずしもタグ付きである必要はなく，
生コーパスを既存の解析システムで解析した結果を用いてもよい．
後節の実験で示すように，
既存の解析システムの精度が90\%程度であったとしても学習コーパスとして
十分に役割を果たすのである．

\section{語順の学習と生成}
\label{sec:learning_and_generation}

\subsection{学習モデル}
\label{sec:model}

この節ではどの語順が妥当であるかを確率として計算するためのモデル
について述べる．
モデルとしては，MEに基づく確率モデルを採用する．
まず，MEの基本について説明し，その後，MEに基づく確率モデルについて述べる．

\subsubsection{ME(最大エントロピー)モデル}
\label{sec:me_model}

一般に確率モデルでは，文脈(観測される情報のこと)とそのときに得られる出力値
との関係は既知のデータから推定される確率分布によって表される．
いろいろな状況に対してできるだけ正確に出力値を予測するためには
文脈を細かく定義する必要があるが，細かくしすぎると既知のデータにおいて
それぞれの文脈に対応する事例の数が少なくなりデータスパースネスの問題が生じる．

MEモデルでは，文脈は素性と呼ばれる個々の要素によって表され，
確率分布は素性を引数とした関数として表される．
そして，各々の素性はトレーニングデータにおける
確率分布のエントロピーが最大になるように重み付けされる．
このエントロピーを最大にするという操作によって，
既知データに観測されなかったような素性あるいは
まれにしか観測されなかった素性については，
それぞれの出力値に対して確率値が等確率になるように
あるいは近付くように重み付けされる．
このように未知のデータに対して考慮した重み付けがなされるため，
MEモデルは比較的データスパースネスに強いとされている．
このモデルは例えば言語現象などのように既知データにすべての現象が現れ得ない
ような現象を扱うのに適したモデルであると言える．

以上のような性質を持つMEモデルでは，
確率分布の式は以下のように求められる．
文脈の集合を$B$，出力値の集合を$A$とするとき，
文脈$b (\in$$B)$で出力値$a (\in$$A)$となる事象$(a,b)$の確率分布$p(a,b)$を
MEにより推定することを考える．
文脈$b$は$k$個の素性$f_j (1\leq j\leq k)$の集合で表す．
そして，文脈$b$において，素性$f_j$が観測され
かつ出力値が$a$となるときに1を返す以下のような関数を定義する．
\begin{eqnarray}
  \label{eq:f}
  g_{j}(a,b) & = & 
  \left\{
    \begin{array}[c]{l}
      1,\ {\rm if}\ exist(b,f_{j})=1 \ \& \ 出力値=a\\
      0,\ それ以外
    \end{array}
  \right.
\end{eqnarray}
これを素性関数と呼ぶ．
ここで，$exist(b,f_j)$は，文脈$b$において素性$f_j$が観測されるか否かによって
1あるいは0の値を返す関数とする．

次に，それぞれの素性が既知のデータ中に現れた割合は
未知のデータも含む全データ中においても変わらないとする制約を加える．
つまり，推定するべき確率分布$p(a,b)$による素性$f_j$の期待値と，
既知データにおける経験確率分布$\tilde{p}(a,b)$による
素性$f_j$の期待値が等しいと仮定する．これは以下の制約式で表せる．
\begin{eqnarray}
  \label{eq:constraint0}
  \sum_{a\in A,b\in B}p(a,b)g_{j}(a,b) 
  & = & \sum_{a\in A,b\in B}\tilde{p}(a,b)g_{j}(a,b)
  \q for\p \forall f_{j}\ (1\leq j \leq k)
\end{eqnarray}
この式で，
$p(a,b)=p(b)p(a|b)\approx\tilde{p}(b)p(a|b)$という近似を行ない以下の式を得る．
\begin{eqnarray}
  \label{eq:constraint}
  \sum_{a\in A,b\in B}\tilde{p}(b)p(a|b)g_{j}(a,b) 
  & = & \sum_{a\in A,b\in B}\tilde{p}(a,b)g_{j}(a,b)
  \q for\p \forall f_{j}\ (1\leq j \leq k) 
\end{eqnarray}
ここで，$\tilde{p}(b)$，$\tilde{p}(a,b)$は，
$freq(b)$，$freq(a,b)$をそれぞれ既知データにおける
事象$b$の出現頻度，出力値$a$と事象$b$の共起頻度として
以下のように推定する．
\begin{eqnarray}
  \tilde{p}(b) & = & 
  \frac{freq(b)}{\displaystyle\sum_{b\in B} freq(b)}\\
  \tilde{p}(a,b) & = & 
  \frac{freq(a,b)}{\displaystyle\sum_{a\in A,b\in B} freq(a,b)}
\end{eqnarray}

次に，式(\ref{eq:constraint})の制約を満たす確率分布$p(a,b)$のうち，
エントロピー
\begin{eqnarray}
  \label{eq:entropy}
  H(p) & = & -\sum_{a\in A,b\in B}\tilde{p}(b)p(a|b)\ log\left(p(a,b)\right)
\end{eqnarray}
を最大にする確率分布を推定するべき確率分布とする．
これは，式(\ref{eq:constraint})の制約を満たす確率分布のうちで
最も一様な分布となる．
このような確率分布は唯一存在し，以下の確率分布$p^{*}$として記述される．
\begin{eqnarray}
  \label{eq:p}
  p^{*}(a|b) & = & \frac{\prod_{j=1}^{k}\alpha_{a,j}^{g_{j}(a,b)}}
  {\sum_{a\in A} \prod_{j=1}^{k}\alpha_{a,j}^{g_{j}(a,b)}}
  \q (0\leq \alpha_{a,j}\leq \infty)
\end{eqnarray}
ただし，
\begin{eqnarray}
  \label{eq:alpha}
  \alpha_{a,j} & = & e^{\lambda_{a,j}}
\end{eqnarray}
であり，$\lambda_{a,j}$は素性関数$g_{j}(a,b)$の重みである．
この重みは文脈$b$のもとで出力値$a$となることを予測するのに
素性$f_{j}$がどれだけ重要な役割を果たすかを表している．
訓練集合が与えられたとき，$\lambda_{a,j}$の推定には
Improved Iterative Scaling(IIS)アルゴリズム
\cite{pietra95}
などが用いられる．
式(\ref{eq:p})の導出については文献
\cite{Jaynes:57,Jaynes:79}
を参照されたい．

\subsubsection{語順モデル}
\label{sec:word_order_model}

本節では語順を学習するためのMEモデルについて述べる．
ここで語順は，ある一つの文節に対しそれに係る文節(係り文節)が複数あるとき，
その係り文節の順序を語順と定義する．
係り文節の数はさまざまであるが，係り文節の数によらず二つずつ取り上げて
その順序を学習するモデルを提案する
\footnote{
  係り文節のうち二つずつではなく，三つあるいはそれ以上ずつ取り上げてその
  順序を学習するモデルを考えることもできる．しかし，データスパースネスの
  問題を考え，本論文では二つずつとりあげて順序を学習するモデルとした．
}．これを語順モデルと呼ぶ．
このモデルは前節のMEモデルにおける式(\ref{eq:p})を用いて以下のように
求められる．
ある文脈$b$において文節$B$に係る文節が二つあるときそれぞれを
文節$B_1$と文節$B_2$とすると，$B_1$の次に$B_2$という順序が適切である
確率$p^{*}(1|b)$は，
出力値$a$を二つの文節の順序が適切であるか否かの1, 0の二値とし，
$k$個の素性$f_j (1\leq j\leq k)$を考えるとき次の式で表される．
\begin{eqnarray}
  \label{eq:p1}
  p^{*}(1|b) & = & \frac{\prod_{j=1}^{k}\alpha_{1,j}^{g_{j}(1,b)}}
  {\prod_{j=1}^{k}\alpha_{1,j}^{g_{j}(1,b)} 
    + \prod_{j=1}^{k}\alpha_{0,j}^{g_{j}(0,b)}}
\end{eqnarray}
この式の$\alpha_{1,j}$，$\alpha_{0,j}$の値を学習するためのデータとしては，
形態素解析，構文解析済みのコーパスを用いる．
一般に係り文節が二つ以上あるときは次のようにする．
ある文脈$b$において文節$B$に係る文節が文節$B_1$，文節$B_2$，
$\ldots$，文節$B_n$ $(n\geq 2)$の$n$個あるとき，
その順序が適切である確率を$P(1|b)$とすると，
この確率は係り文節を二つずつ取り上げたときそれぞれの順序が適切である確率，
つまり，$P(\{W_{i,i+j}=1|1\leq i\leq n-1, 1\leq j\leq n-i\}|b)$で表される．
ここで，$W_{i,i+j}=1$は文節$i$と文節$(i+j)$の順序がこの順で
適切であることを表す．
このとき，$W_{i,i+j}$はそれぞれ独立であると仮定すると，
$P(1|b)$は次の式で表される．
\clearpage
\begin{eqnarray}
  \label{eq:p2}
  P(1|b) 
  & = & P(\{W_{i,i+j}=1|1\leq i\leq n-1, 1\leq j\leq n-i\}|b) \nonumber\\
  & \approx 
  & \prod_{i=1}^{n-1}\prod_{j=1}^{n-i}P(W_{i,i+j}=1|b_{i,i+j}) \nonumber\\
  & = & \prod_{i=1}^{n-1}\prod_{j=1}^{n-i}p^{*}(1|b_{i,i+j})
\end{eqnarray}
ここで，$b_{i,i+j}$は文節$B$とそれに係る文節$B_i$，文節$B_{i+j}$に
着目したときの文脈を表す．

例えば，コーパスに「昨日／太郎は／テニスを／した．」
(／は文節の区切りを表す．)という文があった場合を考える．
動詞「した」に係る文節は「昨日」，「太郎は」，「テニスを」の
三つである．語順モデルでは，このうち二文節ずつ，つまり
「昨日」と「太郎は」，「昨日」と「テニスを」，「太郎は」と「テニスを」
の三つのペアを取り上げ，それぞれこの語順が適切であると仮定して学習する．
素性としては文節の持つ属性などを考える．
例えば，「昨日／太郎は／した．」という関係からは「時相名詞」の方が
「固有名詞」より前に来るという情報，
「太郎は／テニスを／した．」という関係からは「は」格の方が
「を」格より前に来るという情報などを用いる．

\subsection{語順の生成}
\label{sec:generation}

本節では学習した語順モデルを用いて語順を生成するアルゴリズムについて説明する．
語順の生成とは，
ある文節に対し複数の係り文節があるものについて，その係り文節の順序を
決めることを言う．
入力は係り受け関係にある文節および素性の有無を判定するのに必要な情報であり，
出力は係り文節の並びである．
ただし，各文節を構成する語の語彙選択はすでになされており，
文節間の係り受け関係は決まっていると仮定する．
素性の有無を判定するのに必要な情報とは，
形態素情報，文節区切り情報，統語情報，文脈情報などである．
実際に実験で用いた情報については \ref{sec:exp}~章で述べる．

語順の生成は次の手順で行なう．

\underline{手順}

\begin{enumerate}
\item 係り文節について可能性のある並びをすべて考える．
\item それぞれの並びについて，
  その係り文節の順序が適切である確率を語順モデルを用いて求める．
\item 全体の確率が最大となる並びを解とする．
  全体の確率としては式(\ref{eq:p2})を用いる．
\end{enumerate}

例えば，再び「昨日／太郎は／テニスを／した．」という文を考えよう．
動詞「した」に係る文節は「昨日」，「太郎は」，「テニスを」の三つである．
この三つの係り文節の順序を以下の手順で決定する．
\begin{enumerate}
\item 二文節ずつ，つまり「昨日」と「太郎は」，「昨日」と「テニスを」，
  「太郎は」と「テニスを」の三つのペアを取り上げ，
  語順モデルの式(\ref{eq:p1})を用いてそれぞれこの語順が適切である確率
  $P_{昨日,太郎は}$，
  $P_{昨日,テニスを}$，
  $P_{太郎は,テニスを}$を求める．
  例えば，ある文脈においてそれぞれ0.6，0.8，0.7であったと仮定する．
\item 六つの語順の可能性すべてについて全体の確率を
  計算し(表~\ref{table:example})
  \footnote{
    式(\ref{eq:p2})を導出する際，二つの係り文節，文節$i$と文節$(i+j)$の順序
    $W_{i,i+j}$はそれぞれ独立であると仮定したため，
    式(\ref{eq:p2})は近似式となっている．
    したがって，式(\ref{eq:p2})により計算される確率の総和は
    必ずしも1にはならない．
    さらに，ここで例としてあげた確率
    $P_{昨日,太郎は}=0.6$，
    $P_{昨日,テニスを}=0.8$，
    $P_{太郎は,テニスを}=0.7$は適当に与えたものであるため，
    表~\ref{table:example} の六つの語順の可能性すべてについて
    全体の確率を計算し，その総和をとっても1にはならない．
    }，最も確率の高いもの
  「昨日／太郎は／テニスを／した．」が最も適切な語順であるとする．
\end{enumerate}

  \begin{table*}[htbp]
    \begin{center}
      \caption{係り文節の順序が適切である確率の計算例}
      \label{table:example}
      \leavevmode
      \renewcommand{\arraystretch}{}
      \begin{tabular}[c]{|l|p{6.5cm}|}
        \hline
        「昨日／太郎は／テニスを／した．」 
        & $P_{ 昨日,太郎は} 
        \times P_{昨日,テニスを} 
        \times P_{太郎は,テニスを}$
        $= 0.6 \times 0.8 \times 0.7 = 0.336$\\
        「昨日／テニスを／太郎は／した．」
        & $P_{ 昨日,太郎は} 
        \times P_{昨日,テニスを} 
        \times P_{テニスを,太郎は}$
        $= 0.6 \times 0.8 \times 0.3 = 0.144$\\
        「太郎は／昨日／テニスを／した．」
        & $P_{ 太郎は,昨日} 
        \times P_{昨日,テニスを} 
        \times P_{太郎は,テニスを}$
        $= 0.4 \times 0.8 \times 0.7 = 0.224$\\
        「太郎は／テニスを／昨日／した．」
        & $P_{ 太郎は,昨日} 
        \times P_{テニスを,昨日} 
        \times P_{太郎は,テニスを}$
        $= 0.4 \times 0.2 \times 0.7 = 0.056$\\
        「テニスを／昨日／太郎は／した．」
        & $P_{ 昨日,太郎は} 
        \times P_{テニスを,昨日} 
        \times P_{テニスを,太郎は}$
        $= 0.6 \times 0.2 \times 0.3 = 0.036$\\
        「テニスを／太郎は／昨日／した．」
        & $P_{ 太郎は,昨日} 
        \times P_{テニスを,昨日} 
        \times P_{テニスを,太郎は}$
        $= 0.4 \times 0.2 \times 0.3 = 0.024$\\
        \hline
      \end{tabular}
    \end{center}
  \end{table*}

\subsection{性能評価}
\label{sec:evaluation}

本節では語順モデルの性能つまりコーパスにおける語順をどの程度学習できたかを
評価する方法について述べる．
性能の評価は，コーパスから係り受け関係にある文節で
複数の係り文節を持つものを取り出し，
これを入力として\ref{sec:generation} 節で述べた方法で語順を生成し，
どの程度元の文における語順と一致するかを調べることによって行なう．
この一致する割合を一致率と呼ぶことにする．
このように元の文とどの程度一致するかを評価の尺度として用いることによって，
客観的な評価が可能となる．
また，一致率によって評価しておけば，学習したモデルがどの程度学習コーパスに
おける語順に近いものを生成できるかを知った上でそのモデルを使うことができる．

一致率の尺度としては以下の二種類のものを用いる．
\begin{description}
\item[二文節単位] 
  二つずつ係り文節を取りあげたとき，
  順序関係が元の文と一致しているものの割合．
  例えば，「昨日／太郎は／テニスを／した．」が元の文で，
  システムによる生成結果が「昨日／テニスを／太郎は／した．」のとき
  二つずつ係り文節を取り上げると，元の文ではそれぞれ
  「昨日／太郎は」，「昨日／テニスを」，「太郎は／テニスを」の
  順序，システムの結果ではそれぞれ
  「昨日／テニスを」，「昨日／太郎は」，「テニスを／太郎は」の
  順序となる．三つのうち二つの順序が等しいので一致率は$2/3$となる．  
\item[完全一致] 
  係り文節の順序が元の文と一致しているものの割合．
  普通の意味での一致の割合である．
\end{description}

\section{実験と考察}
\label{sec:exp}

この章では，語順生成の実験をいろいろな角度から分析する．
実験には，京大コーパス(Version 2)
\cite{kurohashi:nlp97} を用いた．
学習には1月1日から8日までと1月10日から6月9日までの17,562文を，
試験には1月9日と6月10日から6月30日までの2,394文を用いた．

\subsection{実験データにおける語順の定義}
\label{sec:exp_data}

ある一つの文節に対しそれに係る文節(係り文節)が複数あるとき，
その係り文節の順序を語順と定義した．
我々が用いた実験データでは，
各文節は係り先(受け文節)の情報を一つだけ持つ．
そして，ある文節$B_{m}$とその受け文節$B_{d}$との間に$B_{d}$と並列の関係にある
文節$B_{p}$がある場合，$B_{p}$にはその受け文節が$B_{d}$であるという情報
とともに並列を表すラベル(P)が付与されている．
これは，文節$B_{m}$が$B_{p}$と$B_{d}$の両方に係り得ることを間接的に示している．
このような場合は文節$B_{m}$が$B_{p}$と$B_{d}$の両方に係るとする．

以上の条件の下では，ある文節$B$の係り文節は以下の手順で同定できる．
\begin{enumerate}
\item $B$を受け文節とする文節は$B$の係り文節とする．
\item $B$にラベルが付与されているとき，
  $B$よりも文頭に近い位置にあり$B$と同じ受け文節を持つ文節は
  $B$の係り文節とする．
\item $B$の係り文節の係り文節うちラベルが付与された文節は
  $B$の係り文節とする．手順(3)を再帰的に繰り返す．
\end{enumerate}
以上の手順で，並列の関係にある文節はすべて同じ文節に係るものとして同定される．
例えば，表~\ref{table:kakari_bunsetsu} の左欄のようなデータからは
それぞれの文節に対し，同表の右欄のような係り文節が得られる．
ここで例えば，「出て，」と「優勝した」が並列の関係にあることから，
「優勝した．」の係り文節である「花子は」は「出て，」にも係る文節として
同定されている．
また，「太郎と」と「花子は」が並列の関係にあることから，
「太郎と」は「花子は」と同じ受け文節に係る文節として同定されている．

  \begin{table*}[htbp]
    \begin{center}
      \caption{実験データから同定される係り文節の例}
      \label{table:kakari_bunsetsu}
      \leavevmode
      \renewcommand{\arraystretch}{}
      \begin{tabular}[c]{|c|c|c|l||l|}
        \hline
        \multicolumn{4}{|c||}{実験データ} 
        & 左欄の各文節を受け文節とする係り文節\\
        \hline
        文節 & 係り先の & ラベル & 文字列 & 係り文節(文節番号) \\
        番号 & 文節番号 & & & \\
        \hline
        0 & 1 & P & 太郎と & \\
        1 & 5 &   & 花子は & 太郎と(0)\\
        2 & 3 &   & テニスの & \\
        3 & 4 &   & 試合に & テニスの(2)\\
        4 & 5 & P & 出て， & 太郎と(0) 花子は(1) 試合に(3)\\
        5 &   &   & 優勝した．& 太郎と(0) 花子は(1) 出て(4)\\
        \hline
      \end{tabular}
    \end{center}
  \end{table*}

\subsection{実験結果}
\label{sec:exp_result}

まず，語順の学習および生成の実験に用いた素性を表~\ref{table:feature1} ，
表~\ref{table:feature2} に示す．
表~\ref{table:feature1} にあげた素性は素性名と素性値から成り，
文節が持ち得る属性の情報，統語情報，文脈情報を表している．
これらを基本素性と呼ぶ．
一方，表~\ref{table:feature2} にあげた素性は基本素性の組み合わせである．
これらの素性は文献\cite{Saeki:98}の「語順を支配する基本条件」を
できるだけ反映するように選んだ．
素性の総数はおよそ19万個である．
そのうち学習には学習コーパスに3回以上観測されたもの51,590個を用いた．

\begin{table}[htbp]
  \scriptsize
  \begin{center}
    \caption{学習に利用した素性(基本素性)}
    \label{table:feature1}
    \leavevmode
    \renewcommand{\arraystretch}{}
    \begin{tabular}[c]{|c|l|l|l|l|l|}
      \hline
      \multicolumn{4}{|c|}{\bf 基本素性} 
      & \multicolumn{2}{|c|}{削除したときの一致度}\\
      \hline
      タイプ & 対象 & 素性名 & 素性値 & 二文節単位 & 完全一致 \\
      & 文節 & & & & \\
      \hline
      \hline
      1 & 係り1，
      & 主辞見出し & (5,066個) & 86.65\% & 73.87\% \\
      & 係り2，& & & ($-$0.79\%) & ($-$1.54\%) \\
      & 受け & & & & \\
      \hline
      2 & 係り1，
      & 主辞品詞(Major) 
      & 動詞 形容詞 名詞 助動詞 接続詞 $\ldots$ & 87.07\% & 75.03\% \\
      & 係り2， & & \q\q\q\q\q\q (11個) & ($-$0.37\%) & ($-$0.38\%) \\
      & 受け & 主辞品詞(Minor) 
      & 普通名詞 サ変名詞 数詞 程度副詞 $\ldots$ & & \\
      & & & \q\q\q\q\q\q (24個) & & \\
      \hline
      3 & 係り1，
      & 主辞活用(Major) & 母音動詞 子音動詞カ行 $\ldots$ & 87.39\% & 75.20\% \\
      & 係り2， & & \q\q\q\q\q\q (30個) & ($-$0.05\%) & ($-$0.21\%) \\
      & 受け & 主辞活用(Minor) & 語幹 基本形 未然形 意志形 命令形 $\ldots$ 
      & & \\
      & & & \q\q\q\q\q\q (60個) & & \\
      \hline
      4 & 係り1，
      & 主辞意味素性(110) & 真 (1個) & 87.21\% & 75.20\% \\
      & 係り2， & 主辞意味素性(111) & 真 (1個) 
      & ($-$0.23\%) & ($-$0.21\%)\\
      & 受け & $\cdots$ & & & \\
      & & 主辞意味素性(433) & 真 (1個) & & \\
      & & (90個) & & & \\
      \hline
      5 & 係り1，
      & 語形(String) & こそ こと そして だけ と に も $\ldots$ 
      & 84.78\% & 70.03\% \\
      & 係り2， & & \q\q\q\q\q\q (73個) & ($-$2.66\%) & ($-$5.38\%) \\
      & 受け & 語形(Major) & 助詞 接尾辞 子音動詞カ行 判定詞 $\ldots$ & & \\
      & & & \q\q\q\q\q\q (43個) & & \\
      & & 語形(Minor) & 格助詞 基本連用形 動詞接頭辞 $\ldots$ & & \\
      & & & \q\q\q\q\q\q (102個) & & \\
      \hline
      6 & 係り1，
      & 助詞1(String) & から まで のみ へ ねえ $\ldots$ & 87.32\% & 75.14\% \\
      & 係り2， & & \q\q\q\q\q\q (63個) & ($-$0.12\%) & ($-$0.27\%) \\
      & 受け & 助詞1(Minor) & (無) 格助詞 副助詞 接続助詞 終助詞 & & \\
      & & & \q\q\q\q\q\q (5個) & & \\
      & & 助詞2(String) & けど まま や よ か $\ldots$ (63個) & & \\
      & & 助詞2(Minor) & 格助詞 副助詞 接続助詞 終助詞 (4個) & & \\
      \hline
      7 & 係り1，
      & 句点の有無 & 無 有 (2個) & 87.39\% & 75.54\% \\
      & 係り2， & & & ($-$0.05\%) & ($+$0.13\%) \\
      & 受け & & & & \\
      \hline
      8 & 係り1，
      & 係り文節数 & A(0) B(1) C(2) D(3以上) (4個) & 87.14\% & 74.86\% \\
      & 係り2 & & & ($-$0.30\%) & ($-$0.55\%) \\
      \cline{2-6}
      & 受け 
      & 係り文節数 & A(2) B(3) C(4以上) (3個) & 87.40\% & 75.35\% \\
      & & & & ($-$0.04\%) & ($-$0.06\%) \\
      \hline
      9 & 係り1，
      & 並列 & P(並列) A(同格) D(それ以外) (3個) & 86.26\% & 73.61\% \\
      & 係り2， & & & ($-$1.18\%) & ($-$1.80\%) \\
      & 受け & & & & \\
      \hline
      10 & 係り1，
      & 係り語形1と語形2が一致 & 真 偽 (2個) & 87.34\% & 75.09\% \\
      & 係り2 & 係り語形2と語形1 & 真 偽 (2個) & ($-$0.10\%) & ($-$0.32\%) \\
      & & が一致 & & & \\
      & & 係り語形1と係り語形2 & 真 偽 (2個) & & \\
      & & が一致 & & & \\
      \hline
      11 & 係り1，
      & 主辞見出しが既出 & 真 偽 (2個) & 87.31\% & 75.14\% \\
      & 係り2， & 係り文節主辞見出しが既出 & 真 偽 (2個) 
      & ($-$0.13\%) & ($-$0.27\%) \\
      & 受け & & & & \\
      \hline
      12 & 係り1，
      & 文脈指示語の有無 & 無 有 (2個) & 87.27\% & 75.12\% \\
      & 係り2 & 文脈指示語(String) & この これ こんな そこ その それ 
      $\ldots$ & ($-$0.17\%) & ($-$0.29\%) \\
      & & & \q\q\q\q\q\q (42個) & & \\
      \hline
    \end{tabular}
  \end{center}
\end{table}

\begin{table}[htb]
  \begin{center}
    \caption{学習に利用した素性(基本素性の組み合わせ)}
    \label{table:feature2}
    \leavevmode
    \renewcommand{\arraystretch}{}
    \begin{tabular}[c]{|l|c|c|}
      \hline
      \multicolumn{1}{|c|}{\bf 基本素性の組み合わせ} 
      & \multicolumn{2}{|c|}{削除したときの一致率}\\
      \cline{2-3}
      \multicolumn{1}{|c|}{} & 二文節単位 & 完全一致\\
      \hline
      \multicolumn{1}{|c|}{二素性} & 87.23\% & 74.65\% \\
      \cline{1-1}
      (係り1:語形, 係り2:語形), & ($-$0.21\%) & ($-$0.76\%) \\
      (係り1:語形, 受け:主辞見出し), & & \\
      (係り1:語形, 受け:主辞品詞), & & \\
      (係り1:語形, 係り1:並列), & & \\ 
      (係り1:語形, 係り語形2と語形1が一致), & & \\ 
      (係り2:語形, 受け:主辞見出し), & & \\
      (係り2:語形, 受け:主辞品詞), & & \\
      (係り2:語形, 係り2:並列), & & \\ 
      (係り2:語形, 係り語形1と語形2が一致), & & \\
      (係り1:主辞見出し, 受け:句点の有無), & & \\
      (係り1:主辞品詞, 受け:句点の有無), & & \\
      (係り1:主辞品詞, 係り1:主辞見出しが既出), & & \\
      (係り2:主辞見出し, 受け:句点の有無), & & \\
      (係り2:主辞品詞, 受け:句点の有無), & & \\
      (係り2:主辞品詞, 係り2:主辞見出しが既出) & & \\
      \hline
      \multicolumn{1}{|c|}{三素性} & 87.22\% & 74.86\% \\ 
      \cline{1-1}
      (係り1:語形, 係り2:語形, 受け:主辞見出し), 
      & ($-$0.22\%) & ($-$0.55\%) \\
      (係り1:語形, 係り2:語形, 受け:主辞品詞), & & \\
      (係り1:語形, 係り1:並列, 受け:語形), & & \\ 
      (係り2:語形, 係り2:並列, 受け:語形), & & \\ 
      (係り1:助詞1, 係り1:助詞2, 受け:主辞見出し), & & \\
      (係り1:助詞1, 係り1:助詞2, 受け:主辞品詞), & & \\
      (係り2:助詞1, 係り2:助詞2, 受け:主辞見出し), & & \\
      (係り2:助詞1, 係り2:助詞2, 受け:主辞品詞) & & \\
      \hline
      \multicolumn{1}{|c|}{上記すべての組み合わせ素性} 
      & 85.79\% & 71.67\% \\
      & ($-$1.65\%) & ($-$3.74\%) \\
      \hline
    \end{tabular}
  \end{center}
\end{table}

表~\ref{table:feature1} ，表~\ref{table:feature2} の素性名で使われている
用語の意味は以下の通りである．
\begin{description}
\item[係り1・係り2・受け] \ref{sec:word_order_model}~節で述べた語順モデルでは，
  ある文節に係る文節を二つずつ取り上げて並べその順序が適切である確率を求める．
  その際の受け文節を「受け」，二つ取り上げて並べた係り文節を前から順に
  「係り1」，「係り2」と呼ぶ．
\item[主辞] 各文節内で，品詞の大分類が特殊，助詞，接尾辞となるもの
  \footnote{これらの品詞分類はJUMAN\cite{JUMAN3.5}のものに従う．}
  を除き，最も文末に近い形態素．
\item[主辞見出し] 主辞の基本型(単語)．
  素性値として用いる単語は，主辞の見出し語として学習コーパスに
  5回以上出現したものとする．
\item[意味素性] 「分類語彙表」\cite{NLRI64aj}
  の上位から3レベル目の階層を意味素性として用いる．
  「分類語彙表」は日本語シソーラスの一つであり，
  7レベルの階層からなる木構造で表現される．
  木構造の葉の部分には単語が割り振られており，
  各単語には分類番号という数字が付与されている．
  表~\ref{table:feature1} で例えば「主辞意味素性(110)」の括弧内の数字は
  その分類番号の上位3桁を表す．「主辞意味素性(110):真」という素性は，
  主辞の単語に付与された分類番号の上位3桁が110であることを意味する．
\item[語形] 各文節内で，特殊を除き最も文末に近い形態素．
  もしそれが助詞，接尾辞以外の形態素で活用型，活用形
  \footnote{JUMANの活用型，活用形に従う．}
  を持つものである場合はその活用部分とする
  \footnote{
    語形は基本的に活用部分を指すが，単独の名詞，副詞などからなる文節
    の場合には語形部分なしとするのではなく主辞と同じであると定義する．
    }~．
\item[語形1・語形2] それぞれ係り1，係り2の語形のこと．
\item[助詞1・助詞2] 各文節内で，一番文末に近い助詞を「助詞1」，
  その次に文末に近い助詞を「助詞2」とする．
\item[係り語形1・係り語形2] それぞれ係り1，係り2の係り語形のこと．
  係り語形は係り文節に係っている文節の語形であると定義する．
\item[主辞見出しが既出] 前の文に同じ主辞見出しが出現していること．
\item[文脈指示語] 着目している文節あるいはその係り文節に現れる指示語のこと．
\end{description}

表~\ref{table:feature1} でタイプ1からタイプ6までは文節内の属性を表し，
タイプ7からタイプ10までは統語的な情報を表す．
タイプ11とタイプ12は文脈的な情報を表す．

\begin{table}[htb]
  \begin{center}
    \caption{実験結果}
    \label{Result} 
    \begin{tabular}{|l@{ }|c@{ }|c@{ }|}
      \hline
      & 一致率(二文節単位) & 一致率(完全一致)\\
      \hline      
      本手法 & 87.44\% (12,361/14,137) & 75.41\% (3,980/5,278)\\
      ベースライン1 & 48.96\% (6,921/14,137) & 33.10\% (1,747/5,278)\\
      ベースライン2 & 49.20\% (6,956/14,137) & 33.84\% (1,786/5,278)\\
      \hline
    \end{tabular}
  \end{center}
\end{table}
次に我々の解析結果を表~\ref{Result} に示す．
第1行は京大コーパス1月9日と6月10日から6月30日までの2,394文のうち
係り文節を二つ以上持つ文節5,278文節に対して，
その係り受け関係にある文節およびそれらの文節に関して
コーパスから得られる形態素情報，文節区切り情報，統語情報，文脈情報を入力とし，
語順を生成させたときの結果である．ただし，統語情報としては
係り受けが並列あるいは同格の関係にあるかどうかおよび
文末であるかどうかの情報のみを与える．
また，文脈情報としては生成の対象となっている文節を含む文の前の文を与える．
ベースライン1としてはランダムに選んだ場合の一致率をあげた．
ベースライン2としては，語順モデルの式(\ref{eq:p1})の代わりに
次の式を用いたときの一致率をあげた．
\begin{eqnarray}
  \label{eq:base2}
  p^{*}(1|b) & = & \frac{freq(w_{12})}{freq(w_{12})+freq(w_{21})}
\end{eqnarray}
ここで，$freq(w_{12})$，$freq(w_{21})$は，
係り文節$B_1$と$B_2$の語形の見出し語を$w_1$，$w_2$，
受け文節$B$の主辞見出しを$w$とするとき，これらが
毎日新聞91年から97年のテキストにおいてそれぞれ「$w_1$／$w_2$／$w$」，
「$w_2$／$w_1$／$w$」の順に現れた頻度を表す
\footnote{
  ただし，
  $w_1$と$w_2$が同じときは係り文節$B_1$と$B_2$の主辞見出しを
  それぞれ$w_1$と$w_2$とした．
  また，一方の頻度が0でもう一方の頻度が5以下の場合は
  $freq(w_{12})$，$freq(w_{21})$としてそれぞれ，
  「$w_1$／$w_2$」，「$w_2$／$w_1$」の順に現れた頻度を用いた．
  さらに$freq(w_{12})$，$freq(w_{21})$がいずれも0のときは0から1までの
  乱数値を与えた．
  }．
式(\ref{eq:base2})を用いると例えば，
「太郎は／テニスを／した．」の場合，「は／を／した」の順に
現れる頻度と「を／は／した」の順に現れる頻度を調べ，頻度が大きい並びを
解とすることになる．

\subsection{素性と一致率}
\label{sec:feature_and_accuracy}

この節では，我々が実験で用いた素性が一致率の向上にどの程度
貢献しているかを示す．

\ref{sec:exp_result}~節にあげた表~\ref{table:feature1} ，
表~\ref{table:feature2} の右欄には，
それぞれの素性を削除したときの一致率と削除したことによる一致率の増減を
示してある．
基本素性を削るときは，それを含む組み合わせの素性も一緒に削った．
最も一致率の増加に貢献していると考えられるのは，語形の情報である．
語形は主に格や活用形を表す部分であり，この部分の情報によって
最も語順が影響を受けているという結果は人間の直観とも合っている．

我々が実験に用いた素性は，
言語学的な研究において「語順を支配する基本条件」とされているものを
できるだけ反映したものである．
その条件がどの程度一致率に影響しているかを示すために，
表~\ref{table:feature1} ，表~\ref{table:feature2} に
素性のまとまりごとにその素性を削除したときの一致率を示した．
しかし，「は」「を」などの助詞をひとまとまりとして削除しているなど，
削除する単位が言語学的に興味のある情報よりも粗い可能性がある．
そのような場合には，興味のある要素に対応する素性のみ，
例えば助詞の「は」のみについて，
その素性を削除したときとしなかったときの一致率を比べることにより，
その重要性を定量的に検証することが可能である．
さらに新たな言語学的成果に対してもそれに対応するような素性を追加して
一致率に有意な増加がみられるかどうかを調べることにより，
同様に検証することができると考えられる．

\subsection{学習コーパスと一致率}
\label{sec:corpus_and_accuracy}

この節では，学習コーパスと一致率の関係について考察する．
まず，図~\ref{fig:learning_curve1} ，
図~\ref{fig:learning_curve2} に学習コーパスの量と一致率の関係をあげる．
これらの図には学習コーパスとテストコーパスのそれぞれを解析した場合の
コーパスの量と一致率の関係を載せている．
学習コーパスに対する実験としては基本的に
京大コーパス1月1日の1,172文を用いた．
学習コーパスが250文，500文のときは1月1日の1,172文のうち
上から250文，500文を用いた．

\begin{figure}[htbp]
  \begin{center}
    \leavevmode
    \epsfile{file=learning1.eps,height=8cm}
\vspace{5mm}
    \caption{学習コーパスの量と一致率(二文節単位)の関係}
    \label{fig:learning_curve1}
  \end{center}
\end{figure}
\begin{figure}[htbp]
  \begin{center}
    \leavevmode
    \epsfile{file=learning2.eps,height=8cm}
\vspace{5mm}
    \caption{学習コーパスの量と一致率(完全一致)の関係}
    \label{fig:learning_curve2}
  \end{center}
\end{figure}

学習コーパスが250文という少ない量でもテストコーパスに対して
二文節単位で82.54\%，完全一致で68.40\%の一致率となっている．
これはベースラインよりもかなり高い一致率である．
この結果は，学習コーパスの量が少なくても
新聞記事に対してはある程度語順の傾向を学習できることを示している．

学習コーパスが17,562文のとき，一致率は完全一致で75.41\%である．
テストコーパスと一致しなかった残りの約25\%のうちいくつかは
学習がうまくできなかったものであり，
残りは語順が比較的自由なもので必ずしもコーパスと一致しなくてもよいもの
であると考えられる．
前者に対しては
誤りを分析して，語順の傾向を効率良く学習する素性をもっと補う必要がある．
そこで，テストコーパスに対する結果を調査した．
係り文節の語順がテストコーパスと一致しなかった1,298文節から，
ランダムに100文節を選び分析した．
そのうち，システムが生成した語順でも不自然ではないものが48個，
不自然なものが52個であった．
この不自然なものがテストコーパスの語順と一致するようになるには，
大量の学習コーパス，および表~\ref{table:feature1} ，表~\ref{table:feature2} に
あげたものとは性質の異なる素性が必要である．
学習コーパスが不十分であると思われるものの中には，
「法治国家が／聞いて／あきれる」，
「創案したのが／そもそもの／始まり」，
「味に／精魂／込める」
などイディオム的な表現を含むものが多かった．
コーパスの量が増えればこのような表現に対しては適切な語順が
学習される可能性が高い．
新たな素性を考慮するべきであると思われるものの中には，
並列関係を含むものが目立った．
これについては今後の言語学的な知見なども考慮しながら有効そうな素性を
追加したい．

今回素性として用いた意味素性および文脈指示語や承前反復語は，
意味解析，文脈解析をした結果を基にしている訳ではない．
これらをより有効に利用できるようにするためには，
意味タグや文脈タグなどが付与されたコーパスおよび
意味解析システムや文脈解析システムを統合して用いていく必要がある．

\subsection{生コーパスからの学習}
\label{sec:raw_corpus}

正しい語順の情報はテキスト上に保存されているため，
学習コーパスは必ずしもタグ付きである必要はなく，
生コーパスに対し既存のシステムを用いて解析した結果を学習に用いることもできる．
本節では，タグ付きコーパスと生コーパスを用いて，あるいは生コーパスのみを用いて
学習したときにどの程度の一致率が得られるかについて実験結果を示し考察する．
生コーパスとしては毎日新聞1994年版の最初の200,000文と，
京大コーパスの1月1日から8日までと1月10日から6月9日までの17,562文，
合計217,562文を用いた．このうち京大コーパスの17,562文はタグの付与されていない
原文を用いた．
生コーパスに対しては，
そこから素性の情報を得るために形態素解析，構文解析を行なう．
形態素解析にはJUMAN，構文解析にはKNP\cite{KNP2.0b6}を用いた．
JUMANは形態素区切りおよび品詞の付与の精度が98\%程度，
KNPは係り受け単位の精度が90\%程度である．
これらはいずれも新聞記事に対する精度である．

テストコーパスに対する一致率は，学習コーパスとして
生コーパスのみ217,562文を用いた場合，
二文節単位で87.64\%，完全一致で75.77\%であり，
学習コーパスとして生コーパス(毎日新聞1994年版の最初の200,000文)と
タグ付コーパス(京大コーパスの17,562文)合計217,562文を用いた場合，
二文節単位で87.66\%，完全一致で75.88\%であった．
いずれの場合もタグ付コーパスのみ17,562文を用いたときに比べて，
0.2\%から0.5\%程度一致率が増加した．
この結果から，タグ付きコーパスが少ない場合は，
既存の解析システムの精度が90\%程度であれば
生コーパスのみでも学習コーパスとして十分に役割を果たすことが分かる．
またこの結果は語順の学習はシステムの解析誤りの影響をあまり受けないということを
示していると言える．

\section{まとめ}

本論文ではコーパスから語順を学習する方法について述べた．
ここで語順は，ある一つの受け文節に対し係り文節が複数あるとき
その係り文節の順序を表すものと定義した．
係り文節の数はさまざまであるが，
係り文節の数によらず二つずつ取り上げてその順序を学習するモデルを提案した．
学習モデルにはME(最大エントロピー)モデルを用いた．
このモデルは，学習コーパスから得られる情報を基に
適切な語順を予測するのに有効な素性を学習することによって得られる．
我々が素性として利用したのは，
文節の持つ属性，統語情報，文脈情報およびそれらの組み合わせである．
これらの素性のうちそれぞれを削除した実験を行なうことによって，
その中でも格や活用部分の情報が語順の傾向を学習する上で特に有効に働くことが
分かった．
また，学習コーパスの量を変えて実験を行なうことによって，
我々の手法が少ない学習データに対しても効率良く語順を学習できるだけでなく，
タグ付コーパスだけでなく生コーパスも学習に利用できることも分かった．
学習したモデルを用いて語順を生成させたとき，
コーパスと一致する割合は，京大コーパスを使用した実験で75.41\%であった．
一致しなかった残りの約25\%をサンプリング調査したところ，
その48\%がモデルを用いて生成した語順でも不自然ではないことが分かった．

今回の実験には新聞記事のような一般的な語順のテキストを用いた．
スタイルが異なれば語順の傾向も異なると考えられるため，
今後，小説などのように新聞記事とはスタイルが異なるテキストを用いて実験し，
我々の提案したモデルがどの程度語順の傾向の違いを学習できるかを調べたい．
また，本論文で扱ったのは日本語の語順であったが，
英語についても同様に語順の傾向を学習できると考えられる．
今後，英語についても同様のモデルを用いて語順を学習し，モデルの評価をしたい．

文生成においては一般に客観的な評価基準がないため評価が難しいが，
本論文で示したようにコーパスに基づく評価方法をとることにより，
少なくとも語順の生成に関しては客観的な評価が可能になったと言えるだろう．

本論文で我々が提案した手法には，以下のような応用が考えられる．
\begin{itemize}
\item 校正支援

  ユーザが作文した文を構文解析して依存構造を得た後，それを入力として
  語順を生成させユーザに提示する．
  語順モデルを用いて生成させた語順の方がユーザの作文における語順より
  自然な語順になっている可能性が高いと考えられる．

\item 機械翻訳における対象言語の語順の生成

  対象言語において，文節間の依存構造が決まり，
  各文節において語彙選択が終了すれば，我々が提案した語順モデルを用いて
  一文全体の語順を決めることができる．
  このとき一文全体の語順としては，一文全体の語順の確率が最大となるものを選ぶ．
  一文全体の語順の確率は，受け文節ごとにその受け文節に係る文節の順序の確率を
  式(\ref{eq:p2})を用いて求め，その積として求める．

\item 構文解析における誤り検出

  構文解析結果に複数の係り文節を持つ文節がある場合，
  その係り文節の順序の確率を式(\ref{eq:p2})を用いて求め，
  その値が著しく低い場合に誤りとして検出する
  \footnote{例えば，A，B，Cの三つの文節からなる文があり，
    [A,[B,C]](AとBがともにCに係る解釈)と[[A,B],C](AがBに，BがCに係る解釈)
    の2つの解析結果が得られたとする．前者の解釈に対しては，
    係り文節の順序の確率を求め，
    その値が著しく低い場合には誤りとして検出することができると考えている．
    後者の解釈では，係り文節の順序の確率を求めることはできないが，
    もう一つの候補である前者の解釈に対して係り文節の順序の確率を求め，
    その値が著しく高い場合には後者の解釈は
    誤りであるとして検出することができると考えている．
    }．

\end{itemize}



\bibliographystyle{jnlpbbl}
\bibliography{jpaper}


\begin{biography}
\biotitle{略歴}
\bioauthor{内元 清貴}{
1994年京都大学工学部卒業．
1996年同大学院修士課程修了．
同年郵政省通信総合研究所入所．研究官．
自然言語処理の研究に従事．
言語処理学会，情報処理学会，ACL，各会員．}
\bioauthor{村田 真樹}{
1993年京都大学工学部卒業．
1995年同大学院修士課程修了．
1997年同大学院博士課程修了，博士（工学）．
同年，京都大学にて日本学術振興会リサーチ・アソシエイト．
1998年郵政省通信総合研究所入所．研究官．
自然言語処理，機械翻訳，情報検索の研究に従事．
言語処理学会，情報処理学会，人工知能学会，ACL，各会員．}
\bioauthor{馬 青}{
1983年北京航空航天大学自動制御学部卒業．
1987年筑波大学大学院理工学研究科修士課程修了．
1990年同大学院工学研究科博士課程修了．工学博士．
1990 $\sim$ 93年株式会社小野測器勤務．
1993年郵政省通信総合研究所入所，主任研究官． 
人工神経回路網モデル，知識表現，自然言語処理の研究に従事． 
日本神経回路学会，言語処理学会，電子情報通信学会，各会員．}
\bioauthor{関根 聡}{
1987年東京工業大学応用物理学科卒．同年松下電器東京研究所入社．
1990-1992年UMIST，CCL，Visiting Researcher．1992年MSc．
1994年からNew York University，Computer Science Department，
Assistant Research Scientist．1998年PhD．
同年からAssistant Research Professor．
自然言語処理の研究に従事．
情報処理学会，人工知能学会，言語処理学会，ACL会員．}
\bioauthor{井佐原 均}{
1978年京都大学工学部電気工学第二学科卒業．
1980年同大学院修士課程修了．博士（工学）．
同年通商産業省電子技術総合研究所入所．
1995年郵政省通信総合研究所
関西支所知的機能研究室室長．自然言語処理，機械翻訳の研究に従事．
言語処理学会，情報処理学会，人工知能学会，日本認知科学会，ACL，各会員．}

\bioreceived{受付}
\biorevised{再受付}
\bioaccepted{採録}

\end{biography}

\end{document}
