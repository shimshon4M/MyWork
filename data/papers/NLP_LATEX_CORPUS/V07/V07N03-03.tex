\documentstyle[jnlpbbl_old]{jnlp_j_b5}



\setcounter{page}{39}
\setcounter{巻数}{7}
\setcounter{号数}{3}
\setcounter{年}{2000}
\setcounter{月}{7}
\受付{1999}{10}{4}
\再受付{2000}{1}{20}
\採録{2000}{3}{27}

\setcounter{secnumdepth}{2}

\title{言語に依存しない形態素解析処理の枠組}
\author{山下 達雄\affiref{NAIST} \and 松本 裕治\affiref{NAIST}}

\headauthor{山下, 松本}
\headtitle{言語に依存しない形態素解析処理の枠組}

\affilabel{NAIST}{
奈良先端科学技術大学院大学 情報科学研究科}
{Graduate School of Information Science, Nara Institute of Science
and Technology}

\jabstract{
  形態素解析処理において，
  日本語などのわかち書きされない言語と英語などのわかち書きされる言語では，
  形態素辞書検索のタイミングや辞書検索単位が異なる．
  本論文ではこれらの言語で共通に利用できる形態素解析の枠組の提案と，
  それに基づいた多言語形態素解析システムを実装を行った．
  また，日本語，英語，中国語での解析実験も行った．
}

\jkeywords{
  形態素解析，多言語処理，辞書検索，トークン認識
  }

\etitle{Framework for \\Language Independent Morphological Analysis}
\eauthor{Tatuo Yamasita\affiref{NAIST} \and Yuji Matsumoto\affiref{NAIST}} 

\eabstract{
This paper takes up the problem of tokenization and part-of-speech
tagging of segmented and
non-segmented languages, and proposes a simple framework that enables
an efficient and uniform treatment of tokenization for both types of
languages.  We also reports a language independent morphological
analysis system based on the proposed idea, and shows running systems
for three different languages, English, Japanese and Chinese.
}

\ekeywords{
  multi-lingual processing,  morphological analysis, part of speech
  tagging, tokenization
  }




\newcommand{\delimi}{}
\newcommand{\sufary}{}
\newcommand{\moz}{}

\begin{document}




\maketitle




\section{はじめに}

形態素解析処理とは文を形態素という文字列単位に分割し品詞情報を付与する
処理である．
すでに成熟している技術であるが，
解析精度や速度の向上のために様々な手法を試みる余地はあり，
そのための技術的な拡張要求もある．
他の自然言語処理処理技術と比べ形態素解析技術は実用に近い位置にあり，
それゆえ，形態素解析システムに対する現場からの
使い勝手の向上のための要求が多い．

その要求の一つに，多言語対応がある．
インターネット上で様々な言語のテキストが行き交う現代において，
特定の言語に依存しない，多種多様な言語を視野に入れた自然言語処理
が必要とされている．
しかし，これまでの形態素解析システムは，
特定の言語，または，同系統の数言語の解析のみを念頭に置いて
開発されている．
本研究の目的の一つは，
特定の言語に依存しない形態素解析の枠組の構築である．
我々は，形態素解析処理の言語に依存した部分を考察し，
その部分をできるかぎり共通化した枠組を提案する．

形態素解析は自然言語処理における基本的なコンポーネントであるが，
ミクロな視点から見れば形態素解析処理自体も複数のコンポーネントから
なりたっている．
本研究では，
完成した単一のシステムとして提供するだけではなく，システムを構成しているコン
ポーネント単位で利用できるように設計・実装を行った．
コンポーネント化により，
変更箇所を最小限におさえることができ，
機能拡張が容易になる．
また，
言語非依存化などの調整や個々のコンポーネントの評価が行いやすくなる．



\ref{tok}章では，
形態素解析処理の言語に依存した部分をできるかぎり共通化した
言語非依存の枠組について解説する．
\ref{comp}章では，
形態素解析システムの主要な内部処理のコンポーネント化を行い，
それを基に形態素解析ツールキットの実装を行った．
個別のコンポーネントについての言語非依存性と汎用性を考察し，
実装の方針について解説する．





\section{言語に依存しないトークン認識と辞書検索}\label{tok}

特定の言語に依存しない一般的な
形態素解析システムを考えてみる．
まず，最初に行う処理は，
システムに与えられた解析対象文を{\bf トークン}と呼ばれる
文字列単位に分割する処理である．
この処理を {\bf トークン認識}(tokenization) と呼ぶ．
トークンとは，英語では単語や数字や記号などに該当するが，厳密な定義は無い．
トークンを発展させた概念が本論文で提案する{\bf 形態素片}である．
\ref{MF}節で導入する．
{\bf 語}は一個以上の連続するトークンから構成されるもので
用意された{\bf 辞書のエントリとして存在するもの}と定義する．
語は{\bf 形態素}と呼ばれることもあるが，
言語学的な形態素の定義とは異なるので別名を与えることにした．
次に行う処理は，
認識されたトークンの列に対して形態素辞書の検索を行い，
語を認識する処理である．
この処理を本論文では単に{\bf 辞書検索}と呼ぶ．

トークン認識と辞書検索は解析対象言語の
コンピュータ上での表記の特性によって処理方法が異なる．
言語によって異なる場合もあるし，同じ言語でも清書法によって異なる場合もある．
本論文では「書かれた言語」を対象に形態素解析することを前提としているの
で，「言語」とは表記，清書法をも含む意味に捉えることにする．
本論文では，言語を表記の特性によって以下の二つに分類する．
\begin{itemize}
\item わかち書きされる言語
\item わかち書きされない言語
\end{itemize}
わかち書きされる言語の例として英語があげられる．
英語では語は{\bf 空白文字}(whitespace) や
{\bf 記号文字}(panctuation mark) によって区切られていると考えられることが多く，
トークン認識は単純明解な処理とみなされあまり重要視されなかった．
しかし，いくつかの問題がある．これについては\ref{1t=1l}節で説明する．
わかち書きされない言語の例として日本語や中国語などがあげられる．
単語の境界が視覚的にはっきりしていないという表記上の特性をもっている．
ゆえに，トークン認識は重要かつ困難な処理である．

トークンの処理はわかち書きされる言語とわかち書きされない言語ではまった
く異なるとみなされてきた．
英語などのわかち書きされる言語では明白な境界が単語の両側にあるが，
日本語や中国語などわかち書きされない言語では
明白な単語境界を表すものが必ずしもあるとは言えない．
わかち書きされる言語とされない言語の違いは，
言語の特性というよりも表記の特性によるものであり，
清書法の方針による分類と言える
\cite{永田97}．

本章ではわかち書きされる言語とされない言語のトークン認識方法の違いに着目し，
どちらの表記法にも適応できる統一的な枠組を提案する．
この統一的な枠組により，
システムの最小限の変更(もしくは変更不要)と
データの入れ換えだけで様々な言語を同じシステムで解析できる．
また，複数の言語が混じった文章を解析することもできる．



\subsection{わかち書きされる言語とされない言語の処理の違い}

わかち書きされない言語では，
トークン認識は全ての文字をトークンとして認識すれば良い．
理論的には文中の全ての部分トークン列(この場合，部分文字列)を
語の候補として考慮する必要がある．
また，語辞書により同じ字面の候補に複数の品詞候補が与えられることが
ある．
そのため，区切りの曖昧性と
品詞付与の曖昧性の二種類の曖昧性が生じる．

わかち書きされる言語では，
トークンは一意に決定され，かつ，1 トークン = 1 語という
単純化を行うことが多く，
その場合は品詞付与の曖昧性だけが生じる．
しかし，
実際はわかち書きされる言語でも，
必ずしも 1 トークン ＝ 1 語とみなすことが困難な場合がある．
これについては \ref{1t=1l}節で説明する．

このような，
わかち書きされる言語とされない言語のトークン認識処理の違いは，
辞書検索の方法に影響を与える．

わかち書きされる言語が語の両側に必ず明白な区切りを持つならば，
辞書検索ではシステムはわかち書きされた文字列が語辞書に
存在するかを問い合わせるだけでよい．
もし存在するなら品詞等の情報を辞書から得る．

一方，わかち書きされない言語では文は明白な単語区切りを持ってい
ないため，
辞書検索ではシステムは全ての部分トークン列(部分文字列)が
語辞書に存在するかそれぞれ問い合わせる必要がある．
一般にわかち書きされない言語の辞書検索は，
文中のある位置から始まる全ての語を辞書から一括で取り出す
{\bf 共通接頭辞検索}(common prefix search)と呼ばれる手法が用いられる．
一般に共通接頭辞検索を効率的に行うために
TRIE というデータ構造を用いる．
図\ref{fig:trieJ} に日本語 TRIE 辞書の一部を示す．
TRIE 構造は一回の問い合わせで文中のある位置からはじまる全ての語 
を返すことを保証しているので，効率的な辞書検索ができる．
例えば，
「海老名へ行く」という文字列を図\ref{fig:trieJ}の TRIE で検索すれば，
枝を一回たどるだけで，
「海(名詞)」「海老(名詞)」「海老名(固有名詞)」という
語が見つかる．
文中の全ての語を探す単純な方法は，
文頭から一文字ずつ文字位置をずらしながら各位置で
共通接頭辞検索を行うというものである．
しかし，
Aho と Corasick により提案された AC法\cite{Aho93} を用いれば，
入力文を一回スキャンするだけで入力文に含まれる全ての語候補
を取り出すことができ，
TRIE による方法と比べ検索速度は格段に向上する．
Maruyama\cite{Maruyama94} はこの AC 法を用いて
日本語形態素解析の辞書検索の高速化を行っている．
だが，辞書のデータ格納領域が大きくなるという欠点がある．
本論文では前者の TRIE による方法を用いて以降の解説を行う．


\begin{figure}[bt]
\begin{center}
\renewcommand{\arraystretch}{}
\begin{tabular}{l}
　　海　\\
─┬─●┬─ {\bf  海} : 名詞\\
　│　　│　　\\
　│　　│老　　\\
　│　　└─●┬─ {\bf  海老} : 名詞\\
　│　　　　　│\\
　│　　　　　│名\\
　│　　　　　└─●─ {\bf  海老名} : 固有名詞\\
　│歩　\\
　├─●┬─ {\bf  歩} : 名詞\\
　│　　│\\
　│　　│く\\
　│　　├─●─ {\bf  歩く} : 動詞\\
　│　　│\\
　│　　│道\\
　│　　└─●┬─ {\bf  歩道} : 動詞\\
　│　　　　　│\\
　│　　　　　│橋\\
　│　　　　　└─●─ {\bf  歩道橋} : 動詞\\
　：\\
\end{tabular}
\end{center}
  \caption{日本語 TRIE 辞書}
  \label{fig:trieJ}
\end{figure}


\subsection{1 トークン = 1 語 の問題}\label{1t=1l}

わかち書きされる言語でも，
単語は常に明白な単語境界文字列で区切られているわけではない．
明白な単語境界を前提とした単純なトークン認識手法には限界がある．
そこで，本論文では，
わかち書きされる言語をわかち書きされない言語と同じ方法で解析する
方法を提案する．
英語を例に，問題点とその解決のための方針を述べる．

\begin{description}
\item[単語内区切り曖昧性の問題 --- 一つのトークンに複数の語が含まれている] 
  ~

  語の構成要素に記号文字が含まれている場合，
  区切りの曖昧性が生じてしまう．

  ``Mr.'' や ``Inc.'' のように，
  ピリオドがトークンの末尾にある場合，
  それが文末記号なのか省略記号なのかという曖昧性がある．
  これは，わかち書きされる言語の「文の認識」
  という大きな課題であり，Palmer ら\cite{Palmer97}によって研究されている．

  アポストロフィがトークンの途中に含まれている場合も曖昧性が生じる．
  所有を表す `` 's'' が辞書にあれば解析文中の 
  ``John's'' を
  ``John'' + `` 's'' という二つの語として認識したい．
  しかし，
  ``McDonald's''(固有名詞) が辞書にあれば解析文中の
  ``McDonald's'' を ``McDonald'' + `` 's'' ではなく
  一つの語としても解析したい．
  それゆえ，区切りの曖昧性を生じることになる．

  ハイフンの場合は，
  ハイフンでつながれた文字列が辞書にあればそれを候補としたいし
  (例： ``data-base'' )，
  無ければハイフンを無視したい(例： ``55-year-old'' )．




  これらの問題は単純なパターンマッチでは対処できない．
  そこで，我々はわかち書きされない言語での方法を適用する．
  記号文字を含むトークン全てを記号文字で分割し
  (例： ``McDonald's.''  →  ``McDonald'' + `` ' '' + ``s'' + ``.'')，
  分割されたトークン列に対して共通接頭辞検索で語辞書を検索する．
  記号文字を含めた形で語が辞書に登録されていればそれも候補になる．

  

\item[複合語問題 --- 複数のトークンが一つ語になる] ~

  語の構成要素に空白文字が含まれている場合について考える．

  空白文字などの明白な境界を利用した単純なトークン認識では，複合語
  固有名詞 ``South Shields'' のような空白文字を含む語が扱え
  ない．このような場合 ``South''(固有名詞) と ``Shields''(固有名詞) を
  辞書に登録して解析することが考えられる．
  しかし，``South''(形容詞) + ``Shields''(名
  詞複数形) と認識されてしまう危険も高くなる．``South Shields''(固有名
  詞) という空白文字を含んだ語を辞書に登録できれば，
  この種の誤りは減るであろう．
  

  Penn Treebank\cite{PennTreebank90} では，``New York'' のような
  空白文字を含む固有名詞は ``New'' と ``York'' に分割され，
  それぞれに「固有名詞」という品詞タグが与えられている．
  そのため，
  このコーパスから得られる語も空白文字を含まない
  分割されたものになってしまう．
  このように空白文字を含んだまま単独の語として扱うべき
  ものをわざわざ分割してしまうと，
  形態素解析処理
  における曖昧性の増加の要因になる．


  この問題を解決するために，
  我々は
  わかち書きされない言語での方法を適用する．
  これは，我々の知る限り Webster ら\cite{Webster92} により
  最初に提示されたアイディアである．
  彼らはわかち書きされる語のイディオムや定型表現などを
  扱うため，空白文字を含む語の辞書登録を可能にし，
  トークン認識時にそれらを Table-look-up matching という方法で検索をする．
  我々は語辞書を共通接頭辞検索で検索するという方法により，
  空白文字を含む語を扱う．





\end{description}


これらの問題を\underline{同時に}解決するためには，
「記号文字，及び，空白文字で区切られた文字列単位をベース
に語辞書を共通接頭辞検索」すれば良いという結論に達した．
そのためには，この文字列単位を，
言語非依存性と処理効率を考慮しきちんと定義する必要がある．
これについては，
次節で詳しく解説する．




\subsection{形態素片}\label{MF}

\subsubsection{形態素片の導入}\label{MFintro}

\ref{1t=1l}節で述べたように，
わかち書きされる言語は単語境界が明白であると考えられてきたにもかかわら
ず，単語内区切り曖昧性，複合語の問題がある．

このような問題を解決する素朴な方法として，
わかち書きされる言語をわかち書きされない言語と同じとみなし，
「わかち書きされない言語を解析する方法」で解析するという方法が考えられる．
英語を例に考えると，
``They've{\delimi}gone{\delimi}to{\delimi}school{\delimi}together.''
という文の全てのスペース({\delimi})を削除して，
``They'vegonetoschooltogether.''
という文を作り，これをわかち書きされない言語を解析する方法で解析すれば
よい\cite{Mills98}．
しかし，このような方法は
 ``They/'ve/gone/to/school/to/get/her/.'' 
のような余計な曖昧性を含む結果を生んでしまう．
スペースを消した場合の影響を調べるため，
簡単な精度測定実験を行った．
Penn Treebank\cite{PennTreebank90} の 128万形態素 から学習された
パラメータ(品詞 trigram による状態遷移表と
出現確率が付与された単語辞書)を用いた
HMM ベースの形態素解析システム \moz(\ref{comp:moz}節を参照)で解析精度を計った．
テストデータは学習に用いた全データを使用した．
図\ref{fig:ORGvsNWS}に実験結果を示す．
スペースを削除した場合は区切りの曖昧性が発生するため，
recall と presicion で評価した．
区切りの曖昧性の影響で精度が落ちていることが分かる．
結果を細かく見てみると，
``a way'', ``a head'',  ``any more'', ``work force''
のような複数の語の連続をそれぞれ
 ``away'', ``ahead'', ``anymore'', ``workforce''
のように一つの語として認識してしまう傾向にある．
recall よりも precision が高いのはこのためである．
また，``a tour'', ``a ton'', ``Alaskan or''
を ``at our'', ``at on'', ``Alaska nor''
のように認識してしまう誤りもある程度見られた．
前者のような区切りの曖昧性は conjunctive ambiguity，
後者のような区切りの曖昧性は disjunctive ambiguity
と呼ばれる\cite{Webster92}\cite{Guo97}．
conjunctive ambiguity による区切り誤りは 11267個，
disjunctive ambiguity による区切り誤りは 223個あった．
区切り曖昧性は精度以外にも性能に影響を与える．
文の全ての位置から検索ができるので検索回数が増え，
それにともない候補となる語も増えるため，
解析時間が増大してしまう．
実験ではスペースを削除した方法の解析時間は，
そうでない場合の約5倍を要した．
これは重大な問題である．

より精度の高い効率的な解析を行うためには，
わかち書きの情報を活かし，
余計な曖昧性をできるかぎり排除できる
単位を定義すべきである．
また，特定の言語に依存しないように考慮する必要がある．

\begin{figure}[bt]
\vspace{-3mm}
  \begin{center}
    \begin{tabular}{|l||c|c|}\hline
      & Recall & Presicion \\\hline\hline
      スペースあり({\tt It{\delimi}is{\delimi}my...})
      & \multicolumn{2}{c|}{
        \begin{tabular}{c}
          97.36\%\\
          $(\frac{1250889}{1284792})$
        \end{tabular}
        } \\\hline
      スペース削除({\tt Itismy...})
      &
        \begin{tabular}{c}
          97.04\%\\
          $(\frac{1246811}{1284792})$
        \end{tabular}
      &
        \begin{tabular}{c}
          97.16\%\\
          $(\frac{1246811}{1283203})$
        \end{tabular}
      \\\hline
    \end{tabular}
  \end{center}
  \caption{英語におけるわかち書きの効果を調べる実験}
  \label{fig:ORGvsNWS}
\end{figure}




本論文では，
わかち書きされる言語のこのような問題を解決するために，
効率的かつ洗練された方法を提案する．
それは，
文中での
「辞書検索を始めて良い位置・終えて良い位置」を言語ごとに明確に定義し，
それを元に共通接頭辞検索で辞書検索するという方法である．
これは，わかち書きされない言語で採用されている方法を一般化したものである．
わかち書きされない言語では「文字」の境界が
辞書検索を始めて良い位置・終えて良い位置となる．
このような
「辞書検索を始めて良い位置・終えて良い位置」
に囲まれた文字列を{\bf 形態素片}\cite{Yamashita2000}と呼ぶ．
この形態素片を各言語ごとに定義すれば，
本節冒頭の英語の例のような非論理的な曖昧性を含むことなく，
わかち書きされる言語とされない言語を統一的に扱える．
形態素片は，
言語非依存性や処理の効率を考慮してより厳密に定義されたトークンと言える．

ある言語の形態素片の集合は，
その言語の辞書中の全ての語を構成できる文字列の最小集合と定義する．
ただし，{\bf デリミタ}と呼ばれる文字列集合は除く．
デリミタは文中で語の境界を表す空白文字などの文字列で，
語の最初と最後には現れないものと定義する．
以降， ``{\delimi}'' と表記する．
英語では
アルファベットのみが連続する文字列，及び，
全ての記号文字は形態素片であると定義できる．
それゆえ，単語中の記号文字で分断される各文字列も形態素片である．
例えば，英語文字列 ``they're'' は ``they'', `` ' '', ``re'' の
3つの形態素片から成る．
当然，複合語を構成する各単語も形態素片である．
辞書に ``New{\delimi}York'' や ``New'' というエントリがあれば，
 ``New'', ``York'' はそれぞれ形態素片であるが，
 ``{\delimi}''  はデリミタなので形態素片にはならない．
また，``{\delimi}York'' といった文字列は定義により語にならない．
日本語，中国語などのわかち書きされない言語では全ての文字が形態素片に
なる．
図\ref{fig:MFrslt}に文から形態素片を認識した例を示す．
認識された形態素片は角括弧で囲って表されている．

\begin{figure}[bt]
\vspace{-5mm}
  \begin{center}
  \begin{tabular}{|l|lll|}    \hline
  英語 &
    {\tt I'm in New York.} & → & [{\tt I}][{\tt '}][{\tt
      m}]{\delimi}[{\tt in}]{\delimi}[{\tt New}]{\delimi}[{\tt
      York}][{\tt .}]
    \\\hline
  日本語 &
    学校へ行きましょう． & → & [学][校][へ][行][き][ま][し][ょ][う][．]
    \\\hline
  \end{tabular}

  \end{center}
  \caption{形態素片認識の例}
  \label{fig:MFrslt}
\end{figure}

我々の方法では，わかち書きされない言語と同様に
わかち書きされる言語の語辞書は TRIE に格納する．
形態素片が枝のラベルになる．
図\ref{fig:trieE} に形態素片ベースの英語 TRIE 辞書を示す．
図\ref{fig:MFrslt}の英語の例の
 [New] の位置から図\ref{fig:trieE} の TRIE を検索すれば，
一回たどるだけで 
``New(形容詞)'', ``New{\delimi}York''(固有名詞) という二つの語
が見つかる．
TRIE 辞書構築時と形態素片認識処理時には，
連続する二つ以上のデリミタは一つのデリミタと見なして処理を行う．
デリミタの連続には，特に言語的な意味は無いと仮定している．



\begin{figure}[bt]
\begin{center}
\renewcommand{\arraystretch}{}
\begin{tabular}{l}
　　\bf I\\
─┬─●─ {\bf  I} : 代名詞 \\
　│ \\
　│\bf '\\
　├─●┬─ {\bf  '} : 記号 \\
　│　　│\\
　│　　│\bf m\\
　│　　├──●─ {\bf  'm} : 動詞\\
　│　　│\\
　│　　│\bf s\\
　│　　└──●─ {\bf  's} : 所有\\
　│ \\
　│{\bf New}\\
　├──●┬─ {\bf  New} : 形容詞\\
　│　　　│\\
　│　　　│\delimi 　　\bf York\\
　│　　　└──●───●─ {\bf  New\_York} : 固有名詞 \\
　：\\
\end{tabular}
\end{center}
  \caption{英語 TRIE 辞書}
  \label{fig:trieE}
\end{figure}


\subsubsection{形態素片認識の方法}\label{MFimpl}


形態素片は辞書を引き始める位置と引き終える位置を明確にし，
わかち書きされる言語でも，効率的な共通接頭辞検索を可能にする概
念である．
しかし，ある言語の形態素片の集合を過不足なく定義することは難しい．
そこで，我々は
「ユーザが簡単に定義できる必要最小限の情報」
のみを用いた疑似的な形態素片の定義法を提案する．


わかち書きされる言語である英語を例に考えてみると
「デリミタと記号文字で区切られる文字(アルファベット)の連続」と
「記号文字」
の二種類が形態素片となり，
形態素片認識にはデリミタと記号文字を定義する必要があることが分かる．
わかち書きされない言語である日本語や中国語を例に考えると，
各「文字」が形態素片になり，「文字」を定義すれば良いことが分かる．
これらの考察により，次の3種類の情報
を用いれば形態素片認識処理ができることが分かる．
\begin{enumerate}
\item 文字の定義，及び，全ての文字が形態素片になりうるかどうかの区別

  わかち書きされない言語では全ての文字が形態素片になりうる．
  つまり，これは，わかち書きされる言語かされない言語かを区別する情報である．

\item デリミタ辞書

  形態素片の境界として働き，
  それ自体は独立した形態素片にはならない文字列の辞書．
 語の開始と終了位置にはデリミタは現れない．

\item 形態素片辞書

  形態素片となる特殊な文字・文字列の辞書．
  記号文字のように形態素片の境界として働き，
  それ自体も形態素片として扱われる文字列などを格納する．

\end{enumerate}

全ての文字が形態素片となるわかち書きされない言語では，
(2),(3) は不要の場合が多い．
  

これらの情報の
英語(Penn Treebank\cite{PennTreebank90} のフォーマットに準拠)での定義例を示す．
\begin{enumerate}
\item 1 文字 ≠ 1 形態素片 (わかち書きされる言語)
\item デリミタ辞書 : 空白文字(\delimi)
\item 形態素片辞書 :
  [{\tt .}][{\tt ,}][{\tt :}][{\tt ;}][{\tt '}][{\tt -}]$\cdots$[{\tt \$}][{\tt \%}]$\cdots$[{\tt n't}]
\end{enumerate}
Penn Treebank では， ``don't'' などの縮約形は ``do'' と ``n't'' に
分割されタグ付与されているので，
形態素片辞書に [{\tt n't}] が必要になる．

日本語での定義例を示す．
\begin{enumerate}
\item 1 文字 ＝ 1 形態素片 (わかち書きされない言語)
\item デリミタ辞書 : 空白文字(\delimi)
\item 形態素片辞書 : なし
\end{enumerate}
日本語はわかち書きされない言語であるが，
デリミタを定義しておくと，わかち書きした文の解析もできる．
わかち書きした日本語文は区切り曖昧性が減少する．

韓国語
の通常の清書法では，
句単位でわかち書きする．
これは，日本語の文節に相当する単位である．
わかち書きをする位置は，新国語表記法\cite{全96}によって定められているが，
必ずしも完全に守られているわけではない\cite{平野97}．
我々の視点では，韓国語は日本語のようなわかち書きされない言語に分類できる．
形態素片の定義例は上記の日本語のものをそのまま用いることができる．
しかし，わかち書きの境界の前後の品詞の分布には偏りがある．
平野ら\cite{Hirano96}\cite{平野97}は，
境界内部では品詞 bigram を用い，
境界を越えての連接には品詞 trigram (境界も品詞の一つ)を
用いることにより，わかち書き境界という情報をうまくとりこんでいる．
わかち書き境界がスペース(\delimi)で表されるとすれば，
この場合の韓国語での定義例は次のようになる．
\begin{enumerate}
\item 1 文字 ＝ 1 形態素片 (わかち書きされない言語)
\item デリミタ辞書 : なし
\item 形態素片辞書 : 空白文字(\delimi)
\end{enumerate}
もちろん，語辞書にスペース(\delimi)を登録し
「わかち書き境界」などといった品詞を持たせておく必要がある．

ドイツ語はわかち書きされる言語であるが，
複合名詞は区切りの曖昧性を持っている．
例えば，{\it Staubecken} は，
{\it Stau-becken} と区切れば「貯水池」，
{\it Staub-ecken} では「ゴミ捨て場」という意味になる\cite{Lezius98}．
このような区切り曖昧性を扱うためには，
わかち書きされない言語として処理を行えば良い．
定義例をあげる．
\begin{enumerate}
\item 1 文字 ＝ 1 形態素片 (わかち書きされない言語)
\item デリミタ辞書 : 空白文字(\delimi)
\item 形態素片辞書 : なし
\end{enumerate}

形態素片認識アルゴリズムは，
文の先頭から末尾まで 1 バイトずつずらしながら，
形態素片・デリミタを探して行くという単純なものである．
しかし，
この方法による形態素片認識結果を用いれば全ての語を認識でき，
実用上の問題は無い．













\section{コンポーネント化と実装}\label{comp}

本章では，
形態素解析システム内部の様々な処理をそれぞれコンポーネント化した
設計・実装について述べる．
コンポーネント化は，
形態素解析以外の用途への利用，
特殊な機能の追加，言語に特化した処理の追加などに
必須である．
本章では，
これらの目的を念頭に置いた設計・実装の方針について解説する．

我々の考える言語非依存の形態素解析処理の流れを次に示す．
\begin{enumerate}
\item 入力された解析対象文を形態素片列として認識し，辞書検索を簡単にす
  る．
\item 形態素片列に対し語辞書検索を行い品詞候補，及び，語自
  体のコストを与える．
\item 語を区切り・品詞の曖昧性を保持したまま
  トレリス(trellis)データ構造に格納する．
  同時に状態遷移の情報もチェックし格納する．
\item
  
  トレリスから最適解(語の列)を選択する．
\item 結果を出力する．
\end{enumerate}
このような処理の流れに基づき各処理をコンポーネントに分割した
設計を行い，形態素解析ツールキット LimaTK を実装した
\cite{LimaTK99}\cite{山下and松本98}\cite{山下99}．
図\ref{fig:TK}に示すようなコンポーネントから成り立っている．
全てのコンポーネントは独立しておりインターフェース等の仕様に従えば
自作のコンポーネントと置き換えが可能である．

これらのコンポーネントのうちで言語依存性の高いものは，
形態素片認識，辞書検索，未定義語処理である．
形態素片認識と辞書検索は形態素片の導入により，
言語依存部分がほぼ解消されたと言える．
形態素片認識の実装については，\ref{tok}章で説明した．
辞書検索の実装については\ref{comp:dic}節，
最適解選択の実装については\ref{comp:lattice}節，
未定義語処理の実装については\ref{comp:udw}節で述べる．
\ref{comp:moz}節では LimaTK を用いて実装した形態素解析システム \moz
について述べる．

\begin{figure}[bt]
  \begin{center}
    \leavevmode
    \renewcommand{\arraystretch}{}
    \begin{tabular}{l}
\fbox{\bf 　　　形態素解析システム　　　}\\
　　　│　　│　　│　　│\\
　　　│　　│　　│　　└── 形態素片認識\\
　　　│　　│　　│　　\\
　　　│　　│　　└── 辞書検索\\
　　　│　　│　　　　　　\\
　　　│　　└── 未定義語処理\\
　　　│　　　　　　　│\\
　　　│　　　　　　　└─ 品詞推定\\
　　　│　　\\
　　　└── 形態素データ管理\\
　　　　　　　　│\\
　　　　　　　　├─ 状態遷移表管理\\
　　　　　　　　│\\
　　　　　　　　└─ 最適解選択\\
\end{tabular}
  \end{center}
  \caption{Modules of LimaTK}
  \label{fig:TK}
\end{figure}






\subsection{辞書検索}\label{comp:dic}

辞書検索コンポーネントは
形態素片列として認識された文から
可能性のある語全てを辞書から獲得する．
これらの処理の詳細については，\ref{tok}章で既に述べた．
現在の辞書検索コンポーネントの実装について述べる．

語辞書のデータ構造である TRIE を
単純に実装すると大きなデータ領域が必要になる．
そこで現在は，データ格納領域が小く済む2種類の方法で実装している．
suffix array\cite{Manber90} を使うものと，
パトリシア木\cite{Morrison68} を使うものである．
パトリシア木はデータ消費量が若干大きいが高速であり，
suffix array は若干低速であるがデータ消費量が小さいという特徴がある．
suffix array による実装は
高速文字列検索ライブラリ{\sufary}\cite{SUFARY99}を使用している．

語としてどのようなものを辞書に入れておくか
ということは言語や用途に依存した問題である．
例えば，英語で動詞イディオム
``look up'' (``looking up'', ``looked up'' なども)
を語として辞書登録したいとする．
すると，以下の例の (1) では，登録された語が辞書検索の結果得られるが，
(2) では分割されているのでイディオムとして認識されない．
\begin{enumerate}
\item 
I looked up the answer.

\item 
I looked the answer up.
\end{enumerate}
Webster ら\cite{Webster92} は
このような連続した文字列で表現できないイディオム，定型表現
などを形態素解析処理の段階で扱うために，
辞書検索とパージングの知識・処理を融合するという枠組を提案している．
コンポーネント化設計により，
文字列として連続していない語の認識処理も，
他のコンポーネントに影響を与えないように
辞書検索コンポーネントなどの内部で実装できる．
しかし，我々はこのような言語の構造に関わる処理は
形態素解析より後の高次の処理で扱うべきであると考える．
これは我々の目標が，
形態素解析システムの単純化・効率化・言語非依存性を目指すことにあるためである．



\subsection{最適解選択処理}\label{comp:lattice}

我々は HMM による最適解の選択方法を採用した．
最適解選択に必要な HMM パラメータは
ある程度の量の品詞タグ付きコーパスがあれば得られるので，
特定の言語の解析が容易に始められるという利点がある．
具体的には，
品詞タグ付きコーパスから，語と品詞 N-gram をカウントし，シンボル
出力確率(品詞別単語出現確率)と状態遷移確率(品詞間，または，状態と品詞
間の遷移確率)を計算し，
動的計画法の一つであるビタビ・アルゴリズムで
出現確率最大の解を求める．
実際の実装は，
積演算より和演算の方が効率的に処理できるという理由から\cite{Manning99}
パラメータ(確率値)の逆数の対数に適当な係数をかけた整数値({\bf コスト})を用いている．
コストの和演算で最適な解を選択する方法は
{\bf コスト最小法}とも呼ばれており\cite{永田97}\cite{長尾96}，
JUMAN\cite{JUMAN98}，茶筌\cite{ChaSen99}
といった日本語形態素形態素解析システムなどで採用されている．
つまり，和演算による実装は，
これらのシステムで長年用いられてきた，
人手によって調整された
コスト体系(単語コスト，接続コストなど)も利用できるという柔
軟性を持っている．

特定の言語のために形態素解析を行うためにユーザが必要なものは，
形態素片認識を行うための情報と，
語と接続表(HMM パラメータ)だけである．
HMM パラメータは，
十分な大きさの品詞タグ付きコーパスと
ユーザの望む統計モデル
(bigram, trigram, variable memory model\cite{Haruno97} など)
に基づいた学習プログラムがあれば得られる．


形態素情報管理コンポーネントは，
前述の方法により最尤解選択を行う．
これは形態素解析における解選択の
一般的な実装方法である\cite{永田97}\cite{長尾96}．
文頭から文末へ向かって，一語ずつトレリス(ラティス)構造に格納してゆき，
そのときにその語までの部分解析のコストを求める．
最適解は，文末から文頭へ向かって，
最適な部分解析のコストを持つノードを順次辿れば得られる．
格納の際に必要になる，
状態遷移(接続)にかかるコストと遷移先状態は，
状態遷移表管理コンポーネントから得る．
状態遷移表管理コンポーネントは，
現在の状態と次の品詞をキーに状態遷移のコストと遷移先状態を返すという
単純な仕事をする．



\subsection{未定義語処理}\label{comp:udw}

未定義語処理コンポーネントは
辞書に登録されていない語に対して品詞推定を行う．
未定義語の品詞推定は統計的な方法と
人手による規則などのヒューリスティックを用いる方法がある．

統計的な方法は，
未定義語が全ての品詞を持つと仮定し，
トレリスでの曖昧性解消処理の段階で
品詞 N-gram の統計値により最適な品詞を自動的に選ぶという方法である\cite{Manning99}．
これは，言語に依存しない実装が可能である．
しかし，この方法ではデータ格納領域が増大してしまい，処理効率が悪い．
そこで，未定義語が全ての品詞を持つのではなく，
あらかじめ「未定義語が推定されうる品詞」の集合を限定する方法が考えられる．
例えば「この言語の未定義語は『名詞』か『固有名詞』である」と定義すれば，
曖昧性解消処理で未定義語の品詞はどちらかに選ばれる．
この方法は完全な推定とは言えないがデータ格納領域の増大を押えることができる
現実的な方法であり，
茶筌\cite{ChaSen99} で採用されている．
実用性と性能のバランスの良さから，
LimaTK の未定義語処理コンポーネントの標準の機能として採用した．

ヒューリスティックによる方法は，
例えば，英語ならば，
「文中で大文字で始まるなら固有名詞」
「-tion で終われば名詞」
といった規則を用いて品詞を推定する方法である．
これは言語に依存する方法なので，
言語ごとに処理系を実装する必要がある．
LimaTK ではこのようなルールを埋め込むためには，
統計的手法による未定義語処理コンポーネントを
修正するか，まったく新しく作り直す必要があるが，
作り直す場合でもインターフェース規約を守れば
他の処理に影響を与えずに実装できる．

未定義語の長さ，すなわち，
未定義語がいくつの形態素片で構成されるかを決定する処理も難しい．
理論的にはある位置から始まる全ての長さの部分形態素片列が未定義語の候補
になる可能性がある．
しかし，これでは候補が増大してしまい処理効率に問題がある．
日本語のように字種にバリエーションのある言語は，
連続する漢字列・カタカナ列・記号列などを一まとめにするといった字種によ
るまとめ処理により未定義語の候補を限定できる\cite{長尾96}．
このような字種による未定義語候補の決定処理は
JUMAN\cite{JUMAN98} や 茶筌\cite{ChaSen99} の様な
日本語形態素解析システムに採用されている．
単純なまとめ処理ではなく，
字種による語の長さの分布の違いに着目して未定義語処理を行うという研究もある
\cite{Nagata99}．
わかち書きされる言語ではこれまでこの問題は起こらなかった．
しかし，本研究では形態素片という概念を導入したため，
わかち書きされる言語でも問題になるようになった．
未定義語に複合語は無いと仮定すれば，
ある位置から始まり次のデリミタまで間の全ての部分形態素片列を
未定義語の候補とすれば良い．
この仮定は正しいものではないが，実用上はさほど問題なく現実的である．

そもそも未定義語処理は言語依存性の高い処理であり，
品詞推定精度の高い共通の枠組を構築するのは困難である．
より精度の高い処理を求めるユーザはやはり
プログラムの調整を行う必要がある．
ゆえに，
我々は各言語共通に利用できる最低限の機能と調整の行いやすい枠組で実装を行った．
言語非依存性とユーザの利便性と処理効率のバランスを考慮した
実装と言える．
実装方針をあげておく．

\begin{itemize}
\item 未定義語の品詞推定 : 
  あらかじめ「未定義語が推定されうる品詞」の集合を定義し，
  最適解選択処理にまかせる

\item 未定義語の長さの決定 : 
  「形態素片」「字種によるまとまり」「デリミタに挟まれた領域」
  を未定義語を構成する単位に選択でき，
  未定義語を構成する「最大単位数」も指定できる
\end{itemize}



\begin{figure*}[htb]

  \baselineskip 3.9mm
\fbox{日本語} ~ 
\begin{verbatim}
そんな感じがします．

そんな 3075 226 3648 [Y:ソンナ BF:そんな P:連体詞 Pr:100/3864]
感じ 6960 198 1585 [Y:カンジ BF:感じ P:名詞-一般 Pr:37/144546]
が 1306 59 744 [Y:ガ BF:が P:助詞-格助詞-一般 Pr:17509/82739]
し 0 117 2688 [Y:シ BF:する P:動詞-自立/サ変・スル/連用形 Pr:10638/10638]
ます 3048 73 1666 [Y:マス BF:ます P:助動詞/特殊・マス/基本形 Pr:813/30431]
． 1 11 2072 [PP:記号-句点 Y:． BF:． P:記号-句点 Pr:27418/27452]
\end{verbatim}

\bigskip

\fbox{英語} ~ 
\begin{verbatim}
What is a word?

What 2208 20 1835 [P:WP Pr:218/3156]
is 946 62 1373 [P:VBZ Pr:8789/27619]
a 1207 28 1384 [P:DT Pr:25820/111243]
word 6629 85 117 [P:NN Pr:59/179722]
? 3771 54 202 [P:. Pr:556/53362]
\end{verbatim}

\bigskip

\fbox{中国語} ~
\begin{verbatim}
人力基盤構築不能促成．

人力 5929 28 28 [P:Na Pr:185/372140]
基盤 8286 28 28 [P:Na Pr:9/372140]
構築 6816 70 70 [P:VC Pr:17/106692]
不能 3989 78 78 [P:D Pr:1003/167440]
促成 8473 32 32 [P:VH Pr:2/105135]
． 2007 31 31 [P:PERIODCATEGORY Pr:6046/79413]
\end{verbatim}




  \caption{形態素解析例}\label{fig:mar}
\end{figure*}


\subsection{ツールキットによる形態素解析システムの実装}\label{comp:moz}

LimaTK を用いて，簡単な多言語対応形態素解析システム \moz を作成した．
日本語，英語，中国語，韓国語など様々な言語を実装した．
図\ref{fig:mar}に解析結果の例を示す．
各行はそれぞれ一つの形態素を表し，各列は左から，
「見出し文字列」「各形態素の持つコスト」「品詞コード」「状態コード」
「解析には用いないその他の情報(角括弧で囲まれている)」
\footnote{
  出力を人間にも読みやすくするための情報であり，解析時には一切使用されない．
  形態素辞書のこの項目に記述されている文字列をそのまま表示する．
  フォーマットは任意であり，後処理の目的に合わせ，
  形態素辞書を作成する段階で自由に変更できる．
  英語，中国語の例では，品詞名(P)と形態素の出現確率(Pr)が，
  日本語の例では，読み(Y)，基本形(BF)，品詞名(P)，形態素の出現確率(Pr)
  がこの項目に含まれている．
  形態素の出現確率は「各形態素の持つコスト」を算出する基になった値であ
  る
  (
  $Pr:この形態素のコーパスでの出現回数/品詞のコーパスでの出現回数$
  )．
}
となっている．
使用した言語データを次に示す．

\begin{description}
\item[日本語]
  RWCPの品詞タグ付きコーパス\cite{IPA97}(約92万形態素)から
  品詞 trigram モデルでパラメータ学習を行い，
  さらに茶筌\cite{ChaSen99}の語辞書エントリを追加した．
  解析精度は，同様の方法で作成された解析用データを用いた
  茶筌のものと同等で，
  インサイドデータで Recall, Precision とも 97\% 程度である．

\item[英語]
  Penn Treebank\cite{PennTreebank90}
  の品詞タグ付きコーパス(約128万形態素)から
  品詞 trigram モデルで学習を行い，
  電子化テキスト版
  Oxford Advanced Learner's Dictionary\cite{OALD92}の辞書エントリを追加した．
  語幹(stem)情報も
  同じくOxford Advanced Learner's Dictionaryから補完した．
  解析精度は
  インサイドデータで 97\%，アウトサイドデータで 95\% 程度である．
  
\item[中国語]
  台湾の中央研究院の品詞タグ付きコーパス\cite{CKIP-TR9502}
  (約210万形態素
  )から
  品詞 bigram モデルで学習を行った．
  解析精度は Recall, Precision とも 
  インサイドデータで 95\%，アウトサイドデータで 91\% 程度である．

\end{description}

未定義語品詞推定のチューニング，高次の N-gram の利用，スムージングなど
より高度な統計的手法を用いれば，それに応じて精度は向上する．
しかし，本研究の目的は精度の向上ではないので，
これ以上は追求しない．



\section{関連研究}

Webster ら\cite{Webster92} は
わかち書きされる言語(英語)のイディオムや定型表現が扱えるように，
英語のデリミタで区切られた単位を
わかち書きされない言語(中国語を想定している)の文字に対応させ
わかち書きされない言語での解析方法を適用している．
しかし，
``They've'' のような単語内区切り曖昧性の問題(\ref{MFintro}節)には
言及していない．
曖昧性の解消については，確率を使った方法が適用できると言及されているの
みである．

わかち書きされる言語における境界の曖昧性解消について，
Millsはわかち書きが不明瞭な古代英語の解析の際に，
わかち書きされない言語
で用いる形態素解析手法を単純に適用している\cite{Mills98}．
これは，\ref{MFintro}節で説明した，
文の全てのスペース({\delimi})が削除された英文を
わかち書きされない言語の解析手法で解析する方法である．
しかし，この研究での解決法は，目的から分かるように，
言語非依存ではない．
また，単純なマッチング，ヒューリスティックによる力まかせな
方法に基づいている．


言語非依存な形態素解析の枠組として，Kimmo の
二段階形態素論(two-level morphology)\cite{Kimmo83}が知られている．
簡単に言うと，
オートマトンを用いてトークン認識と品詞付与を同時に行う方法である．

我々は，わかち書きされる言語で考えられる分割の可能性，及び，
わかち書きされる言語とされない言語との整合性も考慮し，
形態素片という概念を導入した．
形態素片はこれまで漠然と定義されていたトークンに替わる概念である．
これにより言語非依存の枠組を構築した．
このアイディアは極めて単純ではあるが，
効率的な TRIE 辞書検索の実現とトークン認識における非論理的な曖昧性の排除を達成
した．

本研究の成果物である形態素解析ツールキット LimaTK の柔軟性を
示す例として文節まとめあげと形態素解析の融合についての研究\cite{浅原99}
をあげる．
英語を念頭に考えると，これは
品詞付与と名詞句(BaseNP)認識を HMM により同時に行う
という方法である．
スペース({\delimi})を
「名詞句始まり」「名詞句終わり」
「名詞句の間(終わりであり始まりである)」
「名詞句途中」「名詞句の外部」のタグ(品詞)を持つ語として
扱い，
名詞句区切り情報と品詞情報の付与されたコーパスから
HMM パラメータを学習し，それを用いて最適なものを選ぶという手法である．
この手法は，形態素片を以下の情報で定義すれば \moz で簡単に実装できる．
\begin{enumerate}
\item 1 文字 ≠ 1 形態素片 (わかち書きされる言語)
\item デリミタ辞書 : なし
\item 形態素片辞書 : スペース({\delimi})
\end{enumerate}
このように，LimaTK は様々な言語を解析できるだけでなく，
形態素解析以外の用途にも適用できる汎用的な
 HMM パーザとして柔軟に利用できる．


\section{おわりに}

本論文では，
言語に依存しない形態素解析の枠組の提案と，
形態素解析の内部処理のコンポーネント化によるツールキットの設計・実装を
行った．
従来は，わかち書きするか否かという言語の特徴により大きく処理が
異なる形態素解析処理を，
形態素片という辞書検索単位を定義したことにより，
言語非依存の共通の枠組で行えるようになった．
また，
形態素解析の内部処理のコンポーネント化により，
言語非依存化のみならず様々な改良や他の言語処理への適用が
行いやすくなった．

本研究に関する情報・ツールキットのパッケージは
下記の URL で入手できる．

\begin{center}
\mbox{\tt $<$http://cl.aist-nara.ac.jp/\symbol{"7E}tatuo-y/ma/$>$}  
\end{center}


\acknowledgment

本研究を進めるにあたって有意義なコメントを戴いた松本研究室の方々に
感謝いたします．また，データを
快く提供していただいた関係各社に深く感
謝いたします．


\bibliographystyle{jnlpbbl_old}
\bibliography{v07n3_03}


\begin{biography}
\biotitle{略歴}
\bioauthor{山下\ 達雄}{
1995年広島大学総合科学部総合科学科卒業．
1997年奈良先端科学技術大学院大学情報科学研究科博士前期課程修了．
2000年同大学同研究科博士後期課程研究指導認定退学．
情報処理学会，
人工知能学会各会員．
}
\bioauthor{松本\ 裕治\ (正会員)}{
1955年生．1977年京都大学工学部情報工学科卒．1979年同大学大
学院工学研究科修士課程情報工学専攻修了．同年電子技術総合研究所入
所．1984〜85年英国インペリアルカレッジ客員研究員．1985〜87年
(財)新世代コンピュータ技術開発機構に出向．京都大学助教授を経て，
1993年より奈良先端科学技術大学院大学教授，現在に至る．
工学博士.
専門は自然言語処理．
情報処理学会，
人工知能学会，
日本ソフトウェア科学会，
認知科学会，
AAAI, ACL, ACM各会員．
}

\bioreceived{受付}
\biorevised{再受付}
\bioaccepted{採録}

\end{biography}



\end{document}


