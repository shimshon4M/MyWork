<?xml version="1.0" ?>
<root>
  <title>言語に依存しない形態素解析処理の枠組</title>
  <author>山下達雄松本裕治</author>
  <jabstract>形態素解析処理において，日本語などのわかち書きされない言語と英語などのわかち書きされる言語では，形態素辞書検索のタイミングや辞書検索単位が異なる．本論文ではこれらの言語で共通に利用できる形態素解析の枠組の提案と，それに基づいた多言語形態素解析システムを実装を行った．また，日本語，英語，中国語での解析実験も行った．</jabstract>
  <jkeywords>形態素解析，多言語処理，辞書検索，トークン認識</jkeywords>
  <section title="はじめに">形態素解析処理とは文を形態素という文字列単位に分割し品詞情報を付与する処理である．すでに成熟している技術であるが，解析精度や速度の向上のために様々な手法を試みる余地はあり，そのための技術的な拡張要求もある．他の自然言語処理処理技術と比べ形態素解析技術は実用に近い位置にあり，それゆえ，形態素解析システムに対する現場からの使い勝手の向上のための要求が多い．その要求の一つに，多言語対応がある．インターネット上で様々な言語のテキストが行き交う現代において，特定の言語に依存しない，多種多様な言語を視野に入れた自然言語処理が必要とされている．しかし，これまでの形態素解析システムは，特定の言語，または，同系統の数言語の解析のみを念頭に置いて開発されている．本研究の目的の一つは，特定の言語に依存しない形態素解析の枠組の構築である．我々は，形態素解析処理の言語に依存した部分を考察し，その部分をできるかぎり共通化した枠組を提案する．形態素解析は自然言語処理における基本的なコンポーネントであるが，ミクロな視点から見れば形態素解析処理自体も複数のコンポーネントからなりたっている．本研究では，完成した単一のシステムとして提供するだけではなく，システムを構成しているコンポーネント単位で利用できるように設計・実装を行った．コンポーネント化により，変更箇所を最小限におさえることができ，機能拡張が容易になる．また，言語非依存化などの調整や個々のコンポーネントの評価が行いやすくなる．章では，形態素解析処理の言語に依存した部分をできるかぎり共通化した言語非依存の枠組について解説する．章では，形態素解析システムの主要な内部処理のコンポーネント化を行い，それを基に形態素解析ツールキットの実装を行った．個別のコンポーネントについての言語非依存性と汎用性を考察し，実装の方針について解説する．</section>
  <section title="言語に依存しないトークン認識と辞書検索">特定の言語に依存しない一般的な形態素解析システムを考えてみる．まず，最初に行う処理は，システムに与えられた解析対象文をトークンと呼ばれる文字列単位に分割する処理である．この処理をトークン認識(tokenization)と呼ぶ．トークンとは，英語では単語や数字や記号などに該当するが，厳密な定義は無い．トークンを発展させた概念が本論文で提案する形態素片である．節で導入する．語は一個以上の連続するトークンから構成されるもので用意された辞書のエントリとして存在するものと定義する．語は形態素と呼ばれることもあるが，言語学的な形態素の定義とは異なるので別名を与えることにした．次に行う処理は，認識されたトークンの列に対して形態素辞書の検索を行い，語を認識する処理である．この処理を本論文では単に辞書検索と呼ぶ．トークン認識と辞書検索は解析対象言語のコンピュータ上での表記の特性によって処理方法が異なる．言語によって異なる場合もあるし，同じ言語でも清書法によって異なる場合もある．本論文では「書かれた言語」を対象に形態素解析することを前提としているので，「言語」とは表記，清書法をも含む意味に捉えることにする．本論文では，言語を表記の特性によって以下の二つに分類する．わかち書きされる言語わかち書きされない言語わかち書きされる言語の例として英語があげられる．英語では語は空白文字(whitespace)や記号文字(panctuationmark)によって区切られていると考えられることが多く，トークン認識は単純明解な処理とみなされあまり重要視されなかった．しかし，いくつかの問題がある．これについては節で説明する．わかち書きされない言語の例として日本語や中国語などがあげられる．単語の境界が視覚的にはっきりしていないという表記上の特性をもっている．ゆえに，トークン認識は重要かつ困難な処理である．トークンの処理はわかち書きされる言語とわかち書きされない言語ではまったく異なるとみなされてきた．英語などのわかち書きされる言語では明白な境界が単語の両側にあるが，日本語や中国語などわかち書きされない言語では明白な単語境界を表すものが必ずしもあるとは言えない．わかち書きされる言語とされない言語の違いは，言語の特性というよりも表記の特性によるものであり，清書法の方針による分類と言える．本章ではわかち書きされる言語とされない言語のトークン認識方法の違いに着目し，どちらの表記法にも適応できる統一的な枠組を提案する．この統一的な枠組により，システムの最小限の変更(もしくは変更不要)とデータの入れ換えだけで様々な言語を同じシステムで解析できる．また，複数の言語が混じった文章を解析することもできる．</section>
  <subsection title="わかち書きされる言語とされない言語の処理の違い">わかち書きされない言語では，トークン認識は全ての文字をトークンとして認識すれば良い．理論的には文中の全ての部分トークン列(この場合，部分文字列)を語の候補として考慮する必要がある．また，語辞書により同じ字面の候補に複数の品詞候補が与えられることがある．そのため，区切りの曖昧性と品詞付与の曖昧性の二種類の曖昧性が生じる．わかち書きされる言語では，トークンは一意に決定され，かつ，1トークン=1語という単純化を行うことが多く，その場合は品詞付与の曖昧性だけが生じる．しかし，実際はわかち書きされる言語でも，必ずしも1トークン＝1語とみなすことが困難な場合がある．これについては節で説明する．このような，わかち書きされる言語とされない言語のトークン認識処理の違いは，辞書検索の方法に影響を与える．わかち書きされる言語が語の両側に必ず明白な区切りを持つならば，辞書検索ではシステムはわかち書きされた文字列が語辞書に存在するかを問い合わせるだけでよい．もし存在するなら品詞等の情報を辞書から得る．一方，わかち書きされない言語では文は明白な単語区切りを持っていないため，辞書検索ではシステムは全ての部分トークン列(部分文字列)が語辞書に存在するかそれぞれ問い合わせる必要がある．一般にわかち書きされない言語の辞書検索は，文中のある位置から始まる全ての語を辞書から一括で取り出す共通接頭辞検索(commonprefixsearch)と呼ばれる手法が用いられる．一般に共通接頭辞検索を効率的に行うためにTRIEというデータ構造を用いる．図に日本語TRIE辞書の一部を示す．TRIE構造は一回の問い合わせで文中のある位置からはじまる全ての語を返すことを保証しているので，効率的な辞書検索ができる．例えば，「海老名へ行く」という文字列を図のTRIEで検索すれば，枝を一回たどるだけで，「海(名詞)」「海老(名詞)」「海老名(固有名詞)」という語が見つかる．文中の全ての語を探す単純な方法は，文頭から一文字ずつ文字位置をずらしながら各位置で共通接頭辞検索を行うというものである．しかし，AhoとCorasickにより提案されたAC法を用いれば，入力文を一回スキャンするだけで入力文に含まれる全ての語候補を取り出すことができ，TRIEによる方法と比べ検索速度は格段に向上する．MaruyamaはこのAC法を用いて日本語形態素解析の辞書検索の高速化を行っている．だが，辞書のデータ格納領域が大きくなるという欠点がある．本論文では前者のTRIEによる方法を用いて以降の解説を行う．</subsection>
  <subsection title="1 トークン = 1 語 の問題">わかち書きされる言語でも，単語は常に明白な単語境界文字列で区切られているわけではない．明白な単語境界を前提とした単純なトークン認識手法には限界がある．そこで，本論文では，わかち書きされる言語をわかち書きされない言語と同じ方法で解析する方法を提案する．英語を例に，問題点とその解決のための方針を述べる．これらの問題を同時に解決するためには，「記号文字，及び，空白文字で区切られた文字列単位をベースに語辞書を共通接頭辞検索」すれば良いという結論に達した．そのためには，この文字列単位を，言語非依存性と処理効率を考慮しきちんと定義する必要がある．これについては，次節で詳しく解説する．</subsection>
  <subsection title="形態素片"/>
  <subsubsection title="形態素片の導入">節で述べたように，わかち書きされる言語は単語境界が明白であると考えられてきたにもかかわらず，単語内区切り曖昧性，複合語の問題がある．このような問題を解決する素朴な方法として，わかち書きされる言語をわかち書きされない言語と同じとみなし，「わかち書きされない言語を解析する方法」で解析するという方法が考えられる．英語を例に考えると，``They'vegonetoschooltogether.''という文の全てのスペース()を削除して，``They'vegonetoschooltogether.''という文を作り，これをわかち書きされない言語を解析する方法で解析すればよい．しかし，このような方法は``They/'ve/gone/to/school/to/get/her/.''のような余計な曖昧性を含む結果を生んでしまう．スペースを消した場合の影響を調べるため，簡単な精度測定実験を行った．PennTreebankの128万形態素から学習されたパラメータ(品詞trigramによる状態遷移表と出現確率が付与された単語辞書)を用いたHMMベースの形態素解析システム(節を参照)で解析精度を計った．テストデータは学習に用いた全データを使用した．図に実験結果を示す．スペースを削除した場合は区切りの曖昧性が発生するため，recallとpresicionで評価した．区切りの曖昧性の影響で精度が落ちていることが分かる．結果を細かく見てみると，``away'',``ahead'',``anymore'',``workforce''のような複数の語の連続をそれぞれ``away'',``ahead'',``anymore'',``workforce''のように一つの語として認識してしまう傾向にある．recallよりもprecisionが高いのはこのためである．また，``atour'',``aton'',``Alaskanor''を``atour'',``aton'',``Alaskanor''のように認識してしまう誤りもある程度見られた．前者のような区切りの曖昧性はconjunctiveambiguity，後者のような区切りの曖昧性はdisjunctiveambiguityと呼ばれる．conjunctiveambiguityによる区切り誤りは11267個，disjunctiveambiguityによる区切り誤りは223個あった．区切り曖昧性は精度以外にも性能に影響を与える．文の全ての位置から検索ができるので検索回数が増え，それにともない候補となる語も増えるため，解析時間が増大してしまう．実験ではスペースを削除した方法の解析時間は，そうでない場合の約5倍を要した．これは重大な問題である．より精度の高い効率的な解析を行うためには，わかち書きの情報を活かし，余計な曖昧性をできるかぎり排除できる単位を定義すべきである．また，特定の言語に依存しないように考慮する必要がある．本論文では，わかち書きされる言語のこのような問題を解決するために，効率的かつ洗練された方法を提案する．それは，文中での「辞書検索を始めて良い位置・終えて良い位置」を言語ごとに明確に定義し，それを元に共通接頭辞検索で辞書検索するという方法である．これは，わかち書きされない言語で採用されている方法を一般化したものである．わかち書きされない言語では「文字」の境界が辞書検索を始めて良い位置・終えて良い位置となる．このような「辞書検索を始めて良い位置・終えて良い位置」に囲まれた文字列を形態素片と呼ぶ．この形態素片を各言語ごとに定義すれば，本節冒頭の英語の例のような非論理的な曖昧性を含むことなく，わかち書きされる言語とされない言語を統一的に扱える．形態素片は，言語非依存性や処理の効率を考慮してより厳密に定義されたトークンと言える．ある言語の形態素片の集合は，その言語の辞書中の全ての語を構成できる文字列の最小集合と定義する．ただし，デリミタと呼ばれる文字列集合は除く．デリミタは文中で語の境界を表す空白文字などの文字列で，語の最初と最後には現れないものと定義する．以降，``''と表記する．英語ではアルファベットのみが連続する文字列，及び，全ての記号文字は形態素片であると定義できる．それゆえ，単語中の記号文字で分断される各文字列も形態素片である．例えば，英語文字列``they're''は``they'',``''',``re''の3つの形態素片から成る．当然，複合語を構成する各単語も形態素片である．辞書に``NewYork''や``New''というエントリがあれば，``New'',``York''はそれぞれ形態素片であるが，``''はデリミタなので形態素片にはならない．また，``York''といった文字列は定義により語にならない．日本語，中国語などのわかち書きされない言語では全ての文字が形態素片になる．図に文から形態素片を認識した例を示す．認識された形態素片は角括弧で囲って表されている．我々の方法では，わかち書きされない言語と同様にわかち書きされる言語の語辞書はTRIEに格納する．形態素片が枝のラベルになる．図に形態素片ベースの英語TRIE辞書を示す．図の英語の例の[New]の位置から図のTRIEを検索すれば，一回たどるだけで``New(形容詞)'',``NewYork''(固有名詞)という二つの語が見つかる．TRIE辞書構築時と形態素片認識処理時には，連続する二つ以上のデリミタは一つのデリミタと見なして処理を行う．デリミタの連続には，特に言語的な意味は無いと仮定している．</subsubsection>
  <subsubsection title="形態素片認識の方法">形態素片は辞書を引き始める位置と引き終える位置を明確にし，わかち書きされる言語でも，効率的な共通接頭辞検索を可能にする概念である．しかし，ある言語の形態素片の集合を過不足なく定義することは難しい．そこで，我々は「ユーザが簡単に定義できる必要最小限の情報」のみを用いた疑似的な形態素片の定義法を提案する．わかち書きされる言語である英語を例に考えてみると「デリミタと記号文字で区切られる文字(アルファベット)の連続」と「記号文字」の二種類が形態素片となり，形態素片認識にはデリミタと記号文字を定義する必要があることが分かる．わかち書きされない言語である日本語や中国語を例に考えると，各「文字」が形態素片になり，「文字」を定義すれば良いことが分かる．これらの考察により，次の3種類の情報を用いれば形態素片認識処理ができることが分かる．文字の定義，及び，全ての文字が形態素片になりうるかどうかの区別わかち書きされない言語では全ての文字が形態素片になりうる．つまり，これは，わかち書きされる言語かされない言語かを区別する情報である．デリミタ辞書形態素片の境界として働き，それ自体は独立した形態素片にはならない文字列の辞書．語の開始と終了位置にはデリミタは現れない．形態素片辞書形態素片となる特殊な文字・文字列の辞書．記号文字のように形態素片の境界として働き，それ自体も形態素片として扱われる文字列などを格納する．全ての文字が形態素片となるわかち書きされない言語では，(2),(3)は不要の場合が多い．これらの情報の英語(PennTreebankのフォーマットに準拠)での定義例を示す．1文字≠1形態素片(わかち書きされる言語)デリミタ辞書:空白文字()形態素片辞書:[.][,][:][;]['][-][][%][n't]PennTreebankでは，``don't''などの縮約形は``do''と``n't''に分割されタグ付与されているので，形態素片辞書に[n't]が必要になる．日本語での定義例を示す．1文字＝1形態素片(わかち書きされない言語)デリミタ辞書:空白文字()形態素片辞書:なし日本語はわかち書きされない言語であるが，デリミタを定義しておくと，わかち書きした文の解析もできる．わかち書きした日本語文は区切り曖昧性が減少する．韓国語の通常の清書法では，句単位でわかち書きする．これは，日本語の文節に相当する単位である．わかち書きをする位置は，新国語表記法によって定められているが，必ずしも完全に守られているわけではない．我々の視点では，韓国語は日本語のようなわかち書きされない言語に分類できる．形態素片の定義例は上記の日本語のものをそのまま用いることができる．しかし，わかち書きの境界の前後の品詞の分布には偏りがある．平野らは，境界内部では品詞bigramを用い，境界を越えての連接には品詞trigram(境界も品詞の一つ)を用いることにより，わかち書き境界という情報をうまくとりこんでいる．わかち書き境界がスペース()で表されるとすれば，この場合の韓国語での定義例は次のようになる．1文字＝1形態素片(わかち書きされない言語)デリミタ辞書:なし形態素片辞書:空白文字()もちろん，語辞書にスペース()を登録し「わかち書き境界」などといった品詞を持たせておく必要がある．ドイツ語はわかち書きされる言語であるが，複合名詞は区切りの曖昧性を持っている．例えば，Staubeckenは，Stau-beckenと区切れば「貯水池」，Staub-eckenでは「ゴミ捨て場」という意味になる．このような区切り曖昧性を扱うためには，わかち書きされない言語として処理を行えば良い．定義例をあげる．1文字＝1形態素片(わかち書きされない言語)デリミタ辞書:空白文字()形態素片辞書:なし形態素片認識アルゴリズムは，文の先頭から末尾まで1バイトずつずらしながら，形態素片・デリミタを探して行くという単純なものである．しかし，この方法による形態素片認識結果を用いれば全ての語を認識でき，実用上の問題は無い．</subsubsection>
  <section title="コンポーネント化と実装">本章では，形態素解析システム内部の様々な処理をそれぞれコンポーネント化した設計・実装について述べる．コンポーネント化は，形態素解析以外の用途への利用，特殊な機能の追加，言語に特化した処理の追加などに必須である．本章では，これらの目的を念頭に置いた設計・実装の方針について解説する．我々の考える言語非依存の形態素解析処理の流れを次に示す．入力された解析対象文を形態素片列として認識し，辞書検索を簡単にする．形態素片列に対し語辞書検索を行い品詞候補，及び，語自体のコストを与える．語を区切り・品詞の曖昧性を保持したままトレリス(trellis)データ構造に格納する．同時に状態遷移の情報もチェックし格納する．トレリスから最適解(語の列)を選択する．結果を出力する．このような処理の流れに基づき各処理をコンポーネントに分割した設計を行い，形態素解析ツールキットLimaTKを実装した．図に示すようなコンポーネントから成り立っている．全てのコンポーネントは独立しておりインターフェース等の仕様に従えば自作のコンポーネントと置き換えが可能である．これらのコンポーネントのうちで言語依存性の高いものは，形態素片認識，辞書検索，未定義語処理である．形態素片認識と辞書検索は形態素片の導入により，言語依存部分がほぼ解消されたと言える．形態素片認識の実装については，章で説明した．辞書検索の実装については節，最適解選択の実装については節，未定義語処理の実装については節で述べる．節ではLimaTKを用いて実装した形態素解析システムについて述べる．</section>
  <subsection title="辞書検索">辞書検索コンポーネントは形態素片列として認識された文から可能性のある語全てを辞書から獲得する．これらの処理の詳細については，章で既に述べた．現在の辞書検索コンポーネントの実装について述べる．語辞書のデータ構造であるTRIEを単純に実装すると大きなデータ領域が必要になる．そこで現在は，データ格納領域が小く済む2種類の方法で実装している．suffixarrayを使うものと，パトリシア木を使うものである．パトリシア木はデータ消費量が若干大きいが高速であり，suffixarrayは若干低速であるがデータ消費量が小さいという特徴がある．suffixarrayによる実装は高速文字列検索ライブラリを使用している．語としてどのようなものを辞書に入れておくかということは言語や用途に依存した問題である．例えば，英語で動詞イディオム``lookup''(``lookingup'',``lookedup''なども)を語として辞書登録したいとする．すると，以下の例の(1)では，登録された語が辞書検索の結果得られるが，(2)では分割されているのでイディオムとして認識されない．Ilookeduptheanswer.Ilookedtheanswerup.Websterらはこのような連続した文字列で表現できないイディオム，定型表現などを形態素解析処理の段階で扱うために，辞書検索とパージングの知識・処理を融合するという枠組を提案している．コンポーネント化設計により，文字列として連続していない語の認識処理も，他のコンポーネントに影響を与えないように辞書検索コンポーネントなどの内部で実装できる．しかし，我々はこのような言語の構造に関わる処理は形態素解析より後の高次の処理で扱うべきであると考える．これは我々の目標が，形態素解析システムの単純化・効率化・言語非依存性を目指すことにあるためである．</subsection>
  <subsection title="最適解選択処理">我々はHMMによる最適解の選択方法を採用した．最適解選択に必要なHMMパラメータはある程度の量の品詞タグ付きコーパスがあれば得られるので，特定の言語の解析が容易に始められるという利点がある．具体的には，品詞タグ付きコーパスから，語と品詞N-gramをカウントし，シンボル出力確率(品詞別単語出現確率)と状態遷移確率(品詞間，または，状態と品詞間の遷移確率)を計算し，動的計画法の一つであるビタビ・アルゴリズムで出現確率最大の解を求める．実際の実装は，積演算より和演算の方が効率的に処理できるという理由からパラメータ(確率値)の逆数の対数に適当な係数をかけた整数値(コスト)を用いている．コストの和演算で最適な解を選択する方法はコスト最小法とも呼ばれており，JUMAN，茶筌といった日本語形態素形態素解析システムなどで採用されている．つまり，和演算による実装は，これらのシステムで長年用いられてきた，人手によって調整されたコスト体系(単語コスト，接続コストなど)も利用できるという柔軟性を持っている．特定の言語のために形態素解析を行うためにユーザが必要なものは，形態素片認識を行うための情報と，語と接続表(HMMパラメータ)だけである．HMMパラメータは，十分な大きさの品詞タグ付きコーパスとユーザの望む統計モデル(bigram,trigram,variablememorymodelなど)に基づいた学習プログラムがあれば得られる．形態素情報管理コンポーネントは，前述の方法により最尤解選択を行う．これは形態素解析における解選択の一般的な実装方法である．文頭から文末へ向かって，一語ずつトレリス(ラティス)構造に格納してゆき，そのときにその語までの部分解析のコストを求める．最適解は，文末から文頭へ向かって，最適な部分解析のコストを持つノードを順次辿れば得られる．格納の際に必要になる，状態遷移(接続)にかかるコストと遷移先状態は，状態遷移表管理コンポーネントから得る．状態遷移表管理コンポーネントは，現在の状態と次の品詞をキーに状態遷移のコストと遷移先状態を返すという単純な仕事をする．</subsection>
  <subsection title="未定義語処理">未定義語処理コンポーネントは辞書に登録されていない語に対して品詞推定を行う．未定義語の品詞推定は統計的な方法と人手による規則などのヒューリスティックを用いる方法がある．統計的な方法は，未定義語が全ての品詞を持つと仮定し，トレリスでの曖昧性解消処理の段階で品詞N-gramの統計値により最適な品詞を自動的に選ぶという方法である．これは，言語に依存しない実装が可能である．しかし，この方法ではデータ格納領域が増大してしまい，処理効率が悪い．そこで，未定義語が全ての品詞を持つのではなく，あらかじめ「未定義語が推定されうる品詞」の集合を限定する方法が考えられる．例えば「この言語の未定義語は『名詞』か『固有名詞』である」と定義すれば，曖昧性解消処理で未定義語の品詞はどちらかに選ばれる．この方法は完全な推定とは言えないがデータ格納領域の増大を押えることができる現実的な方法であり，茶筌で採用されている．実用性と性能のバランスの良さから，LimaTKの未定義語処理コンポーネントの標準の機能として採用した．ヒューリスティックによる方法は，例えば，英語ならば，「文中で大文字で始まるなら固有名詞」「-tionで終われば名詞」といった規則を用いて品詞を推定する方法である．これは言語に依存する方法なので，言語ごとに処理系を実装する必要がある．LimaTKではこのようなルールを埋め込むためには，統計的手法による未定義語処理コンポーネントを修正するか，まったく新しく作り直す必要があるが，作り直す場合でもインターフェース規約を守れば他の処理に影響を与えずに実装できる．未定義語の長さ，すなわち，未定義語がいくつの形態素片で構成されるかを決定する処理も難しい．理論的にはある位置から始まる全ての長さの部分形態素片列が未定義語の候補になる可能性がある．しかし，これでは候補が増大してしまい処理効率に問題がある．日本語のように字種にバリエーションのある言語は，連続する漢字列・カタカナ列・記号列などを一まとめにするといった字種によるまとめ処理により未定義語の候補を限定できる．このような字種による未定義語候補の決定処理はJUMANや茶筌の様な日本語形態素解析システムに採用されている．単純なまとめ処理ではなく，字種による語の長さの分布の違いに着目して未定義語処理を行うという研究もある．わかち書きされる言語ではこれまでこの問題は起こらなかった．しかし，本研究では形態素片という概念を導入したため，わかち書きされる言語でも問題になるようになった．未定義語に複合語は無いと仮定すれば，ある位置から始まり次のデリミタまで間の全ての部分形態素片列を未定義語の候補とすれば良い．この仮定は正しいものではないが，実用上はさほど問題なく現実的である．そもそも未定義語処理は言語依存性の高い処理であり，品詞推定精度の高い共通の枠組を構築するのは困難である．より精度の高い処理を求めるユーザはやはりプログラムの調整を行う必要がある．ゆえに，我々は各言語共通に利用できる最低限の機能と調整の行いやすい枠組で実装を行った．言語非依存性とユーザの利便性と処理効率のバランスを考慮した実装と言える．実装方針をあげておく．未定義語の品詞推定:あらかじめ「未定義語が推定されうる品詞」の集合を定義し，最適解選択処理にまかせる未定義語の長さの決定:「形態素片」「字種によるまとまり」「デリミタに挟まれた領域」を未定義語を構成する単位に選択でき，未定義語を構成する「最大単位数」も指定できる[htb]3.9mm日本語~そんな感じがします．そんな30752263648[Y:ソンナBF:そんなP:連体詞Pr:100/3864]感じ69601981585[Y:カンジBF:感じP:名詞-一般Pr:37/144546]が130659744[Y:ガBF:がP:助詞-格助詞-一般Pr:17509/82739]し01172688[Y:シBF:するP:動詞-自立/サ変・スル/連用形Pr:10638/10638]ます3048731666[Y:マスBF:ますP:助動詞/特殊・マス/基本形Pr:813/30431]．1112072[PP:記号-句点Y:．BF:．P:記号-句点Pr:27418/27452]verbatim英語~Whatisaword?What2208201835[P:WPPr:218/3156]is946621373[P:VBZPr:8789/27619]a1207281384[P:DTPr:25820/111243]word662985117[P:NNPr:59/179722]?377154202[P:.Pr:556/53362]verbatim中国語~人力基盤構築不能促成．人力59292828[P:NaPr:185/372140]基盤82862828[P:NaPr:9/372140]構築68167070[P:VCPr:17/106692]不能39897878[P:DPr:1003/167440]促成84733232[P:VHPr:2/105135]．20073131[P:PERIODCATEGORYPr:6046/79413]verbatimfigure*</subsection>
  <subsection title="ツールキットによる形態素解析システムの実装">LimaTKを用いて，簡単な多言語対応形態素解析システムを作成した．日本語，英語，中国語，韓国語など様々な言語を実装した．図に解析結果の例を示す．各行はそれぞれ一つの形態素を表し，各列は左から，「見出し文字列」「各形態素の持つコスト」「品詞コード」「状態コード」「解析には用いないその他の情報(角括弧で囲まれている)」となっている．使用した言語データを次に示す．未定義語品詞推定のチューニング，高次のN-gramの利用，スムージングなどより高度な統計的手法を用いれば，それに応じて精度は向上する．しかし，本研究の目的は精度の向上ではないので，これ以上は追求しない．</subsection>
  <section title="関連研究">Websterらはわかち書きされる言語(英語)のイディオムや定型表現が扱えるように，英語のデリミタで区切られた単位をわかち書きされない言語(中国語を想定している)の文字に対応させわかち書きされない言語での解析方法を適用している．しかし，``They've''のような単語内区切り曖昧性の問題(節)には言及していない．曖昧性の解消については，確率を使った方法が適用できると言及されているのみである．わかち書きされる言語における境界の曖昧性解消について，Millsはわかち書きが不明瞭な古代英語の解析の際に，わかち書きされない言語で用いる形態素解析手法を単純に適用している．これは，節で説明した，文の全てのスペース()が削除された英文をわかち書きされない言語の解析手法で解析する方法である．しかし，この研究での解決法は，目的から分かるように，言語非依存ではない．また，単純なマッチング，ヒューリスティックによる力まかせな方法に基づいている．言語非依存な形態素解析の枠組として，Kimmoの二段階形態素論(two-levelmorphology)が知られている．簡単に言うと，オートマトンを用いてトークン認識と品詞付与を同時に行う方法である．我々は，わかち書きされる言語で考えられる分割の可能性，及び，わかち書きされる言語とされない言語との整合性も考慮し，形態素片という概念を導入した．形態素片はこれまで漠然と定義されていたトークンに替わる概念である．これにより言語非依存の枠組を構築した．このアイディアは極めて単純ではあるが，効率的なTRIE辞書検索の実現とトークン認識における非論理的な曖昧性の排除を達成した．本研究の成果物である形態素解析ツールキットLimaTKの柔軟性を示す例として文節まとめあげと形態素解析の融合についての研究をあげる．英語を念頭に考えると，これは品詞付与と名詞句(BaseNP)認識をHMMにより同時に行うという方法である．スペース()を「名詞句始まり」「名詞句終わり」「名詞句の間(終わりであり始まりである)」「名詞句途中」「名詞句の外部」のタグ(品詞)を持つ語として扱い，名詞句区切り情報と品詞情報の付与されたコーパスからHMMパラメータを学習し，それを用いて最適なものを選ぶという手法である．この手法は，形態素片を以下の情報で定義すればで簡単に実装できる．1文字≠1形態素片(わかち書きされる言語)デリミタ辞書:なし形態素片辞書:スペース()このように，LimaTKは様々な言語を解析できるだけでなく，形態素解析以外の用途にも適用できる汎用的なHMMパーザとして柔軟に利用できる．</section>
  <section title="おわりに">本論文では，言語に依存しない形態素解析の枠組の提案と，形態素解析の内部処理のコンポーネント化によるツールキットの設計・実装を行った．従来は，わかち書きするか否かという言語の特徴により大きく処理が異なる形態素解析処理を，形態素片という辞書検索単位を定義したことにより，言語非依存の共通の枠組で行えるようになった．また，形態素解析の内部処理のコンポーネント化により，言語非依存化のみならず様々な改良や他の言語処理への適用が行いやすくなった．本研究に関する情報・ツールキットのパッケージは下記のURLで入手できる．</section>
</root>
