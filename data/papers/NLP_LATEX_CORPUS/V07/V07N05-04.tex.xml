<?xml version="1.0" ?>
<root>
  <title>３つ以下の候補から係り先を選択する係り受け解析モデル</title>
  <author>金山博鳥澤健太郎	光石豊辻井潤一</author>
  <jabstract>本稿では、日本語係り受け解析のための統計的手法について述べる。この手法は、統計値の計算方法が従来の手法と異なる。従来の手法では、２つの文節間が依存関係にある確率をそれぞれの文節の組に対して計算するが、本研究で提案する「３つ組／４つ組モデル」は、係り元の文節と係り先の文節の候補となる全ての文節に関する情報を確率の条件部として、ある文節が係り先として選択される確率を求める。なお、係り先の候補は、HPSGに基づいた文法及びヒューリスティクスによって高々３つに絞られる。確率の推定には最大エントロピー法を用いており、我々の構文解析器はEDRコーパスに対して文節正解率88.6%という高い解析精度を達成した。</jabstract>
  <jkeywords>係り受け解析,統計的手法,文法,ＭＥ法</jkeywords>
  <section title="はじめに">本稿では、人手で記述された文法及び統計情報を用いて日本語の係り受け関係を求める手法について述べる。特に、文法とヒューリスティクスにより文節の係り先の候補を絞った時に構成することができる新しいモデルを提案し、それにより高い係り受けの精度（文節正解率88.6%）が得られることを示す。我々のグループでは、何らかの意味表現を構成できるような高機能な構文解析器を実現することを最終目標とし、HPSGの枠組みに基づいた文法を作成している。現状では意味表現の構成こそできていないが、新聞や雑誌などの実世界の文章の殆どに対して構文木を出力できる、被覆率の高い日本語文法SLUNGを開発した。しかしながら、文法的に可能な構造を列挙するだけでは、曖昧性が大きいため、実用に耐えない。また、今後の課題である意味構造の自動学習のためにも、曖昧性の解消が要求される。本研究では、文法を用いた構文解析の結果の曖昧性解消を目的として、文節単位の係り受け解析によって、最も可能性の高い統語構造を選択できるようにする。また、係り受け解析を行う際に文法を用いることが精度の向上に寄与している。係り受け解析は以下のような手順でなされる。まず、文法SLUNGで構文解析し、各文節の係り先の候補を、文法が許す文節に絞る。文法により絞った係り先候補が４つ以上存在する場合、それを係り元から見て(1)最も近い文節、(2)二番目に近い文節、(3)最も遠い文節の３つに制限する。これは、上記の三文節のいずれかが正解となる場合が98.6%を占めるという観察に基づいている。この制限により、以下で考える統計モデルにおいて、係り先の候補は常に３つ以下であるとみなせる。係り元文節がそれぞれの候補に係る確率を、３つ組／４つ組モデルを用いて求める。このモデルは、係り元の文節と、２つまたは３つの係り先候補の全てを同時に考慮するという特徴があり、最大エントロピー法を用いて推定される。文法が出力するそれぞれの部分木（文節間の係り受けに相当する）に上記の統計値を割り当てて、最も高い優先度が割り当てられた文全体の構文木が選択される。本研究で用いるモデルと他の研究でのモデルの違いについてであるが、従来の統計モデルでは、係り元文節i・係り先文節jに対して、係り元文節の属性_i及び係り先文節の属性_i,j（係り元と係り先の文節間の属性を含む）を前件として、係り受けが成立する（Tが出力される）条件付き確率enumsを求めていた。これに対し、本研究で用いる３つ組／４つ組モデルでは、係り元文節iの候補t_nに関して、iの属性を_i、t_k及びiとt_kの文節間の属性を_i,t_kとするとき、_iと全てのt_kに対する_i,t_kを前件として、n番目の候補が選ばれる条件付き確率P(it_n)=&amp;P(n_i,_i,t_1,_i,t_2)&amp;（候補が２つのとき:n=1,2）enums(it_n)=&amp;P(n_i,_i,t_1,_i,t_2,_i,t_3)&amp;（候補が３つのとき:n=1,2,3）eqnarrayenumsを求める。上記の(),~()式をそれぞれ３つ組モデル・４つ組モデルと呼ぶ。なお、ここでのn番目の候補とは、表層文中で係り元から数えてn番目の文節ではなく、文法的に許される係り先のうち２つまたは３つに絞ったものの中で、係り元からn番目に近い文節である。~節では、従来の統計方式の日本語係り受け解析に関する関連研究、本研究で用いる日本語文法、及び最大エントロピー法を紹介する。~節では、上記で概観した我々の手法を順に詳しく述べる。~節の実験結果で、対照実験の結果とともに３つ組／４つ組モデルの有効性を示す。そして、~節で、具体的なパラメータの観察や他研究との比較を行う。</section>
  <section title="関連研究">本節では、これまでに提案されてきた日本語構文解析のための統計的アプローチと、本研究で構文解析に用いる日本語文法SLUNG、及び確率モデルの推定に用いる最大エントロピー法を紹介する。</section>
  <subsection title="従来の統計的構文解析手法">日本語の係り受け解析のための統計的手法として、様々なモデルが考案されており、次の２つに大別される。前者に属するものとして、確率文脈自由文法を用いたものや、確率一般化LR法を用いたものなどがある。これらは、数学的に妥当な確率を用いることができ、形態素解析など様々なレベルとの統合が容易であるという利点があるものの、現状では係り受け解析の精度は最高でも白井らの85〜86%にとどまっている。一方、後者の手法は、比較的学習が容易なため、高い解析精度が得られる手法が多数提案されている。実際、最大87.9%と、生起確率に基づくものよりも高い精度が報告されている。本研究の手法もこのアプローチに基づいており、以下でいくつかの研究を紹介する。これらの手法及び本稿で提案する手法は、上記のP(ij)の求め方に違いがある。決定木を用いたモデル、最大エントロピー法を用いたモデル、距離確率と語彙確率を用いたモデルでは、係り元文節iの品詞や語彙や読点の有無など、係り先文節jの品詞や語彙、そして二文節間の距離・読点や副助詞「は」の数などを属性として、ある属性を持った二文節が存在する時にそれが係り受け関係にある確率を二文節i,j間の係り受けのしやすさとしている。英語の統計的構文解析において二語間の距離が係り受けを決定する重要な要素となるのと同様に、日本語の解析においても二文節間の距離が重要であるとされ、上記のモデルではいずれも文節間にある文節数を属性として用いている。これらのモデルでは、文節iとj以外の文節の情報は、文節間の距離などの属性を除いては反映されない。係り元・係り先とそのまわりの文節を考慮するモデルでは、係り元文節iの係り先文節jへの係りやすさの計算に、iより右側にある全ての文節の情報を用いている。そのために、二文節間の関係を、「係る」「係らない」の二値ではなく、「越えて、遠くの文節に係る」「係る」「手前の文節に係る」の三値を出力するものとして学習する。そして、iがjに係る確率を、iがi,j間の文節を「越える」確率とiがjより右側の文節より「手前に係る」確率の積で補正する。これにより、ある種の文脈情報が取り扱えることになり、解析精度がより約1%向上したことが報告されている。但し、このモデルでは、係り元文節がそれより右側にあるそれぞれの文節に「係る」か「越える」か「手前に係る」かを互いに独立であると仮定しなければならない。本研究で用いる３つ組／４つ組モデルでは、２つまたは３つの係り先候補の属性を同時に考慮できるため、文脈情報が扱えるうえ、さまざまな望ましい点がある。これに関しては本論文の~節以降で詳しく解説する。</subsection>
  <subsection title="日本語文法SLUNG">本論文で提案する手法では、人手で書かれた文法で候補を絞ることが必須である。我々が用いるSLUNGは、HPSGの枠組みで記述された日本語文法であり、8つのスキーマと、48個の語彙項目テンプレート、105個の語彙項目からなる。EDR日本語コーパスの文に対して98.4%と、非常に高い被覆率（構文木を一つでも返した文の割合）を示している。文法自体は曖昧性解消の機構を持っていないため、SLUNGを用いて構文解析した場合、文法的に許される全ての構文木が出力される。本研究では、文節係り受けの統計モデルを用いることにより、出力された構文木から最も優先度の高いものを選び出すことができるようになる。</subsection>
  <subsection title="最大エントロピー法">統計モデルの推定に、最大エントロピー法(ME法)を用いる。ME法では、「学習コーパス中の履歴の特定の条件を満たし、かつ特定の出力値を得る場合」（素性）の頻度を得て、様々な素性に対するパラメータを、出力値の確率分布が最も一様分布に近づくように調整して求める。別の素性に対し、それぞれ満たす集合に重なりがあってもよく、抽象度の高い素性と低い素性を任意に混ぜることができるため、統計モデルを構築する際のデータスパースネスの問題を軽減できる。日本語係り受け解析でもME法は非常に有用で、品詞の情報だけでなく、頻度の高い単語に対しては語彙的情報も加えるといった柔軟な素性の追加が容易である。本稿での実験における精度は、単純な相対頻度で推定した３つ組／４つ組モデルよりも約1.9%向上しているが、その要因として、ME法を用いることで以前よりも多くの素性を追加できたことが挙げられる。</subsection>
  <section title="本研究の手法">本節では、「３つ組／４つ組モデル」を用いて係り受け解析をする手順を解説する。係り受け解析の全体の流れは図~のようになっている。３つ組／４つ組モデルの準備として、~節で述べる手法により、各文節の係り先候補を３つ以下に制限する。まず、文法を用いて、各文節の係り先として文法的に正しいものを列挙する。その中で係り元から一番近い文節・二番目に近い文節・最も遠い文節を選び出し、他を無視する。そして、係り先の候補の集合の中で、ある要素が係り先として選択される確率を、係り元文節と全ての係り先の候補の属性を同時に考慮するモデル（３つ組／４つ組モデル）で推定する。~節では、モデルの特徴及び利点について述べる。最後に、上記のモデルを用いて文全体の最適な係り受けを選択する方法を、~節で解説する。</section>
  <subsection title="準備：係り受け候補の制限"/>
  <subsubsection title="文法の利用">本システムでは、文を入力とし、JUMANで形態素解析をした後、文法SLUNGで構文解析する。SLUNGは、JUMANの形態素を解析の単位として、文法的に正しい全ての構文木を出力する。これを係り先候補の制限に使うために、それぞれの構文木中の部分木を、図~のようにして、文節単位の係り受け構造に帰着させる。部分木Mの左部分木L、右部分木Rの最も右側にある語をそれぞれl,rとし、それらが属する文節をb(l),b(r)とするとき、b(l)はb(r)に係ることになる。一つの構文木は一つの係り受け構造に対応するが、可能な構文木が複数あるため、一つの係り元文節に対して、係り先候補となる文節が複数求まる。以下では、その候補の中から正しいものを選び出すことを考える。人手で記述する文法を用いることには、~節で述べたような我々の最終目標に達するための要件である他に、決してありえない構造を排除することができるという利点がある。文法の制約が過剰でないことは、~節で述べたようにSLUNGの被覆率が高いことが保証している。</subsubsection>
  <subsubsection title="係り先候補の３つ以下への制限">日本語の文節の係り先の傾向として、(1)近くから遠くになるに従って割合が減少すること、(2)最も遠い文節に係る場合だけは比較的多いことが知られている。この傾向は例えばで分析されている。SLUNGにより係り先候補を絞った場合にもこの傾向はやはり顕著である。EDRコーパスの文をSLUNGで解析した際の、係り先候補の数、及び正しい係り先の位置の関係の分布を表~に示す。表中の「第一」「第二」…は、文法で制限された係り先候補のうち、係り元文節から近い順に何番目が正しい係り先であるかを意味する。「最遠」は係り元から最も遠い候補である。このデータより、係り元文節から(1)最も近い文節・(2)二番目に近い文節・(3)最も遠い文節のいずれかに係る場合だけで98.6%を占めることがわかる。この性質を利用して、係り先の候補が４つ以上存在する場合にも上記の３文節だけを考え、その他の文節を無視することにする。この制限によって、係り受け精度の上限は98.6%となるが、わずか1.4%の犠牲により問題を大幅に単純化することができ、次節で述べる３つ組／４つ組モデルの構成が可能になる。</subsubsection>
  <subsection title="３つ組／４つ組モデル">３つ組／４つ組モデルは、文節iが文節t_nに係る確率P(it_n)を式(),~式()で計算する。但し、t_nは文節iの係り先の（３つ以下に限定された）候補、_iは文節iの属性、_i,t_nはt_n及び二文節i,t_n間の属性を表す。P(it_n)=&amp;P(n_i,_i,t_1,_i,t_2)&amp;（係り先の候補が２つのとき:n=1,2）enums(it_n)=&amp;P(n_i,_i,t_1,_i,t_2,_i,t_3)&amp;（係り先の候補が３つのとき:n=1,2,3）eqnarrayenumsこのモデルの特徴は、上記の式から推測される通り、「係り元文節と、係り先の候補となる全ての文節の属性を同時に考慮すること」、そして「それぞれの係り先の候補の係りやすさを求めるのではなく、各候補が選ばれる確率を求める」ことである。これらの意義は次の３点にある。2mm2mm以下で、これらの意義について順に述べる。</subsection>
  <subsubsection title="意義1~:~候補の中での相対的位置">文節間の距離は、係り受け解析における重要な要素として考えられているが、係り先の候補の中の位置の方が重要な場合がある。例として、()の各文における「彼が」の係り先を推定する時を考える。両者とも、「走るのを」が正しい係り先と考えられる。2mmequation2mm文法を用いずに文節数を距離とするモデルでは、「彼が」と「走るのを」の文節間距離はaでは1、bでは2と異なっている反面、aでの「彼が→見た」とbでの「彼が→走るのを」が、係り元からの距離が2である動詞であるという点で、似た事象であると見なされる。一方、文法で係り先を絞った場合、a,~bとも「彼が」の係り先の候補は「走るのを」と「見た」の２つとなる。このように、係り先の候補のみに着目すれば、両者を同じ事象として扱えるので、より効率のよい学習が行えるようになる。</subsubsection>
  <subsubsection title="意義2~:~文脈の考慮">()において、「私の」の係り先を考える。正解は、それぞれ「娘に」「友人の」である。2mmequation2mm係り元文節と係り先文節、及び文節間距離を考えるモデルでは、a,bにおける「私の→娘に」は区別されることなく、全く同じ係り受け確率が付与される。しかしながら、この確率は非常に低くなる。なぜなら、実際にEDRコーパスの一部を観察したところ、aの「N_1のAN_2」とAはそれぞれ名詞、形容詞を表す。という構文に対し、bのような「N_1のN_2N_3」の構文の頻度が４倍程度あり、後者の構文では、N_1は近くのN_2を修飾する場合が約75％と、圧倒的に多いからである。従って、aにおいて、「私の→娘に」に比べて「私の→かわいい」の確率のほうが高くなり、解析誤りを引き起こす。係り元と係り先の３つの候補全てを同時に考慮すると、この誤りを防ぐことができる。aにおいて「私の」と、その係り先候補である「かわいい」「娘に」「会った。」を同時に考えて、三者のそれぞれが選ばれる確率を計算した場合、第二候補であっても、第一候補の形容詞連体形よりも高い確率が割り当てられ、正しく係り先を求めることができる。このような現象は、第一候補である形容詞や副詞を飛び越えて第二候補に係るケースなどで一般的に数多く見受けられる。</subsubsection>
  <subsubsection title="意義3~:~同じ条件下での係りやすさの計算">これは意義2~とも関連するが、ある一つの係り元に対する係り受けの確率を、共通の前件を持った条件付き確率で計算できるという利点である。(a)の「私の」の係り先を考える際には、従来の手法は式()、我々の手法は式()を求めることになる。()ではそれぞれの条件付き確率の前件が異なるため、５つの値の和は1にならないのに対し、式()では３つの和が1になる。従って、３つ組／４つ組モデルにおいて推定する条件付き確率は、係り元とその係り先候補がある文脈において、それぞれの係り先候補が選ばれる確率に一致することになる。なお、考慮する条件を図示すると、それぞれ図~、図~のようになる。enumsenums</subsubsection>
  <subsection title="最適な係り受けの選択">各文節間の係りやすさP(ij)を求めるにあたって、係り元文節に対する係り先文節の候補の数に依って、次のようなモデルを用いることにする。係り先候補が１つの場合：その係り先に確定するため、P(ij)=1.0となる。係り先候補が２つの場合：係り元と２つの係り先の文節の情報を考慮する「３つ組モデル」を用いる。係り先候補が３つ以上の場合：係り先の候補のうち、係り元に最も近い文節、二番目に近い文節、最も遠い文節の３つだけを考え、係り元とその３つの文節の情報を考慮する「４つ組モデル」を用いる。こうして求まった値を用いて、SLUNGの出力した全ての部分木Mに対して、統計値Q()を以下のようなアルゴリズムで割り振る。なお、SLUNGの出力する構文木の終端記号は、文節単位でなく、単語（JUMANの出力する形態素）を単位としている。部分木Mがただ一つの単語からなる場合、Q()=1.0そうでない場合、図~の部分木において、左部分木Lの最も右側の単語をl、右部分木Rの最も右側の単語をrとして、l、rの属する文節をそれぞれb(l)、b(r)とする。このとき、enums文全体に対応する構文木で、この統計値が最大になるようなものを探索し、その構文木を再び文節の係り受け関係に変換して出力する。こうして得られた文の係り受けは、必ず文法的に正しい構文木に対応しており、係り受け同士が交差することはない。</subsection>
  <section title="実験結果">３つ組／４つ組モデルを用いた係り受け解析の実験環境と用いた素性、及び実験結果を示す。さらに、学習コーパスの量を変えた実験や、３つ組／４つ組モデルを導入したことの効用を確かめるための対照実験の結果を載せる。</section>
  <subsection title="実験環境">EDR日本語コーパスの208,157文のうち、192,778文を学習、3,372文をテストに用いた。節で述べたような観察や、次節で述べる考察などにはその他の6,744文を用いている。これは、テストコーパスの解析結果を人が見てモデルを修正することによるコーパスへの特化を防ぐためである。前節で述べた通り、係り先の候補が２つの場合のための「３つ組モデル」と候補が３つ以上の場合のための「４つ組モデル」の二つのモデルを別個に作る。学習コーパス中の文をSLUNGで構文解析して、係り先候補が２つである文節に対して、係り元文節と２つの係り先候補の属性の組を履歴として「３つ組モデル」を構成する。そして、係り先候補が３つ以上である文節に対しては、節で述べた方法で候補を３つに制限し、係り元文節と３つの係り先候補の属性の組を履歴として「４つ組モデル」を構成する。これらは最大エントロピー法のツールChoiceMakerMaximumEntropyEstimatorを使って推定される。推定の際に用いた素性を表~に示す。素性の値は~に倣っており、品詞の分類などにはJUMANの出力結果を用いている。但し、京大コーパスを用いた実験と違って、形態素解析の正解は与えられておらず、誤りを含む場合がある。以下で各素性について解説する。なお、主辞とは、品詞大分類が「特殊」「助動詞」「助詞」「接尾辞」「判定詞」のいずれかであるものを除いて、文節内で最も右側にある語、語形とは、品詞大分類が「特殊」であるものを除いて、文節内で最も右側にある語である。表~中の「異なり数」とは各素性の取りうる値の総数であり、素性番号19〜27の組み合わせ素性に関しては、それぞれの要素の積を記してある。実際には、履歴の数と出力値の数（2または3）の積だけの素性が用いられる。また、係り先に関する素性（素性番号8〜27）は、それぞれの係り先候補（３つ組モデルでは２つ、４つ組モデルでは３つ）に対して素性が割り振られる。このうち、コーパス中で３回以上出現したものが有効素性となる。</subsection>
  <subsection title="対照実験">~節で述べた３つ組／４つ組モデルの有効性を示すために、以下のような対照実験を行った。これらのモデルでは、他の統計的係り受け解析モデル~~と同様に、二つの文節及び文節間の属性から、二文節間の係りやすさを独立に計算する。また、係り先候補の中での位置を出力とする代わりに、係り元と係り先の文節間の距離（「１」「２から５」「６以上」の３値）を導入している。ME法による推定において~節に示した素性と同じ素性を使っており、その全てに対して上記の距離の属性を組み合わせている。対照実験の結果は表~の通りである。「３つ組／４つ組モデル」は「２つ組モデル」と比べて精度が0.9%ほど向上している。このデータから、３つ組／４つ組モデルが有効であることを次節にて論じる。</subsection>
  <section title="考察">ここでは、本稿で提案する手法がどのように精度向上に寄与しているかの観察、及び他研究との比較を行う。</section>
  <subsection title="「３つ組／４つ組モデル」の効用">表~にある対照実験の結果は、以下の理由から３つ組／４つ組モデルの有効性を示しているといえる。「３つ組／４つ組モデル」の精度は「２つ組モデル」の精度よりも約0.9%上回っている。両者とも、文法とヒューリスティクスにより係り先候補を３つ以下に限定しているが、それらの係り先候補を同時に考慮するモデルを用いた方が精度が上がることが確認された。「２つ組モデル」は、「文法なしモデル」より1.0%、「候補限定なしモデル」よりも0.3%高い精度を出している。従って、文法を用いることや係り先候補を３つに限定することは妥当な措置であり、「２つ組モデル」は「３つ組／４つ組モデル」の比較対象として適当である。次に、両者のモデルで実際に解析を行う時の、具体的なMEのパラメータを観察してみる。例として、文()の「子供たちの」の各候補への係りやすさを計算する。「子供たちの」の係り先候補は、「甲高い」「声で」「騒然となる。」の３文節で、正解は「声で」である。2mmenums()そんなとき、子供たちの甲高い声で騒然となる。equation2mm各候補への係りやすさを２つ組モデル・４つ組モデルで推定する際のME法のパラメータ_kのうち主な（|_k|が大きい）ものを、それぞれ表~,表~に示す。パラメータ_kのうち、履歴a、出力値bに対応する素性のものを掛け合わせるので、_kの値が1.0より大きいものは出力値をbにすることを助長するパラメータ、1.0より小さいものはbにすることを抑制するパラメータである。「_kの積」の項は、表に載せていないものも含め、対応する出力値に関する全てのパラメータの積である。</subsection>
  <subsubsection title="２つ組モデルの場合">このモデルでは、係り先ごとに別々の条件で係りやすさを計算する。各係り先への係りやすさP(ij)は、出力値Ｔに対する_kの積を、出力値Ｔ,~Ｆに対する_kの積の和で割ったものである。例えば、P(子供たちの甲高い)は、0.93/(0.93+0.81)=0.53となる。「声で」に係る場合のパラメータに注目すると、係り元助詞「の」は隣の文節に係る傾向が強いことから、文節間距離が「２から５」に対するパラメータが小さくなっている。そのため、「甲高い」に係る確率の方が高くなってしまう。</subsubsection>
  <subsubsection title="４つ組モデルの場合">全ての係り先への係りやすさを共通の確率分布を用いて計算する。出力値bは1,2,3の３値をとり、第一候補への係りやすさP(it_1)は出力値１に対する_kの積を、３つの出力値に対する_kの積の和で割ったものであり、表~の例では0.682/(0.682+2.39+0.106)=0.215となる。出力値が2となる場合のパラメータに着目する。係り元が「の」で、第一候補が「形容詞」であること、第二候補が「名詞」であること、第三候補が「形容詞」であることの全てが第二候補に係るパラメータを高めており、第二候補に係る確率が第一候補に係る確率を上回っている。特に、出力値bと異なる候補（この場合、第一・第三候補）に関係する素性も強い影響を及ぼしていることが興味深い。</subsubsection>
  <subsection title="他研究との比較"/>
  <subsubsection title="EDRコーパスでの精度の比較">係り受けの精度判定にEDRコーパスを用いている他研究と比較してみる。決定木を用いた手法での精度は84〜85%、語の共起確率を用いた手法では、86.8%となっている。我々の手法はこれらを上回っており、EDRコーパスに対してテストした中では最も高い水準といえよう。また、では、３つ組／４つ組モデルを単純な相対頻度を用いて構成している。そこでの精度は86.7%であり、ME法の利用によって約1.9%精度が向上したことになる。精度向上の要因は、ME法によってデータスパースネスの問題が軽減でき、従来は入れられなかった語彙や活用に関する素性を追加できたことであると思われる。</subsubsection>
  <subsubsection title="京大コーパスでの精度の比較">いくつかの研究では、京大コーパスを用いて精度を測っている。構文的・語彙的情報を統合して構文木の生起確率を求めている手法での精度は85〜86%である。本研究と同様に、ME法を用いた研究~,~では、京大コーパスの1月9日分の1,246文を用いている。比較のために、同じコーパスでテストした結果は、表~のようになった。文末から決定的に係り先を決定するモデルの精度は87.14%で我々と同程度、後方文脈を考慮するモデルは87.93%で我々の精度よりも高くなっている。その原因として、以下のことが考えられる。我々は、学習データとしてEDRコーパスを用いている。などと比べて約24倍の学習データがあるとはいえ、括弧付けの方針の違いなどから、京大コーパスでの解析の誤りを引き起こすことが多い。関根ら、内元らは京大コーパス中にある形態素解析・文節区切りの結果を用いているのに対し、我々はJUMANで解析したものを用いているため、形態素解析の誤りを含み、解析誤りの原因となっている。文法SLUNGがEDRコーパスの括弧付けの方針に従って作られており、京大コーパスにあるような係り方を許さない場合がある。現在のところ、京大コーパスの解析には被覆率・精度ともに充分でないが、文法やシステムの改変により対処した上で本論文で提案する手法を有効に適用できるようにすれば、より高い精度が得られると考えている。</subsubsection>
  <subsubsection title="学習量の比較">図~より、最高値に近い精度を得るためには、10〜15万文の学習コーパスを要している。この学習量は、EDRコーパスを用いている研究と同程度であり、などの京大コーパスを用いた場合より、20倍程度の学習量になっている。一般に、ME法を用いることにより学習量を減らすことができると考えられているが、３つ組／４つ組モデルでは、複数の係り先に関する属性を同時に捉える条件付き確率を用いているため、区別される事象の数が大きくなり、多くの学習量が必要になっている。我々のモデルは、EDRコーパスのような多くの学習データを有効に利用できるモデルであるといえる反面、京大コーパスのように学習データ量が限られている時には、より効率のよい素性選択などが要求されるであろう。</subsubsection>
  <subsubsection title="解析速度の比較">本研究での係り受け解析は、あくまで詳細な構文構造を得るという目標の前段階であるため、速度に焦点を当ててはいないが、参考のために比較しておく。文末から決定的に係り先を決定するモデルでは、一文当たり平均0.03秒（SunUltra10,300MHz）で解析できるのに対し、一方、我々のシステムではEDRコーパスの文に対して平均約0.5秒（PentiumIII,500MHz:経験的に、上記の計算機の約３倍の速度）を要する。両者には大きな差があるものの、我々の速度も非実用的なものではない。また、そのほとんどはHPSGパーザによる部分木の生成の時間である。単に係り受け構造を求めるだけなら速度を向上する余地は多分にあるうえ、HPSGパーザ自体の高速化も研究されており、速度の問題は深刻であるとは考えていない。</subsubsection>
  <section title="まとめ">本稿では、文法を用いて係り受け解析をする際に望ましい統計モデルについて論じた。係り先の候補を文法が許すものに制限した後、係り元から最も近い文節・二番目に近い文節・最も遠い文節のみに絞る。これにより、係り元と全ての係り先候補の属性を同時に考慮する「３つ組／４つ組モデル」を用いることができるようになり、88.6%という高い係り受け精度を達成した。また、このモデルが精度向上に確かに寄与していることを示した。plain++document</section>
</root>
