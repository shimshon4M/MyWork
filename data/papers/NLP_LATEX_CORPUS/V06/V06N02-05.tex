



\documentstyle[jnlpbbl,epsf]{jnlp_j_b5}

\setcounter{page}{83}
\setcounter{巻数}{6}
\setcounter{号数}{2}
\setcounter{年}{1999}
\setcounter{月}{1}
\受付{1998}{4}{8}
\採録{1998}{8}{31}

\setcounter{secnumdepth}{2}

\title{発話単位の分割または接合による \\ 言語処理単位への変換手法}
\author{竹沢 寿幸\affiref{ATR} \and 森元 逞\affiref{ATR}\affiref{Fukuoka-U}}

\headauthor{竹沢 寿幸・森元 逞}
\headtitle{発話単位の分割または接合による言語処理単位への変換手法}

\affilabel{ATR}{ATR音声翻訳通信研究所}
{ATR Interpreting Telecommunications Research Laboratories}
\affilabel{Fukuoka-U}{現在，福岡大学工学部電子情報工学科}
{Currently with Department of Electronics Engineering and Computer
Science, Fukuoka University}

\jabstract{
自然で自発的な発話を対象とする音声翻訳ないし音声対話システムへの入力と
しての発話は文に限定できない．一方，言語翻訳処理における処理単位は文で
ある．話し言葉における文に関して，計算機処理から見て十分な知見は得られ
ていないので，文の代わりに「言語処理単位」と呼ぶことにする．まず，一つ
の発話を複数の言語処理単位に分割したり，複数の発話をまとめて一つの言語
処理単位に接合する必要があることを，通訳者を介した会話音声データを使っ
て示す．次に，ポーズと細分化された品詞の $N$-gram を使って，発話単位か
ら言語処理単位に変換できることを実験により示す．}

\jkeywords{発話単位，言語処理単位，ポーズ，統計的言語処理，会話音声}

\etitle{Transformation into Meaningful Chunks by \\ Dividing or
Connecting Utterance Units}
\eauthor{Toshiyuki Takezawa\affiref{ATR}
 \and Tsuyoshi Morimoto\affiref{ATR}\affiref{Fukuoka-U}} 

\eabstract{
The utterance units that serve as input to speech translation and/or
spoken dialogue systems that handle spontaneous speech are not always
sentences.  However, the processing units of language translation are
sentences.  Since we do not have enough knowledge about the sentences
of spoken languages, we use the term ``meaningful chunks'' instead of
sentences.  First, using conventionally interpreted dialogue data, we
show that utterance units sometimes need to be divided into several
meaningful chunks, and sometimes need to be connected to make up a
single meaningful chunk.  Next, we propose a method of transforming
from utterance units to meaningful chunks based on pause information
and the $N$-gram of fine-grained part-of-speech subcategories.  We
have conducted experiments and have confirmed that our method yields
good results.}

\ekeywords{Utterance units, meaningful chunks, pause, statistical
language processing, conversational speech.}

\begin{document}
\maketitle



\section{まえがき}

自然で自発的な発話を対象とする音声翻訳ないし音声対話システムの構築を目
指している．読み上げ文を対象とする音声認識研究においては文が処理単位と
なっている．また，従来の音声翻訳ないし音声対話システムへの入力は，文節
区切りのようなゆっくり丁寧に発話された文を単位とする音声であった
\cite{Morimoto96}．ここで，音声翻訳システムや音声対話システム等の音声
認識応用システムへの入力となる機械的に自動処理可能な単位を「発話単位」
と呼ぶことにすると，自然で自発的な発話を対象とする音声翻訳ないし音声対
話システムへの入力としての発話単位は文に限定できない．

一方，言語翻訳処理における処理単位は文である．書き言葉を対象とする自然
言語処理システムにおける処理単位も一般に文である．話し言葉を対象とする
言語翻訳処理における処理単位も文である\cite{Furuse97}．音声対話システ
ムにおける問題解決器のための解釈の処理単位も暗黙の内に文ないし文相当の
ものを想定していると考えられる．

ところで，本稿では文の定義の議論はしない．例えば，文献\cite{Masuoka92}
等に文に関する説明がある．また，話し言葉における文は，無音と韻律に代表
される表層のレベル，構造のレベル，意味のレベルで特徴付けられると言われ
るが，計算機処理から見て十分な知見は得られていない\cite{Ishizaki96}．
そこで，本稿では文という術語は使わず，翻訳や解釈のための自然言語処理単
位という観点から「言語処理単位」と呼ぶことにする．

まず，{\bf 2}で一つの発話を複数の言語処理単位に分割したり，複数の発話
をまとめて一つの言語処理単位に接合する必要があることを，通訳者を介した
会話音声データを使って示す．次に，{\bf 3}でポーズと細分化された品詞の 
$N$-gram を使って，発話単位から言語処理単位に変換できることを実験によ
り示す．最後に{\bf 4}で全体をまとめ，今後の展望を述べる．

\section{発話単位から言語処理単位への変換の必要性}

\subsection{音声翻訳システムへの入力としての発話単位}

音声翻訳研究のために，日本語話者と英語話者の，通訳者を介した対話を収集
し，データベース化している\cite{Morimoto94,Takezawa98b}．通訳の質を高
めるために，日英方向と英日方向の2名の通訳者を介した．「近未来の音声翻
訳システム」のための基礎資料を目指しているため，通訳者は1回の発話毎に
逐次的に通訳を行う．通訳者が正確に伝えられるために，1回の発話は10秒以
内とした．また，相手の話している間に割り込むことは禁止した．ホテル担当
者やホテル滞在者であるという設定資料を用意し，それをもとに模擬対話を行っ
ている．役割や設定をいろいろ変化させた上で，多くの話者に演じてもらい，
多様な音声言語現象を収録した．

このようにして集めた会話音声データにおいて会話参加者が逐次通訳者に渡す
発話は音声認識応用システムへの入力となる機械的に自動処理可能な単位とは
異なるが，機械的に振る舞う逐次通訳者に会話参加者が渡す発話を入力と仮定
することは使い勝手の良い音声認識応用システムの研究開発につながるものと
期待できる．そこで，機械的に自動処理可能な単位との関連も含めて，まず第
一段階としてこのような会話データの調査を行うことにする．

\subsection{発話単位の分割による言語処理単位への変換}

句や節を単位として漸進的に翻訳処理を行う研究\cite{Mima97}も開始されて
いるが，現段階では，言語翻訳は文を単位とすることが妥当である
\cite{Furuse97}．応答の「はい」等を含む感動詞はそれだけで文を構成する
と言われている\cite{Masuoka92}．しかしながら，我々のデータベースでは
「はい．それで結構です．」と書き起こされていたり，「はい，それで結構で
す．」と書き起こされていたりする．無音区間の長さと韻律的な情報に着目す
るよう指示したガイドラインにより区別されているが，その都度物理量を測定
することはせず，書き起こし作業者の判断に委ねられている部分もある．言語
処理単位の手がかりとして，句点で区切られた節境界に注目する．

まず，一つの発話を複数の言語処理単位へ分割する必要がある例を示す．
ポーズの長さの情報を[]内に記す．

\begin{itemize}
\item[(1)] お待たせいたしました．[440ms]シングル一泊一万円のお部屋でしたね．
\item[(2)] お部屋を調べます．[170ms]しばらくお待ちください．
\end{itemize}

例(1) (2)共に一つの発話が二つの言語処理単位で構成されている．今回調査
対象としている対話音声データベースの一部を使って我々が以前に行った実験
\cite{Takezawa96}では，対数パワーとゼロ交差数の二つの特徴量を用い，
300~ms を閾値としてポーズを自動検出したところ，促音と区別してポーズを
検出できた．そこで，一応の目安として 300~ms を閾値として機械的に自動処
理することを想定すれば，例(1)のポーズは 300~ms より長いので，二つの言
語処理単位に分割することができる．しかし，例(2)のポーズは 300~ms より
短いので，二つの言語処理単位に機械的に分割することはできない．したがっ
て，無音区間に関する物理量のみで発話を言語処理単位に自動分割することは
難しい．

\subsection{発話単位の接合による言語処理単位への変換}

次に，複数の発話を一つの言語処理単位に接合する必要がある例を示す．

\begin{itemize}
\item[(3)] 
\begin{itemize}
\item[(a)] カードの番号が，[680ms]五二七九．
\item[(b)] 三九二零．
\item[(c)] 二四六九．
\item[(d)] 零零九八[410ms]でございますね．
\end{itemize}
\end{itemize}

例(3)はカードの番号を確認する際に，数字の桁毎に区切って通訳者に渡した
事例である．(a) (b) (c) (d) は別の発話となっており，それぞれに対して通
訳が個別に挿入されている．現在の我々のデータベースでは，発話の最後は必
ず句点で書き起こす決まりになっているため，(a) (b) (c) (d) の最後に句点
が置かれている．

文献\cite{Takezawa95}で我々が「箇条発話」と名付けたものは，話者ないし
項目の数と内容により，同じ発話となったり，別の発話に分けられたりする．
通訳の単位としては無理に接合しなくとも良いが，何らかの問題解決のための
解釈の単位としては接合した方が良い可能性もある．接続することを明示する
ためには，例えば，例(3')のように書き起こせば良い．

\begin{itemize}
\item[(3')] 
\begin{itemize}
\item[(a')] カードの番号が，[680ms]五二七九，
\item[(b')] 三九二零，
\item[(c')] 二四六九，
\item[(d')] 零零九八[410ms]でございますね．
\end{itemize}
\end{itemize}

発話の最後に読点を挿入するか，あるいは句点を挿入しないことで，発話とし
ては終了していたとしても，言語処理単位としては継続することを表現できる．
音声翻訳ないし音声対話システムの枠組みでも，言語処理単位が終了していな
い発話の最後には読点を挿入するか，あるいは句点を挿入しなければ，発話の
接合による言語処理単位への変換をインタフェースとして実現することができ
る．

\begin{itemize}
\item[(4)] お待たせいたしました．[1600ms]洋室は，[1130ms]一泊二食付き，
[450ms]二万円で，補助ベッドが入ります．
\end{itemize}

既に述べたように，我々のデータベースでは通訳者に渡す発話を単位として音
声波形ファイルを切り出している．したがって，例(4)は一つの単位として切
り出され，データベース化されている．しかしながら，我々の目標とする自然
で自発的な発話を対象とする音声翻訳システム\cite{Takezawa98a}において音
声の終端検出を自動的に行う\cite{Reaves98}と，この音声波形切り出し単位
は細かく分割される可能性がある．例えば，1秒より長い無音区間を終端とみ
なせば\cite{Reaves98}，例(4)は三つの発話単位から構成される．その場合，
「洋室は，」という発話単位はその次の発話単位と接合した方が良い可能性が
ある．

\begin{itemize}
\item[(5)] シングルの[390ms]シャワー付きのお部屋が$\cdots$
\end{itemize}

300msを閾値として発話単位に分割すると，例(5)は翻訳のための言語処理単位
よりも小さい単位に分割され過ぎてしまう事例である．

\section{発話単位から言語処理単位への変換手法}

\subsection{単語・品詞並びを使った句点相当の節境界検出}

英語の自然で自発的な発話を対象とする音声認識結果を構文解析する研究
\cite{Lavie96}において，ロバストなパーザの探索空間を削減するために，節
境界情報を利用する試みが検討されている．そこでは，確定した音声認識結果
全体を入力とし，現在位置の前後2単語(合計4単語)までの範囲を参照して，そ
の位置の節境界らしさの値を求め，それが閾値を越えれば節境界とみなしてい
る．

そこで，単語・品詞並びを使った句点相当の節境界検出手法を検討する．先行
研究\cite{Lavie96}で提案されている推定式を次に示す．$\bullet$ の位置が
句点相当の節境界の位置である．その前に二つの単語 $w_1 w_2$ があり，そ
の後に二つの単語 $w_3 w_4$ がある．
\begin{equation}
\tilde{F}([w_1 w_2 \bullet w_3 w_4])=\frac
 {C([w_1 w_2 \bullet])+C([w_2 \bullet w_3])+C([\bullet w_3 w_4])}
 {C([w_1 w_2])+C([w_2 w_3])+C([w_3 w_4])}
\end{equation}
ここで，$C([w_i w_j \bullet])$ はバイグラム $[w_i w_j]$ の右に句点相当
の節境界が現れる回数であり，$C([w_i w_j])$ はバイグラム $[w_i w_j]$ が
訓練セットに現れる総数である．他の記号も同様である．

訓練データ中にバイグラムと一緒に現れる句点相当の節境界の回数から求めら
れる個別の頻度 $F([w_1 w_2 \bullet])$, $F([w_2 \bullet w_3])$,
$F([\bullet w_3 w_4])$ の平均や線形結合よりも効果的であったと報告され
ている\cite{Lavie96}．理由は，十分信頼できる情報を含んでいない低い出現
頻度のバイグラムを，他の要因と同じように使わないためである．なお，
$F([w_i w_j \bullet])$ は次式で表される．
\begin{equation}
F([w_i w_j \bullet])=\frac{C([w_i w_j \bullet])}{C([w_i w_j])}
\end{equation}
$F([w_i \bullet w_j])$ と $F([\bullet w_i w_j])$ も同様に求められる．

$\tilde{F}$ の値が閾値より大きければそこを句点相当の節境界とする．閾値
の値は人手で設定し，訓練セットに対して最良の性能が得られるように調整す
る．

文献\cite{Lavie96}では単語のみ検討しているが，我々は日本語を対象とし，
次の3通りの組合わせを調べる．
\begin{enumerate}
\item 品詞のみ
\item 品詞・活用形・活用型(プレターミナル\cite{Takezawa96}と同等)
\item 表層表現・品詞・活用形・活用型(単語と同等)
\end{enumerate}
さらに，それぞれに対して，現在位置の前後2単語(合計4単語)の範囲を参照す
る場合(式(1))と，前2単語と後1単語の合計3単語の範囲を参照する場合(次式
(3))を検討する．
\begin{equation}
\tilde{F}([w_1 w_2 \bullet w_3])=\frac
 {C([w_1 w_2 \bullet])+C([w_2 \bullet w_3])}
 {C([w_1 w_2])+C([w_2 w_3])}
\end{equation}

\vspace{-3mm}
\subsection{分割または接合による言語処理単位への変換}
\vspace{-1mm}

先行研究\cite{Lavie96}では長い発話を分割することのみを検討していた．我々
は分割のみならず接合による言語処理単位への変換も検討する．単語・品詞並
びを使った句点相当の節境界検出のための統計モデルとポーズ情報を組み合わ
せた手法を考える．

{\bf\dg 表\ref{t:combination}}において，(A) と (B) には句点「．」を挿
入する．(D) には何も挿入しない．(C) は箇条発話が相当し，扱いが難しいが，
原則的には読点「，」を挿入する．

\begin{table}
\caption{分割または接合による言語処理単位への変換}\label{t:combination}
\begin{center}
\begin{tabular}{|l||c|c|}
\hline
 & \multicolumn{1}{l|}{閾値より長い無音あり}
 & \multicolumn{1}{l|}{閾値より長い無音なし} \\
\hline\hline
単語・品詞並びの統計モデルが成功する & (A) 句点挿入 & (B) 句点挿入 \\
\hline
単語・品詞並びの統計モデルが失敗する & (C) 読点挿入 & (D) 挿入なし \\
\hline
\end{tabular}
\end{center}
\end{table}

ATR音声言語データベース \cite{Morimoto94} の618会話から音声認識と言語
翻訳を接続する評価実験用のホテル予約9会話を選択した．日本語話者が客の
役割を務めているものが4会話，日本語話者がホテル担当者を務めているもの
が5会話である．そのホテル予約9会話には166ターン(発話権の交代)，216発話
あった．内容を確認したところ，体言止めの箇条発話は含まれていたが，発話
を接合して言語処理単位に変換する必要のある事例はその9会話には含まれて
いなかった．つまり，例(3)ないし(3')のような発話の仕方はまれである．

一方，その166ターン，216発話の書き起こしテキストに含まれる句点の数は
289個であった．発話の最後は必ず句点で書き起こす決まりになっているので，
ターンの途中にある句点の数は123個，発話の途中にある句点の数は73個であ
る．つまり，通訳者へ渡す発話を分割する必要のある事例の頻度は多い．

発話の最後が言語処理単位の最後になっているのか，それとも，言語処理単位
としてはまだ継続するのかに関して，単語・品詞並びの統計モデルおよび韻律
的な特徴等により判断すれば，発話の接合による言語処理単位への変換をイン
タフェースとして実現できる．

しかしながら，発話を接合する必要のある事例は少ないので，以下では，まず，
発話を分割する手法について検討する．

\subsection{発話単位の分割に関する実験}

\subsubsection{準備}

評価実験用のホテル予約9会話以外の609会話を訓練に用いた．訓練は発話権の
交代(ターン)を単位として行った．箇条発話は話者ないし項目の数と内容によ
り同じ発話となったり，別の発話となったりするため，その影響を除くことを
意図した．ターンの始めには開始記号を挿入し，ターンの終りには終了記号を
挿入した．発話の開始と終了の情報は使わなかった．書き起こしテキストの句
点をそのまま句点相当の正しい節境界とみなした．

\subsubsection{書き起こしテキストを用いた実験結果}

書き起こしテキストを用いた予備実験を行った．句点と読点を除いた形態素列
を入力とした．訓練時と同様に，発話の開始と終了の情報は使わず，発話権の
交代(ターン)毎に一つの入力単位とした．ターンの途中にある句点123個が評
価対象となる．書き起こしテキストの句点を正解として，再現率と適合率を求
め，評価する．その際，結果を三つに分類する．

\begin{enumerate}
\item 句点相当の節境界で成功する: 正解 [○]
\item 句点相当の節境界で失敗する: 誤り [×]
\item 句点相当の節境界ではない場所で成功する: 涌き出し誤り [※]
\end{enumerate}

\begin{equation}
再現率=\frac{○}{○+×}
\end{equation}
\begin{equation}
適合率=\frac{○}{○+※}
\end{equation}

まず，閾値を 0.10 にそろえ，粒度および参照する範囲の違いの比較・検討を
行った．結果を{\bf\dg 表\ref{t:hikaku}}に示す．

\begin{table}
\caption{粒度および参照する範囲の違いの比較}\label{t:hikaku}
\begin{center}
\begin{tabular}{|l||r||r|r|r|r|r|r|}
\hline
 & \multicolumn{1}{c||}{条件} 
                 & \multicolumn{2}{c|}{品詞のみ}
                 & \multicolumn{2}{c|}{品詞・活用形・活用型}
                 & \multicolumn{2}{c|}{単語} \\
\cline{2-8}
                 & 閾値 & 再現率 & 適合率 & 再現率 & 適合率 & 再現率 & 適合率 \\
\hline\hline
前後2単語        & 0.10 & 87.9\% & 24.8\% & 96.7\% & 32.4\% & 96.7\% & 31.9\% \\
\hline
前2単語と後1単語 & 0.10 & 86.2\% & 26.7\% & 96.7\% & 39.9\% & 92.7\% & 41.6\% \\
\hline
\end{tabular}
\end{center}
\end{table}

粒度の違いについては，品詞・活用形・活用型の場合が最も良い結果となった．
文献\cite{Takezawa96}においても，品詞では粒度が荒らすぎ，単語では被覆
率の観点で良くなかったため，妥当な結果と考えられる．また，参照する範囲
については，前2単語と後1単語の方が前後2単語(合計4単語)よりも良かった．

そこで，品詞・活用形・活用型の並びに関して，前後2単語を参照する場合と，
前2単語と後1単語を参照する場合について，さらに最適な閾値を探してみた．
結果を{\bf\dg 表\ref{t:result}}に示す．

\begin{table}
\caption{最適な閾値に基づく再現率と適合率}\label{t:result}
\begin{center}
\begin{tabular}{|l||r||r|r|}
\hline
 & \multicolumn{1}{c||}{条件} & \multicolumn{2}{c|}{品詞・活用形・活用型} \\
\cline{2-4}
                 & 閾値 & 再現率 & 適合率 \\
\hline\hline
前後2単語        & 0.37 & 80.5\% & 64.7\% \\
\hline
前2単語と後1単語 & 0.43 & 88.6\% & 65.7\% \\
\hline
\end{tabular}
\end{center}
\end{table}

やはり，前2単語と後1単語の品詞・活用形・活用型の並びを利用した場合が最
も良い．誤りおよび涌き出し誤りの内容を次に示す．あらかじめ要約すると，
その分析内容も，前2単語と後1単語の範囲を見れば十分であることを示唆して
いる．

\vspace{-1mm}
\subsubsection{誤りの分析}
\vspace{-1mm}

閾値を0.43として，前2単語と後1単語の品詞・活用形・活用型の並びを利用し
た場合の誤りは14件あった．その内容を分析する．

発話の途中の感動詞の直後が2件あった．発話の途中の感動詞の直後は読点で
書き起こされることが多いためである．次に例を示す．行の先頭の「×」記号
は誤り例を意味する．「$+$」記号は単語の区切り位置を示す．[]記号の中に
ポーズの長さや発話開始・終了等の情報を加えた．「$\bullet$」記号が現在
位置を示す．

\begin{itemize}
\item[×] 様 $+$ ありがとうございました[60ms] $\bullet$ また
\end{itemize}

接尾辞の直後が5件あった．そのうちの3件は別の発話となっている．同じ発話
に含まれるものは2件あり，そこには 285ms と 350ms のポーズがあった．例
を示す．

\begin{itemize}
\item[×] 千 $+$ 円[発話終了] $\bullet$ 和室
\item[×] 鈴木 $+$ 様[285ms] $\bullet$ それでは
\end{itemize}

名詞類の直後が2件あった．そのうち1件は別の発話となっている．同じ発話に
含まれる1件については，615ms のポーズが挿入されていた．例を示す．

\begin{itemize}
\item[×] 零 $+$ 零[発話終了] $\bullet$ ご
\item[×] ご $+$ 滞在[615ms] $\bullet$ 零
\end{itemize}

接続助詞の直後が5件あった．1秒程度以上の長いポーズが挿入されるか，発話
が終わらない限り，接続助詞の直後は読点で書き起こされているためと考えら
れる．そのうち4件は別の発話となっている．同じ発話に含まれる1件について
は 990ms のポーズが挿入されていた．例を示す．

\begin{itemize}
\item[×] す $+$ が[発話終了] $\bullet$ 予約
\item[×] た $+$ もんですから[990ms] $\bullet$ あ
\end{itemize}

\subsubsection{涌き出し誤りの分析}

閾値を0.43として，前2単語と後1単語の品詞・活用形・活用型の並びを利用し
た場合の涌き出し誤りは57件あった．その内容を分析する．

発話の先頭の感動詞の直後が45件あった．発話の先頭の感動詞の直後は句点で
書き起こされていることが多いためである．これらの事例は句点とみなしても
構わない．次に例を示す．行の先頭の「※」記号は涌き出し誤り例を意味する．
他の記号は同様である．

\begin{itemize}
\item[※]  $+$ はい[640ms] $\bullet$ いつ
\item[※]  $+$ はい[110ms] $\bullet$ そう
\end{itemize}

終助詞の直後の涌き出し誤りが7件あった．これらもすべて句点とみなしても
構わない．例を示す．

\begin{itemize}
\item[※] す $+$ か[590ms] $\bullet$ じゃあ
\end{itemize}

その他の事例が5件あった．すべて頻度のまれな個別的な事例であった．例を
示す．

\begin{itemize}
\item[※] し $+$ た $\bullet$ っけ
\item[※] 大変 $+$ 申し訳ございません $\bullet$ が
\end{itemize}

\subsubsection{ヒューリスティックス導入の効果}

涌き出し誤り57件のうち，発話の先頭の感動詞の直後45件と終助詞の直後7件
の合計52件については，句点相当の節境界とみなして良い．そこで，それらは
すべて句点を正解と見なした場合の実験を行った．

さらに既に列挙した誤りおよび涌き出し誤りに関する事例のうち，直感により
普遍的に成立すると判断したものをヒューリスティックスとして導入する．そ
の内容は次の通りである．

\begin{itemize}
\item 感動詞の直後に接続助詞が続かない限り句点相当の節境界とする．
\item 感動詞の直後に接続助詞が続く場合は句点相当の節境界とはしない．
\item 助動詞終止形と終助詞の間は句点相当の節境界としない．
\end{itemize}

今回導入したヒューリスティックスはすべて細分化された品詞並びに関する情
報からなるもののみである．

このようにして求めた再現率と適合率を{\bf\dg 表\ref{t:result2}}に示す．
今回の実験は開発実験データから得られた誤りおよび涌き出し誤りの事例から
ヒューリスティックスを作成していることもあり，再現率，適合率ともに改善
できた．さらに別の評価実験データ(オープンデータ)を用いた実験は今後の課
題とする．

\begin{table}
\caption{ヒューリスティックス導入の効果}\label{t:result2}
\begin{center}
\begin{tabular}{|l||r|c|c||r|r|}
\hline
 & \multicolumn{3}{c||}{条件} & \multicolumn{2}{c|}{品詞・活用形・活用型} \\
\cline{2-6}
 & 閾値 & 正解句点の追加 & ヒューリスティックス & 再現率 & 適合率 \\
\hline\hline
前2単語と後1単語 & 0.43 & なし & なし & 88.6\% & 65.7\% \\
\hline
前2単語と後1単語 & 0.43 & あり & なし & 92.0\% & 97.0\% \\
\hline
前2単語と後1単語 & 0.43 & あり & あり & 97.7\% & 99.4\% \\
\hline
\end{tabular}
\end{center}
\end{table}

\subsection{音声認識結果への適用実験}

文献\cite{Shimizu96}の音声認識器の結果を用いて，句点相当の節境界を検出
する実験を行った．書き起こしテキストによる評価実験を行ったホテル予約9
会話を対象とした．書き起こしテキストを用いた予備実験では発話権の交代
(ターン)毎に一つの入力単位としたが，音声認識結果を対象とする場合は音声
波形切り出し単位を一つの入力単位とした．第1位候補に対する例を示す．

\begin{itemize}
\item[(音声認識結果例1)]
\item {\bf\dg 書き起こし} お待たせいたしました．申し訳ございません．シ
ングルは満室となっております．
\item {\bf\dg 認識結果} {\tt お+待/た/し/いた+し+ま+し+た○/申し訳ござ
いません○/十/五/満室/に+な+っ+てお+り+ま+す○}
\end{itemize}

認識結果の「{\tt /}」記号は音声認識で使っている単語辞書の区切りを表す．
認識結果の「{\tt +}」記号はデータベースの形態素辞書の区切りを表す．
「○」は検出できた句点相当の節境界のうち，正解とみなせるものを示す．

\begin{itemize}
\item[(音声認識結果例2)]
\item {\bf\dg 書き起こし} ［んー］ちょっと高いですね．もっと安い部屋は
無いですか．
\item {\bf\dg 認識結果} {\tt 二※/ちょっと/高/い/で+す+ね○/オー/で+す
※/いや/な/い/で+す+か○}
\end{itemize}

書き起こしの［んー］は間投詞を表す．「※」は涌き出し誤りを示す．音声認
識で使っている単語辞書では，話し言葉の文末表現に相当するものを一つの長
い単位で扱うことが多いため，文末表現の位置に誤認識が少ない．

音声認識に対する評価実験を行った結果を{\bf\dg 表\ref{t:recognition}}に
示す．第1位候補を対象にした．第1位の単語認識率は 77.4\% (挿入誤りを除
けば 83.6\%)であった．今回の実験対象は，ある基準(機械的に振る舞う逐次
通訳者に会話参加者が渡した発話)で音声波形が切り出されてデータベース化
されているものである．そこでまず発話途中のみを対象に分割に関する数値評
価を行った．また，発話末に対してもそこが境界位置かどうかに関する判定を
行うことができる．そこで次にその位置も対象に含めた数値評価も行ってみた．

\begin{table}
\caption{音声認識結果への適用実験}\label{t:recognition}
\begin{center}
\begin{tabular}{|l||r|r|}
\hline
 & \multicolumn{1}{c|}{再現率} & \multicolumn{1}{c|}{適合率} \\
\hline\hline
発話途中のみを対象     & 95.6\% & 94.8\% \\
\hline
発話途中と発話末を対象 & 98.4\% & 98.1\% \\
\hline
\end{tabular}
\end{center}
\end{table}

音声の終端検出を自動的に行う\cite{Reaves98}場合は分割のみならず接合す
る必要のある現象が出現する可能性がある．接合するかどうかの判定は発話末
位置が境界になるかどうかの情報で行うことができる．

\subsection{発話単位の接合に関する実験}

我々が対象としている会話音声は通訳者を介したものであり，人間の通訳者が
その場で対処できないような断片的な発話はデータベース化していない
\cite{Morimoto94}．そのため，データベースの中には接合しなければいけな
いような発話は含まれていない．まず第一段階として，現時点ではこのように
して集めた会話音声を研究対象としている．

さて，我々の音声翻訳実験システム\cite{Takezawa98a}においてオンラインで
音声入力する場合の音声の終端検出では1秒より長い無音区間があれば終端と
みなしている\cite{Reaves98}．そこで，1秒を閾値として，今回の評価実験用
のホテル予約9会話の発話を自動分割してみた．例えば，先に挙げた例(4)は次
の例(4')のような発話単位に分割される．

\begin{itemize}
\item[(4')] 
\begin{itemize}
\item[(a)] お待たせいたしました．
\item[(b)] 洋室は，
\item[(c)] 一泊二食付き，二万円で，補助ベッドが入ります．
\end{itemize}
\end{itemize}

1秒を閾値として分割できた発話単位のうち，その終端が句点でないものは5例
あった．その5例について提案手法を適用したところ，すべて句点とはみなさ
れなかった．したがって，{\bf\dg 表\ref{t:combination}}のモデルにおいて，
閾値を1秒とした場合の (C) と判断できるので，そこにはすべて読点を挿入す
ることができる．

さらに参考実験として300msを閾値とする自動分割も行ってみた．300msを閾値
として分割できた発話単位のうち，その終端が句点でないものは99例あった．
この99例について提案手法を適用し，句点とみなされなければ接合に成功した
ものとしてすべて正解と集計した\footnote{{\bf\dg 表\ref{t:combination}}
のモデルによれば読点が挿入される．書き起こしテキストのその位置に読点が
書き起こされていても書き起こされていなくても読点で正解とみなした．}場
合の結果を{\bf\dg 表\ref{t:connect}}に示す．

\begin{table}
\caption{発話単位の接合に関する実験}\label{t:connect}
\begin{center}
\begin{tabular}{|l||r|r|}
\hline
                         & 再現率 & 適合率 \\
\hline\hline
ヒューリスティックスなし & 67.0\% & 84.3\% \\
\hline
ヒューリスティックスあり & 90.8\% & 86.8\% \\
\hline
\end{tabular}
\end{center}
\end{table}

涌き出し誤りは，予約内容の確認の発話で名詞類を並べて発話するような箇条
発話における名詞類の直後に多い．対処法としては，パワーの変化や韻律情報，
さらに音韻の継続時間長等との関係を調べることが考えられる．それらは今後
の課題とする．

\section{むすび}

細分化された品詞並びの統計モデルとポーズ情報を用いる句点相当の節境界を
検出する手法を提案し，音声認識結果に適用する実験を行ったところ，良好な
結果を得た．箇条発話の扱いが今後の課題である．パワーの変化や韻律情報，
さらに音韻の継続時間長等を組み合わせることも今後の課題である．次発話予
測の研究\cite{Iwadera96}で用いている発話タイプと関連付ける研究にも発展
させていく．その際，韻律情報を用いた発話タイプの識別
\cite{Fujio95,Fujio96}も考慮する予定である．

また，音声認識過程で構文規則を利用する研究\cite{Takezawa96}と組み合わ
せれば，統語構造の情報を併用することも可能である．そこで用いている部分
木\cite{Takezawa96}と同時通訳方式の実現に向けた処理単位\cite{Mima97}と
の関係も調べる予定である．なお，言語翻訳知識を利用して音声認識候補から
もっともらしい部分を見つける研究\cite{Wakita97}も行われているが，そこ
での入力となる言語翻訳単位はあくまで文である．したがって，文献
\cite{Wakita97}と組み合わせる場合であっても，本稿で提案した処理とメカ
ニズムは必須となる．

\acknowledgment

実験に協力いただいた大槻直子，林輝昭 両氏に感謝いたします．


\bibliographystyle{jnlpbbl}
\bibliography{v06n2_05}

\begin{biography}
\biotitle{略歴}
\bioauthor{竹沢 寿幸}{
1961年生．1984年早稲田大学理工学部電気工学科卒業．1989年同大学院理工学
研究科博士後期課程修了．工学博士．1987年より同大学情報科学研究教育セン
ター助手．1989年よりATR自動翻訳電話研究所勤務．現在，ATR音声翻訳通信研
究所，主任研究員．音声翻訳システムの研究に従事．インタラクションの研究
に興味を持つ．情報処理学会，電子情報通信学会，人工知能学会，日本音響学
会，言語処理学会 各会員．}
\bioauthor{森元 逞}{
1946年生．1970年九州大学大学院修士課程修了．同年日本電信電話公社入社．
1987年よりATR自動翻訳電話研究所ならびにATR音声翻訳通信研究所に勤務．
1998年福岡大学電子情報工学科教授．現在に至る．博士(工学)．電子情報通信
学会，情報処理学会，人工知能学会，言語処理学会，日本音響学会 各会員．}

\bioreceived{受付}
\bioaccepted{採録}

\end{biography}

\end{document}
