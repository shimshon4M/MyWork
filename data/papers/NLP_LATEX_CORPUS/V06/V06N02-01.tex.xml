<?xml version="1.0" ?>
<root>
  <title>単語単位による日本語言語モデルの検討</title>
  <author>伊東伸泰西村雅史荻野紫穂山崎一孝</author>
  <jabstract>日本語では単語の境界があいまいで，活用等のルールに基づいて定義された単位である形態素は必ずしも人が認知している単語単位や発声単位と一致しない．本研究では音声認識への応用を目的として人が潜在意識的にもつ単語単位への分割モデルとその単位を用いた日本語の言語(N</jabstract>
  <jkeywords>音声認識，ディクテーション，N</jkeywords>
  <section title="はじめに">音声認識技術はその発達にともなって，その適用分野を広げ，日本語においても新聞など一般の文章を認識対象とした研究が行なわれるようになった．この要因として，音素環境依存型HMMによる音響モデルの高精度化に加え，多量の言語コーパスが入手可能になった結果，文の出現確率を単語N個組の生起確率から推定するN-gramモデルが実現できるようになったことが挙げられる．日本語をはじめとして単語の概念が明確ではない言語における音声認識を実現する場合，どのような単位を認識単位として採用するかが大きな問題の1つとなる．この問題はユーザーの発声単位に制約を課す離散発声の認識システムの場合に限らない．連続音声の認識においても，ユーザーが適時ポーズを置くことを許容しなければならないため，やはり発声単位を考慮して認識単位を決める必要がある．従来日本語を対象とした自然言語処理では形態素単位に分割することが一般的であり，またその解析ツールが比較的-gramモデル作成においても「形態素」を単位として採用したものがほとんどである．しかしながら，音声認識という立場からあらためてその処理単位に要請される条件を考えなおしてみると，以下のことが考えられる．認識単位は発声単位と同じか，より細かい単位でなければならない．形態素はその本来の定義から言えば必ずこの条件を満たしているが，実際の形態素解析システムにおいては，複合名詞も１つの単位として登録することが普通であるし，解析上の都合から連続した付属語列のような長い単位も採用している場合があるためこの要請が満たされているとは限らない．長い認識単位を採用する方が，音響上の識別能力という観点からは望ましい．つまり連続して発声される可能性が高い部分については，それ自身を認識単位としてもっておく方がよい．言語モデルを構築するためには，多量のテキストを認識単位に分割する必要があり，処理の多くが自動化できなければ実用的ではない．これらは，言い換えれば人間が発声のさいに分割する(可能性がある)単位のMinimumCoverSetを求めることに帰着する．人が感覚的にある単位だと判断する察した研究は過去にも存在する．原田は人が文節という単位について一貫した概念を持っているかについて調査し，区切られた箇所の平均一致率が76%であり付属語については多くの揺れがあったと報告している．また横田，藤崎は人が短時間に認識できる文字数とその時間との関係から人の認知単位を求め，その単位を解析にも用いることを提案している．しかしながら，これらの研究はいずれも目的が異なり，音声認識を考慮したものではない．そこで，われわれは，人が潜在意識としてもつ単語単位を形態素レベルのパラメータでモデル化するとともに，そのモデルに基づいて文を分割，N-gramモデルを作成する手法を提案し，認識率の観点からみて有効であることを示した．本論文では主として言語処理上の観点からこの単語単位N-gramモデルを考察し，必要な語彙数，コーパスの量とパープレキシティの関係を明らかにする．とくに新聞よりも「話し言葉」に近いと考えられるパソコン通信の電子会議室から収集した文章を対象に加え，新聞との違いについて実験結果を述べる．</section>
  <section title="単語単位への分割">本節ではわれわれが採用した単語単位と，同単位への分割手法について述べる．日本語を分割して発声する場合，その分割点はきわめて安定している点と，人，または時によって分割されたりされなかったりする不安定な点がある．例として「私は計測器のテストを行っています．」という文を考えよう．これは形態素解析により，たとえば[私:+:は:+:計測:+:器:+:の:+:テスト:+:を:+:行:+:っ:+:て:+:い:+:ます:+:．]と分割されるが，動詞の活用語尾である「っ」や接続助詞の「て」はほぼ確実に「行」と結合して「行って」と発声されるのに対し，接辞である「器」は分割される場合もあれば，結合されることもあるだろう．そこで文がある位置で「分割」される確率を形態素のレベルでモデル化することを考える．そして人が分割した学習用テキストと同じテキストを形態素解析により分割した結果を照合し，各形態素の遷移ごとに当該点で分割される確率を得る．その後，より大量のテキストをそのモデルに基づいて分割すれば(このプログラムを以後セグメントシミュレータと呼ぶ)，人が分割した傾向をもったわかち書きテキストを容易に得られる．「分割」される位置としては，形態素の境界(形態素単位への分割)と途中(文字単位への分割)がある．ここで分割記号としてを使用し，し，「結合」は「NULL」が生起すると考えれば，前者はある形態素から別の形態素に遷移したときにその間に「」が生起する確率として[P(_iMorpheme_iMorpheme_i+1)]となる．後者のそれはMorphemeを文字列C_1C_2,,C_nで表すと，番目の文字の後にが生起する確率と考えれば[P(_jMorpheme,:C_jC_j+1)]と表現できる．モデルのパラメータ(形態素の属性)としては，)，連接属性(PartofSpeech:PoS)，，そして表記(String)を採用し，(KoW[PoS],String)と表現する．ここで品詞，連接属性とはわれわれの用いた形態素解析プログラムの出力として得られるものであり，品詞は81，連接属性は119に分類されている(PartofSpeech)とすべきであろうが，ここでは文献の記法にしたがった．．したがってでは6個，文字単位への分割では4個のパラメータで記述されることになるが，そうすると明らかに多量の学習用テキスト(人が分割したもの)が必要となる．そこで頻度が閾値以下であるような場合については，パラメータを特定の順序で縮退させた確率値を用意しセグメントシミュレータの実行時も，確率が記述されているレベルまで同様の順序で縮退し，当該確率値で代用することを考える．縮退の順序にはさまざまなものが考えられるが，モデルのパラメータについてその種類数を考えると表記，連接属性，品詞の順に少なくなることは明らかであり，縮退もそれにしたがうのが妥当であろう．また基本的にはある出現回数を閾値としたときより多くの種類の遷移確率が得られることが望ましい．このような観点からいくつかの予備実験を行い経験的に縮退順序を決定した．この順序と参照される確率値を木構造で表現したのが図である．各ノードには形態素の属性とその属性が満たされた場合に分割される確率が対応する．たとえば図中[P(V.:infl.[29]Conj.:p.p.[69],:て)]は形態素単位への分割に対する記述例で，形態素の属性が動詞活用語尾[29]から接続助詞[69]「て」へ遷移したときに，その間で分割される確率を意味するはVerbinflection，Conj.p.p.はConjunctivepost-positionalparticleの略．．こでは「て」）が省略される．ただし品詞が名詞の場合には文字数が分割確率を記述するパラメータとして有効と考えられるので，た．さらに上位レベルでは，連接属性番号も省略し，からConj.p.p.への遷移に対たとえば，「積んで」という文節を形態素に分割すると[積(Verb[8]):+:ん(V.:infl[30]):+:で(Conj.:p.p.[69])]となるが，その中に現れる「ん」と「で」の間で分割されたカウント等もマージした上で算出された確率となる．このように木はリーフから上位のノードに行くにしたがって縮退されたパラメータ，言い換えればより大まかなパラメータとなる．一方，前節で述べたように人は形態素として定義されたトークンをさらに文字単位で分割する場合もある．これは形態素解析の都合上連続した付属語列を1つの形態素としてとり扱うことが行なわれるためである．たとえばわれわれの用いた形態素解析用文法では「...かどうか」という付属語列が助詞として扱われているが「か」+「どうか」と分割されることもある．そこで形態素レベルの分割よりもさらに詳細なレベルとして，文字レベルの分割をモデル化した．このような確率木はつぎのように構成することができる．つまりもっとも細かい分類における各パラメータについて，人が分割した結果と形態素解析の結果を照合してカウントし，その値をリーフから上位ノードに伝搬させた後，確率値に正規化すればよい．全カウント数が少ないと当該確率(推定値)の信頼性が低いので，カウント，マージ作業を行なって，頻度がある閾値以上のノードを最終的なノードとして採用することにする．このモデル化では学習データの量に応じて，そのデータから得られる情報を最大限に利用することができる．たとえば，2文字漢語から接尾辞への遷移には，非常に多くのものがあるが，その分割されやすさは接尾辞の種類によって異り，それらを捨象してモデル化したのでは，あいまいさが大きくなってしまう．しかし逆にそのすべてを細分化したのでは，頻度が低い接尾辞に対するルールが得られないか，または信頼性の低い確率推定値となってしまう．本手法によれば学習データ中に頻度が高いものについてはより細かい分類でモデル化され，頻度が下るにしたがって統計として信頼にたる単位まで縮退されたパラメータによる確率値が得られることになる．</section>
  <section title="形態素解析プログラムの変更"/>
  <subsection title="現代語書き言葉以外の表現への文法の対応">形態素解析システムは，一般に新聞記事に代表される現代語書き言葉を処理できるように開発されてきた．しかし近年，データとして使用されるコーパスの大規模化に伴い，現代語書き言葉以外の表現，特に，会話風の表現（以下，口語体と示す）を扱う試みが増加してきた．われわれが従来使用してきた形態素解析の現代語書き言葉に対応したもので，究で用いる学習用テキストは新聞に限らず，パソコン通信の投稿テキストが含まれており，口語体への対応なくしては充分な精度の解析結果を得ることができない．以下の点を考慮して，より多様な文に対応できるよう形態素解析の文法を記述した．元の文法に対する変更を少なくして派生的な影響を抑える．口語体によく現れる縮退形で，五段活用連用形に接続する「ちゃ」には，接続助詞「て」および係助詞「は」の連なり「ては」の縮退と（例:書いちゃいけない）と，接続助詞「て」および補助動詞「しまう」の語幹の連なり「てしま」の縮退（例:書いちゃう）とがある．前者は直後で文節を切ることができる非活用語，後者はワア行五段活用をするので，ワア行五段活用語尾が接続し，かつ直後で文節末に遷移できる「ちゃ」という形態素の規則を作成すれば形態素解析処理を行うことができる．しかし，品詞や活用形を単語分割モデルで利用すると，「ちゃ」に品詞として接続助詞を付与すれば「接続助詞にワア行五段活用語尾が接続する」という一般化が，また動詞を付与すれば「五段動詞語幹が文節末に遷移する」という一般化が行なわれかねない．これを避けるには，「ちゃ」に新たな品詞を付与するか，または「ちゃ」に二種類あるとするという対応が考えられるがわれわれは後者の方法を採った．形態素解析としては前者が望ましいと思われるが，後の単語分割モデルに影響を及ぼす可能性がある場合は，元の文法規則への影響がより少ないものを採用した．また，文語活用の残存形などで，現代語活用に全く同じ形があるものについては，現代語活用の形態素に接続条件を加えて対処した．縮退形の品詞付与では元の形態素列のうち活用語尾や自立語がもつ品詞を優先する．形容詞仮定形活用語尾「けれ」および接続助詞「ば」の連なりの縮退である「きゃ」「けりゃ」の前連接属性は「けれ」，後連接属性は「ば」にほぼ等しい．こうした縮退形の品詞は，元の形態素列のもつ連接属性のうち活用語尾や自立語のものを優先して付与した．省略による空文字列は次形態素への遷移を追加して対処する．「勉強しよ」「読も」などのように形態素末が落ちる縮退の場合，前者は助動詞「よう」の縮退「よ」を定義すればよいが，後者は助動詞「う」そのものが脱落しているので，動詞未然形から「う」の次の形態素への遷移を追加して対処する．</subsection>
  <subsection title="複合名詞の分割">形態素解析の辞書には，現在までの使用目的に応じて複合語が一語扱いで登録されていることが多いが，単語分割モデル構築のための形態素解析としては短単位に分割されていた方が都合がよい．そこで，複合語の中でも特に多い複合名詞を分割対象として，分割データベースとヒューリスティック規則により，形態素解析で複合名詞分割を行なうことにした．複合名詞の分割データベースは，2カ月分の新聞記事（産経新聞）を形態素解析しての頻度で出現する3文字以上の名詞を抜き出した後，人手で，分割する位置の情報を付与することにより作成した．このデータベースには約25,000語の複合名詞が含まれている．ヒューリスティック規則は，以下の条件を満たすように作成した．1語の名詞よりも2語以上の名詞連続のコストが小さい．名詞連続中では，2語のコストがもっとも小さく，次第にコストがる．これは複合名詞を分割する際，あまり細かく切り過ぎないようにするためである．1文字名詞は他の名詞に比べてコストが大きい．上記と同様，過分割を防ぐためである．分割対象は3文字以上の複合名詞とする．1文字ずつに過分割しないためである．未知語のコストは1語の名詞より大きい．*3mmまた，分割の結果に3文字以上の名詞が含まれている場合は，再帰的にそれを分割し，分割が不可能になるまで繰り返す．</subsection>
  <section title="分割モデルの作成と分割過程"/>
  <subsection title="分割確率の推定">分割ルールとその確率を推定するため，計17人の被験者カ月および産経新聞2カ月）ル」の電子会議室（以下電子会議室）．新聞や日本語用例集はいわゆる「書き言葉」のスタイルであるのに比較して電子会議室の文章はより口語体に近く，これらは分割モデルにも影響を与える可能性がある．そこで両者のデータは別々に取り扱って分割モデル(確率木)を構成した．その結果前者は2,829個，後者は2,269個のノードからなる木が得られた．表に一例を示す．ただしノードとして採用するか否かの閾値には当該ノードの出現回数(カウント)を用い，その値は学習データ中の単語数に比例させた．2つの確率木について得られたノードをいずれに含まれるかで分類し数を示したものが図である．得られたノードは，かなりの異なりがあることがわかる．データから得られた確率木にのみ存在するノードの中で出現回数の多いものから上位3個をあげると以下のようになる．xxx=xxxxxxxxxxxxxxxxx=xxxxxx=xxxxxxxxxxxxxxxxx=1.&gt;接続助詞[69]&gt;&gt;活用語尾[31]「る」2.&gt;助動詞[62]&gt;&gt;接続助詞[73]「が」3.&gt;助動詞[48]&gt;&gt;接続助詞[73]「けど」tabbingこれらの遷移を含む例文を上げると1.読ん+で+る,2....です+が,3....だ+けどなどであり，明らかに口語体特有の言い回しに伴う遷移が抽出されている．一方新聞データから学習した確率木にのみ存在するノードをみると体言止めi.e.「...を議論+．」)や漢語の接辞(名詞[19]::接辞[19],「会」)等の文章では比較的頻度が低いと考えられるものが多かった．について分割確率のついてはほとんど違いはなく，２つの確率木の違いはノードつまりルールそのものに現れていることがわかった．これらのモデルに基づいて以下のように多量の(形態素解析された)テキストを分割・統合する．各形態素およびその遷移について，連接属性番号，品詞，形態素の表記を得て，確率木のリーフに記述があるかどうかを調べる．なければ，木作成の説明で述べた順にパラメータ値を縮退させ，確率木に記述があるかどうかを調べる．記述があれば，0から1の範囲の乱数を発生し，その値がノードに付随する確率以下であれば当該位置で分割し，そうでない場合は分割しない．記述がなければ，縮退を繰り返す．もっとも上位のノードにも該当しない場合，形態素の分割点であれば当該位置で分割し，それ以外は分割しない．なおN-gramモデル作成には，乱数による分割処理(セグメントシミュレータ)は必ずしも必要ではなく，形態素解析の結果と分割確率を使って-gramの生起確率を推定することも可能である．</subsection>
  <subsection title="単語カバレージ">われわれの提案した単語単位に基づく語彙を作成するための予備実験として日経新聞3カ月分(合計446,079文)を用い，前節の手続きを適用して分割，の報告によれば形態素を単位とした場合，キストで収集できる（言い換えれば飽和する）ことがわかっている．語は合計で約10^7個，のべ216,904種類の単語が生成された．図はそれらを頻度の高いのものから順にとった場合のカバレージを示している．ただし数字表現，姓名はカウントから除いている．一方同じテキストから形態素は132,164個が生成された．これによれば単語単位を採用すると，形態素よりはより多くの種類が必要ではあるものの，決して発散するものではなく，たとえば上位約25,000個(種類)の単語で全トークンの約95%がカバーでき，彙数であることがわかる．このとき確率木の各ノード（ルール）がどのような割合で使われたかを示したのが表である．表から明らかなように全体の約60%の場合には，一番詳細なレベルのルールが適用されていることがわかる．</subsection>
  <section title="語彙とコーパス"/>
  <subsection title="コーパスの前処理">用意したコーパスのソースは日経新聞（93年から96年），毎日新聞（91年と92年），EDRコーパス，稿された電子会議室の記事である．ただし日経，産経の両紙は示した期間のすべてではなく，月単位で時期が重複しないように選択したサブセットである．新聞についてはその本文を句点単位で文として取り出し，前節で述べた処理を行った．ただし数字については形態素解析で１単語（品詞「数字」）として扱われてしまうので当該トークンをすべて桁付きの漢数字に変換した後，西村に記載された数字の読み上げ単位に合わせて分割した．すなわち整数については「十，百，千，万，億」を位と定義し先行する数字と位で１つの単位として取り扱い，小数点以下の位については１桁づつに分割する．たとえば１２３４．５６は「千」「二百」「三十」「四」「・」「五」「六」と変換・分割されることになる．一方ディクテーションのアプリケーションや一般ユーザーが入力するであろう文，言い回しを考えると新聞だけでは明らかに不足である．そこでより口語体に近いデータとしてパソコン通信「ピープル」から約90の電子会議室に投稿されたテキストを用意した．会議室・話題の種類そして投稿時期について特に恣意的な選択は行っていないが，結果としてはパソコン関連の話題が多く，テキスト量でみて約半分を占めている．電子会議室の投稿文は文ばかりではなく，文字を利用した表，絵などが多数含まれている他，他人の記述を引用する場合が多く，これらを含めてしまったのでは学習用コーパスとして不適切であることは明らかである．そこでルールベースでこれらをとり除くフィルターを作成した．主なルールとしては以下のようなものがある．引用記号（「＞＞」など）をもとに引用部分だと判断した行は除く．記号文字(「−」「＊」など）の一定以上の繰り返しを含む行は除く．フェースマーク（「:-)」など）のリストを作成し，それにマッチした箇所は特別な１個の記号に置き換え，未知語の扱いとする．このフィルターを通した後，句点に加え空白行，一定数以上の連続した空白を手がかりとして文を取り出し，形態素解析，セグメントシミュレータの処理を行った．</subsection>
  <subsection title="語彙の作成">以上の分割済みテキストの内，日経新聞，産経新聞，EDR，そして電子会議室について，95%以上のカバレージをもつ語彙を作成したところ，約44,000語の単語からなるセット(44K語彙)が得られた．このようにして得られた語彙は，人が日本語について単語単位だと感覚的に思うセットを示していると考えられる．たとえば「行う」という動詞とその後続の付属語列からはxxx=xxxxxxxxxxxxxxx=xxxxxxxxxxxxxxx=xxxxxxxxxxxxxxx=&gt;行い&gt;行いたい&gt;行う&gt;行うべき&gt;行え&gt;行えば&gt;行える&gt;行った&gt;行ったら&gt;行って&gt;行っても&gt;tabbingの計11単語が生成された．また「たい」や「べき」といった単語も生成されており，分割に揺れがある部分では複数の分割に対応した単語が得られることがわかる．</subsection>
  <subsection title="学習コーパス文の選択">前節の結果得られた各文は局所的に見ると記号ばかりであったり，姓名の列挙部分であったりして学習コーパスには適さないものが含まれている．また電子会議室のテキストはフィルターのルールでカバーしきれなかった部分で単語ではないトークンが無視できない程度に生じていた．このような文については人手で採用するかどうかを決める，あるいは当該部分を除くことが望ましいが，多量のコーパスについてそのような作業を行うのは不可能なため，ここでは以下の条件のいずれかに当てはまる文は採用しないことにした．２単語以下から構成される文文の単語数に対する記号の数が一定以上の文44K語彙に対して未知語の数が一定以上の割合で含まれる文音声認識用のコーパスにおいて句読点や括弧表現をどのように取り扱うべきかについてはさまざまな議論がある．松岡らはカギ括弧以外の括弧（（）【】など）について内容ごと削除しており，伊藤らは括弧の用いられ方（引用，強調など）に応じて削除すべきかどうかを自動判別している．括弧による表現には確かに読み上げに適さないものも含まれているが，本研究では文章入力手段としての音声認識システムの構築を重視し，これらを削除しないことにした．また同じ理由で句読点も削除していない．その結果得られた文の数をソース別に示す（表）．</subsection>
  <section title="単語単位による言語モデル">前節にしたがって単語単位に分割されたテキストを学習データとしてN-gramモデルを学習するわけであるが，生起確率の計算上考慮すべきこととして数字，時刻などとくに各単語に確率上の差をつけるべき理由がないもの，および意味がまったく同じでありながら表記の異なる揺らぎが生じているものの取り扱いがある．前者については各単語をクラスにまとめて確率を計算することにし，合計36クラス作成した．後者は新聞の場合，用語統一がなされているため影響は少ないと考えられるが，電子会議室のテキストでは「コンピュータ」と「コンピューター」，「組み合わせ」と「組合せ」といった単語は両者とも多数含まれており明らかに無視できない．そこで44K語彙について「読み」をもとに同義語の候補を抽出した上でチェックを行い，約1,800エントリの別名リストを作成した．N-gramをカウントするさい，このリストを参照して１つの表記に統一した上で学習を行っている．一方，テストデータとして新聞3種類，電子会議室のテキストを別に用意し，被験者(単語分割モデルの学習データを作成したのそれぞれについて文数，形態素数，単語数，そして44K語彙のカバレージを表に示す．この表から１文あたりの単語数は形態素数に比較して12-19%程度少なくなることがわかる．験の目的は単語を単位としたN-gramモデルの有効性，コーパスの必要量を評価する．新聞と電子会議室において単語N-gramモデルから見た違いを明らかにする．の２点である．そこで新聞，電子会議室のそれぞれについてその種類，時期の違いを捨象するため，全学習データを文単位でシャッフルした上で８個に分割したサブセット(新聞:N-1,..,8，電子会議室F-1,...,8）を作成した．そして各サブセットをさらに95%と5%の比率で分割し前者をN-gramカウント，後者をHeld-out補間のパラメータ学習用に用いた．まず新聞について学習データ(N-1,..,8)を順に増加させながら言語モデルを作成し，各モデルをテストセットパープレキシティで評価した．ただし学習データに１回でも出現したN-gram(trigramまで）はを図に示す（電子会議室に関するデータは「Forum」と表記している）．予想されるようにいずれのテストデータでも学習コーパスの増加にともなってパープレキシティは緩やかに改善されるが次第に飽和する傾向がみてとれ，いずれの場合も学習データセットを7個から8個に増やしたときのパープレキシティの改善率は1-2%程度でしかない．パープレキシティの絶対値には相当の差があり，新聞といってもひとくくりにできないことは明らかだが，その値(100-170)は音響識別上対応可能な値であると考えられる．一方電子会議室のテストデータはもっとも良いケースでも400以上のパープレキシティを示しており新聞の学習データだけでは対応できていないことがわかる．われわれの目的は新聞にとどまらず，より口語体に近い電子会議室に投稿される文にも対そこで新聞データすべてを使用した言語モデルをベースとし，電子会議室の学習データ(F-1,..,8)を加えていくことにより各テストデータのパープレキシティがどのようになるかを評価した．結果を図に示す．使用したデータ量の範囲(約25M単語)では，新聞に対する影響はほとんどなかった．一方電子会議室室の）テストデータを評価すると152.1ら作成した言語モデルは新聞・電子会議室の双方に対応できることがわかる．これは双方の統計的異なりが共通しているN-gramの確率が相違しているというよりも，N-gramの種類に，より大きく現れていることを示唆している．一方コーパスのサイズと結果として得られたモデルのサイズ，すなわちN-gramの異なり数これは新聞データ(N-1,..,8)の場合であるが，bigram，trigramとも飽和する傾向は見てとれない．電子会議室テキストを加えた場合も同様でN-1,...,8，F-1,...,8すべてを学習データに使用した場合のN-gram数はtrigramが31M個，bigramが5.6M個に達した．とくにtrigramは学習データサイズの増分に対しほとんど比例して増加している．今後主記憶，外部記憶の容量がさらに増加するとしてもこのN-gram数（異なり）のままでは，実装することが難しい．そこでN-gramの中で低頻度のものを除くことが，パープレキシティにどのような影響を与えるかを検証する実験を行った．結果を図に示す．N-gramの異なりの多くを占めるのはtrigramなので，学習データはN-1,...,8，F-1,...,8すべてを使用した上で，言語モデルを作成するときtrigramの最低出現回数を設定することにより，る．図からtrigramの異なり数が5M個以下になるとパープレキシティが見てとれるが，一方モデルサイズを1/3〜1/5にした程度ではパープレキシティの差は小さいことがわかる．</section>
  <section title="おわりに">このように，本研究では比較的少量の人による分割データから揺らぎを含めた分割傾向を推定する手法について述べ，新聞およびパソコン通信の電子会議室を学習データとして，そのモデルからつくられた単語の集合と言語モデルについて考察した．結果として，人が単語と意識する単位はその揺らぎを含めても発散することはなく，約44Kで94-98%得られること，形態素に比較して１文あたりの要素数が12-19%程度減少すること，電子会議室と新聞では，N-gramモデルからみた統計量に相当の差があり，予想されたように新聞単体では十分に対応できないものの，新聞をベースとして電子会議室のテキストを混合させたデータから作成した言語モデルは新聞のテストデータに対するパープレキシティを増大させることはほとんどなくその双方に対応可能であることがわかった．分割モデル，N-gramモデルのいずれも，データの種類(新聞，パソコン通信）に依存している．これ自体は容易に予想できることであるが，その異なりが共通する事象の確率が異なるというよりも事象自体の異なりにより大きく現れていることは興味深い．形態素との効率比較という意味では，同一学習データから作成した言語モデルを用いて単位長さ（たとえば文）あたりのパープレキシティを比較する必要がある．これについて学習，テストデータ量は少ないものの，すでに報告を行っており，文あたりパープレキシティがほぼ等しく，したがって単位長が長い分，より有利な単位となっていることを確認している．コーパス量とパープレキシティの関係について，とくに日本語に関して報告された例はほとんどないため，他の研究と比較して議論することが難しい．本研究の実験からは400万文強のデータではまだパープレキシティが減少するが，その改善率は低く数倍以上のデータがないと意味のある改善が難しいことを示唆している．人が感覚的にある単位だと判断する日本語トークンについて考察した他の研究との関連についても述べておきたい．原田は人のもつ文節単位の概念に関する調査結果から，「文字列またはモーラ長が一定以上になると分割しようとする動機がたかまる」という仮説を提起している．われわれの分割モデルでは分割が2形態素の遷移情報のみで独立に起こることを仮定しているが，この独立性については検討が必要であろう．横田，藤崎が短時間に認識できる文字数とその時間との関係から求めた認知単位は，とくに平均長は述べられていないものの，例をみる限りわれわれの単位より明らかに長い．同論文では「人は文を文字単位で処理しているのではない」と結論しているが，加えて，分割できる最小単位の列として知覚されているのでもないということになる．今後は，コーパスサイズをより大きくするとともに句読点を削除した場合との比較・考察や，単語分割モデルの分割確率とポーズ位置との関係，さらに上記で述べた分割の独立性について検討したいと考える．本研究にテキストデータ使用を許諾していただいた，産経新聞社，日本経済新聞社，毎日新聞社（CD-毎日新聞91-95），そして(株)ピープルワールドカンパニーに感謝いたします．document</section>
</root>
