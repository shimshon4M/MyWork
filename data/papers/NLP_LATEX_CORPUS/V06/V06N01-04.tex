

\documentstyle[epsf,nlpbbl]{jnlp_e_b5}

\setcounter{page}{59}
\setcounter{巻数}{6}
\setcounter{号数}{1}
\setcounter{年}{1999}
\setcounter{月}{1}
\受付{June}{24}{1998}
\採録{July}{25}{1998}

\newcommand{\ebox}[1]{}
\newcommand{\ccbox}[1]{}
\newcommand{\ccfbox}[1]{}
\def\myatari(#1,#2){}

\setcounter{secnumdepth}{2}

\title{}
\author{}
\jkeywords{}

\etitle{ 
The Concept of Sensitive Word in Chinese \\
- A Survey in a Machine-Readable Dictionary
}

\eauthor{Fuji Ren\affiref{Hiroshimacu}   \and
        Jian-Yun Nie\affiref{UMontreal}}


\headauthor{Ren and Nie}
\headtitle{The Concept of Sensitive Words in Chinese}

\affilabel{Hiroshimacu}
          {Hiroshima City University, Faculty of Information Sciences}
     {Hiroshima City University, Faculty of Information Sciences}
\affilabel{UMontreal}
          {Universit\'e de Montr\'eal, Dept. Informatique et recherche op\'erationnelle}
     {Universit\'e de Montr\'eal, Dept. Informatique et recherche op\'erationnelle}

\eabstract{
In Machine Translation (MT), using compound words or phrases often makes the translation process easier. For 
example, the phrase \ebox{59-2-1.eps} corresponds unambiguously to ``information highway''. It is not necessary to 
break it down to \ebox{59-3-1.eps} (information), \ebox{59-4-1.eps} (high speed) and \ebox{59-4-2.eps} (road). However, some compound words 
(phrases) in Chinese are composed of simpler words which $\,$can $\,$play $\,$significantly $\,$different roles in sentences when they 
are broken down. For example, the compound word \ebox{59-7-1.eps} (machine translation) may be broken into \ebox{59-7-2.eps} 
(machine) and \ebox{59-8-1.eps} (translate), as in the sentence \ebox{59-8-2.eps} (He uses a machine to translate papers). We 
call such a compound word ``Sensitive Word''.   During Chinese MT processing, if the first segmentation result in which 
a sensitive word is segmented as a single word leads to a failure, the alternative solution with the sensitive word broken 
down is considered as the preferred one. This allows us to reach at a higher efficiency by avoiding examining unlikely 
segmentation solutions. In this paper, we describe the problems related to sensitive words. A machine readable 
dictionary has been examined, and 764 sensitive words have been found among 87 600 words. This shows that 
sensitive word is a common phenomenon in Chinese that is worth closer examination.
}

\ekeywords{Sensitive words, Chinese, Word formation pattern, Machine-readable dictionary}


\begin{document}

\maketitle

\section{Introduction}

Natural language processing (e.g. Machine Translation, message understanding) usually starts from words. The 
identification of words in Indo-European languages is a trivial task. However, this problem has been, and is still (as we 
will argue) problematic in Chinese. In Chinese, sentences are written as continuous character strings without word 
separation. For example, \ebox{59-20.eps} (Computers have been used in every area). 

Much work has been dedicated to segmenting Chinese correctly. It has been often reported that the segmentation 
accuracy may reach at as high as over 99\% \cite{Chen92,Chiang92,Yeh91}.
This gives one the impression that the segmentation problem has 
been successfully solved. However, the tests for segmentation have been oversimplified: segmentation has been tested 
as if it is the final step of Chinese processing. In reality, it is only the 
first step in most cases.
If segmentation is considered together with further processing (e.g. 
tagging), many hidden problems will be unveiled. In addition, the test corpora were often built by the same people who 
programmed the segmentation. So there may be a bias in performance measurement. If the segmentation problem is 
considered together with the use we will make of it (e.g. for machine translation), the accuracy rate is much lower. This 
fact has been confirmed by a recent survey \cite{Liu96} conducted by a group of experts in China. They used several test corpora 
for different tests. This survey gave the following conclusions about current state of Chinese NLP software:

1) On the test for word segmentation and tagging:
The best software reached at an 89.4\% accuracy for word segmentation, and only 79.58\% of words have been 
tagged correctly in the best case.

2) On segmentation of some specific words:
This test aimed to segment a set of pre-determined words in texts. The best success rate in determining word boundaries is 60\%, and the global 
accuracy is 89.4\%~\footnote{Accuracy for word boundary $= 1 - \frac{\#errors}{\#boundaries}$, 
Global accuracy $= 1 - \frac{\#errors}{\#words / p}$
Where $p$ is the average word length ($=1.62$).}. 
The accuracy for tagging correctly segmented texts is 86.6\%.

3) On the test for solving ambiguities in word segmentation:
A set of sentences containing segmentation ambiguities was used in this test. The systems had success rates varying 
from 61\% to 78\% for dealing with overlapping ambiguities (see next section for a description), and from 36\% to 
59\% for dealing with combinatory ambiguities (see next section).

These conclusions make it clear that the claimed over 99\% accuracy rate hid many unsolved problems. These problems 
will greatly affect the quality of further processing on Chinese.

In this paper, we intend to deal with one of the segmentation problems - the combinatory ambiguity, which is the most 
difficult to solve as shown by the tests mentioned above. Our goal is not to determine the correct segmentation. This is 
extremely difficult to achieve. Instead, we intend to determine the places where different segmentation solutions due to 
combinatory ambiguities have a significant impact on the later processing (syntactic and semantic analyses). In a later 
analysis, if the first segmentation solution leads to a failure, these places are first checked in order to find alternative 
segmentation solutions.

To do this, it is not sufficient to determine all the combinatory ambiguities. Not all combinatory ambiguities have a 
significant impact on later analyses. They do only when the component words, when separated, may play different roles 
from their compound in sentences. For example, the string \ebox{60-30.eps} as a single word is an adverb (in the future), but it may 
also be separated as two words \ebox{60-31.eps}, an auxiliary verb and a verb sequence (will / come). We call such words 
sensitive words.  The segmentation of sensitive words has a great impact on the syntactic structure of the entire 
sentence. 

In the remaining of the paper, we will first describe the segmentation problem and the impact of sensitive words on it. 
Then we will report the result of our preliminary examination of a machine readable Chinese dictionary which unveils a 
great number of sensitive words in it. 

\vspace{2mm}
\section{Ambiguities in segmentation}

Let us show some examples first.

\noindent Example 1:
\begin{minipage}[t]{3in}
\ebox{61-1-1.eps} \\
(Wang Laoda will come to study at Tokyo)
\end{minipage}
\medskip

This sentence may be segmented into several legitimate word sequences. Two of them are as follows:

\noindent Segmentation 1.1 (correct): 
\begin{minipage}[t]{3in}
\ebox{61-5-1.eps} \\
Wang Laoda / will  / come /  Tokyo  /  study /.
\end{minipage}
\medskip

\noindent Segmentation 1.2 (incorrect):
\begin{minipage}[t]{3in}
\ebox{61-7-1.eps} \\
Wang Laoda / in the future / Tokyo / study /.
\end{minipage}
\medskip 

In this example, the string \ebox{61-9.eps}  may be segmented in two different ways: as \ebox{61-9.eps}  - in the future, or as \ebox{60-31.eps}  - will 
come. Thus, there is a segmentation ambiguity. The first segmentation is the correct one in this sentence whereas the 
second one is not grammatical.

\noindent Example 2:   
\begin{minipage}[t]{3in}
\ebox{61-13.eps} \\
(He studies biochemistry)
\end{minipage}
\medskip

Two possible segmentation solutions are as follows:

\noindent Segmentation 2.1 (correct):
\begin{minipage}[t]{3in}
\ebox{61-15.eps} \\
he  /   study   / biochemistry /  .
\end{minipage}
\medskip

\noindent Segmentation 2.2 (incorrect):  
\begin{minipage}[t]{3in}
\ebox{61-17.eps} \\
he  / graduate student / object /  chemistry /.
\end{minipage}
\medskip

The character \ebox{61-19-1.eps}  may be grouped either with the precedent characters -- \ebox{61-19-2.eps} to form -- \ebox{61-20.eps}  (graduate student) or 
with the next characters \ebox{61-20-2.eps}  to form \ebox{61-20-3.eps}  (biochemistry). In this example, the first segmentation is the 
correct one.

Usually, segmentation ambiguities may be separated into combinatory ambiguity and overlapping ambiguity, according 
to their configuration. Examples 1 and 2 show these cases respectively: the words \ebox{61-24-1.eps}  (will) and \ebox{61-24-2.eps}  (come) may be 
{\sl combined} to form another word \ebox{61-9.eps}  (in the future) in the first example; the words \ebox{61-20.eps}  (graduate student) and 
\ebox{61-20-2.eps} (biochemistry) {\sl overlap} in the second example.

We can also separate segmentation ambiguities according to their nature (morphological, syntactic and semantic).
\medskip

\noindent {\bf - Morphological ambiguity}
\smallskip

Both examples shown above contain a morphological ambiguity. This kind of ambiguity may be solved using syntactic 
analysis. In the two examples, the second incorrect segmentation may be ruled out because they are not grammatical.

\medskip
\noindent {\bf - Syntactic ambiguity}
\smallskip

Let us look at the following example.

\noindent Example 3:  
\begin{minipage}[t]{4in}
\ebox{62-10.eps} \\
(Physics is difficult to learn)
\end{minipage}
\medskip

It may be segmented into two grammatically correct word sequences:

\ebox{62-13.eps}

physics / learn / very / difficult / . (=Physics is difficult to learn)
\medskip

\noindent or

\ebox{62-16.eps}

physics  / stand up / very / difficult / . (=Physics is difficult to stand up)
\medskip

\noindent
This kind of ambiguity may only be solved by using the semantic information that tells ``stand up'' is incompatible with 
``physics''.

\medskip
\noindent {\bf - Semantic ambiguity}
\smallskip

\noindent Example 4: \ebox{62-21.eps}
\medskip

This sentence may be interpreted as ``The United States will take measures to support Japan'', or ``The Congress of the 
United States takes measures to support Japan''. Both interpretations are syntactically and semantically correct. 
They correspond to the following segmentation solutions:

\noindent Segmentation 1: 
\begin{minipage}[t]{4in}
\ebox{62-27.eps} \\
the U.S.A. / will / take / measure / support / Japan / . 
\end{minipage}
\medskip


\noindent Segmentation 2: 
\begin{minipage}[t]{4in}
\ebox{62-29.eps} \\
the U.S.A. / congress / take / measure / support / Japan / .
\end{minipage}
\medskip

This kind of ambiguity is extremely difficult to solve. It may be done only by analyzing the context in which the 
sentence is used.

\section{Approaches to Chinese segmentation}

There have been two main groups of approaches to Chinese segmentation: dictionary-based approaches and statistical 
approaches.

Dictionary-based (also called rule-based) approaches \cite{Chen92,Jin93,Li91,Liang84,Yeh91}
operate according to a very simple concept: a correct 
segmentation result should consist of legitimate words - the words stored in a dictionary or derivable from some rules. 
In general, however, several legitimate word sequences may be obtained from a Chinese sentence. The longest 
matching algorithm is then used to select the word sequence that contains the $\!$longest $\!$(or $\!$equivalently, $\!$the $\!$fewest)$\!$
words. For example, in Example 2, Segmentation 2.1 is preferred to Segmentation 2.2 because it contains less words 
(and longer words). However, this approach fails for Example 1 for which the wrong solution 1.2 will be preferred by 
the longest-matching algorithm.

Statistical approaches \cite{Chang91,Chiang92,Dunning93,Sproat91,Sproat96}
rely on statistical information such as word and character (co-)occurrence 
frequencies in the training data - often a set of manually segmented texts. The statistical data describe how probable a 
character string may be a word, or how probable a word may follow another word. Using these data, the segmentation 
process consists of first determining the probability of a word sequence, and then choosing the one with the highest 
probability. For example, in Segmentation 2.2 of Example 2, the character \ebox{63-1.eps} (object) alone is rarely used as a word. 
So the probability for it to be a word is low. This will make the whole word sequence in Segmentation 2.2 to have a 
lower probability than Segmentation 2.1. Thus the correct segmentation may be chosen.

Both approaches have difficulty to deal with Example 1. We mentioned above that 
the longest-matching algorithm will choose the wrong solution 1.2. By statistical approaches, as the word \ebox{61-9.eps}  (in the future) is 
at least as common as \ebox{61-24-1.eps}  (will) and \ebox{61-24-2.eps}  (come) separately, Segmentation 1.2 will likely also have a higher probability 
than segmentation 1.1. So the segmentation solution suggested first will be wrong, and it will lead to a failure during 
syntactic analysis. Solving such an ambiguity will require syntactic and even semantic analyses. This is difficult to 
achieve because now there does not seem to have a NLP system for Chinese which may proceed a syntactic analysis of 
Chinese with a high success rate. We rather suggest to not try to solve the ambiguities at any cost, but to provide the 
best segmentation solution according to some limited criteria (longest-matching, or statistical criterion). The best 
solution is passed to the later analysis (syntactic and semantic analyses). If the later analysis fails and requires another 
segmentation solution, then we try to locate the next best solution that may change the later analysis significantly. 

The simple solution used in many current Chinese MT systems is to examine all the next best alternatives in turn if the first best 
solution fails. For Example 1, to the longest-matching algorithm, the following segmentation solution is the best:

\ebox{64-5.eps}

(Wang Laoda / in the future / Tokyo / study / .)
\medskip

\noindent
It will fail in the later syntactic analysis (if any). So the next best solution is sought. The following segmentation 
solutions are equivalent.
\medskip

\epsfile{file=image/64.eps}








 If these alternatives are examined by syntactic analysis one by one in turn, several wrong solutions would have been 
examined before the correct solution is reached at. The analysis of the wrong solutions decreases the 
efficiency of the whole NLP. The efficiency may be improved if we are able to re-rank the alternatives so that the correct solution is ranked at the top. This is possible for this example because the correct solution is the only one which 
segments a sensitive word (\ebox{61-9.eps}) in a different way from the first best solution.

\section{Sensitive words and their impact on MT}

If we analyze the words involved in Segmentation 1.1 of Example 1, we can observe that, although several words may 
be broken into simpler words, these simpler words do not have equal chances to be correct words in Chinese sentences. 
For example, it is rare, if not impossible, that the string \ebox{64-24.eps} (Tokyo) will be separated into words \ebox{64-24-2.eps}  (east) and \ebox{64-24-3.eps} 
(capital) in normal Chinese sentences. Words such as \ebox{64-25.eps} (study) can be separated in a more natural way into \ebox{64-26-1.eps}  (read) 
and  \ebox{64-24-2.eps} (book). However, they will form together a verb phrase \ebox{64-26-3.eps} (read / book) which has exactly the same function 
as (and similar meaning to) \ebox{64-25.eps} together in a Chinese sentence. So 
breaking these words does not lead to a significantly different solution for later analyses.
It will likely lead to a failure, too.

On the other hand, the word \ebox{61-9.eps}  (in the future) is different from the two last cases. Its components \ebox{61-24-1.eps}  (will) and \ebox{61-24-2.eps}  (come) are both 
common words in Chinese, and these two words can be naturally put side by side in Chinese sentences. In addition, 
when they are separated, they play roles significantly different from that of their compound \ebox{61-9.eps} (in the future). We call such a word 
a sensitive word. More precisely, a sensitive word is a compound word in which:

\begin{itemize}
\item the component words can be naturally put side by side in the same order as in the compound word in normal 
modern Chinese sentences;

\item and the roles of the component words in such sentences are different from that of the compound.
\end{itemize}

The first criterion is important. Otherwise, most of the words could be broken down into two or more words, 
but in many of them, the component words are only rarely used or used in an archaic way. For example, the word \ebox{65-10-1.eps} 
(democracy) can be broken down into \ebox{65-10-2.eps} (people, govern). However, the character \ebox{65-11.eps}  alone used as a verb 
(govern) is rare in modern Chinese. So we do not consider such a word as a sensitive word.

If a sentence contains a sensitive word, and it has been proposed as a single word in the first segmentation solution that 
finally fails, then the sensitive word should be first separated into two words. The result of such a separation has a 
significantly different syntactic structure from the first segmentation. In Example 1, if the string \ebox{61-9.eps}  is identified as a 
sensitive word, once Segmentation 1.2 fails, Segmentation 1.1 will be considered as the next best segmentation. This 
allows us to avoid examining other less likely word sequences.

This approach is perfectly compatible with the common practice in MT to group words into long phrases. The 
advantage of grouping words into phrases is its high efficiency: if an expression may be unambiguously recognized, it 
is unnecessary to analyze its composition. For example, it is of no help, and even misleading to analyze the composition of 
\ebox{65-20.eps} (The People's Republic of China) as follows:

\medskip

\epsfile{file=image/65-21.eps}

  
 
  

Long phrases are extensively used in Chinese MT. In fact, many items stored in Chinese dictionaries are indeed 
phrases. Some of them are sensitive words. In previous NLP on Chinese, sensitive words have not been distinguished 
from non-sensitive words. Much effort has been spent to analyze unlikely alternative word sequences. We think the 
distinction of sensitive and non-sensitive words is an effective way to increase NLP efficiency of Chinese. The next 
step of our study is to analyze a machine-readable Chinese dictionary (87599 items) to find sensitive words in it. The 
aim of the analysis is to see the extent to which sensitive words are spread in Chinese.

The meaning of the symbols we will use is given in Table 1.

\begin{table}
\caption{Symbols and their meaning.}
\medskip
\vspace{-3mm}
\begin{center}
\epsfile{file=image/66-1.eps}
\end{center}
\vspace{-3mm}
\end{table}

In many cases, compound words (phrases) are constructed from simpler words according to some rules. The 
rules commonly used are given in Table 2.

\begin{table}
\caption{Rules of word formation.}
\label{RULES}
\medskip
\begin{center}
\epsfile{file=image/66-2.eps}
\end{center}
\end{table}

However, there is no complete set of word composition rules. Many exceptions are observed. Sensitive words are not 
due to the fact that we do not know all the composition rules. Rather they are because Chinese words or characters may 
correspond to several categories. A composition may be broken down, and its components separately may play roles 
different from that of their compound. For example, \ebox{66-20.eps} (guard the door) together is an intransitive verb. It may be 
considered as composed of \ebox{66-21-1.eps} (guard) and \ebox{66-21-2.eps}  (door). This corresponds to the rule V + NC $\rightarrow$  VINT. However, \ebox{66-21-1.eps} 
is an ambiguous word. It may be a verb (guard), or more often, a preposition in the structure ``\ebox{66-21-1.eps} Something Verb'', 
meaning ``to Verb Something''. The string \ebox{66-20.eps} can also be considered as a sequence of ``\ebox{66-21-1.eps} Something'' if a verb 
follows. For example, in \ebox{67-2-1.eps} (he opens the door = he \ebox{66-21-1.eps} door open), \ebox{66-20.eps} should be considered as 
such a sequence, and be separated into two words. Therefore, \ebox{66-20.eps} is a sensitive word. 

In our examination, we intuitively determined a set of compositions that are the most propitious for sensitive words 
according to Chinese grammar. These compositions are shown in Table 3, together with some examples. For example 
the composition ADV+VAUX means that the components of a word may also be separated into an adverb and an 
auxiliary verb. For example, \ebox{67-10.eps} may be considered as a single word which means ``all-round'' as in \ebox{67-10-2.eps}, (all-round 
champion). However, it can also be separated into two words \ebox{67-11.eps}(all, can), as in the sentence \ebox{67-12.eps} (they / all / can / enter / university = They can all go to university). 

\begin{table}
\caption{Compositions of sensitive words and examples.}
\label{EXAMPLES}
\medskip
\epsfile{file=image/68-1.eps}
\end{table}

\begin{table}
\caption{Statistics of sensitive words in the dictionary.}
\medskip
\begin{center}
\begin{tabular}{l|r|rp{15mm}}
\multicolumn{1}{c}{Composition}
& \multicolumn{1}{|c}{No. Words possibly}
& \multicolumn{2}{|c}{No. Sensitive} \\ 
& \multicolumn{1}{|c}{having the composition}
& \multicolumn{2}{|c}{words(percentage)} \\ \hline
ADV + ADV & 401 \hspace*{7mm} & \makebox[10mm][r]{10} & (2.5\%)\\
ADV + NC & 2230 \hspace*{7mm} & 103 &  (4.6\%) \\
ADV + PREP & 294 \hspace*{7mm} & 9  & (3.1\%) \\
ADV + QUAN-CL & 9 \hspace*{7mm}& 4 & (44.4\%) \\
ADV + VAUX & 148 \hspace*{7mm}& 3  & (2.0\%) \\
ADV + VINT & 1265 \hspace*{7mm}& 20&   (1.6\%) \\
ADV + VTR & 1155 \hspace*{7mm}& 87 &  (7.5\%) \\
CONJ + NC & 131 \hspace*{7mm}& 14  & (10.7\%) \\
CONJ + VTR & 94 \hspace*{7mm}& 24  & (25.5\%) \\
NC + PREP & 628 \hspace*{7mm}& 45  & (7.2\%) \\
NC + VAUX & 249 \hspace*{7mm}& 95  & (38.2\%) \\
NC + VTR & 3994 \hspace*{7mm}& 252 & (6.3\%) \\
PREP + NC & 748 \hspace*{7mm}& 39  & (5.2\%) \\
PREP + V & 453 \hspace*{7mm}& 15   & (3.3\%) \\
VAUX + PREP & 96 \hspace*{7mm}& 5  & (5.2\%) \\
VAUX + VTR & 363 \hspace*{7mm}& 19 & (5.2\%) \\
VTR + NC & 5976 \hspace*{7mm}& 57  & (1.0\%) \\
\end{tabular}
\end{center}
\end{table}

Table 4 shows the number of sensitive words that we determined manually from a machine-readable dictionary for each 
word composition. 
12357 possible sensitive words have been identified 
according to the compositions from 87600 words in the dictionary. Among them, 764 are actual sensitive words. The 
numbers shown in Table 4 are higher because some words fall into several categories. The proportion of sensitive 
words is surprisingly high. It is near 1\% of all the items in our dictionary. In addition, a high proportion of sensitive 
words are very common words, as we can see in the examples in Table 3 and in Appendix A.

We can further observe the high percentage of sensitive words for the NC+VAUX composition (38.4\%). The main 
reason is that most words ending with \ebox{67-20.eps}   (association) and \ebox{67-21.eps} (energy) may also be separated into two words, and once 
separated, \ebox{67-20.eps}   and \ebox{67-21.eps}  are usually interpreted as auxiliary verbs (\ebox{67-20.eps}  - will, \ebox{67-21.eps}  - can). 

Most sensitive words involve nouns (NC). The highest numbers of sensitive words are obtained from NC+VTR and 
ADV+NC compositions. In addition, the percentages of sensitive words of these compositions are relatively high. 

From these observations, it becomes clear that we cannot consider the items in a dictionary as inseparable. It is 
important to make distinction between sensitive and non-sensitive words. Precaution should also be paid when one 
adds new items into a machine readable dictionary for MT, especially for words having a composition highly propitious 
for sensitive words.

This study is still ongoing. Several other possible compositions have to be examined. For example, we did not check 
the NC+NC composition. Intuitively, this is the composition the less affected by sensitive words, and many items in the 
dictionary (12571 of them) may correspond to this composition. However the sensitive word problem does occur in this case, 
but in a different manner. The first NC may be syntactically associated with some $\!$precedent words before being 
associated with the second NC. For example, \ebox{69-1-1.eps} (plain-clothes policemen) is an item in our dictionary. But this 
string should be separated into two words in the following sentence: 
\ebox{69-3-1.eps}
(Once in plain clothes, policemen can be hardly recognized). 
In this sentence, \ebox{69-4-1.eps}  (plain clothes) is the object complement of 
the verb \ebox{69-4-2.eps} (wear), but \ebox{69-5-1.eps} (policemen) is the subject of another verb phrase \ebox{69-5-2.eps} (be recognized).

\section{ Conclusions and Future work}

In this paper, we described a new concept - sensitive word - in Chinese language. We first described how sensitive 
words may affect Chinese segmentation. The purpose of distinguishing sensitive words from non-sensitive words is to 
increase the efficiency of Chinese analysis and MT. We argue that if a segmentation solution fails to produce a 
translation, it is not of much help to try to break non-sensitive words into their components. On the other hand, sensitive words 
may be broken down, and this usually leads to a new syntactic structure for the whole sentence. Thus we have a higher 
chance to reach at the correct interpretation of the sentence.

In order to see the scale of sensitive words in Chinese, we examined a machine-readable Chinese dictionary. The 
number of sensitive words found is surprisingly high. We found 12357 words that correspond to the composition 
patterns we determined among 87600 words in the dictionary. Among them, 764 are actual sensitive words. In other 
words, about 0.9\% of dictionary items are sensitive words. This shows that sensitive words are widely spread in 
Chinese. It is worth more intensive study in the future.

We will continue this study by a more thorough analysis of the machine-readable dictionary, and try to extract a set of 
common rules to detect sensitive words. Such a set of rules will be very useful when one builds or enriches a 
Chinese dictionary. The concept of sensitive words will be applied in the Chinese MT system we are developing. The 
impact of considering sensitive words on the efficiency of Chinese MT will be reported in a later paper.


\acknowledgment

This work has been partly supported by the Education Ministry of Japan under Joint Research Grant in Aid for 
International Scientific Research JCE-TC (09044179).

\bibliographystyle{nlpbbl}
\bibliography{v06n1_04}

\begin{biography}

\biotitle{}

\bioauthor{Fuji Ren}
{
Fuji Ren received his Ph.D. degree in 1991 from Faculty of Engineering in
Hokkaido University, Japan. Since then, he has worked in CSK, Japan, where
he was a chief researcher of NLP. From 1994, he joined the Faculty of
Information Sciences in Hiroshima City University as an Associate
Professor. He also holds a professor position at Dalian University of
Technology and Harbin University of Technology, and is an
Advisory Professor at Beijing University of Posts and Telecommunications,
China. He was a visiting professor at CRL Computing Research Laboratory in
New Mexico State University, USA, from 1996-1997. His current resarch
interests include natural language processing, machine translation, and
artificial intelligence. 
}

\bioauthor{Jian-Yun Nie}
{
Jian-Yun $\,$Nie $\,$received $\,$his $\,$PhD $\,$of $\,$Computer $\,$Science $\,$in Universit\'e de Grenoble in France in 1990. 
Since then, his research has been oriented to NLP-based and logical information 
retrieval. 
He is currently an associate professor in Universit\'e de Montr\'eal, Canada. His e-mail address is: nie@iro.umontreal.ca
}

\bioreceived{Received}
\bioaccepted{Accepted}

\end{biography}

\bigskip


\vspace{-4mm}
{\Large\bf
\bigskip
\noindent
\begin{tabular}{@{}p{0.22\textwidth}@{\quad}p{0.7\textwidth}}
  Appendix A &
  Some Lists of Sensitive words found in the Machine Readable Dictionary\\
\end{tabular}
\bigskip
}

How to read the data?

Example:
\begin{center}
ADV-ADV (10)

\ebox{71.eps} (at dawn / at the beginning, more) ADJ/ADV ADV/NC/VTR
\end{center}


We found 10 words in our dictionary with a possible ADV-ADV composition. The string \ebox{71.eps}
as a single word means ``at dawn'', but when separated into \ebox{72-1-2.eps},
it may be an ADV-ADV sequence, meaning ``at the beginning'' and ``more'' respectively.
The tags ADJ/ADV ADV/NC/VTR are the possible tags for the two component words.

\bigskip
\bigskip

\unitlength=1mm

\epsfile{file=image/72.eps}

\newpage \null\vspace*{7mm}
\epsfile{file=image/73.eps}

\newpage \null\vspace*{7mm}
\epsfile{file=image/74.eps}

\newpage \null\vspace*{7mm}
\epsfile{file=image/75.eps}

\newpage \null\vspace*{7mm}
\epsfile{file=image/76.eps}

\newpage \null\vspace*{7mm}
\epsfile{file=image/77.eps}

\newpage \null\vspace*{7mm}
\epsfile{file=image/78.eps}

\end{document}
