\documentstyle[epsf,jnlpbbl]{jnlp_j_b5}

\setcounter{page}{3}
\setcounter{巻数}{6}
\setcounter{号数}{7}
\setcounter{年}{1999}
\setcounter{月}{10}
\受付{1998}{12}{9}
\再受付{1999}{3}{1}
\再々受付{1999}{5}{25}
\採録{1999}{6}{9}



\title{形態素解析結果から過分割を検出する統計的尺度}
\author{内山 将夫\affiref{shinshu-u}}

\headauthor{内山}
\headtitle{形態素解析結果から過分割を検出する統計的尺度}

\affilabel{shinshu-u}{信州大学工学部電気電子工学科}
{Department of Electrical and Electronic Engineering, Faculty of Engineering, Shinshu University(現在，郵政省通信総合研究所，Communications Research Laboratory, Ministry of Posts and Telecommunications)}

\jabstract{本稿では，形態素解析の結果から過分割(正解が分割していないと
  ころを形態素解析システムが分割している個所)を検出するための統計的尺
  度を提案する．もし，形態素解析の結果から過分割を検出できれば，それを
  利用して形態素解析結果の過分割を訂正する規則を作成できるし，人手修正
  済みのコーパスで除去しきれていない過分割を発見し取り除くこともできる
  ため，そのような尺度は有用である．本稿で提案する尺度は文字列に関する
  尺度であり，文字列が分割される確率と分割されない確率との比に基づいて
  いて，分割されにくい文字列ほど大きな値となる．したがって，この値が大
  きい文字列は過分割されている可能性が高い．本稿の実験では，この尺度を
  使うことにより，規則に基づく形態素解析システムの解析結果から，高精度
  で過分割を検出できた．また，人手で修正されたコーパスに残る過分割も検
  出できた．これらのことは，提案尺度が，形態素解析システムの高精度化に
  役立つこと，及び，コーパス作成・整備の際の補助ツールとして役立つこと
  を示している．}

\jkeywords{コーパス,日本語形態素解析,分割誤り}

\etitle{Statistical Measure for Detecting \\
Over-Segmentations in Results of \\
Japanese Morphological Analysis}
\eauthor{UTIYAMA Masao \affiref{shinshu-u}}

\eabstract{This paper proposes a statistical measure for detecting
  over-segmentations, which are errors in segmentation where a
  morphological analyzer segments places which should not be
  segmented, in results of Japanese morphological analysis.  Such a
  measure is useful because we can use detected over-segmentations for
  creating error correction rules or for removing remaining errors in
  manually debugged corpora.  The measure proposed in this paper is
  based on the ratio of the probability of a whole string to that of
  the string being segmented into two parts.  Therefore, the value of
  the measure is high when a given string is rarely segmented into two
  parts. Consequently, a string rated high by the measure is likely to
  contain over-segmentations.  In the experiments, the measure
  detected over-segmentations in the results of rule-based
  morphological analyzers very precisely and it also detected
  remaining over-segmentations in manually debugged corpora.  These
  results show that the proposed measure is useful for developing high
  quality Japanese morphological analyzers and for
  developing/debugging corpora.}

\ekeywords{corpus, Japanese morphological analysis, segmentation error}

\parindent 1em
\newcommand{\B}{}
\newcommand{\E}{}

\begin{document}

\maketitle

\section{はじめに}
\label{sec:introduction}

日本語の形態素解析は，日本語の自然言語処理にとって基本的なものであるの
で，多くの研究・開発が行われている．形態素解析システム\footnote{以下，
  システムとは，形態素解析システムのことであり，解析結果あるいは形態素
  解析結果とは，形態素解析システムの解析結果のことである}には，主に，
人手で作成された規則に基づくシステム\cite[など]
{kurohashi97,matsumoto97,washizaka97,fuchi98}と確率に基づくシステム
\cite[など]{nagata94,mori98,yamamoto97}がある．

本稿では，人手で作成された規則に基づく形態素解析システムを対象として，
形態素解析の結果から半自動的に誤りを検出することを試みる．

形態素解析結果から誤りが検出できた場合には，次のような利点がある．
\begin{enumerate}
\item{}形態素解析の誤りは，形態素解析システムの弱点を示していると考え
  られるので，誤りを分析することにより，システムの性能を向上できる可能
  性がある．
\item{}形態素解析が誤るような表現を連語として登録することで，そのよう
  な誤りが再び起きないようにできる\cite{yamachi96,fuchi98}．
\item{}形態素解析の誤りから誤り訂正規則を作成できるので，その規則を利
  用して形態素解析の精度を向上できる\cite{yokoh97,hisamitsu98}．
\item{}形態素解析の誤りに基づいて，形態素解析の規則に割当てるコストを
  調整したり\cite{komatsu98}，品詞分類を変更する\cite{kitauchi98}こと
  ができる．
\end{enumerate}
これらのことから，形態素解析結果から誤りを検出することは，形態素解析シ
ステムの高精度化に役立つことがわかる．

しかし，形態素解析の結果から誤りを見付けるのは，形態素解析の精度が97〜
99％\cite{fuchi98}に達している現在では，困難になっている．ところが，従
来の研究で，形態素解析結果の誤りを利用して形態素解析の精度を向上させよ
うとしている研究では，それらの誤りを人手で発見すること，あるいは，人手
で作成されたコーパスと形態素解析結果とを比較することにより発見すること
が前提になっている．そのため，形態素解析の誤りを発見することはコストが
高い作業となっている．

一方，本稿では，従来の研究で人手で発見されることが前提となっていた解析
誤り(特に過分割)を，生のコーパスを形態素解析した結果から半自動的に抽出
することを目指し，そのための統計的尺度を提案する．更に，本稿では，人手
により誤りが修正済みのコーパスに対しても提案尺度を適用し，人手で除去し
きれていない誤りを検出することも試みる．もし，人手修正されたコーパスか
ら誤りを検出できたら，提案尺度はコーパス作成・整備の際の補助ツールとし
て役立つことになる．

以下，\ref{sec:measure}章では，本稿が検出対象とする誤り(過分割)の定義
を述べ，それを検出するための統計的尺度について述べる．
\ref{sec:experiments}章では，提案尺度を，公開されている形態素解析シス
テム\cite{kurohashi97,matsumoto97,washizaka97}，および，人手で修正され
たコーパス\cite{edr95,kurohashi98}に適用した結果について述べると共に，
提案尺度を各種統計的尺度と定量的に比較する．\ref{sec:discussion}章では，
提案尺度の有効性などを論じる．\ref{sec:conclusion}章は結論である．

\section{過分割を検出する統計的尺度}
\label{sec:measure}

まず，形態素解析システムの解析結果における分割誤りを分類し，検出対象で
ある過分割を定義づける．次に，過分割を検出する統計的尺度について述べる．

\subsection{形態素解析結果における分割誤り}
\label{sec:error}

分割点という用語を導入し，それを用いて，形態素解析結果における分割誤り
を分類する．

まず，長さ$n$の文字列$S$を，$S=c_1,c_2,...,c_n$とする．このとき，$S$の
$i$番目の分割候補点とは，文字$c_i$と文字$c_{i+1}$の間をいう．また，分
割候補点が分割点であるとは，その分割候補点が形態素境界である場合をいい，
その分割候補点は分割されているという．

たとえば，「$S=休憩室$」とすると，1番目の分割候補点は「休」と「憩」の
間であり，2番目の分割候補点は「憩」と「室」の間である．更に，$S$が「休
憩/室」のように分割されているとすると，2番目の分割候補点は分割点である．

次に，形態素解析結果の分割誤りとは，正解と形態素解析結果とで，分割点が
異なる場合をいう．そして，分割誤りのなかで，過分割とは，正解で分割され
ていない分割候補点をシステムが分割している場合をいう．また，分割不足と
は，正解で分割されている分割候補点をシステムが分割していない場合をいう
\footnote {ここで定義した過分割と分割不足とは，形態素境界(分割点)に注
  目したものであるので，形態素自体に注目した過分割/分割不足の定義とは
  異なる．たとえば，\cite{hisamitsu98}では，分割の誤りを以下の3種類に
  分類している．なお，「正」で示される分割は，当該文字列の正しい分割を
  示し，「誤」で示される分割は，形態素解析システムによる誤った分割を示
  す．
  \begin{description}
  \item[(形態素自体に注目した)過分割]
    \begin{tabular}{ll}
      正: & 今日/の/金/相場/は,...\\
      誤: & 今日/の/金/相/場/は,...\\
    \end{tabular}
  \item[(形態素自体に注目した)分割不足]
    \begin{tabular}{ll}
      正: & ユニックス/ワークステーション\\
      誤: & ユニックスワークステーション\\
    \end{tabular}
  \item[その他の誤り(語境界交差型)]
    \begin{tabular}{ll}
      正: & 病気/が/まん延\\
      誤: & 病気/がまん/延\\
    \end{tabular}
  \end{description}
  この定義では，形態素境界を直接取り扱えないので，本稿での目的には不適
  切である．なお，語境界交差型の分割誤りは，本稿の定義では，過分割と分
  割不足が複合したものとなる．たとえば，上の例では，「が」と「ま」の間
  の分割候補点が分割不足であり，「ん」と「延」の間の分割候補点が過分割
  である．}．

たとえば，「休憩室」の分割の正解が「休憩/室」であるとき，システムが
「休/憩室」と分割したとすると，1番目の分割候補点(「休」と「憩」の間)は
過分割であり，2番目の分割候補点(「憩」と「室」の間)は分割不足である．

なお，形態素解析結果の誤りには，他には品詞付けの誤りがある．これは，形
態素への分割自体は正しいが，品詞が間違った場合である．この誤りの検出に
ついては，分割不足と同様に，本稿では考察しない．

\subsection{過分割の検出尺度の定義}
\label{sec:detecting-method}

ここで定義する尺度は，文字列に関する尺度であり，与えられた文字列が分割
される場合と分割されない場合とで確率を比較し，分割されない確率が高いほ
ど大きな値をとる尺度である．そのため，この尺度の値が大きいような分割を
されている文字列は，誤った分割(過分割)をされている可能性が高い．

より厳密には，与えられた文字列を$S =
a_1,a_2,\ldots,a_k,b_1,b_2,\ldots,b_l$とし，$S$の二つの部分文字列(形態
素\footnote{(\ref{eq:L})式では，形態素の文字列のみを考慮し，品詞は考慮
  しない．なぜなら，そうすることで，計算が単純になり，かつ，確率推定に
  おけるスパースネスを避けることができるからである．})を
$A=a_1,\ldots,a_k$と$B=b_1,\ldots,b_l$とするとき，
\begin{equation}
  \label{eq:L}
  L(A,B) = \log \frac{\Pr(\B,a_1,\ldots,a_k,b_1,\ldots,b_l,\E)}{\Pr(\B,a_1,\ldots,a_k,\E)\Pr(\B,b_1,\ldots, b_l,\E)}
\end{equation}
は，以下で述べるように，文字列$S$の形態素$A$と$B$への切れにくさを表現
している．ただし，$\B$と$\E$は，それぞ
れ，形態素の前後に付ける区切り記号であり，$\Pr(\cdots)$は，文字列の生
起確率である．すなわち，$\Pr(\B,c_1,\ldots,c_k,\E)$については，$c_{0}=\B, c_{k+1}=\E$とすれば，
\begin{eqnarray}
  \label{eq:n}
  \lefteqn{\Pr(\B, c_1,\ldots, c_k, \E)}\nonumber \\
  & = & \Pr(\B)\prod_{i=1}^{k+1}\Pr(c_i|c_0,\ldots,c_{i-1}) \nonumber \\
  & \simeq & \Pr(\B)\prod_{i=1}^{k+1}\Pr(c_i|c_{i-n+1},\ldots,c_{i-1})
\end{eqnarray}
である．ただし，適当な$n$により，$\Pr(c_i|c_0,\ldots,c_{i-1})$を
$\Pr(c_i|c_{i-n+1},\ldots,c_{i-1})$で近似する．

(\ref{eq:L})式で定義した尺度$L(A,B)$は，文字列$S=AB$の形態素$A$と$B$へ
の切れにくさを表している．すなわち，$L(A,B)$が大きいときには，$S$は$A$
と$B$には分割されがたい．なぜなら，$L$の分子は，$S$が一つの形態素とし
て(区切り記号が途中に入らずに)生起する確率であるのに対して，分母は，
$S$が二つの形態素に分れて独立に生起する確率であるから，$L$が大きいほど，
$S$が一つの形態素として生起する比が大きいからである．そのため，$L$が大
きいほど分割が誤っている可能性が高い．すなわち，$a_k$と$b_1$の間の分割
は過分割である可能性が高い．

たとえば，\ref{sec:experiments}章の実験によると，「$S=休憩室$」のとき，
「$A=休$」，「$B=憩室$」とすると，$\log \Pr(\B,休,憩,室, \E)=-24.3$，
$\log \Pr(\B,休,\E)=-12.9$， $\log \Pr(\B,憩,室, \E)=-29.4$であるので，
$L(休,憩室) =-24.3+12.9+29.4=18.0$となる．一方，「$A=休憩$」，「$B=室$」
とすると，$\log \Pr(\B,休,憩,\E)=-13.7$，$\log \Pr(\B,室, \E)=-10.4$で
あるので，$L(休憩,室) =-24.3+13.7+10.4=-0.20$となる．二つを比較すると，
$L(休,憩室) >L(休憩,室) $であるので，尺度$L$によると，「休憩室」につい
ては，「休/憩室」という分割の方が「休憩/室」という分割よりも起り難い．
すなわち，過分割である可能性が高い．これは我々の言語感覚と一致する．

\subsubsection*{尺度$L$の適用範囲}

尺度$L$が効果的に検出できるような過分割は，脚注2に示した分割誤りのうち
で，形態素自体に注目した過分割である．それに対して，語境界交差型におけ
る過分割の検出への尺度$L$の有効性は，形態素自体に注目した過分割に対す
るものよりも低いと予想される\footnote{このことは査読者に指摘していただ
  いた．}．なぜなら，問題にしている分割点が語境界交差型の分割誤りの場
合，たとえば，「が/まん延」を「がまん/延」と間違えている場合には，$L
(がまん,延)$を計算するのだが，このとき，$\Pr(\B, が, ま, ん, 延, \E)$
が高い確率値を示し，$\Pr(\B，が, ま, ん, \E)\Pr(\B, 延 \E)$が低い確率
値を示すことは必ずしも期待できないからである．なぜなら，コーパス中で
「$\B{}がまん延\E$」が頻出し，「$\B{}がまん\E$」や「$\B{}延\E$」が出現
しないということは保証できないからである．

ただし，実際上は，(\ref{eq:n})式の確率を推定するときには，$n=2$や$n=3$
により近似するので，(\ref{eq:L})式の計算に関係するのは，分割点の近傍の
文字だけになり，その結果，語境界交差型の分割誤りと形態素自体に注目した
過分割とで尺度$L$の有効性の違いは小さくなると考えられる．

\subsection{尺度$L$と相互情報量との違い}
\label{sec:rel-to-mi}

本節では，尺度$L$と，よく知られた統計量である相互情報量\cite{Kita96}と
の違いを述べる．

形態素$A$と形態素$B$の相互情報量$MI(A,B)$は，$\Pr(A)$と$\Pr(B)$を形態
素$A$と$B$の生起確率とし，$\Pr(A,B)$を，形態素$A$と形態素$B$が，この順
番で隣接して生起する確率とすると，
\begin{equation}
  \label{eq:mi}
  MI(A,B) = \log \frac{\Pr(A,B)}{\Pr(A)\Pr(B)}
\end{equation}
である．次に，(\ref{eq:mi})式を，(\ref{eq:L})式と同様に，文字の連鎖と
して表すと，
\begin{equation}
  \label{eq:mi-char}
  MI(A,B) = \log \frac{\Pr(\B,a_1,\ldots,a_k,\E,\B,b_1,\ldots,b_l,\E)}{\Pr(\B,a_1,\ldots,a_k,\E)\Pr(\B,b_1,\ldots, b_l,\E)}
\end{equation}
となる．$MI(A,B)$が大きいときには，形態素$A$と$B$が共起する確率は，そ
れぞれが独立に生起する確率よりも大きいといえるので，$MI(A,B)$は，形態
素$A$と形態素$B$との共起関係の強さ(共起強度)を表す尺度として利用できる．

(\ref{eq:L})式で表される尺度$L$と，(\ref{eq:mi-char})式で表される相互
情報量とが異なることは，$\Pr(\B,a_1,\ldots,a_k,b_1,\ldots,b_l,\E) \ne
\Pr(\B,a_1,\ldots,a_k,\E,\B,b_1,\ldots,b_l,\E)$であることから分る．また，
定性的にいっても，$\Pr(\B,a_1,\ldots,a_k,b_1,\ldots,b_l,\E)$は，
\ref{sec:detecting-method}節で述べたように，$S=AB$が一つの形態素として
生起する確率であるのに対して，$\Pr(\B,a_1,\ldots,a_k,\E,\B,b_1,\ldots,b_l,\E)$は，形態素$A$と形態素$B$と
が二つの形態素として隣接して生起する確率であるので，これら二つの確率は
異なる．

定性的な違いを要約すると，尺度$L$は，形態素$S=AB$が形態素$A$と形態素
$B$に分割されるときの分割の困難さを表すが，相互情報量は，形態素$A$と形
態素$B$が隣接して生起するときの生起の容易さを表すと言える．これらは関
連していることは確かであるが，基本的には異なる．

なお，\ref{sec:experiments}章では，実験の一つとして，尺度$L$と相互情報
量を含む五つの尺度について，過分割の検出精度を比較する．

\section{実験}
\label{sec:experiments}

\subsection{実験概要}
\label{sec:overview}

\subsubsection*{実験事項}

三つの実験を行った．実験1では，定性的な評価として，種々の形態素解析シ
ステムの解析結果，および，人手修正されたコーパスについて尺度$L$を適用
し，目視により適用結果を評価した．実験2では，訓練コーパスのサイズを変
えたときの，尺度$L$の過分割検出精度を定量的に評価した．実験3では，五つ
の尺度(尺度$L$/相互情報量/尤度比/改良Dice係数\cite{kitamura97}/Yates補
正された$\chi^2$)について過分割の検出精度を定量的に比較した．

\subsubsection*{確率推定の際の設定}

\paragraph{教師なし学習/教師あり学習}
尺度$L$を求めるためには，(\ref{eq:n})式の確率を求める必要があるので，
形態素に分割された訓練コーパスが必要である．そのようなコーパスとしては，
形態素解析システムにより分割されたコーパスをそのまま用いる場合(教師な
し学習)と，形態素解析結果の誤りを人手で修正したコーパスを用いる場合(教
師あり学習)の二通りが考えられる．そのため，実験1,2,3では，この二つの場
合について，尺度$L$の過分割検出精度などを調べた．

\paragraph{パラメータ推定法} (\ref{eq:n})式の確率を求めるには，$n$を設
定し，かつ，確率推定法も適当に決める必要がある．そのために，本稿では，
実験1と実験2においては，n-gram確率推定のためのツールとして広く使われて
いるCMU-Cambridge Toolkit\cite{clarkson97}を用いて，$n=3$の場合につい
て，バックオフスムージングにより推定した．このときのディスカウント法は
Witten-Bell discounting\cite{placeway93}を用い，カットオフは，文字バイ
グラムと文字トライグラムの双方で1とした\footnote{CUM-Cambridge Toolkit
  は，与えられたn-gramである$c_{i-n+1},\ldots,c_{i}$の頻度が0のとき，
  そのn-gram確率$\Pr(c_i | c_{i-n+1},\ldots,c_{i-1})$を(n-1)-gram確率
  $\Pr(c_i | c_{i-n+2},\ldots,c_{i-1})$から推定する．これをバックオフ
  スムージングという\cite{Kita96}．バックオフスムージングには，種々の
  方法があるが，それらは，ディスカウントといって，頻度が0より大きい
  n-gramの頻度から幾らか割引いて，割引いた分を頻度が0のn-gramに分け与
  える方法により特徴付けられる(これにより頻度が0のn-gramの確率が0より
  大きくなる)．そのディスカウントの一手法が Witten-Bell discounting で
  ある．また，カットオフとは，ある値$x$以下の頻度で生起したn-gramの頻
  度を0として確率を計算する場合の$x$のことである．カットオフ以下の頻度
  のn-gramは，頻度が0として扱われるが，バックオフスムージングにより0よ
  り大きい確率が付与される．}．一方，実験3においては，最尤推定により求
めた確率により尺度$L$を計算した．その理由は，尺度$L$以外の尺度において
は，通常，最尤推定を用いて，確率を計算しているので，それに合せるためで
ある．また，比較を簡単にするために，$n=2$の場合について各種の尺度を比
較した．

\subsubsection*{コーパス}

実験1,2,3で共通に用いるコーパスは京都大学テキストコーパス
version2.0\cite{kurohashi98}である．京都大学テキストコーパスは，CD-毎
日新聞95年度版から約2万文を抽出したものであり，形態素・構文解析されて
いる．このコーパスを均等に2分割し，実験に用いた．以下では，その一方を
京大コーパスAと呼び，他方を京大コーパスBと呼ぶ．京大コーパスAは主に確
率推定のための訓練コーパスとして用い，京大コーパスBは主に過分割の検出
精度を評価するためのテストコーパスとして用いた．

\subsection{実験1：目視による尺度$L$の評価}
\label{sec:look}

実験1では，定性的な評価として，種々の形態素解析システムの解析結果，お
よび，人手修正されたコーパスについて尺度$L$を適用し，目視により適用結
果を評価した．

\subsubsection*{実験材料：コーパスと形態素解析システム}

\paragraph{教師なし学習の場合}

教師なし学習では，確率推定用の訓練コーパスと過分割検出用のテストコーパ
スとが同一である．つまり，確率を推定したコーパス中における過分割を検出
する．

このときのコーパスとしては，京大コーパスBとEDR日本語コーパスversion
1.5\cite{edr95}の全文を用いた．なお，EDR日本語コーパスは，新聞・雑誌・
辞典などの流通文書から1文単位でとられた約21万文からなるコーパスであり，
各文は，形態素・構文・意味解析されている．

これらのコーパスにおける生の文を分割する形態素解析システムとしては，公
開されている形態素解析システムのうちから，JUMAN version
3.5\cite{kurohashi97}，茶筅version 1.51\cite{matsumoto97}, すもも
version 1.3\cite{washizaka97}を用いた．これらの形態素解析システムは，
全て，規則に基づいて形態素解析をするものである．なお，これらの形態素解
析システムを用いるときには，ただ一つの(ベストの)解析結果を出力させた．

これらのコーパスと形態素解析システムとの組み合わせは，EDRコーパスに対
しては，三つの形態素解析システム全てを適用したが，京大コーパスBについ
ては，JUMANのみを適用した\footnote{こうした理由は，京大コーパスは主に
  実験2,3における定量的な評価に使用することを意図したものであり，実験1
  では，EDRコーパスを主な対象としたからである．}．また，二つのコーパス
の元々の分割(人手修正済みの分割)についても試した．すなわち，全部で6種
の形態素分割に対して尺度$L$を適用した．

\paragraph{教師あり学習の場合}

教師あり学習では，確率推定用の訓練コーパスと過分割検出用のテストコーパ
スとが異なる．

実験1では，京大コーパスAの元々の分割(JUMANの解析結果を人手修正したもの)
を訓練データとして(\ref{eq:n})式の確率を推定した．そして，その推定値を
利用して，JUMANにより形態素解析された京大コーパスBに対して尺度$L$を適
用した．

\subsubsection*{実験方法}

7種(=教師なし6種+教師あり1種)の形態素分割のそれぞれに対して，その全て
の分割点について，前後の形態素から尺度$L$を計算した．たとえば，「休/憩
室/は/広い/。」のように分割されている文については，四つの分割点におい
て，それぞれ，$L(休,憩室)$，$L(憩室,は)$，$L(は,広い)$，$L(広い,。)$を
計算した．このとき，(\ref{eq:n})式の確率は，\ref{sec:overview}節で述べ
たように，$n=3$としてバックオフスムージングにより計算した．

\subsubsection*{実験結果}

7種の形態素分割のそれぞれに対して，全ての分割点を尺度$L$により降順に
(同一尺度値の場合はランダムにtieを解消して)ソートし，その上位から異な
り150個を選んだ\footnote{たとえば，「休/憩室/は/広い/。」と「休/憩室/
  は/狭い/。」という文があるとき，それぞれの文について，$L(休,憩室)$が
  求まるが，この二つの$L$は同じ文字列を同じように分割しているので，異
  なりとしては一つである．ただし，二つの分割点を比べたとき，分割点の前
  後の形態素の字面は同じであっても，品詞が異なる場合には異なる分割点と
  して扱った．すなわち，たとえば，$L(A,B)$と$L(a,b)$という二つの分割点
  があり，字面上は，$A=a$,$B=b$であったとしても，$A$と$a$の品詞が異な
  るか，$B$と$b$の品詞が異なる場合には，それらの分割点は，異なるものと
  して扱った．そうした理由は，尺度$L$が検出できるのは過分割だけであっ
  ても，我々が実際に興味があるのは品詞付けの誤りを含めた形態素解析結果
  の誤りだからである．}．そして，それぞれの異なりについて，一個の分割
点を無作為に抽出し，それが過分割であるかを判定した\footnote{過分割かど
  うかの判定，すなわち，形態素解析システムによる分割点が実際に切って良
  いかどうかの判定は，複合名詞について困難であるが，もしも，形態素への
  分割の結果として生じる括弧付けが，筆者の内省に基づいた括弧付けと交差
  する(cross bracketing)なら，その分割点による分割は誤り(過分割)とする．
  たとえば，「東南アジアツアー」は，筆者の内省によれば(((東南)アジア)
  ツアー)という構造をしているので，「東南/アジアツアー」という分割は過
  分割とする．なぜなら，この分割では，((東南) (アジアツアー))という構
  造になるので，括弧が交差するからである．一方，「東南アジア/ツアー」
  は((東南アジア)ツアー)という構造なので正解とする．なお，分割の正誤の
  判定が困難なものについては，品詞を参照し，もし品詞が誤っていたら分割
  も誤りとした．ただし，上位異なり150個については，付録の表
  \ref{tab:edr}から表\ref{tab:cv}にある例と同様に，形態素の途中で分割
  されているものがほとんどであるので，分割の正誤の判定に迷うような例は
  少ない．}．なお，判定は筆者による．

\begin{table}[htbp]
  \begin{center}
    \caption{上位異なり150分割点における過分割の数}
    \begin{tabular}{|l|l|l|c|}
      \hline
      学習方法 & コーパス         & 解析システム     & 過分割の数 \\
      \hline
      \hline
      教師なし & EDR              & 人手(元々の分割) & 43 \\
      &                           & JUMAN            & 126\\
      &                           & 茶筌             & 128\\
      &                           & すもも           & 125\\
      \cline{2-4}
      &          京大コーパスB    & 人手(元々の分割) & 49\\
      &                           & JUMAN            & 98\\
      \hline
      教師あり & 京大コーパスB    & JUMAN            & 125\\
      \hline
    \end{tabular}
    \label{tab:errs}
  \end{center}
\end{table}

判定した150個の分割点について，それが実際に過分割であった数を表
\ref{tab:errs}に示す．表から分かるように，これら150個の中に過分割が占
める割合は非常に高い．たとえば，表\ref{tab:errs}では，茶筅には128個の
過分割がある．一方，平均的には，茶筌の分割が過分割であるのは，1.5％以
下であると言ってよい\footnote{\cite{fuchi98}によると，茶筌のEDRコーパ
  スにおける形態素解析結果の適合率(=100$\times$(茶筌の形態素解析結果の
  形態素で正解と一致したものの数/茶筌の形態素解析結果の形態素の総数))
  は，字面が一致していた場合を一致とすると，98.5％である．ここで，形態
  素の適合率は，実験2でも示すように，分割点の適合率よりも低くなる．な
  ぜなら，形態素が一致するためには，その前後の分割点も一致しなくてはな
  らないため，形態素が一致するというのは，分割点の一致よりも厳しい条件
  であるからである．そのため，分割点が過分割であるのは1.5％以下と言っ
  て良い．}．つまり，茶筅の150個の分割点のうちで，過分割は，平均的には，
$150 \times 0.015=2.25$個以下である．よって，茶筅の解析結果から128個の
過分割を検出するためには，平均的には，$(128/2.25) \times 150 \simeq
8533$個以上の分割点を調べなければならないことになる．同様なことが，他
の形態素解析システムによる分割結果，あるいは，人手で修正された分割結果
についても言える．これより，尺度$L$を用いることにより，形態素解析結果
から過分割を効率的に抽出できるといえる．なお，表\ref{tab:errs}において，
JUMANで解析された京大コーパスBからの過分割検出結果について，教師なし学
習の場合と教師あり学習の場合とを比べると，教師あり学習の方が検出個数が
多い．これは，教師あり学習の方が，(\ref{eq:n})式の値を正確に推定できる
からであると解釈できる．

さらに，教師なし学習の場合の6種の形態素分割のそれぞれについて，上位異
なり150個中の過分割から上位12個の過分割を付録の表\ref{tab:edr}と表
\ref{tab:kyoto2.0}に示す．表で「数」とある欄には，そのような過分割を含
む文の数がある．また，「形態素/品詞」とある二つの欄は，尺度$L$を計算し
た分割点の前後の形態素と品詞を示す．なお，品詞は，それぞれの形態素解析
システムの品詞である．また，表の解析結果は，各解析システムが一つだけ解
析結果を出力した場合のものである．もし，複数の解析結果も出力するように
すれば，表中の文について，当該の形態素解析システムが正解を含む解を出す
ことはある．

これらの表に示されている過分割の中には，何らかの規則性があるとすぐに分
るものもある．たとえば，EDRコーパスの元々の分割に含まれる過分割(表
\ref{tab:edr})においては，「引き下げ/よう」が「引き下/げ/よう」と分割
されていたり，「掲げ/、」が「掲/げ/、」のように分割されるなど，動詞の
語幹が分割される例が大半である\footnote{EDRコーパスの元々の分割におい
  ては，「掲げ、」という文字列を含む文が14例あるが，そのうち表
  \ref{tab:edr}の1例のみが，「掲/げ/、」という分割であり，その他の13例
  は，「掲げ/、」という分割である．このことは「掲/げ/、」が過分割であ
  ることを傍証している．これと同様なことが，表\ref{tab:edr}のその他の
  例についても言える．なお，「掲げ/、」という分割を含む例には，「虹/を
  /描/い/た/旗/を/掲げ/、/高らか/に/歌/う/。」や「独特/の/理想/を/掲げ
  /、/実行/し/た/人/だっ/た/。」のような例がある．}．一方，EDRコーパス
に対する茶筌の過分割では，「結果」が「結(普通名詞) /果(普通名詞)」と分
割されていたり，「考えて」が「考(普通名詞)/えて(普通名詞)」と分割され
ているが，このような例に含まれる規則性は，もしあったとしても，容易には
分らない．

いずれにしろ，尺度$L$を使うことにより，ある程度の量の，形態素解析結果
の過分割が，教師なし学習により容易に抽出できることが分かる．このような
例を集めるのは人手では手間が掛る．また，尺度$L$は人手修正後のコーパス
に残る過分割も検出できるため，コーパス作成・整備の際の補助ツールとして
も役立つと考える．

また，付録の表\ref{tab:cv}には，教師あり学習の場合について，尺度$L$の
値が上位12個の過分割を示す．ここで，教師あり学習の結果である表
\ref{tab:cv}におけるJUMANの過分割と，教師なし学習の結果である表
\ref{tab:kyoto2.0}におけるJUMANの過分割とを比べると，表\ref{tab:cv}に
おいては「護/煕」など固有名詞が占める割合が多いが，表
\ref{tab:kyoto2.0}では固有名詞は一つ(「若/乃/花」)しか存在しないことが
わかる．表\ref{tab:kyoto2.0}に固有名詞が少ないのは，固有名詞は未知語で
ある場合が他の品詞と比べて多いため，常に過分割される場合も多くなり，そ
の結果として尺度$L$の値が小さくなる場合が多いためである．このように，
形態素解析システムが常に過分割してしまうような場合を検出するためには，
人手修正済みコーパスが必要であると言える．

\subsection{実験2：過分割検出精度の定量的評価}
\label{sec:size}

教師なし学習により何か統計的に興味のある言語現象を発見するような応用
\cite[など]{shiNnou95,ikehara95,shimohata95,hisamitsu97}においては，新
聞記事などの大規模なコーパスが比較的用意に入手できるので，訓練コーパス
のサイズは深刻な問題ではない．これは本稿における過分割検出の場合でも同
様である．しかし，教師あり学習の場合には，訓練コーパスを構築するのはコ
ストが掛るため，なるべく小さな訓練コーパスであることが望ましい．そこで，
実験2では，主に教師あり学習の場合を対象として，訓練コーパスのサイズと
過分割検出精度との関係を調べた．ただし，教師なし学習の場合についても，
教師あり学習と比較するために，訓練コーパスのサイズと過分割検出精度との
関係を同様に調べた．

\subsubsection*{実験材料：コーパス}

確率推定用の訓練コーパスとしては京大コーパスAを用い，過分割検出の精度
を調べるテストコーパスとしてはJUMANにより分割された京大コーパスBを用い
た．このことは，教師あり学習と教師なし学習とで共通である．ただし，教師
あり学習では京大コーパスAの元々の分割から(\ref{eq:n})式の確率を推定し，
教師なし学習では，京大コーパスAをJUMANにより形態素解析した結果から
(\ref{eq:n})式の確率を推定した\footnote{教師なし学習において，京大コー
  パスAを訓練コーパスにした場合と，京大コーパスBをJUMANにより形態素解
  析した結果を訓練コーパスとした(訓練コーパスとテストコーパスが同一の)
  場合とでは，(後述する(\ref{eq:examination})式で定義する分割点調査率の
  意味における)過分割検出精度は，ほぼ等しい．}．

\paragraph{テストコーパスの各種統計}

テストコーパスである京大コーパスBについて，その元々の分割を正解と看倣
して\footnote{実験1で見付けた過分割についても修正はしていない．}，分割
の正誤を判定したときの統計を表\ref{tab:stat}に示す．

\begin{table}[htbp]
  \begin{center}
    \caption{京大コーパスBにおける分割点についての統計}
    \begin{tabular}{|lr|}
      \hline
      正解における分割点の数 & 232572 \\
      JUMANによる分割点の数 & 233048 \\
      一致した分割点の数 & 231816 \\
      分割点の再現率 & 99.7％ \\
      分割点の適合率 & 99.5％ \\
      \hline
      過分割の数 &  1232 \\
      分割不足の数 & 756 \\
      分割の間違いの数(過分割の数+分割不足の数) & 1988\\
      100$\times$(過分割の数/分割の間違いの数) & 62.0％ \\
      100$\times$(過分割の数/JUMANによる分割点の数) & 0.5％ \\
      \hline
    \end{tabular}
    \label{tab:stat}
  \end{center}
\end{table}

表\ref{tab:stat}より，分割の間違いに占める過分割は62.0％である．加えて，
過分割の周辺には分割不足も起りやすいと言えるので，過分割が検出できれば，
その周囲も調べることにより，分割誤りの多くが検出できると言える．

しかし，分割点の再現率(=100$\times$(一致した分割点の数/正解における分
割点の数))と適合率(=100$\times$(一致した分割点の数/JUMANによる分割点の
数))は，それぞれ，99.7％，99.5％と非常に高い\footnote{参考のため，
  \cite{nagata94}の基準による，形態素の再現率(=100$\times$(一致した形
  態素の数/正解における形態素の数))と適合率(=100$\times$(一致した形態
  素の数/JUMANによる形態素の数))を求めると，それぞれ，99.1％と98.9％に
  なる(ただし，字面が一致していれば形態素が一致したと看倣す)．これらか
  らも分るように，形態素の再現率と適合率とは分割点のものに比べて低い．}．
また，JUMANの分割点全体の中で過分割である分割点は0.5％(=100％$-$適合率)
であるので，過分割を見付けるのは人手では困難であると考える．

\subsubsection*{実験方法}

約1万文からなる訓練コーパスから，約1000，2000, ..., 10000文を選び，そ
れぞれの場合について，$n=3$としてバックオフスムージングにより
(\ref{eq:n})式の確率を推定し，それを利用して約1万文からなるテストコー
パスにおける全分割点の尺度$L$の値を計算した．そして，全ての分割点を尺
度$L$により降順にソートし，上位の分割点から，過分割かどうかを，テスト
コーパスの元々の分割を正解として調べた．

\subsubsection*{実験結果}

まず，全訓練データを使用した場合についての実験結果を述べ，次に訓練デー
タを1000文ずつ増加した場合についての実験結果を述べる．

\paragraph{全訓練データを使用した場合}

図\ref{fig:10000}には，約1万文の訓練データ全てを使って確率推定した場合
について，教師あり学習と教師なし学習のそれぞれについて，過分割検出の再
現率(percent recall)に対する適合率(percent precision)および分割点調査
率(percent examination)を示す．ここで，
\begin{displaymath}
  \label{eq:recall}
  再現率 = 100 \times \frac{検出された過分割の数}{テストコーパスにおける過分割の数},
\end{displaymath}
\begin{displaymath}
  \label{eq:precision}
  適合率 = 100 \times \frac{検出された過分割の数}{尺度Lの上位から順番に調べた分割点の数},
\end{displaymath}
\begin{equation}
  \label{eq:examination}
  分割点調査率 = 100 \times \frac{尺度Lの上位から順番に調べた分割点の数}{テストコーパスにおける全分割点の数}.
\end{equation}

\begin{figure}[htbp]
  \begin{center}
    \epsfile{file=10000.eps}
    \caption{再現率と適合率/分割点調査率}
    \label{fig:10000}
  \end{center}
\end{figure}

図\ref{fig:10000}の 教師あり学習の場合の適合率(supervised-precision)お
よび教師なし学習の場合の適合率(unsupervised-precision)のプロットから分
かるように，上位における過分割検出の適合率は非常に高い．たとえば，再現
率が10.0％のとき，適合率は，教師あり学習の場合に90.5％であり，教師なし
学習の場合に46.8％であるが，これらは，JUMANの分割点全体の中で過分割が
占めるパーセンテージである0.5％の，180倍以上，および，90倍以上である．
この適合率の高さは，実験1での結果を裏付けるものである．

また，図\ref{fig:10000}の教師あり学習の場合の分割点調査率
(supervised-examination)および教師なし学習の場合の分割点調査率
(unsupervised-examination)から分かるように，一部の分割点を調べるだけで
多くの過分割を検出できると言える．たとえば，全体の過分割のなかから再現
率50％で過分割を見付けるためには，教師あり学習の場合には，全分割点の
0.5％を調べればよく，教師なし学習の場合には，全分割点の2.0％を調べれば
よい．さらに，90％の過分割を見付けるためには，教師あり学習の場合には，
全分割点の7.8％を調べればよく，教師なし学習の場合には，全分割点の12.2
％を調べればよい．一方，もし，無作為に分割点を調べるという方法により，
過分割を検出しようとしたならば，50％の過分割を見付けるためには，平均的
には，全分割点の50％を調べる必要があり，90％の過分割を見付けるためには，
90％の分割点を調べる必要がある．

以上より，尺度$L$を使うことにより，過分割の検出が効率良くできると言え
る．

なお，再現率，適合率，分割点調査率の間には
\begin{equation}
  \label{eq:rel}
  適合率  = K \times \frac{再現率}{分割点調査率}.
\end{equation}
という関係が成立する．ただし，$K$はテストコーパスに固有の定数であり，
\begin{displaymath}
K = 100 \times \frac{テストコーパスにおける過分割の数}{テストコーパスにおける全分割点の数}.
\end{displaymath}
(\ref{eq:rel})式から，分割点調査率と再現率が決まれば適合率が決まること
が分かる(e.g.,分割点調査率が小さければ適合率は高い)．そのため，以下で
は，再現率に対する分割点調査率のみに基づいて過分割の検出精度を評価する．
そして，同一の再現率に対して分割点調査率が小さいとき過分割の検出精度が
高いと言い，その逆のときに過分割の検出精度が低いと言うことにする．

\paragraph{1000文ずつ訓練データを増やした場合}

図\ref{fig:incr-exam}には，過分割検出の再現率が25,50,75％の場合
(recall25,recall50,recall75)について，教師あり学習の場合と教師なし学習
の場合における，訓練文数(Num. of training sentences)と分割点調査率の関
係を示す．


\begin{figure}[htbp]
  \begin{center}
\vspace{1mm}
    \epsfile{file=exam25-50-75.eps}
\vspace{2mm}
    \caption{訓練データを増やした場合の再現率と分割点調査率}
    \label{fig:incr-exam}
  \end{center}
\end{figure}

図\ref{fig:incr-exam}から，教師あり学習の場合
(supervised-recall25,50,75)については，訓練文数が増加すると，再現率が
50％と75％においては，分割点調査率が明確に減少していると言える．また，
再現率が25％についても緩やかに分割点調査率は減少している．一方，教師な
し学習の場合(unsupervised-recall25,50,75)については，訓練文数が増えて
いっても，2000文以上については，分割点調査率は(若干の変動はあるが)ほ
ぼ横ばいである．

このことは，教師あり学習については，訓練データが多くなれば多くなるだけ，
(\ref{eq:n})式の確率を精密に推定できるが，教師なし学習については，訓練
データが多くなったとしても，その確率推定に対する効果は，教師あり学習の
場合に比べれば，小さいことを示している．

\vspace{17mm}

\subsection{実験3：各種尺度の比較}
\label{sec:comp}

実験3では，尺度$L$，相互情報量，尤度比，改良Dice係数\cite{kitamura97}，
Yates補正された$\chi^2$，の五つの尺度について，過分割の検出精度を比較
した．ここで，尤度比，改良Dice係数，Yates補正された$\chi^2$は，
\cite{hisamitsu97}において，有用な括弧表現を抽出するために有効であると
された尺度である．また，尤度比は，\cite{kageura97}でも，2文字間の連関
の尺度として，漢字列の分割に有効であることが示されている．

以下では，まず，本実験のテストコーパスとした京大コーパスBについて，そ
こでの分割点の出現頻度の統計について述べる．この出現統計は，あとで，各
尺度間の過分割検出精度の違いを説明するときの資料に用いる．次に，各尺度
を定義し比較する．

\subsubsection*{テストコーパスにおける分割点の出現統計}

形態素$A$の最後の文字を$a$,形態素$B$の最初の文字を$b$とし，$a$と$b$に
挟まれるような分割点を，前後1文字で区別される分割点と呼ぶ．実験3では，
分割点といえば，前後1文字で区別される分割点のこととする．つまり，
「ab/cd」と「xb/cy」のような分割点は，分割点の前後1文字が同じであるの
で，区別しないで同一タイプの分割点として扱う．

表\ref{tab:freq}は，テストコーパスとした京大コーパスBにおける分割点に
ついて，過分割である分割点とそうでない分割点のそれぞれに対して，出現頻
度ごとの，分割点の異なり数などを調べたものである．ここで，分割点の総数
を$F$,頻度$r$における分割点の異なり数を$k_r$とすると，頻度$r$における
延べ数は$f_r = r \times k_r $であり，$F = \sum_r f_r$である．表
\ref{tab:freq}では，頻度$r$における「延べ％」は$100 \times f_r/F$であ
り，「累積％」は$\sum_{s=1}^{r} 100 \times f_s/F$である．

\begin{table}[htbp]
  \begin{center}
    \caption{京大コーパスBにおける分割点の出現統計}
    
    \begin{tabular}{|c||ccc|ccc|}
      \hline
      & \multicolumn{3}{c|}{過分割である分割点}
      & \multicolumn{3}{c|}{過分割でない分割点}\\
      \cline{2-7}
      \raisebox{1.5ex}[0pt]{頻度} & 異なり数 & 延べ％ & 累積％ & 異なり数 & 延べ％ & 累積％ \\
      \hline
      1 & 645 &52.4 &52.4  &24180 &10.4 & 10.4\\
      2 & 104 &16.9 &69.2  &7187  &6.2  & 16.6\\
      3 & 29  &7.1  &76.3  &3579  &4.6  & 21.3\\
      4 & 11  &3.6  &79.9  &2133  &3.7  & 24.9\\
      5 & 9   &3.7  &83.5  &1413  &3.0  & 28.0\\
      6 & 5   &2.4  &86.0  &1062  &2.7  & 30.7\\
      7 & 2   &1.1  &87.1  &764   &2.3  & 33.0\\
      8 & 3   &1.9  &89.0  &644   &2.2  & 35.3\\
      9 & 1   &0.7  &89.8  &496   &1.9  & 37.2\\
      10& 1   &0.8  &90.6  &411   &1.8  & 39.0\\
      \hline
  11以上& 8   &9.4  &100.0 &3586  &61.0 & 100.0 \\
      \hline
    \end{tabular}
    \label{tab:freq}
  \end{center}
\end{table}

表\ref{tab:freq}から，過分割である分割点の出現頻度は，そうでない場合に
比べて，低頻度であると言える．これは，過分割である分割点の数自体が少な
いことが主な原因である．また，過分割である場合とそうでない場合の分布の
様子を比べると，過分割である分割点の場合には，頻度が1か2であるような場
合が全体の50％以上を占めていることから分かるように，低頻度の方に分布が
偏っている．

\subsubsection*{各尺度の定義}

まず，$n=2$として，(\ref{eq:n})式を用いて，(\ref{eq:L})式を変形すると，
\begin{equation}
  \label{eq:Ln2}
  L(A,B) = \log \frac{\Pr(a_k,b_1)}{\Pr(a_k,\E)\Pr(\B,b_1)}
\end{equation}
となる．一方，(\ref{eq:mi})式を同様に変形すると
\begin{displaymath}
  MI(A,B) = \log \frac{\Pr(\E,\B)}{\Pr(\E)\Pr(\B)}
\end{displaymath}
という無意味な値になるので，区切り文字とそれに隣接する文字は特に強く結
合すると仮定し，
\begin{displaymath}
  \Pr(\B, c_1,\ldots, c_k, \E) = \Pr(\B, c_1)\Pr(c_2|\B, c_1)\cdots\Pr(\E, c_k|c_{k-1})
\end{displaymath}
のような変形をすると，
\begin{equation}
  \label{eq:MIn2}
  MI(A,B) = \log \frac{\Pr(a_k,\E,\B,b_1)}{\Pr(a_k,\E)\Pr(\B,b_1)}
\end{equation}
となる．なお，以下では，$a=a_k$,$b=b_1$とする．

(\ref{eq:MIn2})式から，$n=2$においては，相互情報量$MI(A,B)$は，$a \E$
と$\B b$をそれぞれ一つの項と看做せば，この2項に関する通常の相互情報量
の式と一致することがわかる．そこで，尤度比，改良Dice係数，Yates補正さ
れた$\chi^2$についても，これら2項に基づいて，その値を計算する．

以下では，\cite{hisamitsu97}に基づいて，尤度比，改良Dice係数，Yates補
正された$\chi^2$を定義する．また，尺度$L$と相互情報量についても，確率
を最尤推定した形で定義する．

各尺度を定義する準備として，まず，$f_{ij}(i,j=1,2)$は，分割表で示すと
\begin{quote}
  \begin{tabular}{|l|c|c|}
    \hline
    & 後続文字が$\B b$ & 後続文字が$\B b$以外 \\
    \hline
    先行文字が$a \E$     & $f_{11}$           & $f_{12}$ \\
    \hline
    先行文字が$a \E$以外 & $f_{21}$           & $f_{22}$ \\
    \hline
  \end{tabular}
\end{quote}
である．より厳密には，$f(\cdots)$を文字列の頻度とし，$v,w,x,y$を，$\B$
と$\E$を含む任意の文字としたとき，
\begin{displaymath}
  \begin{array}{rcl}
    f_{11}& = &f(a, \E, \B, b)\\
    f_{12}& = &\sum_{xy \ne \B b} f(a,\E,x,y)\\
    f_{21}& = &\sum_{vw \ne a \E} f(v,w,\B,b)\\
    f_{22}& = &\sum_{vwxy} f(v,w,x,y) - f_{11} - f_{12} - f_{21}
  \end{array}
\end{displaymath}
である．また，
\begin{displaymath}
  \begin{array}{rcl}
    f_{i.}& = & f_{i1}+f_{i2}\\
    f_{.j}& = & f_{1j}+f_{2j}\\
    F     & = &\sum_{i,j}f_{ij}
  \end{array}
\end{displaymath}
である．

\paragraph{尤度比}

ここでの「尤度比」は，$a \E$と$\B b$の2項が従属とした場合と独立とした
場合との最尤推定量による尤度比であり，
\begin{equation}
  \label{eq:lambda}
  \lambda = 2 \sum_{i,j} f_{ij}\left \{\log\frac{f_{ij}}{F} - \log\frac{f_{i.}f_{.j}}{F^2} \right \}
\end{equation}
である．なお，上式では，分割点のソートに無関係な項は除いてある．

$\lambda$は，2項が従属して生起する度合が強いとき，正で大きな値をとる．
しかし，これだけでは必ずしも共起強度が強いとは言えない．たとえば，
\begin{tabular}{|l|l|}
  \hline
  10 & 1   \\
  \hline
  1  & 10  \\
  \hline
\end{tabular}
と
\begin{tabular}{|l|l|}
  \hline
  1  & 10   \\
  \hline
  10  & 1  \\
  \hline
\end{tabular}
は同じ$\lambda$となる．これらのうち前者は共起強度が強いが，後者は弱
い(反発している)．このことを考慮して，$\lambda>0$のときには，
\cite{kageura97}と同様に，Yuleの
$Y (=\frac{\sqrt{f_{11}f_{22}}-\sqrt{f_{12}f_{21}}}{\sqrt{f_{11}f_{22}}+\sqrt{f_{12}f_{21}}})$
の符合を付けることにより，分割点をソートした．

\paragraph{Yates補正された$\chi^2$}

$\lambda$と同様に独立性の判定に用いられる尺度である．
\begin{equation}
  \label{eq:chi2}
  \chi^2 = \frac{F(|f_{11}f_{22}-f_{12}f_{21}|-F/2)^2}{f_{1.}f_{2.}f_{.1}f_{.2}}
\end{equation}
なお，$\chi^2$に関しても，$\lambda$と同様な理由から，Yuleの$Y$の符合
を付けて分割点をソートした．

\paragraph{改良Dice係数}

\cite{kitamura97}で，対訳単語間の類似度として，提案されている尺度であ
る．
\begin{equation}
  \label{eq:dice}
  \mbox{改良Dice係数} = (\log f_{11})\frac{2f_{11}}{f_{1.}+f_{.1}}.
\end{equation}

\paragraph{相互情報量}
(\ref{eq:MIn2})式より，分割点のソートに無関係な項は除くと，
\begin{equation}
  \label{eq:mi2}
  MI^\prime = \log \frac{f_{11}}{f_{1.}f_{.1}}.
\end{equation}

\paragraph{尺度$L$}
(\ref{eq:Ln2})式より，分割点のソートに無関係な項は除くと，
\begin{equation}
  \label{eq:L2}
  L^\prime = \log \frac{f(a,b)}{f_{1.}f_{.1}}
\end{equation}

ここで，上記の各尺度について，もし，$f_{ij}=0$，あるいは，$f(a,b)=0$と
なる場合には，それぞれを0.1として計算した．

\subsubsection*{教師なし学習の場合での各尺度の比較}

各尺度について，JUMANにより形態素解析された京大コーパスBを訓練およびテ
ストコーパスとして，過分割の再現率に対する分割点調査率を評価した結果を
図\ref{fig:unsup-cmp}に示す．

図\ref{fig:unsup-cmp}から分るように，改良Dice係数(Dice)，
$\lambda$(lambda)，$\chi^2$(chi＾2)の分割点調査率は，$L^\prime$や
$MI^\prime$と比べて大きい．すなわち，過分割検出精度は低い．この原因は，
これらの尺度が，統計的に有意と言えないような低頻度の共起関係をノイズと
して排除するような尺度であるからである．すなわち，頻度が1とか2とかの共
起関係の尺度値は，これらの尺度では大きくならない\footnote{特に，改良
  Dice係数では，頻度が1の共起関係については，値が0となる}ため，(表
\ref{tab:freq}に示されるように)低頻度である過分割が排除されるためであ
る．

このような性質は，\cite{hisamitsu97}や\cite{kageura97}や
\cite{kitamura97}のような，一般的に共起強度が高い共起関係を必要とする
ような応用に対しては適していたが，低頻度事象である過分割を検出するには
適さない．

一方，$MI^\prime$の過分割検出精度は，再現率50％程度のところまでは，
$L^\prime$とほぼ同じである(実際には若干低い)．これは，$MI^\prime$が低
頻度の共起関係を過大評価する\cite{hisamitsu97}からであろう．つまり，再
現率が低いところでは，低頻度で，かつ，共起強度の強い表現を選択的に拾っ
てくるが，そのようなものは過分割であることが多いため，検出精度が高いと
解釈できる．しかし，再現率が上ってくると，比較的頻度が高い過分割も増え
てくるため，共起強度だけでは，過分割なのか，そうでない分割かが区別でき
なくなり，検出精度が下がると言える．

これらの尺度に対して，$L^\prime$は，分割されるか分割されないかを直接モ
デル化した尺度であるため，再現率が高くなっても検出精度が高いものと考え
る．

\begin{figure}[htbp]
  \begin{center}
\vspace{4mm}
    \epsfile{file=measure-exam.eps}
\vspace{1mm}
    \caption{過分割の再現率と分割点調査率(教師なし学習)}
    \label{fig:unsup-cmp}
  \end{center}
\end{figure}


\begin{figure}[htbp]
  \begin{center}
\vspace{3mm}
    \epsfile{file=sup-measure-exam.eps}
\vspace{1mm}
    \caption{過分割の再現率と分割点調査率(教師あり学習)}
    \label{fig:sup-cmp}
  \end{center}
\end{figure}
\vspace{-3mm}
なお，筆者は，予備実験として，相互情報量とYate補正した$\chi^{2}$を，隣
接する形態素間について，(文字ではなく)形態素を単位とする2項関係に基づ
いて計算してみたが，その性質は尺度$L$とは非常に異なっていた．相互情報
量の性質とYate補正した$\chi^{2}$の性質とは，互いに若干は異なるが，おお
まかには，二つの尺度とも，固有名詞(「福沢/諭吉」など)や四字熟語(「不眠
/不休」など)を取ってくる傾向が強かった．これらの隣接形態素は，それ自体
は有用な表現ではあるが，これらの隣接形態素間の分割が間違っているわけで
はないので，本稿での目的である過分割の検出には適さない．

その他，共起や定型表現を抽出する研究として，特に文字列レベルに関係する
ものでは，\cite[など]{shiNnou95,ikehara95,shimohata95}がある．これらの
研究では，大量の生テキストコーパスから，統計量を用いることにより，「に
関して」や「に対しては」などの定型的な表現を抽出する．これらの表現は有
用な表現ではあるが，「に対して」を「に/対/し/て」と分割しても，過分割
ではないことからも分るように，これらの手法は，本稿での目的である過分割
の検出には適さない．

\subsubsection*{教師あり学習の場合での各尺度の比較}

各尺度に対して，京大コーパスAの元々の分割を訓練コーパス，京大コーパスB
をテストコーパスとして，過分割の再現率に対する分割点率調査を評価した結
果を図\ref{fig:sup-cmp}に示す．

図\ref{fig:sup-cmp}では，図\ref{fig:unsup-cmp}と同様に尺度$L$の分割点
調査率が一番小さい．そして，図\ref{fig:sup-cmp}と図\ref{fig:unsup-cmp}
を比べると，尺度$L$については，図\ref{fig:sup-cmp}の教師あり学習の方が
図\ref{fig:unsup-cmp}の教師なし学習の場合よりも分割点調査率が小さい．

一方，尺度$L$以外の尺度については，教師あり学習の方が分割点調査率は大
きくなっている．これは，教師あり学習の場合の方が，教師なし学習の場合よ
りも，テストコーパスにおいて，過分割の前後の文字の共起強度が小さいこと
を示している．この理由は，教師あり学習においては，訓練コーパスで過分割
であるような分割点が人手により除かれているため，テストコーパスで過分割
であるような分割点は訓練コーパスで出現することが稀となり，その結果，共
起強度が小さくなるからである．このことから，尺度$L$以外の尺度について
は，教師あり学習をしても過分割検出精度が高くならないことが分かる．


\section{考察と今後の課題}
\label{sec:discussion}

\subsubsection*{確率推定法と尺度$L$}

本稿の実験では，(\ref{eq:n})式の確率を推定するために，
\ref{sec:overview}節に述べたような$n$の値と確率推定法を用いたが，確率
推定の方法には，最尤推定やバックオフスムージングの他にも様々な方法があ
り，さらに，バックオフスムージングについても様々な discounting の方法
があるので，これらを適用した場合の尺度$L$の過分割検出精度について網羅
的に調べることを今後の課題としたい．

本稿でこのことを網羅的に調べなかった理由は，本稿での主要な目的は，
\ref{sec:introduction}章で述べたように，従来の研究で人手で発見されるこ
とが前提となっていた過分割を，尺度$L$を用いることにより半自動的に抽出
できることを示すことにあったので，そのことを示すためには，なんらかの
(代表的な)確率推定法を利用した場合について示すだけで十分であったからで
ある．すなわち，確率推定法の優劣(尺度$L$と併用したときの過分割検出精度
の良否)を調べることは副次的な事柄であったからである．

なお，予備実験として，京大コーパスを利用し，
\begin{itemize}
\item n=2，または，n=3
\item Witten-Bell discounting によるバックオフスムージング，または，最尤推定
\end{itemize}
から作られる四つの組合せのそれぞれについて(\ref{eq:n})式の確率を推定し，
尺度$L$による分割点調査率を調べた結果は，教師あり学習と教師なし学習の
双方について，$n=2$に最尤推定を組み合わせたものと$n=3$にバックオフスムー
ジングを組み合わせたものとがほぼ等しく良く(教師なし学習では前者が若干
良く，教師あり学習では後者が若干良い)，その他の組み合わせ($n=3$と最尤
推定および$n=2$とバックオフスムージング)は，この二つよりも劣っていた．
このような結果の原因としては，$n$や確率推定法の違いの他に，訓練データ
のサイズが約1万文と比較的少ないことが影響していると考えられる．

\subsubsection*{確率に基づく形態素解析システムへの適用}

確率に基づいた形態素解析システム(あるいは単語分割システム)には，文字の
連鎖確率に基づいたシステム\cite{yamamoto97,oda98}と単語や品詞の連鎖確
率に基づいたシステム\cite[など]{nagata94,itoh97,mori98}がある．

\cite{yamamoto97,oda98}では，本稿とは実現手法は異なるが，形態素境界の
情報を文字に取り込むことにより，文字列により形態素列を表現している．そ
して，入力文に対して，(形態素境界情報を含む)文字の連鎖確率が最大になる
ような解を求めることにより，最適な形態素列を得ている．一方，\cite[など]
{nagata94,itoh97,mori98}では，単語や品詞n-gramに基づいて形態素解析をし
ており，文字情報を直接用いているのは未知語モデルに限定されている．

これらの形態素解析システムの解析結果からも尺度$L$が過分割を検出できる
かを調べることは今後の課題であるが，\cite{yamamoto97,oda98}と\cite[な
ど] {nagata94,itoh97,mori98}を比べた場合，前者は，文字の連鎖確率を直接
用いて形態素列への分割を行なっている点が，尺度$L$と極めて類似している
ため，前者に尺度$L$を適用した場合の過分割検出精度は，(文字レベルでの分
割の最適化を行なっていない)後者に適用した場合と比較して劣ることが予想
される．しかし，\cite[など] {nagata94,itoh97,mori98}の品詞や単語の
n-gramに基づくシステムに尺度$L$を適用した場合についても，規則に基づく
形態素解析システムに比較すれば，最適な形態素列を求めるときに，可能な分
割を相互に比較し最高確率のものを出すという形で，尺度$L$に用いた情報が
既に用いられているとも言えるため，尺度$L$の有効性は低いと予想される．

\subsubsection*{分割不足の検出}

筆者は，予備実験として，実験1,2と同様の確率推定法で，JUMANにより解析さ
れた京大コーパスB全体を訓練およびテストコーパスとして，教師なし学習で
の確率推定値を用い，尺度$L$により分割不足の検出を試みた．つまり，尺度
$L$の値が小さい位置が形態素として結合されている場合について，それが実
際に分割不足かを確かめた．

その結果は，分割不足の再現率が10％の時点で，既に適合率が4％であり，実
験2における，再現率が10％のときの適合率が47％と比べて非常に劣っていた．

その理由の一つは，形態素解析システム中の形態素に比較的長い単位が多く，
かつ，分割不足として抽出されたものの多くが，その長い単位の形態素を短い
単位に分割しようとしているためである．たとえば，分割不足として抽出され
たものの上位には，'/'が候補位置とすると，「穴を/あけた」「目を/見張る」
「あっという/間」などがある．

このような場合と，明確に間違いである分割不足とを区別することは尺度$L$
には不可能なので，尺度$L$により分割不足を検出するのは，過分割の場合ほ
どには上手くいかない．

\section{おわりに}
\label{sec:conclusion}

本稿では，形態素解析結果から過分割を検出する統計的尺度を提案した．その
尺度は，文字列に関する尺度であり，文字列が分割される確率と分割されない
確率との比に基づいていて，分割されにくい文字列ほど大きな値となる．した
がって，この値が大きい文字列は過分割されている可能性が高い．

提案尺度を使うことにより，規則に基づいた形態素解析システムの解析結果か
ら高精度で過分割を検出できたし，人手で修正されたコーパスに残る過分割も
検出できた．また，提案尺度の過分割検出精度は，その他の統計的尺度と比べ
て高かった．これらのことは，提案尺度が，形態素解析システムの高精度化に
役立つこと，及び，コーパス作成・整備の補助ツールとして役立つことを示し
ている．

今後は，提案尺度を実際に使い，形態素解析システムの精度向上やコーパスの
整備に役立てたい．


\acknowledgment

本稿に対して有益なコメントを下さった筑波大学山本幹雄助教授，および，日
頃議論して下さる信州大学音声信号処理研究室の各位に感謝する．本稿では，
一般に公開されている，JUMAN，茶筌，すもも，京都大学テキストコーパス，
CMU-Cambridge Toolkitを利用させていただいた．このことに対して関係者の
方々に感謝する．

\appendix
\input{tab2-short_2.tex}

\bibliographystyle{jnlpbbl} 
\bibliography{v06n7_01}


\begin{biography}
\biotitle{略歴}
\bioauthor{内山 将夫}{
筑波大学第三学群情報学類卒業(1992).
筑波大学大学院工学研究科博士課程修了(1997).
信州大学工学部電気電子工学科助手(1997)．
郵政省通信総合研究所非常勤職員(1999).
博士(工学).
}

\bioreceived{受付}
\biorevised{再受付}
\biorerevised{再々受付}
\bioaccepted{採録}
\end{biography}

\end{document}
