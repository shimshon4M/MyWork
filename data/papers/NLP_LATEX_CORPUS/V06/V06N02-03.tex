



\documentstyle[epsf,jnlpbbl]{jnlp_j_b5}

\renewcommand{\topfraction}{}
\renewcommand{\bottomfraction}{}
\renewcommand{\textfraction}{}
\renewcommand{\dbltopfraction}{}

\newcommand{\lw}[1]{}


\setcounter{page}{41}
\setcounter{巻数}{6}
\setcounter{号数}{2}
\setcounter{年}{1999}
\setcounter{月}{1}
\受付{1998}{4}{3}
\採録{1998}{7}{27}

\setcounter{secnumdepth}{2}

\title{品詞および可変長形態素列の \\
複合Ｎ─ｇｒａｍを用いた日本語形態素解析}
\author{政瀧 浩和 \affiref{ATR} \and 匂坂 芳典\affiref{ATR}}

\headauthor{政瀧 浩和・匂坂 芳典}
\headtitle{品詞および可変長形態素列の複合Ｎ─ｇｒａｍを用いた日本語形態素解析}

\affilabel{ATR}{（株）エイ・ティ・アール 音声翻訳通信研究所，京都府}
{ATR Interpreting Telecommunications Research Laboratories}

\jabstract{
本論文では，
日本語連続音声認識用のN-gram言語モデルの学習に用いる形態素データを，
テキストデータから自動的に生成することを目的として，
品詞および可変長形態素列の複合N-gramを用い，
日本語テキストデータを自動的に形態素解析する手法を提案する．
複合N-gramは，品詞，形態素，形態素列を単位としたN-gramで，
少ないデータ量から高い予測精度を持つ言語モデルである．
また，品詞から未知語が出現する確率を定式化することにより，
未知語の形態素解析を行えるようにモデルの改良を行った．
形態素解析実験の結果，
複合N-gramの形態素同定率は最高99.17\%で，
従来のルールベースによる方法よりも正確に形態素の同定が行えることが判明し，
提案手法の有効性を確認した．
また，読みまで含めた評価を行った場合でも，
最高98.68\%の正解率が得られた．
未知語を含む文の形態素解析では，
全ての語いが辞書に登録されている場合と比較して
0.8\%程度の低下に抑えることができた．
}

\jkeywords{連続音声認識, 形態素解析，N-gram, 複合N-gram，未知語}

\etitle{Japanese Morphlogical Analysis using\\ 
Composite Part-of-speech and Morpheme\\
 Sequence N-gram}
\eauthor{Hirokazu Masataki \affiref{ATR} \and Yoshinori Sagisaka\affiref{ATR}} 

\eabstract{
In this paper, Japanese morphological analyzer is proposed
using composite part-of-speech(POS)
and morpheme sequence N-gram(Composite N-gram).
Composite N-gram is a N-gram type language model
whoes unit is POS class, morpheme and morpheme-sequence,
which can give an excellent prediction ability from small corpus.
In order to apply unknown words,
we improved the composite N-gram by considering the probability that
unknown word appears from POS class.
Experimental results showed that morpheme accuracy
using composite N-gram reached a maximum of 99.17\%,
which was better than using conventional rule based method.
Considering the pronounciation to the evaluation,
the accuracy was 98.68\%.
When applied to sentences including unknown words,
the fall of the morpheme accuracy was only about 0.8\%.
}

\ekeywords{Continuous speech recognition, N-gram, Composite N-gram, Unknown word}

\begin{document}
\maketitle

\clearpage


\section{はじめに}
近年，連続音声認識において，
N-gram言語モデルによる言語制約を用いた手法が幅広く用いられている．
N-gramは，大規模なテキストデータを統計的に解析し，
直前の{\it N-1} 個の単語から次の単語への遷移を確率的に与える
非常に単純な言語モデルである．
しかし，その構築・実装の容易さ，統計的音響モデルとの相性の良さ，
認識率向上や計算時間の短縮の効果が大きい等の理由から，
連続音声認識にはでは盛んに用いられている\cite{Bahl}\cite{Woodland}．

N-gramは当初，英語の連続音声認識に対して適用され，その有効性が示された．
英語の文章は，単語がスペースで区切られており，
テキストデータから単語を単位としたN-gramが容易に構築できる．
しかし，日本語の文章は文字が連続しており単語の境界が明らかではなく，
テキストデータのみでは単語N-gramを構築することはできない．

このため，
我々は日本語の連続音声認識の認識単位として形態素を用いているが，
その有効性について２章で明らかにしている．
形態素を単位としたN-gramを構築する場合，
テキストデータに形態素を付与する，いわゆる形態素解析を行う必要がある．
しかし，N-gramを構築するのに必要な，
大量のテキストデータを全て人手で形態素解析を行うには
多大な労力と時間が必要であり，
また，かなりの経験がある人が作業を行わなければ，
付与された形態素の揺れも大きくなると考えられる．
従って，大量のデータをより正確に形態素解析を行うためには，
自動的に形態素解析する手法が望ましい．
自動形態素解析は，
従来人手で作成したルールにより解析を行う方法が主流であったが，
ルールの作成の作業は相当の知識・経験が必要であり，
また，話し言葉等のより自然な文を全てカバーできかつ
矛盾のないルールを作成するのは困難であると考えられる．
これに対し，
本論文ではN-gram統計に基づく形態素解析手法を考える．
N-gramを構築するためには，
事前に形態素体系の構築や定義を行う必要はあるが，
従来の形態素解析で必要であった
形態素間の接続ルールの作成・重みの変更等の作業に代わり，
ある程度の量の形態素データを収集するという
比較的単純な作業で構築できる利点がある．
また，より自然な発話文に対しても，
データさえ収集できれば容易に適用可能である．
３章では，N-gramを用いた形態素解析の原理を説明する．

統計的モデルにより形態素解析を行うためには，
通常は統計モデルの学習用として形態素解析済みの言語コーパスが整備されていることが前提となる．
このため，山本らは\cite{Yamamoto}辞書と接続コストのみを用いて
文コーパスから形態素ネットワークを生成し，
生成された形態素ネットワークから隠れマルコフモデルを学習し形態素解析を行うことにより，
形態素解析された言語コーパスが存在しない場合でも形態素解析が可能な手法を提案している．
しかしこの方法では，形態素解析にかかるコストは非常に小さいという長所はあるが，
形態素解析の結果はモデル化能力の低いとされる品詞Bigramと大きくは変わらず，
形態素解析の正解率の適合率が93.5\%程度と報告されており，
正しい形態素データを学習しない方法には精度に限度があると考えられる．
形態素解析の精度は連続音声認識の精度にも大きく影響すると考えられるため，
我々は高い精度でかつできるだけコストを抑えた形態素解析の手法を考える．
このため，本論文では，
形態素解析のためのN-gram言語モデルとして，
より少ない量の形態素解析された言語コーパスから精度の高い予測精度の言語モデルを得るため，
品詞と可変長形態素列の複合N-gram\cite{Masataki}を用いることを提案する．
複合N-gramは，基本的には品詞を単位としたN-gramであるが，
言語モデルとしての精度を高めるため，
特定の形態素は品詞クラスから分離させ独立して扱い，
さらに特定の形態素列を結合させて新たな単位として扱うモデルである．
このため，品詞という単位では表現できない形態素独自の特徴を表現でき，
かつ長い範囲の形態素間の連接関係を効率良く表現することができるモデルである．
４章では，品詞と可変長形態素列の複合N-gramについて解説する．

通常連続音声認識では，
辞書に登録されている語いを対象とした認識が行われている．
しかし形態素解析では，
大量のテキストデータをまとめて処理するため，
辞書に登録されていない未知語が含まれている場合も多く存在する．
このため，
形態素解析においては，
未知語を含む文に対しても正確に処理が行えることが重要であると考える．
本論文では，
品詞から未知語が出現確率する確率を考えることにより，
未知語の形態素解析も行えるよう，５章で定式化を行った．
本論文で使用した複合N-gramは，
品詞を基本単位としたN-gramであるため，
このような未知語処理が容易である．

第６章では，形態素解析実験により，
形態素N-gramや品詞N-gramに対する複合N-gramの有効性を示し，
最後の７章で本論文の結論を述べる．



\section{日本語連続音声認識のための形態素解析}
  英語等の言語では，単語を認識単位とした連続音声認識システムが構成されている．
  しかし，日本語の文は連続した文字列から構成されているため，
  単語の定義が明らかでなく，適切な認識単位が定かではない．
  連続音声認識の適切な単位として，以下の条件を満たすが重要であると考えられる．
\begin{itemize}
\item 認識単位から読みの対応が明確であること \\
  日本語の場合，漢字の読みや，
  助詞の「は」「へ」など，その文字だけでは読みが決定できない場合が多い．
  できる限りヴァリエーションの少ない読みが決定できる認識単位を選択することが，
  連続音声認識の探索空間の削減のために有効である．
\item ポーズ（無音区間）の挿入位置が限定できること \\
  連続音声といっても，発声文にはポーズが挿入される．
  連続音声認識では，認識単位間にはポーズの挿入が可能であるとして認識を行うため，
  連続音声認識の探索空間の削減および認識率の向上のためには，
  認識単位をできるだけ長くしてポーズが挿入される個所を少なくし，
  かつ認識単位中にはポーズは挿入されないように決定するのが好ましい．
\item 言語理解システムとの整合性が良いこと \\
  音声認識のアプリケーションとして，
  機械翻訳等の言語理解と結合した音声理解システムが考えられる．
  言語理解には形態素解析・構文解析等の処理が必要である．
  このため，認識誤りにより認識結果の解析が不能に陥ることが少ない単位が望まれる．
\end{itemize}
以上の条件をよく満たす単位として，形態素が挙げられる．
しかし，形態素を単位としてN-gramを構築する場合，
テキストデータから形態素を切り出す必要があり，
この作業をすべて手作業で行うにはかなりのコストが必要となり，
N-gram構築上の大きな問題となる．
このため，形態素切り出し作業を省くことを目的として，
文字あるいは文字列を単位とした手法が提案されているが
\cite{Yamada1}\cite{Yamada2}\cite{Itoh}，
上記３条件には適合せず，連続音声認識にとって好ましい単位とは言えない．
従って，テキストデータから大量の形態素データを得るためには，
自動で形態素解析を行う必要があると考えられる．

\vspace{3mm}

また，音声認識の対象となる文は基本的に話し言葉であり，
文章の読み上げなどの場合を除いては，
通常の発話において書き言葉を喋ることはまずありえない．
このため，連続音声認識用の言語モデル構築のためには，
話し言葉のテキストデータに対して形態素解析が可能でなければならない．
現在形態素解析の手法としては，
形態素間の接続ルールや重み付けを人手で作成する
ルールベースの手法が広く用いられているが，
接続ルールは通常書き言葉を対象に作成されており，
音声認識で必要な話し言葉に対して，十分な性能が得られない可能性がある．
このため，本論文では，
統計的な言語モデルを用いた形態素解析の手法を採用した．
統計的モデルは，データから自動的に構築できるため，
接続ルールや重み付けを手作業で行うことと比較して，
話し言葉に対しても適用が容易であると考えられる．
また，統計的言語モデルは，
連続音声認識の言語モデルとしても盛んに使用されているため，
形態素解析にも統計的モデルを用いることにより，
認識用言語モデルに適した形態素解析結果が得られると考えられる．



\section{N-gram言語モデルによる形態素解析}
日本語の形態素解析は，
文の文字列$L$から，
それに対応する形態素列$W_L$を獲得することである．
統計的手法では，
$L$に対して最も高い確率を与える形態素列$W_L$\mbox{を探索する
ことにより}形態素解析を実現する．
これは，以下の式で与えられる．
\begin{equation}
W_L ~=~ \arg \max_{W} P(W|L)
\end{equation}
ベイズ則により本式は下式のように変形される．
\begin{equation}
W_L ~=~ \arg \max_W \frac{P(L|W)P(W)}{P(L)}
\label{eqn:Bayes1}
\end{equation}
本式において，$P(L)$は右式の最大値を与えるためには無関係な量である．
従って，式\ref{eqn:Bayes1}は下式と等価となる．
\begin{equation}
W_L ~=~ \arg \max_W P(L|W)P(W)
\end{equation}
右辺の確率$P(L|W)$は形態素から文字列を与える確率であるが，
これは，形態素の表記と文字列が一致する場合は必ず$1$であり，
一致しない場合は$0$である．
また，確率$P(W)$は，形態素列$W$の生成確率である．
従って，統計的手法による形態素解析は，
与えられた文字列と一致する全ての形態素列の中から，
生成確率が最も高くなる形態素列を探索することによって実現できる．

形態素列$W$を$w_1, w_2 , \ldots w_m$とすると，
その生成確率$P(W)$は次のように表される．
\begin{equation}
P(W) ~=~ \prod_{t=1}^m P(w_t|w_1^{t-1})
\end{equation}
本式において，$w_x^y$は$x$番目$y$から番目までの形態素列を表す．
\mbox{右辺の確率を直接求めるのは困}難であるから，
各形態素は，直前のN-1形態素から確率的に予測できると近似する．
これが，形態素N-gramである．
N-gramを用いると，上式の形態素列$W$の遷移確率は次のように近似される．
\begin{equation}
P(W) ~=~ \prod_{i=1}^m P(w_i|w_{i-N+1}^{i-1})
\end{equation}
N-gramは，Nが大きくなるほど，パラメータ数が飛躍的に増大するため，
通常は直前の形態素から次の形態素を予測するBigram(2-gram)，
および直前の2形態素から次の形態素を予測するTrigram(3-gram)
程度がよく使用される．



\section{品詞と可変長形態素列の複合N-gram}

\subsection{品詞N-gram・可変長形態素列N-gram}
形態素N-gramのパラメータ数，
すなわち形態素遷移の組合せは$V^N$（$V$は語い）であり，
{\it N}を大きくするとパラメータ数が格段に多くなるため，
それぞれの値の推定が困難になる．
例えば，語いが10,000語の時，
Trigramのパラメータ数は$10,000^3 = 10^{12} (=1兆)$となり，
それぞれのパラメータを推定するためには，
数兆語からなるテキストデータが必要となるが，
これほどの大規模のデータを収集することは事実上不可能に近い．
実際には，平滑化\cite{Jelinek}\cite{Katz}の手法を用いて
データ上に出現しない形態素遷移に対しても，確率を与えることができるが，
その結果，実際には有り得ない形態素の遷移に対しても
比較的高い確率を与える可能性がある．

この問題を解決するため，
品詞を単位としたN-gramが使用される場合がある\cite{Nagata}．
これは，形態素の代りに品詞間の遷移を考えることによりパラメータ数を削減し，
推定量の信頼性を高めるものである．
$m$形態素からなる文の生成確率は一般に下式で表される．
\begin{equation}
 P(w_1^m) = \prod_{t=1}^m P(w_t | c_t) \cdot P(c_t | c_{t-N+1}^{t-1})
\end{equation}
（$c_t$は$w_t$の属する品詞を，
$c_x^y$は$x$番目から$y$番目の形態素列に対応する品詞列を表す）

上式で，$P(c_t|c_{t-N+1}^{t-1})$は
直前の {\it N-1} 形態素列に対応する品詞列から
次の形態素の属する品詞への遷移確率を表し，
$P(w_t | c_t)$は，
次品詞から次形態素が出現する確率を表す．
品詞数を100とした時，Trigramの全ての品詞間の遷移の組は
$ 100^3=10^6 (=100万)$であるから，
形態素N-gramに比べてパラメータ数は極めて少なく，
比較的信頼できる遷移確率が求めることができる．
しかし，品詞単位でのモデルでは，
それぞれの形態素特有の接続関係を表現することができないため，
言語モデルとしての性能は劣ると考えられる．

また，N-gramの単位を結合させ部分的に長くする手法も提案されている\cite{Giachin}．
日本語の場合は特定の形態素列を結合させてN-gramの単位として扱い，
固定長のN-gramと比較して，局所的にNを大きくさせる効果があり，
パラメータ数の増大を抑えながら，
より長い範囲の形態素間の関係を表現するものである．
$m$形態素からなる文の生成確率は一般に下式で表される．
\begin{equation}
 P(w_1^m) = \prod_{t=1}^{\hat{m}} P(ws_t | ws_{t-N+1}^{t-1})
\end{equation}
但し，$ws_t$ は文章の $t$ 番目の形態素列（単独の形態素も含める）を意味する．
また，$\hat{m}$ は形態素列に分割した際の形態素列の個数を表し，
$\hat{m} \leq m$ である．
可変長形態素列N-gramは，
長い範囲の形態素間の連接関係を表現するのには有効であるが，
パラメータ数が同じNの形態素N-gramより多くなり，
少量の学習データから，信頼性の高いパラメータ推定を行うのは困難である．





\subsection{品詞と可変長形態素列の複合N-gram}
本論文では，形態素解析のための言語モデルとして，
品詞と可変長形態素列の複合N-gramを使用することを提案する．
品詞と可変長形態素列の複合N-gramは，
品詞N-gramを基本としたN-gramであるが，
品詞全体の性質とは異なった性質を呈する特殊な形態素は
その品詞から分離させ独立して扱う．
さらに，結合させることにより，言語モデルの精度を向上させる特定の形態素列は
結合させた単位として扱う．
従って，品詞と可変長形態素列の複合N-gramは，
前節で示した品詞N-gramと可変長形態素列N-gramとのそれぞれの長所を生かしながら，
それぞれの短所を補い合うことにより，
少ないパラメータで高い予測精度が得ることを可能とした言語モデルである．
N=2の場合，すなわちBigramを例にして，
形態素N-gram，品詞N-gram，可変長形態素列N-gram，
複合N-gramとの比較を図\ref{fig:probability comparison}に示す．

複合N-gramは，品詞クラス，形態素，
形態素列を同時に扱うため複雑なモデルとなるが，
本論文では，表現を簡単にするため，
複合N-gramを次の３種類のクラス間のN-gramとして表現する．
\begin{description}
\item[A)] 品詞クラス
\item[B)] 独立した１形態素のみで構成されるクラス
\item[C)] １形態素列で構成されるクラス
\end{description}

\def\probability_FIG
{}
\probability_FIG


このクラス分類を用いると，複合N-gramによる文の生成確率は，
下式のクラスN-gramの形で与えることができる．
\begin{equation}
P(w_1^m) = \prod_{t=1}^{\hat{m}} P(ws_t | c_t) \cdot P(c_t | c_{t-N+1}^{t-1})
\end{equation}
但し，\( ws_t \) は文章を上記のクラス分類を用いた場合の，
 t 番目の形態素列（単独の形態素も含める）を意味する．
また，$\hat{m}$ は文章の形態素列の個数を表し，$ \hat{m} \leq m $である．
例として，次の文章（５形態素）を考える．
\begin{center}
「わたくし - 橋本 - と - 言い - ます」
\end{center}
「橋本」は出現頻度が高くないため．
固有名詞クラスとして扱う方が適切であると考えられる．
「わたくし」および「と」は日本語の文章で頻繁に出現する形態素であるため．
品詞クラスより分離して単独で扱う．
また，「言い - ます」は日本語で頻繁に用いられるフレーズであるため．
結合させて一単位として扱う方が効果的であると考えられる．
従って，この文章の生成確率は，次の式で与えられる．

\begin{eqnarray*}
 &P(w_1^m)& = P(わたくし|\{わたくし\}) \cdot P(\{わたくし\}) \nonumber \\
 &\cdot& P(橋本|\langle 固有名詞 \rangle) \cdot P(\langle 固有名詞 \rangle|\{わたくし\}) \nonumber \\
 &\cdot& P(と|\{と\}) \cdot P(\{と\}|\langle 固有名詞 \rangle) \nonumber \\
 &\cdot& P(言います|[言います]) \cdot P([言います]|\{と\})
\end{eqnarray*}

但し，\(\langle \rangle \{ \}[ ]\)はそれぞれ，
クラス A) B) C) に属していることを表す．
B)およびC)のクラスは，形態素（列）とクラスの出現頻度は等しいため
(\(P(w_t)=P(c_t)\))，
上式は次のように変形することができ，複合N-gramと等価であることがわかる．

\begin{eqnarray*}
 &P(w_1^m)& = P(わたくし) \\
 & \cdot & P(橋本|\langle 固有名詞 \rangle) \cdot P(\langle 固有名詞 \rangle|わたくし) \\
 & \cdot & P(と|\langle 固有名詞 \rangle) \\
 & \cdot & P(言います|と)
\end{eqnarray*}



\subsection{複合N-gramの生成方法}
より少ないパラメータで次形態素予測精度の高い効率的な複合N-gramを得るためには，
初期クラスから独立させる形態素，
および結合させる形態素列を適切に選択する必要がある．
本論文では，品詞クラスを初期クラスとし，
初期クラスからの形態素独立によるクラス分離，
および形態素列結合によるクラス分離
の２種類のクラス分離を逐次的に行うことによって，
複合N-gramのためのクラス分類を決定する方法を提案する．
形態素独立，および形態素列結合候補の決定は，
式\ref{eqn:entropy}により求められるエントロピーを
最小にさせる候補を１つのみ選択する．

\begin{eqnarray}
\label{eqn:entropy}
&H&(\{c_i\}) =-\sum_i P(c_i) \nonumber \\
& & \sum_k P(ws_k | c_j) \cdot P(c_j | c_i) \log_2 \{ P(ws_k | c_j) \cdot P(c_j | c_i) \} \nonumber \\
& &  where ~ws_k \in c_j
\end{eqnarray}

エントロピーはあいまいさを表す尺度であり，
また，エントロピーを$H$としたときパープレキシティは$2^H$で与えられる．
すなわち，エントロピーが小さいことはあいまいさが小さく，
また，次形態素予測の分岐も少なく，言語モデルの精度が高いことを意味する．
従って，
クラス分離を行う際に，常にエントロピーを最小にする候補を選択する本手法は，
より少ないパラメータで精度の高い複合N-gramを生成するために
適した手法であると考えられる．
なお，本手法において，エントロピーの減少は常に正になることが保証されており，
クラス分離によって，学習データに関してエントロピーは単調に減少する．

\begin{figure}[tb]
  \setlength{\unitlength}{1mm}
  \footnotesize
  \begin{center}
  \begin{picture}(60,65)(0,0)
    \put(10,55){\framebox(40,7){形態素→品詞の分類}}
    \put(10,45){\framebox(40,7){\shortstack{分離クラス候補の \\ リストアップ}}}
    \put(10,35){\framebox(40,7){\shortstack{各候補に対する \\ エントロピー減少量の算出}}}
    \put(10,25){\framebox(40,7){\shortstack{エントロピー減少量最大候補の \\ クラス分離実行}}}
    \put(10,17){\line( 4, 1){20}}
    \put(10,17){\line( 4,-1){20}}
    \put(50,17){\line(-4, 1){20}}
    \put(50,17){\line(-4,-1){20}}
    \put(12,13){\makebox(40,7){所定の分離クラス数?}}
    \put(50,10){\makebox(10,7){{\bf No}}}
    \put(30, 7){\makebox(10,7){{\bf Yes}}}
    \put(10, 2){\makebox(40,7){終了}}
    \multiput(30,55)(0,-10){4}{\vector(0,-1){3}}
    \put(30, 12){\vector(0,-1){3}}
    \put(50,17){\line(1,0){5}}
    \put(55,17){\line(0,1){36.5}}
    \put(55,53.5){\vector(-1,0){25}}
  \end{picture}
  \end{center}
  \normalsize
  \caption{複合N-gramの生成アルゴリズム}
  \label{fig:generation algorithm}
\end{figure}

\section{未知語を含んだ文の形態素解析}

本論文では，未知語の形態素解析を行うために，
品詞クラス$c_{\xi}$に対して，
同一品詞の未知語のためのクラス$\hat{c_{\xi}}$を導入する．
クラス$\hat{c_{\xi}}$は，任意の文字を１次を出力するクラスであり，
同一\break
未知語クラス$\hat{c_{\xi}}$が連続した場合は，
それらをまとめて一つの未知語とみなす．
図\ref{fig:UnknownWord}に，
\mbox{「政瀧」と}\break
いう未知語を含んだ文の
品詞Bigramを使用した形態素解析の処理例を示す．
以下に，$\hat{c_{\xi}}$に関
する確率の導出を行う．

\def\unknown_FIG
{}
\unknown_FIG

Turing推定によると，
データ上に$r$回出現する形態素は，
次式の$r^*$回と推定される．
\begin{equation}
r^* ~=~ (r+1) \frac{n_{r+1}}{n_r}
\end{equation}
ただし，$n_r$はデータ上に$r$回出現した形態素の種類数を表す．
\mbox{従って，$r$回出現する形態素$w$の}品詞からの出現確率$P(w|c_{\xi})$は，
\begin{equation}
P(w|c_{\xi}) ~=~ \frac{r^*}{N(c_{\xi})}
\end{equation}
となる．
これを，クラス$c_{\xi}$に属する全ての形態素について計算し，
$1$から引いた残りが品詞$c_{\xi}$から
未知語出現する確率$P(\hat{c_{\xi}})$である．
\begin{equation}
P(\hat{c_{\xi}}) ~=~ 1 - \sum_{w \in c_{\xi}} P(w|c_{\xi})
\end{equation}
品詞$c_{\xi}$の未知語の文字$l$の出現する確率$P(l|\hat{c_{\xi}})$は，
本来文字毎に与えられるべきであるが，
固有名詞等では，学習データにも出現しない文字が出現する可能性もあり
文字毎に正確な確率を与えるのは困難であるため，
全ての文字が等しい確率で出現するとし，
未知語出現確率$P(\hat{c_{\xi}})$から均等に割り当てる．
\begin{equation}
P(l|\hat{c_{\xi}}) ~=~ \frac{P(\hat{c_{\xi}})}{V}
\end{equation}
ただし，$V$は文字の種類数とする．

また，$P(\hat{c_{\xi}}|\hat{c_{\xi}})$は，
未知語が連続する確率である．
本モデルにより生成される未知語の長さは二項分布に従うため，
実際の未知語の長さも二項分布に従うという仮定を設けると，
その品詞に属する語$w$の文字列$len(w)$より
$P(\hat{c_{\xi}}|\hat{c_{\xi}})$は下式により求められる．
\begin{equation}
P(\hat{c_{\xi}}|\hat{c_{\xi}}) ~=~ \sum_{w(\in c_{\xi})} \frac{len(w)-1}{len(w)}
\end{equation}



\section{評価実験}


\subsection{各種N-gramモデルの形態素解析性能評価}
自然発話旅行会話データベース\cite{Morimoto}を用いて
形態素解析の評価実験を行った．
本データベースには，
間投詞や感動詞のほか，
ら抜き表現，助詞落ち等の自然発話特有の言語現象が頻出する．
また，本データベースは旅行会話という限定された内容の言語データであるが，
実際のアプリケーションを想定した連続音声認識システムとしては，
情報案内システム\cite{Tsutsumi}・予約システム\cite{Kawahara}等
用途を限定し音声認識の精度を向上させている例が多く，
このような内容の限定されたデータで評価を行うことは適切であると考えられる．
データベースは，1,334対話，44,091文，559,711形態素から成り，
語いは7,724語である．
このうち，約４分の１(334対話, 11,321 文，137,691 形態素)を評価用データとし，
残り(1, 000対話，32,770 文，402,020 形態素)を言語モデル学習に使用した．

形態素解析精度の比較対象として，
複合N-gramと形態素N-gram，および品詞N-gramを構築した．
複合N-gramは，活用形，および活用型を含めた234品詞を初期状態とし，
最大2,000クラスまで分離を行い，500分離おきにデータを採取した．
また，形態素N-gram，品詞N-gram，複合N-gramともに，
形態素および品詞クラスの遷移確率をback-off Smoothing \cite{Katz}
により学習データに出現しない形態素および品詞クラス遷移に対して
$0$でない確率を与えた．
また，本節の実験では，辞書には学習データ，評価データに出現する
全ての形態素が登録されており，未知語は存在しない．
ただし，学習データに出現しない形態素に対する遷移確率は，
全てのモデルにおいて 1/(100*語数) という確率を与えた．
これは，他に候補が無い場合はこの形態素を割り当てるために，
０でない小さい値を与えることを目的としている．

形態素の正解率の評価には，
音声認識で広く用いられている単語正解率(Accuracy)にならい
形態素Accuracyを用いた．
形態素Accuracy(\%)は下式で表される．
\begin{equation}
100 \times \frac{W-S-D-I}{W}
\end{equation}
ただし，W:正解の形態素数，S:置換誤り形態素数，
D:削除誤り形態素数，I:挿入誤り形態素数を表す．

\begin{table}[tb]
\begin{center}
  \caption{各種言語モデルの形態素解析性能比較(品詞のみの評価)}
  \label{tbl:ModelComparison1}
  \begin{tabular}{|l||c|c|c|c|c|c|}
    \hline
    \lw{ } & \lw{形態素N-gram} & \lw{品詞N-gram} & \multicolumn{4}{c|}{複合N-gram（分離クラス数）} \\
   \cline{4-7}
           &  &  & ~~500~~ & ~~1000~~ & ~1500~ & 2000 \\ \hline\hline
   Bigram  & 98.90 & 98.56 & \underline{99.13} & 99.07 & 99.02 & 99.01 \\ \hline
   Trigram & 98.95 & 98.94 & \underline{99.17} & 99.08 & 99.01 & 99.03 \\ \hline
  \end{tabular}
\end{center}
\end{table}
通常，形態素解析では，形態素の分割が正しく，
かつ付与された品詞が正しければ，正解とみなされれる．
この場合の形態素Accuracy(\%)を表\ref{tbl:ModelComparison1}に示す．
また，形態素解析結果を音声認識に用いることを考えると，
同一品詞の形態素でも読みが異なるものは，
別単位として扱うことが好ましい．
形態素に読みまで考慮した場合のた場合の形態素Accuracy(\%)を
表\ref{tbl:ModelComparison2}に示す．
ただし，読みの推定は，
表記が同一の形態素でも読みが異なるものは別の形態素として扱い，
異なる単位としてN-gramを構築し形態素解析を行うことにより実現している．

\begin{table}[tb]
\begin{center}
  \caption{各種言語モデルの形態素解析性能比較(品詞と読みを含めた評価)}
  \label{tbl:ModelComparison2}
  \begin{tabular}{|l||c|c|c|c|c|c|}
    \hline
    \lw{ } & \lw{形態素N-gram} & \lw{品詞N-gram} & \multicolumn{4}{c|}{複合N-gram（分離クラス数）} \\
   \cline{4-7}
           &  &  & ~~500~~ & ~~1000~~ & ~~1500~~ & 2000 \\ \hline\hline
   Bigram  & 98.54 & 96.78 & 98.62 & \underline{98.64} & 98.62 & 98.63 \\ \hline
   Trigram & 98.64 & 97.12 & \underline{98.68} & \underline{98.68} & 98.61 & 98.66 \\ \hline
  \end{tabular}
\end{center}
\end{table}

表中で下線を付した値が，その次数の複合N-gramの最高の形態素正解率を示す．
表\ref{tbl:ModelComparison1}および\ref{tbl:ModelComparison2}より，
いずれの評価の場合も，
複合N-gramの最も高い形態素正解率は，
同次数の形態素N-gramおよび品詞N-gramよりも高い正解率を得ることができ，
複合N-gramの他のモデルに対する優位性が実験的に示された．
形態素正解率最高の値を与える分離クラス数は，
品詞のみの評価の場合は分離クラス数500，
読みも含めた評価の場合は分離クラス1000であり，
それ以上増やしても逆に形態素正解率は低下する傾向にある．
これは，クラス数が増加すると共に，パラメータ数も増加するため，
各パラメータの確率推定が正しく行われないことに起因すると考えられる．
このため，適切なクラス数を決定する必要があるが，
これは，ニューラルネットワークの学習回数の決定等で用いられる
Cross Validationの手法を用いることにより，
適切なクラス数を実験的に求めることができる．
以下に手順を示す．
\begin{enumerate}
\item 学習データの一部を仮想的なテストデータとする
\item クラス数を徐々に増加させながらN-gramを学習する
\item 仮想的なテストデータに対し形態素解析を行い，
      形態素解析の性能が頭打ちになる所をクラス数とする
\end{enumerate}


３種類のモデルを比較すると，
品詞N-gramは読みを含めた評価の場合に他のモデルと比較して
形態素正解率が著しく低下している．
これは，ある形態素の読みはその前後の形態素の読みに影響されると考えられるが，
品詞という枠組みでは，前後の読みの関係が表現できないためと考えられる．
形態素N-gramと複合N-gramでは，読みまで含めた形態素を単位として扱うことが
できるため，このような大きな低下は見られない．
また，複合N-gramと形態素N-gramとの正解率の差は大きくはないが，
５章で示した未知語処理の容易さとを考えると，
複合N-gramが有利である．

また，山本らの手法\cite{Yamamoto}では，
タグなしコーパスから形態素ネットワークを生成する際に，
ノイズを調整するための信頼性係数なるパラメータを変化させると，
隠れマルコフモデルと品詞BigramおよびTrigramの形態素解析の精度は同等であると報告されている．
しかし本実験の結果では，品詞Bigram，品詞Trigram，形態素Bigram，形態素Trigramの順に精度が向上しており，
正解形態素列を学習させることにより，モデル化能力が形態素解析の正解率に反映されていると考えられる．
また，複合N-gramは，複合Bigramでさえ
隠れマルコフモデルよりもモデル化能力が高いとされる品詞Trigram
よりも形態素の精度が高くなっている．
従って，山本らの手法と比較し，
正解形態素列を与えること，および複合N-gramを使用することにより，
精度の高い形態素解析が可能であることが示せた．

\subsection{学習データ量と形態素解析率との関係}
前節の実験で，約40万語のデータより構築した複合N-gramモデルは，
読みまで考慮した形態素解析率が98\%以上の，高い解析率が得られることが分かった．
しかし，40万語の形態素データを集めることはそれほど容易ではなく，
連続音声認識に使用するN-gramを学習するための，
大量の形態素データを容易に集めるという本研究の目的と矛盾する．
従って，データ量が少ない時にどの程度の形態素解析率が得られるかは，
本論文の趣旨において重要なことである．
これを調査するため，
前節の実験で用いたデータを量を1/2,1/4から最小1/64とした時の
形態素正解率を調べた．
言語モデルには，複合Bigramの分離クラス数500と1000を用い，
形態素正解率は読みも含めた場合の形態素Accuracyで評価した．
実験結果を\ref{tbl:DataAmount}に示す．

\begin{table}[tb]
\begin{center}
  \caption{複合N-gramの学習データ量と形態素解析性能の関係}
  \label{tbl:DataAmount}
  \begin{tabular}{|l||c|c|c|c|c|c|c|}
    \hline
     &  \multicolumn{7}{c|}{データ量} \\ \hline
 全学習データに対する割合 &  1/64 &  1/32 &  1/16 &  1/8  & 1/4 & 1/2 & 1/1 \\ \hline
    学習形態素数 &  6,306& 14,293& 25,931& 50,794&101,227&200,105&402,020 \\ \hline \hline
複合Bigram(500)  & 94.45 & 95.87 & 96.97 & 97.53 & 98.02 & 98.45 & 98.62 \\ \hline
複合Bigram(1000) & 94.27 & 95.66 & 96.85 & 97.50 & 97.98 & 98.41 & 98.64 \\ \hline
  \end{tabular}
\end{center}
\end{table}

表\ref{tbl:DataAmount}より，データ量が減少するに比例して，
形態素正解率は低下することが分かる．
しかし，データ量が全体の1/64の場合は，形態素数がわずか6,306であるが，
このような非常に少ない量の学習データから構築したモデルでも，
94\%程度の比較的高い正解率が得られる．
94\%の形態素正解率は自動で形態素解析を行うには高い精度とは言えないが，
自動形態素解析の結果を見て，人手で誤り個所を修正するような，
半自動の形態素解析としては，使用に耐える性能であると考える．

全学習データを使用した場合は，複合Bigramの分離クラス数1000の場合が
分離クラス数500の場合よりも正解率が高いが，
データ量が減少するにつれて，正解率は逆転している．
これは，パラメータ数の多い分離クラス1000のモデルでは，
データ量が少ない場合では，正確なパラメータ値を推定することが困難に
なることが原因であると考えられる．

以上より，大量の形態素データを得るためには，
まず，数千形態素程度のデータを人手で作成し，
クラス数の少ない複合N-gramを構築して半自動の形態素解析を行い，
数十万形態素程度のデータが集まった段階で，
クラス数の大きい複合N-gramを構築し，
その後は自動で形態素解析を行う，
というのが効果的な手段であると考えられる．
以下に，この作業に必要なコストについて検討した．
まず，最初のN-gram学習用の形態素データを作成する必要があるが，
これは，１形態素のデータ作成に１分あれば十分であるとして，
6,000形態素のデータ作成にかかる時間は，
１分×6,000＝6000分＝100時間である．
これを基にして作成した複合N-gramで95\%程度の正解率が得られるため，
形態素解析したデータの修正には，
1形態素あたりでは形態素データ作成時の1/20の3秒程度で可能であると考えられる．
40万形態素のデータを作成するためには，
40万×3秒＝120万秒＝2万分＝約333時間となる．
一日８時間労働としても，２ヶ月程度で
正解率98\%以上の形態素解析システムの構築が可能であることになる．
また，修正を行うだけなら比較的単純な作業であり，
多数の人間で平行して行うことができるため，
さらにシステム構築の期間を短縮することが可能である．
なお，以上の議論では文章データ収集のためのコストを無視している．
しかし，英語・日本語に限らず音声認識システムを構築するためには
文章データを収集することは必須の作業であり，
この部分のコストに関してし議論するのは無意味である．
このため形態素解析の作業量のみを議論した．



\subsection{ルールベースの形態素解析との比較}

形態素解析システムJUMAN \cite{Kurohashi} (Version 3.5)との比較により，従
来のルールベースの形態素解析に対する有効性を示す．ただし，我々の形態素解
析とJUMANとでは，用いている形態素の体系や辞書に登録されている形態素の語
数等が異なるため，できるだけ公平になるよう次のような方法で比較を行った．

\begin{itemize}
\item 辞書サイズの均等化 \\
辞書サイズが，本論文の実験では約7千語であるのに対しJUMANでは約58万語あり，
さらに，本論文の実験では評価データの全ての形態素が登録されている等，
条件はJUMANが圧倒的に不利である．このため，名詞，動詞，形容詞等の
自立語の語彙を我々のシステムと同一にした．
ただし，「えー」「あのー」等の語は我々のシステムでは間投詞としているが，
JUMANには間投詞という品詞は存在しないため感動詞とした．
\item 評価方法 \\
我々のシステムとJUMANとでは形態素の体系が異なり，
評価データに対してJUMANの形態素体系の正解は存在しない．
このため，提案方法よびJUMAN共に，形態素解析結果を目視して正誤の判定を行った．
ただし，形態素の切り分けや品詞の判断は専門家でも困難な部分もあるため，
明らかに誤りであると判断できる個所のみを誤りと判断している．
また，評価データ約1万文を，目視により全て検査するのは時間を要するため，
最初の200文のみを評価の対象とした．
\end{itemize}
6.1節の実験で，最も正解率の高かった複合Trigram（クラス数1000）とJUMANに
関し，形態素数と形態素解析の品詞付与および読み付与正解率とで比較した結果
を表\ref{tbl:RuleComparison}に示す．

表より，形態素数はほぼ同じであり，両システムの形態素体系はは同程度の長さ
であることがわかる．形態素解析の精度に関しては，品詞付与で約4\%，読み付
与で約5\%と本論文の提案手法の方が優れている．JUMANの誤り個所を調べると，
大部分は感動詞と，数字の読みに関する誤りである．以下に代表例を示す．
\begin{itemize}
\item 「たぶんえー大丈夫だと思います」 \\
 →「たぶん(副詞) え(動詞) ー(記号) 大丈夫だ(形容詞) と(助詞) 思い(動詞) ます(接尾辞)」
\item 「九月十一日ご一泊」
 →「きゅう つき じゅう いち にち ご いち はく」
\end{itemize}
\begin{table}[h]
\begin{center}
  \caption{ルールベースの形態素解析との比較}
  \label{tbl:RuleComparison}
  \begin{tabular}{|l||c|c|c|}
    \hline
      & ~~形態素数~~ & 品詞付与正解率(\%) & 読み付与正解率(\%) \\ \hline \hline
  JUMAN   &   2,158  &       95.83        &      94.21 \\ \hline
  提案法  &   2,129  &       99.91        &      99.91 \\ \hline
  \end{tabular}
\end{center}
\end{table}
\newpage
これらの誤りの大部分は，接続ルールや重みを変更することで対応できると考え
られる．しかし，そのためには，相当数のルールの追加・変更が必要になると考
えられる．このような，修正を行うためには，試験的に形態素解析を行って形態
素解析の誤り個所を見つけ，誤りの個所が修正でき，かつ正解個所の解析結果は
変化しないように接続ルールや重みを変更する必要があると予想される．この作
業を行うためにはルールの作成において相応の経験・知識を持つ人が，相当な時
間をかける必要があると考えられる．これに対してN-gramでは，前節の実験でデー
タ量が増えるに比例して形態素解析率は向上していることから，形態素解析の誤
り部分を修正するだけで形態素解析精度が向上でき，日本語において多少の文法
的知識を持つ人なら容易に作業が可能であり，ルールベースの方法より精度の改
善が容易であると考えられる．


\subsection{未知語を含む文の形態素解析結果}
次に，未知語を含む文の形態素解析実験を行った．
学習・評価には，6.1節の実験と同一データを使用した．
ただし，辞書には学習データに出現した形態素しか登録しておらず，
評価データのみにしか出現しない形態素が未知語となる．
このような未知語は632語存在し，
評価データ中の137,691形態素中の859形態素(約0.6\%)を占める．
ただし，形態素N-gramは，この処理は行えないため，
品詞N-gramと複合N-gramのみで比較実験を行った．
ただし，処理時間の都合上，両モデル共にBigramのみを用いた．

形態素解析の評価は，品詞付与の形態素Accuracy(\%)のみで評価した．
これは，現在の我々の形態素解析システムでは，
未知語に対し読みを付与する機能がないためである．
未知語に読みを付与するためには漢字毎の読みの情報があることが最低条件となるが，
現在そのようなデータを持ちあわせていないことがその理由である．
また，未知語，特に固有名詞の読みは人間でも間違う場合が多く，
これを自動で行うのは技術的にも困難であると考えられる．



表\ref{tbl:UnknownWord}に結果を示す．
未知語処理を行った場合でも，
複合Bigramが品詞Bigramよりも高い正解率を得た．
辞書に全語いが登録されている6.1節の実験では正解率が99.13\%であったから，
0.8\%程度低下はしているものの，98\%以上の比較的高い正解率が得られた．
また，未知語の形態素解析誤りを分析したところ，
「防音」が「防」と「音」のように，１形態素が複数の形態素に分割された
例が多数見られた．これは，「音」という形態素が辞書に登録されているため
「防」という文字のみが未知語として解析された結果生じた現象である．
「防」も「音」も両方普通名詞であるから，
これらの語を結合させることにより，誤りを低減することが可能であると考え

\begin{table}[h]
\begin{center}
  \caption{未知語を含む文の形態素解析性能}
  \label{tbl:UnknownWord}
  \begin{tabular}{|c||c|c|c|c|}
    \hline
    \lw{品詞Bigram} & \multicolumn{4}{c|}{複合Bigram（分離クラス数）} \\
    \cline{2-5}
          & ~~500~~ & ~~1000~~ & ~1500~ & 2000 \\ \hline\hline
    97.66 & \underline{98.31} & 98.26 & 98.24 & 98.26 \\ \hline
  \end{tabular}
\end{center}
\end{table}

\newpage
られる．



\section{むすび}
本論文では，
連続音声認識用のN-gram言語モデルを構築するのに
必要な形態素データを大量に収集することを目的として，
品詞と可変長形態素列の複合N-gramを用い，
テキストデータから自動で形態素解析を行う方法を提案した．
形態素解析実験の結果，最高99.17\%の精度であり，
読みまで考慮した結果でも最高98.68\%の精度を得ることができた．
これは，従来のルールベースの手法よりも高い精度であり，
提案手法の有効性が示された．
また，実験により，６千語程度の少ない学習データから学習したモデルでも
94\%程度の精度が得られることを確認した．
さらに，品詞から未知語の出現確率を考えることにより
未知語を含む文の形態素解析が行えるよう改良を行い，
実験の結果，未知語が登録されている場合と比較して
形態素解析精度の低下は0.8\%程度であることを確認した．

今後の課題としては，
形態素解析に最適な複合N-gramの分離クラス数を自動決定することが重要と考える．
また，未知語に関しては，
同一品詞の未知語と既知語とを結合させ，新たな未知語と考えること等により
形態素解析率を向上させ，
さらに，音声認識に直接活用できるよう，
未知語に対して読みを自動的に付与する手法の開発も行いたい．



\bibliographystyle{jnlpbbl}
\bibliography{v06n2_03}

\begin{biography}
\biotitle{略歴}
\bioauthor{政瀧 浩和}{
1989年 京都大・工・電子工卒．1991年 同大大学院工学研究科修士課程修了．
同年住友金属工業株式会社システムエンジニアリング事業本部入社．
1995年よりATR音声翻訳通信研究所に出向．
自然言語処理，音声認識の研究に従事．
電子情報通信学会，日本音響学会各会員．}
\bioauthor{匂坂 芳典}{
1973年 早大・理工・物理卒．1975年 同大大学院修士課程修了．
同年日本電信電話公社（現，NTT）武蔵野電気通信研究所入社．
1986年より国際電気通信基礎技術研究所（ATR）に出向．
現在，ATR 音声翻訳通信研究所，第 1 研究室室長．
工博．音声合成・音声認識を中心とした，音声情報処理，言語情報処理の研究に従事．
電子情報通信学会，日本音響学会，IEEE，米国音響学会各会員．}

\bioreceived{受付}
\bioaccepted{採録}

\end{biography}

\end{document}
