<?xml version="1.0" ?>
<root>
  <title>単語知識を必要としない高精度な言語モデル</title>
  <author>森大毅	阿曽弘具	牧野正三</author>
  <jabstract>本論文では，知識に依存しない，する．このモデルはsuperwordと呼ぶ文字列の集合の上のn-gramとして定義され，従来の単語や文字列のn-gramモデルを包含するものになっている．superwordは訓練テキスト中の文字列の再現性のみに基づいて定義される概念であり，Forward-Backwardアルゴリズムによって学習される．実験の結果，superwordに基づくモデルと文字のtrigramモデルを複数融合させたモデルの優位性が示され，形態素解析に基づく方法および高頻度文字列に基づく方法を上回る性能が得られた．</jabstract>
  <jkeywords>言語モデル，superword，n-gram，音声認識，文字認識</jkeywords>
  <section title="はじめに">音声認識・文字認識の精度向上のため，より高い性能を持つ言語モデルを求めることは重要である．近年は，モデル構築やメンテナンスの容易さの点から，コーパスに基づく統計的言語モデルの研究が盛んである．大語彙ないしタスク非依存のシステムのための統計的言語モデルとして今日もっとも有望視されているものに，n-gramが挙げられる．n-gramは大量のテキストコーパスからの単純な数え上げによって得られる統計量であり，強力かつ頑健性に優れている．英語などのヨーロッパ系言語においては，n-gramの単位として単語を用いることが多い．大語彙のシステムでは単語はカテゴリ数が非常に大きくなるため，単語の代わりに品詞を用いる，または単語クラスタリングによって得られる単語クラスを用いることが多い．これらの言語においては単語は分かち書きされるため機械的に取り出すことができ，数え上げも容易に行える．これに対し，日本語や中国語には分かち書きの習慣がない．朝鮮語は文節ごとに分かち書きをするが，その分かち方は一定しないうえ，n-gramの単位としては大き過ぎて汎化性に難がある．よって，これらの言語をn-gramによってモデル化する際には，テキストコーパスに何らかの前処理が必要である．これには次の可能性が考えられる．人手によって分割されたタグ付きコーパスを使う自動形態素解析システムによって単語に分割する経験的な統計基準によって文字列に分割するこのうちタグ付きコーパスを使う方法には，コーパス自体の入手が質的・量的な困難を伴うという欠点がある．形態素解析に基づく方法は有効であるが，モデルを学習するためにはまず形態素解析システムを用意せねばならないうえ，特定タスクに対して高い性能を得るためには予め辞書をチューニングする必要があると考えられ，メンテナンスのコストがかかる．また，形態素解析システムの文法規則によっては機能語が短めに分割される傾向があり，n-gramの性能を必ずしも最大にするものではない．これらの手法に対して，伊藤らは統計的な基準によって文定し，その文字列に分割されたテキストを使ってn-gramを学列を選定する基準としては，単純な頻度，および語彙の自動獲得のために提案されている正規化頻度の高いものから選ぶ方式が析を必要としない点で優れている．しかし，抽出すべき文字列の最適な個数を見出す方法については述べられていない．また，用いられている基準と言語モデルの能力との理論的関係は浅く，最良の分割方法である保証はない．さらに，この手法ではテキストが明示的に分割される．このため，接辞を伴った語や複合語などの長い文字列が抽出された場合，その文字列を構成するもっと短い語は出現しなかったのと同様な扱いを受けることになる．有限のテキストから汎化性の高い言語モデルを構築したい場合に，このような明示的な分割が最良の結果を与えるとは限らない．本論文では，高い曖昧性削減能力を持つ新しい言語モデルを提案する．このモデルは，superwordと呼ぶ文字列の集合の上のn-gramモデルとして定義される．superwordは訓練テキスト中の文字列の再現性のみに基づいて定義される概念であり，与えられた訓練テキストに対して一意に定まる．具体的な確率分布は，訓練テキストからForward-Backwardアルゴリズムによって求める．訓練テキストを明示的に分割せぬまま学習を行うため，長い文字列中の部分文字列を「再利用」することが可能となり，少量の訓練テキストでも効率の良いモデル化が期待できる．本論文ではまた，いくつかのモデルの融合による汎化性の向上についても検討する．実時間性が要求される大語彙連続音声認識システムにおいては，緩い言語モデルを用いてく２パス処理が一般的である．本論文で提案するような字面の適格性を与える言語モデルは，ディクテーションシステムの第２パス，すなわち後処理用の言語モデルとして有用であるものと考えられる．また，文字n-gramを用いた認識手法を本手法に応用することも可能である．</section>
  <section title="superwordモデルの定式化">単語や文字列のn-gramでは与えられた系列を単語ないし文字列に分割するやり方が一意に決まらないため，これらのモデルは直前の(n-1)個の単語や文字列を状態とする，隠れマルコフモデルの一種と考えられる．単語や文字列の集合は，語彙知識として人手で与えられるか，あるいは経験的な規則に基づいて訓練テキストから抽出されるものである．ここで定義するsuperwordとはこれら単語や文字列を一般化したものであるが，それらと対照的なのは，訓練テキスト中の任意の文字列を含み得る点である．ただし，言語モデルとして意味を持つために必要最小限のヒューリスティクスは導入せねばならない．そこで，次の条件を満たす文字列をsuperwordと定義する．訓練テキスト中に最低2回出現するまたは長さ1の文字列である訓練テキストにおける再現性の仮定は，ある文字列が何らかの言語的なまとまりを成すか否かに対する基準となるものであり，そのような基準として考え得る制約の中でもっとも緩い条件として与えてある．すなわち，ある文字列が訓練テキスト中で1回しか出現しない，または1回も出現しないならば，その文字列が何らかのまとまりを成すだろうという証拠は，他に人間が知識として与えない限り得られない．また，再現性とは独立に，長さ1の文字列は全てsuperwordと定義している．これにより，全ての文は少なくとも1通りのsuperwordの系列として表現できることが保証される．superwordn-gram確率(P(w_i|w_i-(n-1)w_i-1))は，直前に(n-1)個のsuperwordの列(w_i-(n-1)w_i-1)が生起したと仮定した時のsuperword(w_i)の条件付き生起確率である．与えられた文(C=C_1C_2C_k)がsuperwordの列(w_1w_2_l)に分割できるとき，(w_1w_2w_lC)と書く．superwordn-gramモデルは，Cの全ての可能な分割に関して計算したsuperwordn-gram確率の積の総和をもってCの発生確率を推定するものである．すなわち，その確率を次式で与える．ここで(n=1)の時，すなわちsuperwordunigramモデルは，文全体の生起確率がそれぞれ独立なsuperwordの生起確率の積で表されるとするものであり，multigramと呼ばれる可変長単語列に基づく言語モデルと同一のものである．superwordn-gramモデルのクラスは，単語や文字列のn-gramモデルのクラスを包含する．この性質は，パラメータさえ適切に与えることができれば，superwordに基づくモデルの性能が単語や文字列のn-gramモデルの性能と同等かそれ以上になることを保証する．</section>
  <section title="superwordモデルの学習法"/>
  <subsection title="superword集合の獲得">モデルの獲得にあたっては，パラメータの学習に先立ち，訓練テキストからsuperwordの集合を求める必要がある．長さ1のsuperwordについては自明であるから，再現性のある文字列を集める作業が核心である．これには，訓練テキストの全ての位置から始まる半無限文字列をソートして任意長n-gram統計を求め，2回以上出現する文字列を記録する方法が考えられる．しかし，再現性のある文字列だけに興味がある場合には，短い文字列から長い文字列へと逐次的に求める簡便な方法で十分である．実験で用いたテキストコーパスでは，長さLのsuperwordの種類は大きなLでは単調に減少することが観察されている．</subsection>
  <subsection title="確率分布のForward-Backward学習">superwordモデルでは，ある状態から別の状態に移る時に，ある確率で一つのsuperwordを出力する．状態は，直前(n-1)個のsuperwordによって定まるものとする．ただし，(n=1)の場合はただ1つの状態のみ存在するものとする．superwordモデルの出力はsuperword列としてではなく文字の系列として観測される．そこで，通常の隠れマルコフモデルと同様に扱うことを可能にするため副状態を導入して，1回の状態遷移で1文字を出力する等価なモデルを考える．副状態は，状態を分割したもので，そこに移る時最後に出力したsuperwordの各文字に対応する．superword(w_i)の表記を(C_1C_jC_L)とし，(w_i)の長さjのプレフィックスを(w_i,j)とする．そして，superwordモデルにおける(w_i)の出力を，等価なモデルでは次のように表す．すなわち，確率(P(w_i|w_i-(n-1)w_i-1))で副状態(w_i-(n-2)w_i-1w_i,1)に移る時に(C_1)を出力し，以後確率1で副状態(w_i-(n-2)w_i-1w_i,j)w_i-1w_i)に至ると考える．例として，図の状態遷移図では「東北大学」というsuperwordを出力して状態（東北大学）に至る様子を示している．等価なモデルでは，本来の確率で副状態（東）に移る時に文字「東」を出力，確率1で（東北）に移る時に「北」を出力，確率1で（東北大）に移る時に「大」を出力，最終的に確率1で（東北大学）に移る時に「学」を出力する．分割前の状態が異なる副状態は同一視しないので，例えば（東北大学）の副状態（東）と（東京）の副状態（東）は異なることに注意すべきである．(n=1)，すなわちsuperwordunigram確率の学習のための初期確率としては，全てのsuperwordが等確率で発生するとして，superwordの数の逆数を与える．(n&gt;1)については，対応するsuperwordの((n-1))-gram確率で初期化する．確率の再推定のために，図のように訓練テキストから全てのsuperwordを洗い出す．次に，連接可能なn個のsuperwordの組に関して，次式によって確率を更新する．P(w_i|w_i-(n-1)w_i-1)=&amp;&amp;_t_t-1(w_i-(n-1)w_i-1)P(w_i|w_i-(n-1)w_i-1)_t(w_i-(n-2)w_i)_t_t(w_i-(n-2)w_i)_t(w_i-(n-2)w_i)eqnarrayただし，,はそれぞれForward確率，Backward確率で，以下のように再帰的に定義する．時刻t((t&gt;1))でsuperword(w_i)の第1字目を出力するとき時刻t((t&gt;1))でsuperword(w_i=C_1C_jC_L)の第j字目((j&gt;1))を出力するときただし同様に時刻t((t&lt;T))でsuperword(w_i)の第1字目を出力するとき時刻t((t&lt;T))でsuperword(w_i=C_1C_jC_L)の第j字目((j&gt;1))を出力するときただし</subsection>
  <section title="長さ制限の導入">再現性のある文字列の長さを十分大きく取れば，前節までに述べたモデルは与えられた訓練テキストに対して一意に求まる．以下では，これを一般superwordn-gramモデルと呼ぶ．しかし，一般モデルのパラメータ数は大きい．特に，(n&gt;2)ではsuperwordの組み合わせが爆発し，現実的ではない．さらに，あまりに長いsuperwordは訓練テキストに特化してしまう恐れがあり，汎化能力の低下を招く．これに対処するため，一般モデルに加えて長さ制限付きのsuperwordモデルを導入する．これは，逐次的な再現性文字列の獲得を早い段階で打ち切って小さなsuperwordの集合をつくり，その集合に基づいてForward-Backward学習を行うことで得ることができる．以下では，長さlに制限されたsuperwordn-gram確率を(P_|w|(w_i|w_i-(n-1)w_i-1))と表記する．長さがlに制限されたsuperwordn-gramモデルは，図に示すような，状態数が高々字種のl乗に制限されたエルゴーディックHMMとなる．ただし，図は(l=2)とした例である．</section>
  <section title="複合モデル">n-gramに代表される確率モデルにおいては，モデルのパラメータを精度良く推定するに足るサンプルが得られないことが多く，パラメータ空間のさまざまなスムージング法が提案されている．その一つに，いくつかのモデルの確率の重み付き線形和で表現する方法がある．これは本来，詳細なモデルの値が信用できない場合に，パラメータの少ない安定したモデルの値を代用するものであるが，性質の異なる複数のモデルを組み合わせてより良いモデルを得るという積極的な利用も可能である．本節では，この線形補間に基づくいくつかの複合モデルを考える．superwordbigram((n=2))モデルに対しては，superwordunigram確率によって補間された確率は次式で与えられる．重み係数(_)は，訓練テキストとは別のサンプル(held-outデータ)またはクロスバリデーションによって得られる仮想的な未知データの確率を最大にするように再推定する．前述したように，一般superwordbigramはパラメータ量が多くなり過ぎるので，実際にはsuperwordの長さを最大lに制限したモデルと組み合わせる．これは次式で与えられる．式()のような制限されたモデルでは，長い語の表現に難があることも考えられる．そこで，長さ制限付きsuperwordbigramモデルと一般superwordunigramモデルの複合モデルを導入する．複合superwordbigram確率は次式で定義される．さらに，複合superwordbigramモデルを，文字のtrigramモデルによってスムージングすることを考える．文字のtrigramモデルはそれ自身で強力な曖昧性削減能力を持っているが，単語n-gramモデルと融合させることにより，認識対象中の未知の文字列の存在による単語解析精度の低下の影響を低減させ，頑健なモデルとすることができる．文字によって補間された複合superwordbigram確率は次式で定義される．ただし，(P_(w_i|w_i-1))はsuperword(w_i)が生起する確率を，補間された文字trigram確率の積によって求めたものである．すなわち，(w_i)の表記を(C_1C_L(w_i))，(w_i-1)の最後の2文字を(C_-1C_0)と書くときただし(P_c(C_j|C_j-2C_j-1))はbigram,unigram等により補間された文字trigram確率である．また，(d(L(w_i)))は文字モデルが生成する単語の長さに関する分布関数である．</section>
  <section title="評価実験">提案した言語モデルの能力を，文字を単位としたパープレキシティによって評価する．パープレキシティは，式()において評価用テキストをCとして次式で求められる．ただし，kは評価用テキストの全字数である．長さ1のsuperwordに対しては，確率が設定した底値を下回る場合には底上げした．対象タスクは朝日新聞「社説」とした．実験に用いたテキストの量を表に示す．表中，held-outとは式(),式(),式()の重み係数を求めるために用いたテキストである．各々のテキストは，共通部分を持たない．長さ制限の効果を見るため，superwordunigramモデルについて最大長を変化させてパープレキシティを求めた．その結果を図に示す．この結果から，長いsuperwordを許してもパープレキシティは上がらないことがわかる．これは，superwordの再現性の条件が適当であったことを示す．以下の実験では，(L=20)の場合を一般superwordunigramモデルとして扱う．表に，提案したモデルおよび従来のモデルのパープレキシティを示す．上から4項目までがsuperwordに基づくモデルである．bigramとあるのは式()の長さ制限付きモデルである．ここでは最大長を3とした．一般unigram+bigramとあるのは式()の複合モデル，一般unigram+bigram+文字とあるのはさらに文字trigramで補間した式()のモデルである．その場合の式()の分布関数としては，指数分布を仮定した．表の残りの4項目は比較のために示してある．単語trigramは，訓練テキストをあらかじめ形態素解析システムJUMANにより分割して求めたものであり，削除補間法によりスムージングしたものである．文字+単語trigramは，さらに文字のtrigramでスムージングしたもので，式()と同様の式を用いている．文字列trigramは，訓練テキストに伊藤らの実験で最も有効であった左最長一致による高頻度文字列への分割法を適用し，さらに文字のでスムージングしたものである．抽出文字列数は約4000から12000まで変化させ，パープレキシティが極小となった約6000個を用いた時の値を示してある．スムージングのためのheld-outデータにはsuperwordモデルと同じものを用いている．この結果から，次のことがわかる．まず，superwordunigramモデルの性能が良くない．図の結果をも考慮すると，これはsuperwordの長さの問題ではなく，unigramでは語と語の連接関係が本質的に表現できないものと考えられる．これはATISデータベースの上でのmultigramの評価といくぶん矛盾する結果であるが，伊藤らも同様の結果を導いている．長さ制限付きsuperwordbigramモデルの導入によって，性能の向上が見られた．しかし，まだその性能は文字trigramモデルに及ばない．長さ制限付きsuperwordbigramモデルと一般superwordunigramモデルを融合させることで，若干の性能向上が見られた．これは，長いsuperwordは単独ではあまり性能に貢献しないが，語と語の連接関係だけでは表現しきれない部分を補う効果を持っているものと考えることができる．語と語の関係に関する知識と語彙知識とを独立に表現する枠組は，形態素解析の原理と類似している．さらに，文字trigramモデルでスムージングすることにより，大きく性能が向上した．その結果，形態素解析を用いたモデルを超える性能が得られた．superwordに基づいたモデル単独では訓練テキストに対して過学習する傾向があり，未知テキストに対して脆弱な側面があるが，未知テキストに対して頑健な文字trigramモデルとの融合によりそれが克服できることを意味する．</section>
  <section title="あとがき">本論文では，superwordの概念に基づいた新しい言語モデルを提案した．このモデルは従来のn-gramの枠組を包含したより一般的なものであり，コーパス以外の知識に全く依存しない．また，本論文で導入した長さ制限モデルとスムージング手法により，現実的なコーパスの量の範囲でモデルの学習が可能となった．評価実験の結果，長さ制限を施したsuperwordbigramモデルを文字trigramモデルと組み合わせて頑健性を向上させたモデルの性能が高く，形態素解析に基づく手法，および高頻度文字列抽出による方法を超える能力が得られた．superwordに基づく言語モデルは，可搬性に優れた強力なものであるが，欠点として訓練テキストに比べモデルの規模が非常に大きいことが挙げられる．superwordunigramモデルのパラメータ数はsuperword集合の大きさにほぼ比例する．通常のn-gramではモデルのサイズの上界がコーパスの量に対して線形のオーダーで与えられるのに対し，superwordの場合にはそれよりも大きくなる可能性がある．これはsuperwordを可能な限り一般的に定義したためであり，特に大規模なコーパスを用いてモデルを学習する場合には，再現性の仮定を見直す必要があることが考えられる．また，superwordbigramモデルは長さ制限を加えた場合でも非常に大きくなる．今回構築した長さ3のsuperwordbigram確率テーブルは約170Mbyteの大きさのファイルとなり，一般superwordunigram確率テーブルの約10倍である．これは，与えられたテキストのsuperwordによる解析結果が極めて曖昧性が大きいものであることが原因である．モデルのサイズを小さくし，実際のパターン認識システムで利用できるようにするためには，モデルの最適化が必要である．すなわち，学習の過程で非常に小さな確率を付与された状態遷移のアークは刈り取る，あるいは外から遷移してくる確率が十分小さな状態は削除する，などである．しかし，この種の枝刈りは，訓練サンプルに特化する危険がある．今後はパープレキシティを上げることなくモデルをコンパクトにするための枝刈り手法の開発が課題である．document</section>
</root>
