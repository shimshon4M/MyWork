<?xml version="1.0" ?>
<root>
  <title/>
  <author/>
  <jkeywords/>
  <subsection title="Briscoe and Carroll's Model (B&amp;C)">BriscoeandCarroll~introducedprobabilitytotheGLRparsingalgorithminthelightofthefactthatLRtablesdoprovideappropriatecontextualinformationforsolvingthecontext-sensitivityproblemsobservableinrealworldNLapplications.TheypointedoutthatanLRparsestateencodesinformationabouttheleftandrightcontextoftheparse.Thisresultsindistinguishabilityofcontextforanidenticalrulereappliedindifferentwaysacrossdifferentderivations.BriscoeandCarroll'smethodallowsustoassociateprobabilitieswithanLRtabledirectly,ratherthansimplywiththerulesofthegrammar.TheyconsidertheLRtableasanondeterministicfinite-stateautomaton.EachrowoftheLRtablecorrespondstothepossibletransitionsoutofthestaterepresentedbythatrow,andeachtransitionisassociatedwithaparticularlookaheaditemtoincludeaterminalsymbolasasecondcomponent.AnLRitemofagrammarGisaproductionofGwithadotatsomepositionoftherightside.Inthiscase,however,BriscoeandCarrollrefertolookaheadasan``inputsymbol''.andaparsingaction.Nondeterminismariseswhenmorethanoneactionispossiblegivenaparticularinputsymbol.ThefollowingisareviewofB&amp;Cintermsofourformalization.BriscoeandCarrollregardaparsederivationasasequenceofstatetransitions(T):-wherea_iisanaction,l_iisaninputsymbolands_iisthestateattimet_i.TheprobabilityoftheparsederivationTisestimatedbyequation~b&amp;c-bayes:-P(T)&amp;&amp;_i=1^nP(l_i,a_i,s_i|s_i-1)&amp;=&amp;_i=1^nP(l_i,a_i|s_i-1)P(s_i|s_i-1,l_i,a_i)eqnarrayBasedonB&amp;C,thefollowingisasummaryoftheschemeforderivingtheactionprobabilities(p(a))fromthecountofstatetransitionsresultingfromparsingatrainingset.Theprobabilityofanactiongivenaninputsymbolisconditionedbythestateitoriginatedfrom.Theprobabilitiesassignedtoeachactionatagivenstatemustsumtoone.Therefore,whereLa(s)isthesetofinputsymbolsatstates,Act(s,l)isthesetofactionsgivenapairofstatesandinputsymboll,andisthesetofallstatesoftheLRtable.ThismeansthattheactionsintheLRtablearenormalizedwithineachstate.Inthecaseofashiftaction(),P(s_i|s_i-1,l_i,a_i)inequation~b&amp;c-bayesisequaltoonebecauseshiftconflictneveroccursinanLRtable.Therefore,Inthecaseofareduceaction(),theprobabilityissubdividedaccordingtothestatereachedafterapplyingthereduceaction.ThereasonforthisisthatBriscoeandCarrollassociateprobabilitieswithtransitionsintheautomatonratherthanwithactionsintheactionpartoftheLRtable.InthiscaseP(s_i|s_i-1,l_i,a_i)inequation~b&amp;c-bayesisnotone.Therefore,p(a_i)=P(l_i,a_i|s_i-1)P(s_i|s_i-1,l_i,a_i)eqnarrayB&amp;Cemploysthegeometricmeanoftheprobabilitiesoftheactions(p(a_i))forstatetransitionsacrossthewholeparsederivationastheprobabilityofaparsederivation,toavoidbiasinfavorofparsesinvolvingfewerrulesorequivalentlysmallertrees.Therefore,</subsection>
  <section title="Introduction">TheprobabilisticGLRlanguagemodel(PGLR)~haspreviouslybeenproventobebetterthanexistingmodels,inparticularthemodelproposedbyBriscoeandCarroll~andthebaselinemodelusingaprobabilisticcontext-freegrammar(PCFG),inparsingstringsofparts-of-speech(non-word-basedparsing)~.Parsingasentencefromthemorphologicallevelmakesthetaskmuchmorecomplexbecauseoftheincreaseofparseambiguitystemmingfromwordsegmentationambiguitiesandmultiplecorrespondingsequencesofparts-of-speech.Inthispaper,weempiricallyevaluatetheprecisenessofaprobabilisticmodelforPGLRagainstthatforBriscoeandCarroll'smodel(B&amp;C),whichisbasedonthesameGLRparsingframework.Wealsoexaminethebenefitsofcontext-sensitivityinGLRparsing,ofthePGLRmodelagainstthe``two-levelPCFG''model~or``pseudocontext-sensitivegrammar''model(PCSG)---recentlypresentedin~---whichhasbeenshowntocapturegreatercontext-sensitivitythantheoriginalPCFGmodel,byempiricalresultsandqualitativeanalysis.LiketheB&amp;Cmodel,PGLRinheritsthebenefitsofcontext-sensitivityingeneralizedLRparsing(GLR).ItsLRparsingtable(``LRtable''forshort)isgeneratedfromacontext-freegrammar(CFG)bydecomposingaparseintoasequenceofactions.EveryactionintheLRtableisdeterminedbythepairingofastateandinputsymbol,sothatitisvalidtoregardthestate/inputsymbolpairasthecontextfordetermininganaction.Asaresult,PGLRinherentlycapturestwolevelsofcontext,i.e.globalcontextoverstructuresfromthesourceCFG,andlocaln-gramcontextfromadjoiningpre-terminalconstraints.Inui~et~al.~showedthatB&amp;ChassomedefectsindistributingparseprobabilitiesovertheactionsofanLRtable.Oneisthat,inB&amp;C,nodistinctionismadebetweenactionswhennormalizingactionprobabilitiesoverthestatesinanLRtable,whilePGLRdistinguishestheactionprobabilitynormalizationofstatesreachedimmediatelyafterapplyingashiftaction,fromstatesreachedimmediatelyafterapplyingareduceaction.B&amp;Crepeatedlycountsthenextinputsymbolwhencomputingtheprobabilities(thoughthenextinputsymbolisdeterministic),ifparsingisatthestatereachedimmediatelyafterapplyingareduceaction.Redundantlyincludingtheprobabilitiesoftheprecedinginputsymbolsinthiscasesignificantlydistortstheoverallparseprobabilities.Moreover,subdividingreduceactionprobabilitiesaccordingtothestatesreachedafterapplyingreduceactionsisalsoredundantbecauseresultantstack-topstatesafterpoppingforreduceactionsarealwaysdeterministic.B&amp;Cthusestimatesparseprobabilitieslowerthantheyshouldbe.Sornlertlamvanich~et~al.~demonstratedthesuperiorperformanceofPGLRoverB&amp;CandPCFGinasyntacticanalysistaskinvolvingadeterminedsequenceofparts-of-speechasinput---non-word-basedparsing.Mostsyntacticparsingmodelevaluationtakesastringofparts-of-speechasinputandleavestheproblemsofwordsegmentationandpart-of-speechdeterminationtoothermorphologicalanalysismodules,suchaspart-of-speechtaggers.SinceGLRparsinghastheabilitytointegratemorphologicalandsyntacticanalysis~,wecaneasilyrealizePGLRparsingformorphosyntacticanalysis,byaddinglexicalprobabilities.Toprovethatlocaln-gramconstraintsinPGLRareeffectiveinmorphologicalparsing,weconductedacharacter-basedexperimentontheATRJapanesecorpus,andcomparedtheresultingperformancewiththeexistingB&amp;Cmodelandtwo-levelPCFG,anextensionofPCFGwhichisclaimedtoyieldasignificantperformanceadvantageovertheoriginalPCFGframework.SincenospacesareplacedbetweenwordsinJapanesesentences,themodelscaninthiswaybeevaluatedintermsofbothmorphologicalandsyntacticanalysis.Parsinghighlyambiguoussentencesorlongsentencescancauseextendedparsetimeandexhaustiveuseofcomputingresources.Weproposeanewtechniqueforpruningparsesthathavealowerprobabilitythanparseswithinapredeterminedbeamwidth,calledthenode-drivenparsepruningalgorithm.UnlikethemethodproposedbyCarrollandBriscoe~,whichhastoparseasentencecompletelybeforeextractingn-bestparses,ourtechniqueallowspruningthelessprobableparsesatanystageofparsing.Byusingourparsepruningtechnique,theparserneedstoextendonlytheprobableparseswithinabeamwidth,resultinginreductionofbothparsingtimeandmemoryspace.Section~brieflyreviewsthevariousprobabilisticmodels,namelyB&amp;C,two-levelPCFGandPGLR,whichareevaluatedthroughcharacter-basedparsingontheATRJapanesecorpus.Section~showstheresultsofexperimentscarriedoutonthethreemodelsandthebaselinemodelofPCFG.WediscusstheempiricalresultsandgivecaseanalysesinSection~.Section~showstherelativeresultsforLALRandCLRtable-basedPGLRandtheirparametertrainability.</section>
  <section title="Probabilistic Models">Inthissection,webrieflydescribetheexistingprobabilisticmodels,namelyB&amp;Candtwo-levelPCFG,whicharetobeevaluatedagainstthePGLRmodel.B&amp;Cisahigh-performanceprobabilisticmodelproposedfortheGLRparsingframework,aspresentedin~.Two-levelPCFGisanextendedPCFGmodelforyieldinggreatercontext-sensitivitythantheoriginalparadigm.ItwasrecentlyexploredmorethoroughlybyCharniakandCarroll~,usingtheterminology``pseudocontext-sensitivegrammar''(PCSG),showingtheimprovementinper-wordcross-entropyovertheoriginalPCFGmodel.OurmotivationinselectingB&amp;Candtwo-levelPCFGforthecomparativeevaluationofPGLRistoexaminetheeffectivenessofLRtablecontext-sensitivity(globalcontextoverCFG-derivedstructuresandlocaln-gramcontextfromadjoiningpre-terminalconstraints),andtheappropriatenessofPGLRforGLRparsing.Incharacter-basedparsing,givenastringofcharactersC=c_1,,c_nasaninput,thejointprobabilityofaparsetree(T)andwordsequence(W)is:P(T,W|C)&amp;=&amp;P(T)P(W|T)P(C|W,T)P(C)&amp;&amp;P(T)P(W|T)eqnarrayThetermP(C|W,T)becomesonewhenwordsequenceWisdetermined,andP(C)isaconstantscalingfactor,independentofTandW,whichisnotworthyofconsiderationinrankingparsetreesandwordsequences.Probabilisticmodelsallowustoestimateparsetreeprobabilities(P(T)).ForthelexicalprobabilityP(W|T),inourevaluation,wenaivelyassumethatwordw_iinwordsequenceW=w_1,,w_mdependsonlyonitspart-of-speech(l_i).Therefore,Theestimationoflexicalprobabilityisappliedidenticallyinallmodels.</section>
  <subsection title="Two-level Probabilistic Context-Free Grammar">Two-levelProbabilisticContext-FreeGrammar(Two-levelPCFG)isanextendedversionofPCFG,derivingfromtheideaofprovidingcontext-sensitivityforacontext-freegrammar.IntheoriginalPCFGmodel,theprobabilityofaparsederivation(T)isregardedastheproductofprobabilitiesoftheruleswhichareemployedforderivingthatparsederivation.Eachproductionruleofthegrammar(r_i)isoftheform&lt;A,P(r_i)&gt;whereP(r_i)istheassociatedprobability,andtheprobabilitiesassociatedwithallruleswithagivennonterminalAontheleft-handsidemustsumtoone.P(|A)istheprobabilityofarulewhoseleft-handsidenonterminalsymbolis``A'',andtheright-handside``''isasequenceofterminaland/ornon-terminalsymbols.Therefore,_P(|A)&amp;=&amp;1P(T)&amp;=&amp;_iP(r_i)eqnarraywhereP(r)correspondstoP(|A).Two-levelPCFGutilizesextrainformationprovidedbytheparentofnonterminalsinexpandingrules(r_i)throughassignmentofruleprobabilities.Thus,theruleprobabilityinequation~pcfgcanberewrittenas:where(A)isthenonterminalthatimmediatelydominatesA(i.e.itsparent).</subsection>
  <subsection title="Probabilistic Generalized LR (PGLR)">Inui~et~al.~recentlyproposedanewformalizationofaprobabilisticmodelforGLRparsing.UnlikeB&amp;C,aparsederivationisregardedasasequenceoftransitionsbetweenLRparsestacks(T)asshownin~pglr-def,where_iisthestackattimet_i,a_iisanaction,andl_iisaninputsymbol.Schema~pglr-defshowstheinherentdiversionfromB&amp;Cinthedefinitionofparsederivation.Basedontheabovedefinition,theprobabilityofacompletestacktransitionsequenceTcanberepresentedwithequation~pglr-bayes,byassumingthat_icontainsalltheinformationofitsprecedingparsederivation:-ToestimateeachtransitionprobabilityP(l_i,a_i,_i|_i-1),wedecomposeitto:BythenatureofGLRparsing,thestack_iisdeterminedifthepreviousstack_i-1,thenextinputsymboll_iandactiona_iaregiven.Therefore,thetermP(_i|_i-1,l_i,a_i)isequaltoone.WehencehavetoestimateonlythetermsP(l_i|_i-1)andP(a_i|_i-1,l_i).Thefirstterm$P(l_i|\st_i-1)isaconditionalprobabilityforestimatinganinputsymboll_i,giventhecurrentstack_i-1.AccordingtotheLRparsingalgorithm,theinputsymbolafterapplyingareduceactionremainsunchanged.Therefore,wehavetoconsiderP(l_i|_i-1)separatelyintermsoftheactionsthatareappliedtoreachthestack_i-1:Ifthepreviousactiona_i-1isashiftaction,andweassumethatthestack-topstaterepresentsthestackinformationbeneathit,thenIfthepreviousactiona_i-1isareduceaction,theinputsymbolisnotchanged.Therefore,Thesecondterm$P(a_i|\st_i-1,l_i)isaconditionalprobabilityforestimatinganactiona_i,giventhecurrentstack_i-1andinputsymboll_i.Bythesameassumptionasappliedabove,thatthestack-topstaterepresentsthestackinformationbeneathit,thistermcanbeestimatedby:Thefirsttermintransitionprobability~pglr-bayes2isestimateddifferentlydependingonthetypeofactionappliedtoreachthecurrentstack.Therefore,wedividestatesintotwoclasses,namelytheclassofstatesreachedafterapplyingashiftaction(),andtheclassofstatesreachedafterapplyingareduceaction().Fortunately,thesetwoclassesaremutuallyexclusivebecausestatetransitioninLRparsingisrepresentablebyadeterministicfiniteautomaton(DFA).Astatecanbereachedbyauniquegrammarsymbol,whichdistinguishesstatesthatarereachedbyaterminalsymbolfromstatesthatarereachedbyanon-terminalsymbol.Therefore,Tosummarize,thetransitionprobabilitycanberewrittenas:suchthat:_lLa(s)_aAct(s,l)p(a)&amp;=&amp;1_aAct(s,l)p(a)&amp;=&amp;1eqnarraywherep(a)istheprobabilityofanactiona,istheclassofstatesreachedafterapplyingashiftaction,includingtheinitialstate,andistheclassofstatesreachedafterapplyingareduceaction.Theprobabilityofaparsederivationistheproductoftheprobabilitiesoftheactions(p(a_i))forstacktransitionsacrossthewholeparsederivation:-</subsection>
  <section title="Experimental Results">Weevaluatedthevariousprobabilisticmodels(i.e.B&amp;C,two-levelPCFGandPGLR)onaportionoftheATRJapanesecorpus,calledtheSpokenLanguageDatabase(SLDB)~.GivenaninputstringofJapanesecharacters,eachmodelproducesprobabilisticallyrankedparsestogetherwiththeassociatedparseprobabilities,computedasdescribedinSection~.Asthedictionarytogeneratewordcandidatesandtheircorrespondingparts-of-speech,wecollectedallthewordsusedinthecorpus.EachwordinthedictionaryretainslexicalprobabilityP(w|l),whichistheprobabilityofgeneratingword`w'fromanarbitrarypart-of-speech`l'.Eachmodelwastrainedequallywiththesamehand-annotatedtrainingset.Forunseenevents,wesimplyaddedpartofacounttosmooththemodelprobabilities.Evaluationofthesmoothingmethodisbeyondthescopeofthispaper.WeadditionallypresenttheresultsoftheoriginalPCFGframeworkasthebaselinefortheevaluation.</section>
  <subsection title="ATR Corpus and Grammar">The``SpokenLanguageDatabase''(SLDB)isatreebank(acollectionoftreesannotatedwithasyntacticanalysis,or``trees''forshort)developedbyATRbasedonJapanesedialogue.AportionofthecorpushasbeenrevisedthroughapplicationofamoredetailedphrasalgrammardevelopedbyTanaka~et~al.~.Werandomlyselectedabout5%oftherevisedcorpustouseasatestsetandtrainedeachparsingmodelwiththeremainingapproximately10,000trees.Table~describesabreakdownofthecorpus.Therangeandaverageofsentencelengthinboththetrainingandtestsetsareveryclose,fromwhichitisplainthatthetestsetwasappropriatelyselectedfromthecorpus.WeimplementedallthemodelsusingaGLRparser.WegeneratedanLALRtablefromthetreebankgoverningcontext-freegrammarof762productionrules,comprisedof137non-terminalsymbolsand407terminalsymbols.ThegeneratedLALRtablecontained856states.</subsection>
  <subsection title="Parsing the ATR Corpus">WeusedPARSEVALmeasures~tocomparetheperformanceofthetop-rankedparsesforeachmodel.[htbp]*-1em*0.5em*-1emtable*Table~showsthatthePGLRmodeloutperformedtheothermodelsineverymetric.LookingatthemetricsofBP,BR,0-CBandm-CB,whereallstructurelabelsareignored,everymodelreturnedverygoodresults(&gt;93%).Thedisparitybetweenmodelsbecomessignificantwhenbracketlabelsaretakenintoconsideration,suchasinLP/LRandPA.Therefore,thedifficultyliesnotinbracketingbutinlabeling,includingdeterminingpre-terminalswhichisanessentialtaskincharacter-basedparsing.Onereasonforthisisthat,thecontext-freegrammarusedforthiscorpusisrelativelyrestrictedintermsofterminalassignment.Informationaboutwordform(e.g.-meisi---Sino-Japaneseverbalnoun),postposition(e.g.-/gaand-/ni),forexample,isexplicitlyincludedinnon-terminalsymbollabels.Therefore,wordconnectionconstraintswithintherulescansomehowexcludetheinvalidwordcombinations.Resultsfromourpreliminarytestonpart-of-speechinputsequencesshowedthatstructuralambiguityhardlyoccurred.Mostofthesentenceshadnoambiguity.Fromthisstudy,itwasobservedthatmostofthesentenceshadonlyoneparseiftheparts-of-speechofthewordsinthatsentenceweredefined.Theambiguityincreases,however,whenweconsiderparsinginputstringsofcharacters.Theaverageparsebase,wherenisthenumberofwordsinasentence,andpisthenumberofparsesforthatsentence.(APB)ofthetestsetisashighas1.348inthecharacter-basedmeasure.ThisiscomparablewiththeSUSANNEcorpus(1.256)andSECcorpus(1.239),asreportedin~.Therefore,theperformanceofcharacter-basedparsinginthistestmainlydependsontheaccuracyofselectingwordsandtheircorrespondingparts-of-speech.Thismeansthatamodelthatcanprovidelocalcontextinadditiontotheglobalcontextwouldresultinhigherperformance.Asexpected,themodelswhichmakeeffectiveuseofthelocalcontextmodelingnatureofGLRparsing,namelyB&amp;CandPGLR,returnedsignificantlybetterresultsthanPCFG-basedparsing.AlthoughthePCFGrulecontextintwo-levelPCFGextendsastephigher(i.e.totheparentofthereducedrule),themodelstillfailedtoincludeappropriatecontextinsomecases.OnesuchcaseisshowninSection~.Parseaccuracy(PA)showsthepercentageofcorrectparsesthatarerankedtopmostaccordingtothemodelprobability.Bythismeasure,PGLRmaintainedthehighestaccuracyinrankingparses,whilethePCFG-basedmodelsdroppeddowntoslightlyhigherthan50%.Sincethecorpusisakindofspokenlanguagedatabase,therearealotofshortresponseutterancesi.e.``yes'',``no''and``takecare''.ThelowertableinTable~isaddedtoshowtheperformanceonsentencesrangingfrom15to42characters.Thedifferenceinperformancebecomesobviousinparsinglongersentences.Parseperformancepartlydependsonthegrammarandthecorpus.AccordingtothereportofanexperimentontheSUSANNEEnglishcorpusbyCarroll~,thedifferencebetweentheperformanceofPGLRandB&amp;Cwasnotsignificantlyobserved.However,theinputtestsetwasasetofpart-of-speechsequences,excludingambiguityinwordandpart-of-speechselection.Evenhere,though,PGLRreturnedthebestresultintermsofthem-CBmetric.</subsection>
  <section title="Discussion">Itisobviousthattwo-levelPCFGshowsthebenefitsofcontext-sensitivityandyieldssignificantgainsovertheoriginalPCFGmodel.However,theresultsarestillfarbelowthosefortheprobabilisticGLR-basedparsingmodels.Onereasonwouldbetheadvantagesoflocalcontext,i.e.pre-terminaln-gramconstraintsencodedintheLRtable.Then-gramconstraintsaredistributedovertheactionsofthetable.Therefore,theparsetreesgeneratedbyprobabilisticGLR-basedparsersincludepre-terminaln-gramconstraintsintheparseprobabilities.TheexamplebelowshowsthatprobabilisticGLR-basedparsingcansuccessfullyexploittheadvantagesofpre-terminaln-gramconstraints,andassignparseprobabilitiesinamoreaccuratemanner.BasedonGrammar-1,thethreeparsetreetypesinFigure~canbegenerated.Supposingthat(S1)and(S2)arefoundoneandtwotimesrespectivelyinourtrainingset,but(S3)doesnotoccur.(S3)canbefoundveryrarely,oralternativelyneveroccurbecauseitmayhavenoobviousmeaning.Thisactuallyhappensformostwide-coveragegrammars.ThecaseshowninFigure~issimplifiedfromoneofthecaseswehavefoundinourtestset.Itisthecaseofselectingtheappropriatepart-of-speechforasentence-endingword``su'',whichcanbe``infl-masu-su''or``infl-desu-su''.Itmustbeassigned``infl-masu-su'',ifitfollowsaword``ma''havingthepart-of-speechof``auxstem-masu'',andassigned``infl-desu-su''ifitfollowsaword``de''havingthepart-of-speechof``auxstem-desu''.Inthiscase,onlythepre-terminaln-gramconstraintsareeffective,ratherthantheconstraintsfromtheparentnodewhicharethesameinbothcases.InFigure~,`a',`b'and`c'correspondto``auxstem-masu'',``auxstem-desu''and``infl-masu-su'',respectively.Grammar-1.Acontext-freegrammar.*1emProbability-1.Ruleprobabilitiesfortwo-levelPCFG.*1emThebracketedvaluesgivenforProbability-1aretheruleprobabilitiesestimatedaccordingtothetwo-levelPCFGmodelfromthetrainingsetinFigure~.Infact,theyarethesameasforPCFGbecausetheparentsofrules(1)and(2)arenotdifferent,andneitheraretheparentsofrules(3)and(4).Thismeansthattheextendedcontextintwo-levelPCFGdoesnothaveanyeffectifdirectparentsarethesame.Weneedmoreinformationtodistinguishthecases.Unfortunately,however,therearenootherparentnodesinthiscase.Table~isanLALRtablegeneratedfromGrammar-1.TheassociatedprobabilitiesbeloweachactionareestimatedaccordingtoB&amp;CandPGLR,indicatedinthefirstandsecondlinesofeachstaterow,respectively.Forthesakeofbrevity,wedonotconsideranysmoothingtechniqueinthistable,althoughsmoothingwasperformedintheexperimentsdescribedinSection~.ApplyingtheprobabilitiespreparedinProb-a-bil-i-ty-1fortwo-levelPCFG(aswellasPCFG),andTable~forB&amp;CandPGLR,toestimatetheparseprobabilitiesof(S1),(S2)and(S3)inFigure~,weobtaintheresultsshowninTable~.Two-levelPCFG(andPCFG)wronglyassignedpreferenceto(S3)over(S1),whereas(S3)neveroccursinthetrainingset.AlthoughB&amp;Cyieldscorrectpreference,theprobabilitiesaresmallerthanwhattheyshouldbe.Inthiscase,thereisnodifferencebetweenB&amp;CandPGLRinrankingtheparses.Theside-effectsofinappropriatenormalizationofprobabilitiesinB&amp;ChasalreadybeenexploredinandempiricallyconfirmedintheevaluationinSection~.</section>
  <section title="Comparative Results for LALR and CLR Table-based PGLR">CanonicalLR(CLR),orLRwithonelookahead,isthemostpowerfulandthemostexpensive.IntheLRtablegeneratingprocess,onelookaheadsymbolistakenintoaccountbyaddingasthesecondcomponentinconstructinganitem,wherethefirstcomponentintheitemisadotnotation,calledthecoreoftheitem.AnLRofagrammarGisaproductionofGwithadotatsomepositionoftherightside.Table~~showstheCLRandLALRtablesgeneratedfromthesamegrammar.ItisobviousthattheLALRtablerequiresfewerstatesthantheCLRtabledoes.AccordingtotheprocessofgeneratingtheLALRtable,state1and6,state2and7,andstate5and9intheCLRtablearemergedtobestate1,2and5,respectively,becausetheyhavethesamecoreintheirLRitem.Consequently,thedegreeofcontextsensitivitywhenparsingwithanLALRandCLRtablecanbedifferent,andtheleftcontextencodedinstatesinanLALRtablecanbedecreasedcomparedwiththatforstatesinaCLRtable.However,thetremedousnumberofstatesintheCLRtablecancausethedatasparsenessproblem.ToverifyourPGLRmodelonbothtables,wethusextendedourexperimenttoexaminetheperformanceofPGLRusinganLALRtable(PGLR(LALR))againstusingaCLRtable(PGLR(CLR)).TheCLRtablecontained3,715states,morethanfourfoldthenumberofstatesinthecorrespondingLALRtable(856states)forthesameATRJapanesegrammar.Table~showsthatPGLR(LALR)returnedslightlybetterresultsthanPGLR(CLR).However,thedifferenceisnotstatisticallysignificant.[htbp]*-1emtable*Basedontheabove,wemayconcludethat,empirically,PGLRusinganLALRtablecanperformaswellasoneusingaCLRtable.StatemergingingeneratinganLALRtabledoesnotaffecttheparsingcontextagreatdeal.Onthecontrary,thedrasticincreaseinstatesfortheCLRtablecausesparametersparseness.InPGLR,wedistributeparseprobabilitiestoeachactionappearinginthetable,sothatthenumberoftrainingparametersisidenticaltothenumberofdistinctactionsintheLRtable.ComparedwithPGLRusingaCLRtable,thepercentageoftrainedactionsforthatusinganLALRtableincreasesmoresignificantly,asshowninFigure~.Thepercentageofactions(bothshiftandreduce)intheLALRtableobservedinparsingthetrainingsetisaroundthreetimeshigherthanintheCLRtable.ThismeansthatactionsintheLALRtablearetrainedmoreeffectivelythanthoseintheCLRtable.Althoughthenumberoftrainedactionstendstoincreaseasthenumberoftrainingsentencesisincreased,thenumberofactionsobservedonlyonceintrainingbecomessaturatedatanearlystageoftraining,withmostoftheobservedactionsrepeatedintheexpansionofthetrainingset.PerhapsmostoftheactionsintheLRtablesarenotreallyused,thoughtheyareallowedundertheoriginalcontext-freegrammar;thisisespeciallythecaseforreduceactionswithaverylowlearningcurve.ItmaybepossibletogenerateamoreefficientLRtableifwecandroptheactionsthatcausetheparsertogeneratepracticallyunacceptableparses.However,itisbeyondthescopeofthispapertodiscussthispossibility.Figure~explicitlyshowsthatwithasmalltrainingset,theresultswhenusinganLALRtablearebetterthanwhenusingaCLRtable.Trainingthemodelwithonly1/8ofthetrainingset,orabout1,250sentences,PGLR(LALR)returnedaparseaccuracyashighas90%,whilePGLR(CLR)neededmoretrainingsentencestocatchupwiththeperformanceofPGLR(LALR).Inthesenseofcontext-sensitivity,statesinaCLRtablearemoredistinguishablethanthoseinanLALRtable,butthedrasticincreaseinstatesfortheCLRtablecausesdatasparsenessintraining.Thus,whereaswewouldexpectthepredictabilityforaCLRtabletobehigherthanthatforanLALRtable,LALRtablesperformevenbetterthanCLRtablesinsomecasesgiventhesamesizeoftrainingdata.Consideringthecostoftablegenerationandparsingtime,usinganLALRtableismuchmoreefficient.</section>
  <section title="Conclusion">TheprobabilisticgeneralizedLRparsingmodel(PGLR)isformalizedbasedonthenatureofGLRparsing,aimedatinheritingthebenefitsofcontext-sensitivityinherentintheGLRparsingalgorithm.Thecontext-sensitivityofGLRparsingreflectsboth(i)globalcontextoverstructuresfromthesourcecontext-freegrammar,and(ii)localn-gramcontextfromadjoiningpre-terminalconstraints.ThecontextisencodedintheLRtablestates,andtheparsinghistoryofGLRisstoredinstacks---inaformofGSStorecordparsingambiguity.Therefore,itiseffectivetoconcentrateonstatetransitionsinLRparsingandviewaparseasasequenceofstackorstatetransitions.SinceGLRparsingisatable-drivenshift-reduceleft-to-rightparserforcontext-freegrammars,wecanefficientlyincorporatestatetransitionprobabilitiesintoparsingactionprobabilities.TheresultsofourexperimentsclearlyshowthatthePGLRmodelisabletomakeeffectiveuseofbothglobalandlocalcontextprovidedintheGLRparsingframework.Asaresult,ourmodeloutperformedbothBriscoeandCarroll'smodelandthetwo-levelPCFGmodelinalltests.Inaddition,PGLRneedsonlytheprobabilityforeachactionintheLRtabletocomputetheoverallprobabilityofeachparse.Itisthustractabletotraining,withthedegreeoffreeparametersassmallasthenumberofdistinctactions,andassociatesaprobabilitydirectlytoeachaction.EmpiricalresultsalsodemonstratedthattheeffectivenessofPGLR(LALR)iscomparablewiththatofPGLR(CLR).wouldliketothankMr.~MasahiroUekiforsupportinusingMSLRandkindlyadaptinghisparsertobeabletoaccumulateactioncountsduringthetrainingphase.Mr.~TaiichiHashimotohelpeduswiththeimplementationoftheprobabilisticGLRparser,andMr.~ToshikiAyabeaidedwithLRtablegeneration,especiallyincomputingthestatelistforusewiththeB&amp;Cmodel.Dr.~ThanarukTheeramunkongofJAISTprovidedhelpfuldiscussionsonaccumulatingprobabilitiesintoaGLRpackedsharedparseforest.Mr.~ImaiHirokihelpedrevisetheATRcorpus.WealsowouldliketothankDr.~JohnCarrollforhishelpfuldiscussionsandeffortsinevaluatingdifferentmodelsonanEnglishcorpusbywayofhisparserandgrammar,duringhisstayatourlaboratory.Mr.~TimothyBaldwinrevisedandgavevariousvaluablecommentsontheoriginalversionofthispaper.document</section>
</root>
