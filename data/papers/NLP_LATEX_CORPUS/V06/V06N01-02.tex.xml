<?xml version="1.0" ?>
<root>
  <title>長さ可変文脈を用いたマルチニューロタガー</title>
  <author>馬青井佐原均</author>
  <jabstract>本稿は最長文脈優先に基づいて長さ可変文脈で品詞タグづけを行うマルチニューロタガーを提案する.マルチニューロタガーはそれぞれ長さの異なる文脈を入力とした複数のニューラルネット（それぞれをシングルニューロタガーと呼ぶ）とそれらの出力を選別する最長文脈優先セレクターで構成される.個々のニューラルネットの訓練はそれぞれ独立に行なわれるのではなく,短い文脈での訓練結果（訓練で獲得した重み）を長い文脈での初期値として使う.その結果,訓練時間が大幅に短縮でき,複数のニューラルネットを用いても訓練時間はほとんど変わらない.タグづけにおいては,目標単語自身の影響が最も強く,前後の単語もそれぞれの位置に応じた影響を与えていることを反映させるために,入力の各構成部分は情報量最大を考慮して訓練データから得られるインフォメーションゲイン（略してIGと呼ぶ）を影響度として重み付けられる.その結果,更に訓練時間が短縮され,タグづけの性能が改善される.計算機実験の結果,マルチニューロタガーは,8,322文の小規模タイ語コーパスを訓練に用いることにより,未訓練タイ語データを94%以上の正解率でタグづけすることができた.この結果は,固定長さを文脈としたどのシングルニューロタガーを用いた場合よりも優れ,マルチニューロタガーはタグづけ過程において動的に適切な長さの文脈を見つけていることを示した.</jabstract>
  <jkeywords>タイ語,コーパス,品詞タグづけ,マルチニューロ,長さ可変文脈,最長文脈優先</jkeywords>
  <section title="はじめに">一つ一つの単語はしばしば複数の品詞（即ち,品詞の曖昧性）を持ち得る.しかしながら,その単語が一旦文に組み込まれば,持ち得る品詞はその前後の品詞によって唯一に決まる場合が多い.品詞のタグづけはこのような曖昧性を文脈を用いることによって除去することである.品詞タグづけの研究は,特に英語や日本語などにおいて多数行なわれてきた.これらの研究を通じ,これまで主に四つのアプローチ,即ち,ルールベースによるもの~,HMMやn-gramを用いた確率モデルに基づいたもの~,メモリベースのもの~,そしてニューラルネットを用いたもの~が提案された.これらの研究では,大量の訓練データ（例えばにおいては1,000,000個のデータ）を用いれば,そのいずれの手法を用いても,未訓練データへのタグづけを95%以上の正解率で行なえることを示した.しかしながら,実際,英語や日本語などを除いた数多くの言語（例えば本稿で取り上げたタイ語）に関しては,コーパス自体もまだ整備段階にあるのが現状で,予め大量の訓練データを得るのが困難である.従って,これらの言語にとっては,如何に少ない訓練データで十分実用的で高い正解率の品詞タグづけシステムを構築するかが重要な課題となる.これまで提案された確率モデルやニューラルネットモデルのほとんどはタグづけに長さが固定の文脈を用いるものであり（HMMモデルにおいても状態遷移を定義するのに固定されたn-gramベースのモデルを用いる）,入力の各構成部分は同一の影響度を持つものとされていた.しかし,訓練データが少ない場合,タグづけ結果の確信度を高めるために,まずできるだけ長い文脈を用い,訓練データの不足から確定的な答えが出ない場合に順次文脈を短くするといったようにフレキシブルにタグづけすることが必要とされよう.そして,客観的な基準で入力の各構成部分の品詞タグづけへの影響度を計り,その影響度に応じた重みをそれぞれの構成部分に与えればより望ましいであろう.そこで,シンプルで効果的と思われる解決法はマルチモジュールモデルを導入することである.マルチモジュールモデルとは,複数のそれぞれ異なった長さの文脈を入力としたモジュールとそれらの出力を選別するセレクターから構成されるシステムのことである.しかし,このようなシステムを例えば確率モデルやメモリベースモデルで実現しようとすると,それぞれ以下に述べる不具合が生じる.確率モデルは,比較的短い文脈を用いる場合には,必要とされるパラメターの数はそれほど多くならない.しかし,ここで提案しているような複数のモジュールを場合に応じて使い分けるようなシステムでは,ある程度の長さの文脈を用いることが必要となり,確率モデルのパラメターの数が膨大になる.例えば,品詞が50種類ある言語を左右最大三つの単語の情報を文脈としてタグづけを行なう場合,その最長文脈を入力としたn-gramベース確率モデルにおいては,サイズが50^7=7.810^11のn-gramテーブルを用意しなければならない.一方,IGTreeのようなメモリベースモデルにおいては,品詞タグづけに実際に用いる特徴の数はそのツリーを張るノード（特徴）の範囲内で可変であり,各特徴のタグづけへの影響度もそれらを選択する優先順位で反映される.しかしながら,特徴の数を大きく取った場合,この手法によるタグづけの計算コストが非常にかかってしまうケースが生じる.実際,Daelmansらのモデル~においてはノードの数は僅か４に設定されており,実質的に固定長さの文脈を用いていると見てもよい.本稿では,複数のニューラルネットで構成されるマルチニューロタガーを提案する.品詞のタグづけは,長さが固定の文脈を用いるのではなく,最長文脈優先でフレキシブルに行なわれる.個々のニューラルネットの訓練はそれぞれ独立に行なわれるのではなく,短い文脈での訓練結果（訓練で獲得した重み）を長い文脈での訓練の初期値として使う.その結果,訓練時間が大幅に短縮でき,複数のニューラルネットを用いても訓練時間はほとんど変わらない.タグづけにおいては,目標単語自身の影響が最も強く,前後の単語もそれぞれの位置に応じた影響度を持つことを反映させるために,入力の各構成部分は情報量最大を考慮して訓練データから得られるインフォメーションゲイン（略してIGと呼ぶ）を影響度として重み付けられる,その結果,訓練時間が更に大幅に短縮され,タグづけの性能も僅かながら改善される.計算機実験の結果,マルチニューロタガーは,8,322文の小規模タイ語コーパスを訓練に用いることにより,未訓練タイ語データを94%以上の正解率でタグづけすることができた.この結果は,どの固定長さの文脈を入力としたシングルニューロタガーを用いた場合よりも優れ,マルチニューロタガーはタグづけ過程において動的に適切な長さの文脈を見つけていることを示した.以下,２章では品詞タグづけ問題の定式化,３章ではインフォメーションゲイン（IG）の求め方,４章ではマルチニューロタガーのアーキテクチャ,そして５章では計算機実験の結果について順に述べていく.</section>
  <section title="品詞タグづけ問題">表１は品詞をタグづけされたタイ語コーパスの例を示す.記号`@'や`/'で区分されている記号列（例えばNCMNやPPRS）はその前にある単語が持ち得る品詞を表し,記号`@'の直後の記号列はその文において唯一に決まった品詞を表している.本稿で用いるタイ語の単語カテゴリの分類法に47種類の品詞が定義されている~.table*入力されるタイ語テキストは電子辞書を用いて単語に分割され,各単語の持ち得る品詞もリストアップされるため,品詞のタグづけ問題は以下に示すような文脈を用いた品詞の曖昧性除去あるいは一種のクラス分け問題と見なせる.ここで,ipt_x(x=l_i,t,r_j,i=1,,l,j=1,,r)を入力IPTの構成部分と呼ぶ.具体的には,ipt_tは目標単語の取りうる品詞に関するもの,ipt_l_iとipt_r_jはそれぞれ目標単語から左へi番目と右へj番目の単語の取りうる品詞（文脈）に関するもの,そして,POS_tは目標単語がその文脈で取る正しい品詞を表すものである.</section>
  <section title="インフォメーションゲイン（IG）">インフォメーションゲイン（IG）は,特徴ベクトルで定義されるデータセットの情報量がある特定の特徴の値を知ることによってどれだけ増えるかを表す量である~.より具体的に言えば,ある特徴のIGとはその特徴がデータのクラス同定にどれだけ重要かを反映する量である.ここで,特徴を入力の構成部分,特徴の値をその構成部分の取りうる品詞,データの属するクラスを目標単語の取りうる品詞にそれぞれ置き換えてやれば,各構成部分のIGはその構成部分の品詞タグづけへの影響度として考えることができる.従って,(1)における入力の各構成部分ipt_x(x=l_i,t,r_j)はそれぞれタグづけへの影響度に応じた重みw_xを持つと仮定すれば,その重みは以下のように求められる.ここで訓練データセットをS,i番目のクラス,あるいはi番目の品詞（i=1,,n,但し,nは品詞の数）をC_iで表す.セットSのエントロピー,即ち,Sの中の一つのデータのクラス（品詞）を同定するのに必要とされる情報の平均量はである.但し,|S|はSの中のデータの数,freq(C_i,S)はそのうちクラスC_iに属するデータの数である.セットSが構成部分ipt_xの持ちうる品詞によってh個のサブセットS_i(i=1,,h)に分割されたとき,新しいエントロピーはこれらのサブセットのエントロピーの重みつき総和で求められる.即ち,この分割（即ち,構成部分ipt_xの品詞を知ること）による情報の増益（IG）は以下になる.従って,構成部分ipt_xのタグづけへの影響度に応じた重みは以下のように設定できる.</section>
  <subsection title="マルチニューロタガー">図２に示すように,マルチニューロタガーはエンコーダー/デコーダー,複数のシングルニューロタガーSNT_i(i=1,,m),そして最大文脈優先セレクターで構成される.SNT_iは入力IPT_iを持つ.入力IPT_iの長さ（即ち,構成部分の数:l+1+r）l(IPT_i)は次の関係を持つ：l(IPT_i)&lt;l(IPT_j)fori&lt;j.目標単語word_tを中心とした,最大長さl(IPT_m)の単語列(word_l_l,,word_l_1,word_t,,word_r_r)がマルチニューロタガーに与えられた時,それぞれ同じく単語word_tを中心とした長さl(IPT_i)の部分単語列が前節に述べた方法でIPT_iに符号化され,個々のシングルニューロタガーSNT_i(i=1,,m)に入力される.それらの入力に対し,個々のSNT_iはそれぞれ独立に品詞タグづけを行ない,出力OPT_iを得る.出力OPT_iは前節に述べた方法でRST_iにデコードされる.RST_iは更に最大文脈優先セレクターに入力され,最終結果は次のように得られる.この式は,タグづけの最終結果はできるだけ長い文脈で得られた出力を優先的に用いることを意味する.</subsection>
  <subsection title="シングルニューロタガー">図1は固定長さの文脈を用いて品詞タグづけをするニューラルネット（シングルニューロタガー,略してSNTと呼ぶ）を示す.単語xが入力の位置y(y=t,l_i,orr_j)に与えられた時,入力IPTの構成部分ipt_yは以下のように重み付けされたパターンで定義される.但し,w_yは(5)で求められた重み,nはタイ語に定義された品詞の数,I_xi=w_ye_xi(i=1,,n)である.もし単語xが既知のもの,即ち,訓練データに出現するならば,各ビットe_xiは以下のように得られる.ここで,Prob(POS_i|x)は単語xの品詞がPOS_iである確率で,訓練データから以下のように推定される.ここで,|POS_i,x|は全訓練データを通じ,xが品詞POS_iを取る回数で,|x|はxが出現する回数である.一方,もし単語xが未知のもの,即ち,訓練データに出現しないならば,各ビットe_xiは以下のように得られる.ここで,n_xは単語xが持ちうる品詞の数である.出力OPTは以下のように定義されるパターンである.OPTはデコードされ,目標単語の品詞として最終結果RSTが得られる：文の各単語を左から右へ順にタグづけしていくとき,左側の単語はつねにタグづけ済みと考えられるため,それらの単語に関する入力を構成するとき,より多くの情報が活用できる.具体的には,(6)-(9)を用いる代わりに,入力は次のように構成される.ここで,tは目標単語の文における位置であり,i=1,2,,lfort-i&gt;0.しかしながら,訓練過程においてはタガーの出力はまだ正確ではないため,それらを直接入力にフィードバックして使うことができない.そのために,訓練過程における入力は以下のように実際の出力と目標出力の重みづき平均を用いて構成する.ここで,DESは目標出力で,w_OPTとw_DESはそれぞれ次のように定義される.ここで,E_OBJとE_ACTはそれぞれ目標誤差と実際の誤差を表す（それらの詳細は4.3節で述べる）.従って,訓練の始めの入力構成では目標出力の比重が大きく,時間が立つにつれゼロへ減っていく.逆に,実際の出力の比重は最初小さく,時間が立つにつれて大きくなっていく.</subsection>
  <subsection title="三層パーセプトロン">図１に示すように,シングルニューロタガーは三層パーセプトロン（詳細はを参照）で構成される.三層パーセプトロンは誤差逆伝播学習アルゴリズム~を用いて品詞のタグづけ済みの訓練用データを学習することによって品詞タグづけ能力を学習できる.訓練段階においては,各訓練データーのペア(IPT^(b),DES^(b))は順番にネットワークに与えられる.但し,上つき記号bはデータの番号を表し,b=1,,P,Pは訓練データの数である.b番目の訓練データIPT^(b)とDES^(b)は次のようなパターンとされる.[IPT^(b)=(ipt_l_l^(b),,ipt_l_1^(b),ipt_t^(b),ipt_r_1^(b),,ipt_r_r^(b))]但し,p=(l+1+r)nである.IPT^(b)の各ビットI^(b)_iは(6)-(9)或は(12)-(15)を用いて得られる.DES^(b)の各ビットD^(b)_iは次のように与えられる.訓練は全訓練データに対し平均出力誤差が目標値以下になるまで繰り返して行なわれる.ここでk回目の繰り返し訓練においてb番目の訓練データのペアが提示されたとする.その時,ネットワークは,入力層に与えられた入力パターンIPT^(b)を以下の(20)-(24)を用いて出力層へ前向きに伝播しながら変換する.入力層はまず以下のようにセットされる.但し,i=1,,p,y_i(k)は入力層のユニットiの出力,i=0はバイアスユニットを表す.ここで,次の層（中間層或は出力層）のユニットjの出力は次のように得られる.但し,v_j(k)はユニットjの内部活動度と呼ばれるもので次のように得られる.ここで,y_i(k)とqはそれぞれ前の層のユニットiの出力とユニットの総数である（入力層においてはq=p）.また,()は次のように定義される.出力層のユニットjの出力には別な記号O^b_jを用いる,即ち,出力O^(b)_j(k)が得られた後,その出力と目標出力間の二乗誤差E^(b)は次のように計算される.そこで,その誤差を出力から入力へ逆伝播して誤差を減らすようにネットワークの重みを次のように修正する.但し,は重みの更新量を決める学習率で,は慣性率である.w_ji(k)は最急降下法で次のように計算される.このように(20)-(27)を通じてk回目の繰り返し訓練においてのb番目のデータの処理が終る.訓練は,下の条件が満足されるまで,即ち,各訓練データと各出力ユニットに対する平均誤差E_ACT(k)が目標誤差E_OBJ以下になるまで,全訓練データを通じて繰り返して行なわれる：品詞タグづけ段階においては,入力IPT=(I_1,,I_p)が与えられた時,ネットワークはその入力パターンを(20)-(24)を用いて入力層から出力層へ前向きに伝播しながら変換する.ここでkは１にセットされ,上つき記号bが取り除かれる.最終的に,出力OPT=(O_1,,O_n)が(24)の代わりに次のように得られる.ここでは出力の閾値で,1()は以下のように定義される.</subsection>
  <subsection title="訓練">品詞タグづけにニューラルネットモデルを用いる主な欠点は訓練コストが高い（即ち,訓練に時間がかかる）ことである.この欠点は複数のニューラルネットの導入によって更に強調されてしまう.しかしながら,実際,もし短い入力のSNT_iの訓練結果（訓練で獲得した重み）を長い入力のSNT_i+1（i=1,,m-1）にコピーして初期値として使えば,SNT_i+1（i=1,,m-1）の訓練時間を大幅に短縮できる.従って,この方法を用いればマルチニューロタガーをシングルニューロタガーとほとんど変わらないコストで訓練することができる.図３にSNT_1（入力の長さ3）とSNT_2（入力の長さ4）の場合の例を示す.この図では実線部分でSNT_1を示し,点線部分を含む全体でSNT_2を示している.図に示しているように,SNT_1が訓練された後,その重みw_1とw_2はSNT_2の対応するところにコピーされ,SNT_2の初期値として使われている.</subsection>
  <subsection title="特徴">例えば品詞が５０種類ある言語を左右それぞれ三つの単語の情報を文脈としてタグづけを行なう場合,n-gramベースの確率モデルは50^7=7.810^11個のn-gram（パラメータ）を推定しなければならない.それに対し,例えば中間層のユニット数が入力層の半分であるような三層パーセプトロンを用いたニューロタガーの場合,必要とされるパラメータ（ユニット間の結合）の数は僅かn_iptn_hid+n_hidn_opt=350175+17550=70,000である.ここで,n_ipt,n_hid,とn_optはそれぞれ入力層,中間層,及び出力層のユニットの数で,n_hid=n_ipt2である.一般的に,システムに必要とされるパラメータの数が少なければ,それらを正しく同定するのに必要な訓練データの数も少なくてよい.そのために,ニューラルネットモデルのタグづけ性能は確率モデルのそれに比べ訓練データの数の少なさに影響されにくい~.また,他モデルに比べ,ニューラルネットモデルは訓練時間がかかる一方,タグづけ速度が非常に速いことも特徴の一つである.*-3.5mm</subsection>
  <section title="実験結果">*-2.5mm実験用データはすでに品詞のタグづけされたタイ語コーパスから得られた10,452の文であった.それを無作為に8,322文と2,130文に分けてそれぞれ訓練とテストに使った.訓練文においては22,311個の単語が複数の品詞を持ち,テスト文においては6,717個の単語が複数の品詞を持ちえた.タイ語には47種類の品詞が定義されているため,式(6),(10),(18)の中のnは47となる.マルチニューロタガーは五つの（入力に用いられる左右の単語の数がそれぞれ(l,r)=(1,1),(2,1),(2,2),(3,2),(3,3)の）シングルニューロタガーSNT_iから構成された.個々のタガーSNT_iは入力長さl(IPT_i)（=l+1+r）で入力層-中間層-出力層にp-p2-n個のユニットを持つ三層パーセプトロンであった.但し,p=nl(IPT_i)=n(l+1+r)である.SNT_iの出力の閾値[式(29)]は0.5に設定された.また,重みの更新量を決める学習率と慣性率[式(26)]はそれぞれ0.1と0.9に,訓練を止める基準である目標誤差E_OBJ[式(28)]は0.005に設定された.訓練セットから得られた各入力部分の重み[式(5)]は(w_l_3,w_l_2,w_l_1,w_t,w_r_1,w_r_2,w_r_3)=(0.575,0.524,0.749,2.667,0.801,0.575,0.649)であった.表２はテストデータへの品詞タグづけ結果を示す.マルチニューロタガーはIGの有無とは関係なく,その正解率はどのシングルニューロタガーのそれよりも高かった.従って,マルチニューロタガーを用いることによって,文脈の長さを事前に経験的に選ぶ必要がなく,いつも状況に応じて適切な長さの文脈を自動的に選んでいると言える.IGを用いる場合,タグづけの正解率は短い文脈を用いた場合では下がり,長い文脈（入力長さが5以上）を用いた場合では上がった.この表は更にシングルニューロタガーだけを用いてもかなり高い正解率でタグづけすることができることを示した.実際,によれば,英語タガーを10,000オーダーのデータを用いて訓練させた場合,タグづけの正解率は僅か85%程度であった.両者の違いはそもそもタイ語のタグづけ問題が英語のそれより容易であることにあるかもしれない.しかしながら,少なくとも訓練そのものに関してはタイ語のほうが英語より難しいと考えられる.なぜならば,英語の場合は線形分離可能な問題しか解決できない二層パーセプトロンでタグづけ問題を学習できたのに対し,タイ語の場合は三層以上でなければ学習が正しくできなかった.*4mm一般的に,訓練データの数が十分でない場合,タグづけの正解率は用いる文脈が長くなるにつれて確定的な答えが少なくなるために落ちていく.しかしながら,本実験ではこのような現象が現れなかった.その理由は新しい訓練方法,即ち,長い入力のタガーの訓練は短いタガーの訓練結果に依存すること,にあると考えられる.これを確かめるために,l(IPT_i)=6でIGなしのシングルニューロタガーSNT_4を改めて前の結果を利用せずに訓練し直した.その結果,SNT_4のタグづけの正解率は91.1%まで下がった.これは短い入力のシングルニューロタガーSNT_i（i=1,2,3）のいずれよりも低い数字であった.図４は異なる条件でのSNT_4の訓練曲線を示す.太い実線,細い実線,そして点線はそれぞれSNT_3の訓練結果を利用した場合,SNT_3の訓練結果を利用しない場合,そしてSNT_3の訓練結果を利用せず,IGも用いない場合である.この図は,訓練時間の大幅な短縮には前の訓練結果の利用だけでなくIGの利用も効果的であることを示している.*-3.5mm</section>
  <section title="結び">*-2.5mm情報量最大を考慮し最長文脈優先に基づいて長さ可変文脈で品詞タグづけを行うマルチニューロタガーを提案した.マルチニューロタガーは,8,322文の小規模タイ語コーパスを訓練に用いることにより,未訓練タイ語データを94%以上の正解率でタグづけすることができた.この結果は,どのシングルニューロタガーを用いた場合よりも優れ,マルチニューロタガーはタグづけ過程において動的に適切な長さの文脈を見つけていることを示した.効率的な訓練方法,即ち,短い文脈での訓練結果を長い文脈での初期値として使うこと,を用いることにより,マルチニューロタガーをシングルニューロタガーとほとんど変わらないコストで訓練することができた.インフォメーションゲイン（IG）を導入することにより,訓練時間は更に大幅に短縮され,タグづけの性能も僅かながら改善された.document</section>
</root>
