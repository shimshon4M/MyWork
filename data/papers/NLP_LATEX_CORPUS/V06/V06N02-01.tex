\documentstyle[epsf,jnlpbbl,leqno,fleqn]{jnlp_j_b5}

\makeatletter
\newcommand{\namelistlabel}[1]{}
\newenvironment{namelist}[1]{}{}
\makeatother
\setcounter{page}{9}
\setcounter{巻数}{6}
\setcounter{号数}{2}
\setcounter{年}{1999}
\setcounter{月}{1}
\受付{1998}{4}{3}
\再受付{1998}{7}{6}
\採録{1998}{8}{10}
\setcounter{secnumdepth}{2}
\oddsidemargin=0cm
\evensidemargin=0cm
\topmargin=0cm
\title{単語単位による日本語言語モデルの検討}
\author{伊東 伸泰\affiref{IBM} \and 西村 雅史\affiref{IBM} \and 荻野 紫穂\affiref{IBM} \and 山崎 一孝\affiref{IBM}}
\headauthor{伊東 伸泰, 西村 雅史, 荻野 紫穂, 山崎 一孝}
\headtitle{単語単位による日本語言語モデルの検討}
\affilabel{IBM}{日本アイ・ビー・エム 東京基礎研究所}
{Tokyo Research Laboratory, IBM Japan, Ltd.}
\jabstract{
日本語では単語の境界があいまいで，活用等のルールに基づいて定義された単位である
形態素は必ずしも人が認知している単語単位や発声単位と一致しない．
本研究では音声認識への応用を目的として
人が潜在意識的にもつ単語単位への分割モデルとその単位を用いた
日本語の言語 ({\it N}-gram)モデルについて考察した．
本研究で用いた単語分割モデルは分割確率が2形態素の遷移で決定される
という仮定を置いたモデルで，人が単語境界と考える点で
分割した比較的少量のテキストデータと形態素解析による分割結果とを
照合することにより，パラメータの推定を行った．
そして多量のテキストを同モデルにしたがって
分割し，単語単位のセット（語彙）と言語モデルを構築した．
新聞3誌とパソコン通信の投稿テキストを用いた実験によれば約44,000語で，
出現した単位ののべ
94-98\%がカバーでき，1文あたりの単位数は形態素に比べて12\%から19\%
少なくなった．一方，
新聞とパソコン通信ではモデルに差があるものの
その差は単語分割モデル，言語モデル双方とも事象の異なりとして現れ，同一事象に対する
確率の差は小さい．このため，
新聞・電子会議室の両データから作成した言語モデルは
その双方のタスクに対応可能であった．}
\jkeywords{音声認識，ディクテーション，{\it N}-gram モデル，形態素解析}
\etitle{A Word-based Japanese Language Model}
\eauthor{Nobuyasu Itoh \affiref{IBM} \and Masafumi Nishimura \affiref{IBM} \and Shiho Ogino \affiref{IBM} \and Kazutaka Yamasaki \affiref{IBM}}
\eabstract{
This paper deals with a word-based language model of Japanese.
In Japanese, word boundaries are not stable and grammatical units do not
necessarily coincide with human intuition. For accurate segmentation
it is therefore necessary to create a vocabulary set that covers human
utterance units.
In our word-segmentation method, a model of word boundary is described by morphological
parameters (i.e. part of speech), which are learned by comparing
results of human segmentation with those of Japanese morphological
analyzer. Then by using pseudo-random number and the model,
it is determined whether each morpheme transition is a word
boundary. As a result, we obtain a vocabulary set and learning data
for Japanese language model automatically. According to our experiments using
articles from three newspaper and appended texts in network-based forums,
about 44,000 words cover 94-98\% of all words in the test data, and the average numbers of
words per sentence are 12-19\% smaller than those of morphemes.
The parameters of word segmentation
model and language model are quite different in newspaper articles and forum's texts.
However,
the difference does not exist in the probabilities of common events, but in the
kinds of events. Therefore the language
model, which was created from newspaper articles and forum's text, gave
the satisfactory results for both test set.}
\ekeywords{Speech recognition, Dictation, {\it N}-gram model, Morphological analysis}
\begin{document}
\maketitle

\section{はじめに}
  音声認識技術はその発達にともなって，その適用分野を広げ，
日本語においても新聞など一般の文章を認識対象とした研究が
行なわれるようになった\cite{MATSUOKA,NISIMURA4}．
この要因として，音素環境依存型HMMによる音響モデルの高精度化に加え，
多量の言語コーパスが入手可能になった結果，文の出現確率
を単語{\it N}個組の生起確率から推定する{\it N}-gramモデルが
実現できるようになったことが挙げられる．
  日本語をはじ\break
めとして単語の概念が明確ではない言語における音声認識を
実現する場合，どのような単位を認識単位として採用する
かが大きな問題の1つとなる．この問題はユーザーの発声単位
に制約を課す離散発声の認識システムの場合に限らない．連続
音声の認識においても，ユーザーが適\break
時ポーズを置くことを許容
しなければならないため，やはり発声単位を考慮して認識単位を
決\break
める必要がある．従来日本語を対象とした自然言語処理では
形態素単位に分割することが一般\break
的であり，またその解析ツールが比較的
\mbox{よく整備されていたことから{\it N}-gramモデル作成におい}ても「形態素」を
単位として採用したものがほとんどである\cite{MATSUOKA,ITOHK}．
しかしながら，音声認識という立場からあらためてその処理単位に要請される
条件を考えなおしてみると，以下のことが考えられる．
\begin{itemize}
  \item 認識単位は発声単位と同じか，より細かい単位でなければならない．
形態素はその本来の定義から言えば必ずこの条件を満たしているが，実際の
形態素解析システムにおいては，複合名詞も１つの単位として登録することが
普通であるし，解析上の都合から連続した付属語列
のような長い単位も採用している場合が
あるためこの要請が満たされているとは限らない．
  \item 長い認識単位を採用する方が，音響上の
識別能力という観点からは望ましい．つまり連続して発声される
可能性が高い部分については，それ自身を認識単位としてもっておく
方がよい．
  \item 言語モデルを構築するためには，多量のテキストを認識単位に分割する
必要があり，処理の多くが自動化できなければ実用的ではない．
\end{itemize}
  これらは，言い換えれば人間が発声のさいに分割する(可能性がある)
単位のMinimum Cover Set
を求めることに帰着する．
人が感覚的にある単位だと判断する
\mbox{日本語トークンについて考}察した研究は
過去にも存在する．原田\cite{HARADA}は人が文節という単位について一貫した概念を
持っているかについて調査し，区切られた箇所の平均一致率が76\%であり
付属語については多くの揺れがあったと報告している．また
横田，藤崎\cite{YOKOTA}は人が
短時間に認識できる文字数とその時間との関係から人の認知単位を求め，その
単位を解析にも用いることを提案している．しかしながら，これらの研究はいずれも
目的が異なり，音声認識を考慮したものではない．
そこで，われわれは，
人が潜在意識としてもつ単語単位を形態素レベルのパラメータで
モデル化するとともに，そのモデルに基づいて文を分割，{\it N}-gramモデルを作成する
手法を提案し，認識率の観点からみて有効であることを示した\cite{NISIMURA3}．
本論文では主として言語処理上の観点からこの単語単位{\it N}-gramモデルを考察し，
必要な語彙数，コーパスの量とパープレキシティの関係を明らかにする．
とくに新聞よりも「話し言葉」に近いと考えられるパソコン通信の電子会議室から収集した
文章を対象に加え，新聞との違いについて実験結果を述べる．
\section{単語単位への分割}
  本節ではわれわれが採用した単語単位と，同単位への分割手法について述べる．
\par
日本語を分割して発声する場合，その分割点はきわめて安定
している点と，人，または時によって分割されたりされなかったり
する不安定な点がある．
例として「私は計測器のテストを
行っています．」という文を考えよう．これは形態素解析により，
たとえば
\[\hspace{-6mm}
  私\:+\:は\:+\:計測\:+\:器\:+\:の\:+\:テスト\:+\: \\
を\:+\:行\:+\:っ\:+\:て\:+\:い\:+\:ます\:+\:．
\]
と分割されるが，動詞の活用語尾である「っ」や接続助詞の「て」
はほぼ確実に「行」と結合して「行って」と発声されるのに対し，
接辞である「器」は分割される場合もあれば，結合されること
もあるだろう．そこで文がある位置で「分割」される
確率を形態素のレベルでモデル
化することを考える．そして人が分割した学習用テキストと同じテキストを
形態素解析により分割した結果を照合し，各形態素の遷移ごとに
当該点で分割される確率を得る．その後，
より大量のテキストをそのモデルに基づいて分割すれば (このプログラムを以後
セグメントシミュレータと呼ぶ)，人が
分割した傾向をもったわかち書きテキストを容易に得られる．
\par
「分割」される位置としては，形態素の境界
(形態素単位への分割)と
\mbox{さらに細かく形態素の}途中 (文字単位への分割)がある．ここで
分割記号として$\sharp$を使用し，\mbox{「分割」は記号「$\sharp$」が
生起}し，「結合」は「NULL」が生起すると考えれば，前者は
ある形態素から別の形態素に遷移したときにその間に「$\sharp$」
が生起する確率として
\[\hspace{-5mm}
  P(\sharp _i \mid Morpheme_i \rightarrow Morpheme_{i+1})
\]
となる．
後者のそれは$Morpheme$を文字列 $C_1C_2,\ldots,C_n$で表すと，
\mbox{その{\it j}番目の文字の後に$\sharp$}が
生起する確率と考えれば
\[\hspace{-5mm}
  P(\sharp _j \mid Morpheme, \: C_j \rightarrow C_{j+1})
\]
と表現できる．
モデルのパラメータ (形態素の属性)としては，
\mbox{品詞情報({\it KoW})，連接属性 (Part} of Speech: {\it PoS})，
，そして表記 ({\it String})を採用し，
$(KoW[PoS], String)$と表現する．
ここで品詞，連接属性
とはわれわれの用いた形態素解析プログラム\cite{MARUYAMA}の出力
として得られるもの
であり，品詞は81，連接属性は119に分類されている
\footnote{品詞情報は学校文法でいう品詞分類（「動詞」「助動詞」など）に
相当するが，解析の都合上一般にその品詞であると認められていない
形態素に当該品詞を割り
当てている場合がある．その場合は，後の処理のため同じ品詞でも単に
「助動詞」とするのではなく「助動詞A」
のように区別しており，結果的に種類が増大している．また連接属性は品詞を
活用型などによりさらに詳細分類したもので，たとえば動詞は17種類に分類されている．
意味からすれば品詞情報を{\it PoS} (Part of Speech)とすべきであろうが，ここでは
文献\cite{MARUYAMA}の記法にしたがった．}．
したがって
\mbox{形態素単位の分割}では6個，文字単位への分割では4個のパラメータで記述さ
れることになるが，そうすると明らかに
多量の学習用テキスト (人が分割したもの)が必要となる．
そこで頻度が閾値以下であるような場合については，
パラメータを特定の順序で縮退させた確率値を用意し
セグメントシミュレータの実行時も，確率が記述されているレベルまで
同様の順序で縮退し，当該確率値で代用することを考える．
縮退の順序にはさまざまなものが考えられるが，モデルのパラメータについてその種類数
を考えると表記，連接属性，品詞の順に少なくなることは明らかであり，縮退も
それにしたがうのが妥当であろう．また基本的にはある出現回数を閾値としたとき
より多くの種類の遷移確率が得られることが望ましい．このような観点から
いくつかの予備実験を行い経験的に縮退順序を決定した．
この順序と参照される確率値を木構造で表現したのが
図\ref{FIG:STATTREE}である．
各ノードには
形態素の属性とその属性が満たされた場合に分割される確率
が対応する．たとえば
図\ref{FIG:STATTREE}中
\[\hspace{-5mm}
  P(\sharp \mid V. \:infl.[29] \rightarrow Conj. \:p.p.[69],\: て)
\]
は形態素単位への分割に対する記述例で，
形態素の属性が動詞活用語尾[29]から接続助詞[69]「て」へ
遷移したときに，その間で分割される確率を意味する\footnote{
{\it V. infl.}は Verb inflection， {\it Conj. p.p.}は Conjunctive
post-positional particleの略．}．
\begin{figure}[htb]
\begin{center}
  \epsfile{file=signl96.fig2.ps,width=12cm}
  \caption{セグメントシミュレータにおけるパラメータ縮退の順序}
  \label{FIG:STATTREE}
\end{center}
\end{figure}
\mbox{1つ上のレベルでは，表記（こ}こでは「て」）が省略される．
ただし品詞が名詞の場合には文字数が分割確率を記述するパラメータと
して有効と考えられるので\footnote{「誤認識」が「誤」「認識」と分割され
るよりは「音声認識」が「音声」「認識」となりやすいなど．}，\mbox{表記を省
略した場合，文字数をパラメータとして残し}た．
さらに上位レベルでは，連接属性番号も省略し，\mbox{品詞{\it V. infl.}から
{\it Conj. p.p.}への
遷移に対}\mbox{して，人が分割する確率を記述する．}
たとえば，「積んで」という文節を形態素に分割すると
\[\hspace{-5mm}
  積 (Verb[8]) \:+\: ん(V. \:infl[30]) \:+\: で(Conj. \:p.p.[69])
\]
となるが，その中に現れる「ん」と「で」の間で分割された
カウント等もマージした上で算出された確率
となる．このように木はリーフから上位のノードに行くにしたがって
縮退されたパラメータ，言い換えればより大まかなパラメータとなる．
\par
  一方，前節で述べたように人は形態素として定義されたトークンをさらに
文字単位で分割する場合もある．これは形態素解析の都合上
連続した付属語列を1つの形態素としてとり扱うことが行なわれるためである．
たとえばわれわれの用いた形態素
解析用文法では「...かどうか」という付属語列が助詞として扱われているが
「か」+「どうか」と分割されることもある．
そこで形態素レベルの
分割よりもさらに詳細なレベルとして，文字レベルの分割をモデル化した．
このような確率木はつぎのように構成することができる．つまり
もっとも細かい分類における各パラメータ
について，人が分割した結果と形態素解析の結果を照合してカウントし，
その値をリーフから上位ノードに伝搬させた後，確率値に正規化す
ればよい．
全カウント数が少ないと当該確率 (推定値)の信頼
性が低いので，カウント，マージ作業を行なって，頻度がある閾値以上のノードを
最終的なノードとして採用することにする．
\par
このモデル化では学習データの量に応
じて，そのデータから得られる情報を最大限に利用することができる．
たとえば，2文字漢語から接尾辞への遷移には，非常に多くのものがあるが，
その分割されやすさは接尾辞の種類によって異り，それらを捨象してモデル
化したのでは，あいまいさが大きくなってしまう．
しかし逆にそのすべてを
細分化したのでは，頻度が低い接尾辞に対するルールが得られないか，また
は信頼性の低い確率推定値となってしまう．本手法によれば学習データ中に頻度が
高いものについてはより細かい分類でモデル化され，頻度が下るにしたがっ
て統計として信頼にたる単位まで縮退されたパラメータ
による確率値が得られることになる．
\section{形態素解析プログラムの変更}
\subsection{現代語書き言葉以外の表現への文法の対応}
形態素解析システムは，一般に
新聞記事に代表される現代語書き言葉を処理できるように開発されてきた．
しかし近年，データとして使用されるコーパスの大規模化に伴い，
現代語書き言葉以外の表現，特に，会話風の表現（以下，口語体と示す）
を扱う試みが増加してきた\cite{KURO}．
われわれが従来使用してきた形態素解析の\mbox{文法規則\cite{MARUYAMA}}\mbox{も，
原則として}現代語書き言葉に対応したもので，\mbox{口語体への対応は十分ではない．
一方本研}究で用いる学習用テキストは新聞に限らず，パソコン通信
の投稿テキストが含まれており，口語体への対応なくしては
充分な精度の解析結果を得ることができない．
以下の点を考慮して，より多様な文に対応できるよう形態素解析の文法を記述した．
\begin{itemize}
\item 元の文法に対する変更を少なくして派生的な影響を抑える．\\
口語体によく現れる縮退形で，五段活用連用形に接続する「ちゃ」
には，接続助詞「て」および係助詞「は」の連なり「ては」
の縮退と（例: 書い{\bf ちゃ}いけない）と，
接続助詞「て」および補助動詞「しまう」の語幹の連なり「てしま」の縮退
（例: 書い{\bf ちゃ}う）とがある．
前者は直後で文節を切ることができる非活用語，
後者はワア行五段活用をするので，ワア行五段活用語尾が接続し，かつ
直後で文節末に遷移できる「ちゃ」という形態素の規則を作成すれば
形態素解析処理を行うことができる\cite{KURO}．
しかし，品詞や活用形を単語分割モデルで利用すると，
「ちゃ」に品詞として接続助詞を付与すれば「接続助詞にワア行五段活用語尾
が接続する」という一般化が，また動詞を付与すれば「五段動詞語幹が
文節末に遷移する」という一般化が行なわれかねない．
これを避けるには，「ちゃ」に新たな品詞を付与するか，または
「ちゃ」に二種類あるとするという対応が考えられるが
われわれは後者の方法を採った．
形態素解析としては前者が望ましいと思われるが，後の単語分割モデルに
影響を及ぼす可能性がある場合は，元の文法規則への影響がより少ない
ものを採用した．
また，文語活用の残存形などで，現代語活用に全く同じ形があるものに
ついては，現代語活用の形態素に接続条件を加えて対処した．
\item 縮退形の品詞付与では元の形態素列のうち活用語尾や自立語がもつ品詞を優先する． \\
形容詞仮定形活用語尾「けれ」および接続助詞「ば」の連なりの縮退である
「きゃ」「けりゃ」の前連接属性は「けれ」，後連接属性は「ば」
にほぼ等しい．こうした縮退形の品詞は，元の形態素列のもつ連接属性のうち
活用語尾や自立語のものを優先して付与した\cite{OGINO}．
\item 省略による空文字列は次形態素への遷移を追加して対処する．\\
「勉強しよ」「読も」などのように形態素末が落ちる縮退の場合，
前者は助動詞「よう」の縮退「よ」を定義すればよいが，後者は
助動詞「う」そのものが脱落しているので，動詞未然形から
「う」の次の形態素への遷移を追加して対処する．
\end{itemize}
\subsection{複合名詞の分割}
形態素解析の辞書には，現在までの使用目的に応じて複合語が
一語扱いで登録されていることが多いが，単語分割モデル構築の
ための形態素解析としては短単位に分割されていた方が都合がよい．
そこで，複合語の中でも特に多い複合名詞を
分割対象として，分割データベースとヒューリスティック規則
により，形態素解析で複合名詞分割を行なうことにした．
複合名詞の分割データベースは，
2カ月分の新聞記事（産経新聞）を形態素解析して
\mbox{その結果から一定以上}の頻度で出現する3文字以上の名詞を抜き出した後，
人手で，分割する位置の情報を付与することにより作成した．
このデータベースには約25,000語の複合名詞が含まれている．
ヒューリスティック規則は，以下の条件を満たすように作成した．
\begin{itemize}
\item 1語の名詞よりも2語以上の名詞連続のコストが小さい．\\
名詞連続中では，2語のコストがもっとも小さく，次第にコストが
\mbox{増大するように設定す}る．これは複合名詞を分割する際，
あまり細かく切り過ぎないようにするためである．
\item 1文字名詞は他の名詞に比べてコストが大きい． \\
上記と同様，過分割を防ぐためである．
\item 分割対象は3文字以上の複合名詞とする． \\
1文字ずつに過分割しないためである．
\item 未知語のコストは1語の名詞より大きい． 
\end{itemize}
\vspace*{3mm}
また，分割の結果に3文字以上の名詞が含まれている場合は，
再帰的にそれを分割し，分割が不可能になるまで繰り返す．
\section{分割モデルの作成と分割過程}
\vspace{-1mm}
\subsection{分割確率の推定}
  分割ルールとその確率を推定するため，計17人の被験者
\mbox{により，新聞5カ月分（日経新聞3}カ月および産経新聞2カ月）
\mbox{，日本語用例集 (合計約26,000文)，そしてパソコン通信「ピープ}ル」
の電子会議室（以下電子会議室）
\mbox{から採取した文章 (約9,500文）を分割する作業を行った}
\footnote{
文選択は文の長さが一定の範囲に入っていることを除けば無作為に行なった．また
被験者には
1. 不自然にならない限り，より細かく分割すること
2. 書かれた文章ではなく発声する場合の分割点を回答すること
という指示を与えた．}．
\par
新聞や日本語用例集はいわゆる「書き言葉」のスタイルであるのに比較して電子会議室の文章は
より口語体に近く，これらは分割モデルにも影響を与える可能性がある．そこで両者のデータは
別々に取り扱って分割モデル (確率木)を構成した．その結果前者は2,829個，後者は2,269個
のノードからなる木が得られた．表\ref{TBL:RULES}に一例を示す．
ただしノードとして採用するか否かの閾値には当該ノードの出現回数 (カウント)
を用い，その値は学習データ中の単語数に比例させた \footnote{新聞データの場合で50である．}．
\begin{table}[t]
\begin{center}
\caption{生成された木に記述された分割確率の例 (新聞データから得られたもの)}
\label{TBL:RULES}
\begin{tabular}{lr} \hline
  パラメータ値                              & 分割確率 \\ \hline
  $名詞[19]\rightarrow 名詞[19],「者」$ & 0.33 \\
  $名詞[19]\rightarrow 名詞[19],「人」$ & 0.71 \\
  $名詞[19]\rightarrow 形容動詞[18],「的」$ &  0.36 \\
  $動詞活用語尾[29]\rightarrow 接続助詞[69],「て」$ & 0.03 \\
  $名詞[19]\rightarrow 格助詞[77],「を」$ & 1.0 \\ \hline
\end{tabular}
\end{center}
\end{table}
\begin{figure}[t]
\begin{center}
  \epsfile{file=16.eps}
\vspace{6mm}
  \caption{確率木のノード数}
  \label{FIG:CMPSTAT}
\end{center}
\end{figure}
  2つの確率木について得られたノードをいずれに含まれるかで分類し
数を示したものが図\ref{FIG:CMPSTAT}である．
得られたノードは，かなりの異なりがあることがわかる．\mbox{たとえば電子会議室}データから
得られた確率木にのみ存在するノードの中で出現回数の多いものから上位3個
\footnote{遷移後の表記$String$が縮退していないレベルのものに限った．}をあげると
以下のようになる．
\begin{tabbing}
  xxx \= xxxxxxxxxxxxxxxxx \= xxxxxx \= xxxxxxxxxxxxxxxxx \= \kill
  1. \> $接続助詞[69]$ \> $\rightarrow$ \> $活用語尾[31]「る」$   \\
  2. \> $助動詞[62]$   \> $\rightarrow$ \> $接続助詞[73]「が」$   \\
  3. \> $助動詞[48]$   \> $\rightarrow$ \> $接続助詞[73]「けど」$ \\
\end{tabbing}
\par
  これらの遷移を含む例文を上げると 1. 読ん+で+{\bf る}, 2. ...です+{\bf が}, 
3. ...だ+{\bf けど}などであり，明らかに口語体特有の言い回しに伴う遷移が抽出されて
いる．一方新聞データから学習し\break
た確率木にのみ存在するノードをみると体言止め
\mbox{に伴う遷移 ($サ変動名詞[13] \:\rightarrow \: 句点[100]「．」$} i.e.「...を議論 + ．」)
や漢語の接辞 ($名詞[19] \:\rightarrow\: 接辞[19],「会」$)\mbox{など直感的にも電子会議室}等の
文章では比較的頻度が低いと考えられるものが多かった．\mbox{また両方の確率木に共通して出}\mbox{現
しているノード1,607個}について分割確率の\mbox{相関係数を求めたところ0.980となりきわめて高}\mbox{い．
したがって共通するノードに}ついてはほとんど違いはなく，２つの確率木の違いはノードつまり
ルールそのものに現れていることがわかった．
\par
これらのモデルに基づいて以下のように多量の
(形態素解析された)テキストを分割・統合する．
\begin{enumerate}
  \item 各形態素およびその遷移について，連接属性番号，品詞，形態素の表記を
得て，確率木のリーフに記述があるかどうかを調べる．
  \item なければ，木作成の説明で述べた順にパラメータ値を
縮退させ，確率木に記述があるかどうかを調べる．
  \begin{itemize}
     \item 記述があれば，0から1の範囲の乱数を発生し，その値が
ノードに付随する確率以下であれば当該位置で分割し，そうでない場合は
分割しない．
     \item 記述がなければ，縮退を繰り返す．
  \end{itemize}
  \item もっとも上位のノードにも該当しない場合，形態素の分割点であれ
ば当該位置で分割し，それ以外は分割しない．
\end{enumerate}
なお{\it N}-gramモデル
作成には，乱数による分割処理 (セグメントシミュレータ)は必ずしも必要では
なく，形態素解析の結果と分割確率を使って\mbox{直接各{\it N}-gramの生起確率を
推定することも可}能である．
\subsection{単語カバレージ}
\begin{figure}[htb]
\begin{center}
  \epsfile{file=signl96.fig3.ps,width=14cm}
  \caption{日経新聞3ヵ月のテキストに対する単語数とカバレージ}
  \label{FIG:COVERAGE}
\end{center}
\end{figure}
われわれの提案した単語単位に基づく語彙を作成するための予備実験として
日経新聞3カ月分 (合計446,079文)を用い，
前節の手続きを適用して分割，\mbox{連結を行う実験を行った．西村ら}
の報告\cite{NISIMURA}によれば
形態素を単位とした場合，
\mbox{約97\%はおよそ3カ月分のテ}キストで収集できる（言い換えれば飽和する）
ことがわかっている．
\mbox{その結果を図\ref{FIG:COVERAGE}に示す．単}語は合計で約$10^{7}$個，
のべ216,904種類の単語が生成された．図はそれらを頻度の高いのものから
順にとった場合のカバレージを示している．ただし数字表現，姓名はカウン
トから除いている．一方同じテキストから形態素は132,164個が生成された．
これによれば単語単位を採用す\break
ると，形態素よりはより多くの種類が必要では
あるものの，決して発散するものではなく，た\break
とえば
上位約25,000個
(種類)の単語で全トークンの約95\%がカバーでき，
\mbox{取り扱いが可能な語}彙数であることがわかる．
\par
このとき確率木の各ノード（ルール）がどのような割合で使われたかを示したのが
表\ref{TBL:USEDRULE}である．表から明らかなように全体の約60\%
の場合には，一番詳細なレベルのルールが適用されていることがわかる．
\begin{table}[htb]
\begin{center}
\caption{適用されたノードの比率(階層別)}
\label{TBL:USEDRULE}
\begin{tabular}{lr} \hline
  パラメータ値                                              & 
比率 (\%)\\ \hline
  $P(\sharp \mid KoW_{1}[PoS_{1}] \rightarrow KoW_{2}[PoS_{2}], String)$     &
59.6 \\
  $P(\sharp \mid KoW_{1}[PoS_{1}] \rightarrow KoW_{2}[PoS_{2}])$             &
29.2 \\
  $P(\sharp \mid KoW_{1} \rightarrow KoW_{2})$     &  3.9 \\
  $P(\sharp \mid KoW_{2})$                         &  6.6 \\
  該当なし                            &  0.7 \\ \hline
\end{tabular}
\end{center}
\end{table}
\section{語彙とコーパス}
\subsection{コーパスの前処理}
用意したコーパスのソースは日経新聞（93年から96年），\mbox{産経新聞（92年10月から97年），}毎日新聞（91年
と92年），EDRコーパス\cite{EDR}，\mbox{そしてパソコン通信「ピープル」に投}稿された電子会議室の記事である．
ただし日経，産経の両紙は示した期間のすべてではなく，月単位で時期が
重複しないように選択したサブセットである．
新聞についてはその本文を句点単位で文として取り出し，前節で述べた処理を行った．
ただし数字については形態素解析で１単語（品詞「数字」）として扱われてしまうので
当該トークンをすべて桁付きの漢数字に変換した後，西村\cite{NISIMURA3}に記載された
数字の読み上げ単位に合わせて分割した．すなわち整数については「十，百，千，万，億」を位と定義し
先行する数字と位で１つの単位として取り扱い，小数点以下の位については１桁づつに分割する
．たとえば１２３４．５６は「千」「二百」「三十」「四」「・」「五」「六」と変換・分割される
ことになる\footnote{電子会議室の文章では電話番号やID番号にともなう数字があり，これらは
位付きで読むことに適さない．そこでルールでそれらに該当すると判断した場合は１桁づつに
分割した．}．
\par
一方ディクテーションのアプリケーションや一般ユーザーが入力するであろう文，言い回し
を考えると新聞だけでは明らかに不足である．
そこでより口語体に近いデータとしてパソコン通信「ピープル」から
約90の電子会議室に投稿されたテキストを用意した．会議室・話題の種類そして投稿時期について特に
恣意的な選択は行っていないが，結果としてはパソコン関連の話題が多く，テキスト量でみて
約半分を占めている．
電子会議室の投稿文は文ばかりではなく，文字を利用した表，絵などが多数含まれている
他，他人の記述を引用する場合が多く，これらを含めてしまったのでは学習用コーパスとして
不適切であることは明らかである．そこでルールベースでこれらをとり除く
フィルターを作成した．主なルールとしては以下のようなものがある．
\begin{itemize}
   \item 引用記号（「＞＞」など）をもとに引用部分だと判断した行は除く．
   \item 記号文字(「−」「＊」など）の一定以上の繰り返しを含む行は除く．
   \item フェースマーク（「:-)」など）のリストを作成し，それにマッチした箇所は特別な１個の記号
に置き換え，未知語の扱いとする．
\end{itemize}
このフィルターを通した後，句点に加え空白行，一定数以上の
連続した空白を手がかりとして文を取り出し，形態素解析，
セグメントシミュレータの処理を行った．
\vspace{-2mm}
\subsection{語彙の作成}
\vspace{-1mm}
以上の分割済みテキストの内，日経新聞，産経新聞，EDR，そして電子会議室について，
95\%以上のカバレージをもつ語彙を作成したところ，約44,000語の単語からなるセット
(44K語\break
彙) が得られた．
  このようにして得られた語彙は，人が日本語について単語単位だと感覚的に思う
セットを示していると考えられる．たとえば「行う」という動詞とその
後続の付属語列からは
\smallskip
\begin{tabbing}
   xxx \= xxxxxxxxxxxxxxx \= xxxxxxxxxxxxxxx \= xxxxxxxxxxxxxxx \= \kill
       \> 行い            \> 行いたい        \> 行う            \\
       \> 行うべき        \> 行え            \> 行えば          \\
       \> 行える          \> 行った          \> 行ったら        \\
       \> 行って          \> 行っても        \>                 \\
\end{tabbing}
の計11単語が生成された．また「たい」や「べき」といった単語も生
成されており，分割に揺れがある部分では複数の分割に対応した単語が
得られることがわかる．

\subsection{学習コーパス文の選択}
  前節の結果得られた各文は局所的に見ると記号ばかりであったり，姓名の列挙部分
であったりして学習コーパスには適さないものが含まれている．また
電子会議室のテキストはフィルターのルールでカバーしきれなかった部分で
単語ではないトークンが無視できない程度に生じていた．このような文については
人手で採用するかどうかを決める，あるいは当該部分を除くことが望ましいが，
多量のコーパスについて
そのような作業を行うのは不可能なため，ここでは以下の
条件のいずれかに当てはまる文は採用しないことにした．
\begin{itemize}
  \item ２単語以下から構成される文
  \item 文の単語数に対する記号の数が一定以上の文
  \item 44K語彙に対して未知語の数が一定以上の割合で含まれる文
\end{itemize}
音声認識用のコーパスにおいて句読点や括弧表現をどのように取り扱うべきかについては
さまざまな議論がある．松岡ら\cite{MATSUOKA}はカギ括弧以外の括弧（（）【】など）について
内容ごと削除しており，伊藤ら\cite{ITOHK}は括弧の用いられ方（引用，強調など）に応じて
削除すべきかどうかを自動判別している．括弧による表現には確かに読み上げに適さないもの
も含まれているが，本研究では文章入力手段としての音声認識システムの構築を重視し，これらを
削除しないことにした．また同じ理由で句読点も削除していない．
その結果得られた文の数をソース別に示す（表\ref{TBL:SOURCE}）．
\begin{table}[htb]
  \caption{ソース別のテキストサイズ（文と形態素の数，Kは1,000，Mは100万を意味する）}
  \label{TBL:SOURCE}
  \begin{center}
  \begin{tabular}{lrr}  \hline
     ソース   & 文数(K)  & 形態素数(M) \\ \hline
     日経新聞 & 715    & 20.9  \\
     産経新聞 & 1,837  & 49.4  \\
     毎日新聞 & 1,401  & 41.4  \\
     EDR      & 169 &  4.4  \\
     電子会議室   & 1,565  & 33.6  \\ \hline
  \end{tabular}
  \end{center}
\end{table}
\section{単語単位による言語モデル}
  前節にしたがって単語単位に分割されたテキストを学習データとして{\it N}-gram
モデルを学習するわけであるが，生起確率の計算上考慮すべきこととして
数字，時刻などとくに各単語に確率上の差をつけるべき理由がないもの，および
意味がまったく同じでありながら表記の異なる揺らぎが生じているものの取り扱い
がある．前者については各単語をクラスにまとめて確率を計算することにし，合計36クラス
作成した．
後者は新聞の場合，用語統一がなされているため影響は少ない
と考えられるが，
電子会議室のテキストでは「コンピュータ」と「コンピューター」，「組み合わせ」
と「組合せ」といった単語は両者とも多数含まれており
明らかに無視できない．そこで44K語彙について「読み」をもとに同義語
の候補を抽出した上でチェックを行い，約1,800エントリの別名リストを作成した．
{\it N}-gramをカウントするさい，このリストを参照して１つの表記に統一した上で学習
を行っている．
\par
一方，テストデータとして新聞3種類，電子会議室
のテキストを別に用意し，被験者 (単語分割モデルの学習データを作成した
\mbox{被験者とは異なる）により分割を行なった．テストデータ}のそれぞれについて
文数，形態素数，単語数，そして44K語彙のカバレージを表
\ref{TBL:TESTDATA}に示す．この表から１文あたりの単語数は形態素数に比較して
12-19\%程度少なくなることがわかる．
\begin{table}[t]
\begin{center}
\caption{テストデータにおける諸元 ({\it イタリック}は１文あたりの平均数)}
\label{TBL:TESTDATA}
\begin{tabular}{lrrrr} \hline
                       &   文数  &  形態素数 & 単語数      &  カバレージ(\%) \\\hline
  日経新聞             &   600   &  21,378   & 18,725      &   98.3          \\
                       &         &{\it 35.6} & {\it 31.2}  &   \\
  毎日新聞	       &   725   &  22,051   & 18,608      &   96.1          \\
		       &         &{\it 30.4} & {\it 25.7}  &   \\
  産経新聞             &   775   &  21,702   & 17,751      &   96.0          \\
                       &         &{\it 28.0} & {\it 22.9}  &   \\
  電子会議室           &   1,381 &  29,979   & 24,204      &   94.4          \\
                       &         &{\it 21.7} & {\it 17.5}  &   \\ \hline
\end{tabular}
\end{center}
\end{table}
\mbox{本実}験の目的は
\begin{itemize}
  \item 単語を単位とした{\it N}-gramモデルの有効性，コーパスの必要量を評価する．
  \item 新聞と電子会議室において単語{\it N}-gramモデルから見た違いを明らかにする．
\end{itemize}
の２点である．そこで新聞，電子会議室のそれぞれについてその種類，時期の違いを捨象するため，
全学習データを文単位でシャッフルした上で８個に分割したサブセット (新聞: N-1,..,8，電子会議室
F-1,...,8）を作成した．そして各サブセットをさらに95\%と5\%の比率で分割し
前者を{\it N}-gramカウント，後者をHeld-out補間のパラメータ学習用に用いた．
\par
\begin{figure}[htb]
\begin{center}
  \epsfile{file=perpnewsc.eps,width=11cm}
  \caption{新聞データから学習したモデルのテストセットパープレキシティ}
  \label{FIG:PERPNEWS}
\end{center}
\end{figure}
  まず新聞について学習データ (N-1,..,8)を順に増加させながら言語モデルを作成し，
各モデルをテストセットパープレキシティで評価した．ただし学習データ
に１回でも出現した{\it N}-gram (trigramまで）は
\mbox{すべて使用しており，また未知語部分については予測を行っていない．
結果}を図\ref{FIG:PERPNEWS}に示す（電子会議室に関するデータは「Forum」と表記
している）．予想されるようにいずれのテストデータでも
学習コーパスの増加にともなって
パープレキシティは緩やかに改善されるが次第に飽和する傾向がみてとれ，
いずれの場合も学習データセット
を7個から8個に増やしたときのパープレキシティの改善率は1-2\%程度でしかない．
パープレキシティの絶対値には相当の差があり，新聞といってもひとくくりにできない
ことは明らかだが
\footnote{コーパスの量では産経新聞が一番多く，学習データ量で
とくに不利に扱われたとは考えにくい．}，その値 (100-170)は音響識別上
対応可能な値であると考えられる\cite{NISIMURA4}．
一方電子会議室のテストデータはもっとも良い
ケースでも400以上のパープレキシティを示しており新聞の学習データだけでは
対応できていないことがわかる．
\begin{figure}[htb]
\begin{center}
  \epsfile{file=perpforum2c.eps,width=11cm}
  \caption{混合した電子会議室データのサイズとテストセットパープレキシティ}
  \label{FIG:PERPFORUM}
\end{center}
\end{figure}
\begin{figure}[htb]
\vspace*{4mm}
\begin{center}
  \epsfile{file=ngramsc.eps,width=11cm}
  \caption{学習データサイズと{\it N}-gramの異なり数 (新聞)}
  \label{FIG:NGRAM}
\vspace{-3mm}
\end{center}
\end{figure}
\par
  われわれの目的は新聞にとどまらず，より口語体に近い電子会議室に投稿される
文にも対\mbox{応できる言語モデルを作成することである．}そこで新聞データすべてを
使用した言語モデルをベースとし，電子会議室の学習データ(F-1,..,8)
を加えていくことにより各テストデータのパープレキシティがどのようになるかを
評価した．結果を図\ref{FIG:PERPFORUM}に示す．
\mbox{この結果，電子会議室につい}\mbox{てそのパープレキシティは改善される一方，}使用したデータ量
の範囲 (約25M単語)では，新聞に対する影響はほとんどなかった
\footnote{細かく見れば，産経新聞はさらに改善されるのに対し，日経新聞はわずかながら
悪くなる傾向があり，新聞間の差を示唆している．}．一方電子会議室
\mbox{のみから作成した言語モデルで（電子会議}室の）テストデータを評価すると152.1
\mbox{であり，若干の差は見られるものの，混合学習データか}ら作成した言語モデルは
新聞・電子会議室の双方に対応できることがわかる．これは双方の統計的異なりが
共通している{\it N}-gramの確率が相違しているというよりも，{\it N}-gramの種類
に，より大きく現れていることを示唆している．
\par
一方コーパスのサイズと結果として得られたモデルのサイズ，すなわち{\it N}-gramの
異なり数\mbox{の関係を見たのが図\ref{FIG:NGRAM}である．}これは新聞データ(N-1,..,8)の
場合であるが，bigram，trigramとも飽和する傾向は見てとれない．電子会議室テキスト
を加えた場合も同様でN-1,...,8，F-1,...,8すべてを学習データに使用した場合の
\hspace{-0.1mm}{\it N}-gram\hspace{-0.1mm}数は
\hspace{-0.1mm}trigram\hspace{-0.1mm}が31M個，bigramが5.6M個に達し\break
た．
とくにtrigramは学習データサイズの増分に対しほとんど比例して増加している．
今後主\break
記憶，外部記憶の容量がさらに増加するとしても
この{\it N}-gram数（異なり）のままでは，実装することが難しい．そこで{\it N}-gram
の中で低頻度のものを除くことが，パープレキシティにどのような影響を与えるかを
検証する実験を行った．結果を図\ref{FIG:SMALLLM}に示す．{\it N}-gramの異なり
の多くを占めるのはtrigramなので，学習データはN-1,...,8，F-1,...,8すべてを
使用した上で，言語モデルを作成するとき\hspace{-0.1mm}trigram\hspace{-0.1mm}の最低出現回数を設定
することにより，\mbox{モデルのサイズを変更してい}る．図から\hspace{-0.05mm}trigram\hspace{-0.05mm}の異なり数が
\hspace{-0.05mm}5M\hspace{-0.05mm}個以下になるとパープレキシティが\mbox{急速に悪くなる傾向が}見てとれるが，
一方モデルサイズを1/3〜1/5にした程度ではパープレキシティの差は小さい
ことがわかる
\footnote{ここでは，言語モデルに含める最低出現回数を1,...,8に設定している．
グラフから
出現回数1のものを除くだけでtrigramの異なり数は31M個から9M個に減少する
ことがわかる．}．
\begin{figure}[htb]
\begin{center}
  \epsfile{file=perpsmallc.eps,width=11cm}
  \caption{trigramの異なり数とテストセットパープレキシティ}
  \label{FIG:SMALLLM}
\end{center}
\end{figure}
\section{おわりに}
  このように，本研究では比較的少量の人による分割データから揺らぎを含めた
分割傾向を推定する手法について述べ，新聞およびパソコン通信の電子会議室を
学習データとして，そのモデルからつくられた単語の集合と言語モデルについて
考察した．結果として，人が単語と意識する単位はその揺らぎを含めても
発散することはなく，約\hspace{-0.05mm}44K\hspace{-0.05mm}で\hspace{-0.05mm}94-98\%\hspace{-0.05mm}\mbox{程度のカバレージが}得られること，
形態素に比較して１文あたりの要素数が12-19\%程度減少すること，
電子会議室と新聞では，{\it N}-gramモデルからみた統計量に相当の差があり，
予想されたように新聞単体では十分に対応できないものの，新聞をベースとして
電子会議室のテキストを混合させたデータから作成した言語モデルは
新聞のテストデータに対するパープレキシティを増大させることはほとんどなく
その双方に対応可能であることがわかった．分割モデル，{\it N}-gramモデルの
いずれも，データの種類 (新聞，パソコン通信）に依存している．これ
自体は容易に予想できることであるが，その異なりが共通する事象の確率が異なるという
よりも事象自体の異なりにより大きく現れていることは興味深い．
\par
形態素との効率比較という意味では，同一学習データから作成した言語モデル
を用いて単位長さ（たとえば文）
あたりのパープレキシティを比較する必要がある．これについて
学習，テストデータ量は少ないものの，すでに報告を行っており，
文あたりパープレキシティがほぼ等しく，
したがって単位長が長い分，より有利な単位となっている
ことを確認している\cite{NISIMURA4}．
\par
  コーパス量とパープレキシティの関係について，とくに日本語に関して報告された
例はほとんどないため，他の研究と比較して議論することが難しい．本研究の実験から
は400万文強のデータではまだパープレキシティが減少するが，その改善率は低く数倍以上の
データがないと意味のある改善が難しいことを示唆している．
\par
人が感覚的にある単位だと判断する
日本語トークンについて考察した他の研究との関連についても述べておきたい．
原田\cite{HARADA}は人のもつ文節単位の概念に関する調査
結果から，「文字列またはモーラ長が一定以上になると分割しようと
する動機がたかまる」という仮説を提起している．われわれの分割モデルでは
分割が2形態素の遷移情報のみで独立に起こることを仮定しているが，この
独立性については検討が必要であろう．横田，藤崎\cite{YOKOTA}が
短時間に認識できる文字数とその時間との関係から求めた認知単位は，とくに
平均長は述べられていないものの，例をみる限りわれわれの単位
より明らかに長い．同論文では「人は文を文字単位で処理しているのではない」
と結論しているが，加えて，分割できる最小単位の列として知覚されているのでも
ないということになる．
\par
  今後は，コーパスサイズをより大きくするとともに
句読点を削除した場合との比較・考察や，単語分割モデルの分割確率とポーズ位置との
関係 \cite{TAKEZAWA}，さらに上記で述べた分割の独立性について検討したいと考える．
\vspace{4mm}
\par
\par
  本研究にテキストデータ使用を許諾していただいた，産経新聞社，日本経
済新聞社，毎日新聞社（CD-毎日新聞91-95），そして(株)ピープルワールドカンパニーに
感謝いたします．
\bibliographystyle{jnlpbbl}
\bibliography{v06n2_01}
\begin{biography}
\biotitle{略歴}
\bioauthor{伊東 伸泰}{
1982年大阪大学基礎工学部生物工学科卒業．
1984年同大学院博士前期課程修了．
同年，日本アイ・ビー・エム（株）入社．
東京基礎研究所において文字認識，音声認識の研究に従事．
情報処理学会会員．
}
\bioauthor{西村 雅史}{
1981年3月大阪大学基礎工学部生物工学科卒業．1983年3月同大学院物理系博士前期課程修了．同年，日本アイ・ビー・エム（株）入社．
以来，同社東京基礎研究所において，音声認識などの音声言語情報処理の研究に従事．工学博士．平成10年情報処理学会山下記念研究賞受賞．情報処理学\break
会，日本音響学会，電子情報通信学会各会員．
}
\bioauthor{荻野 紫穂}{
1986年東京女子大学文理学部日本文学科卒業．1988年同大学院文学研究科修士課程修了．
同年，日本アイ・ビー・エム（株）入社．東京基礎研究所に勤務．
現在，音声認識システムの研究開発に従事．情報処理学会，人工知能学会，
計量国語学会各会員．
}
\bioauthor{山崎 一孝}{
1988年東京工業大学工学部情報工学科卒業．1990年同大大学院総合理工学研究科
システム科学専攻修士課程修了．
1993年同大大学院理工学研究科情報工学専攻博士課程修了．工学博士．
同年，日本アイ・ビー・エム（株）入社．
東京基礎研究所に勤務．文字認識，音声認識の研究および製品開発に従事．電子情報通信学会会員．
}
\bioreceived{受付}
\biorevised{再受付}
\bioaccepted{採録}

\end{biography}
\end{document}
