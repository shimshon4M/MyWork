



\documentstyle[jnlpbbl]{jnlp_j}

\setcounter{page}{39}
\setcounter{巻数}{8}
\setcounter{号数}{2}
\setcounter{年}{2001}
\setcounter{月}{4}
\受付{2000}{10}{18}
\再受付{2000}{12}{11}
\採録{2001}{1}{12}

\setcounter{secnumdepth}{2}

\title{常識的判断のための概念間の関連度評価モデル}
\author{渡部 広一\affiref{KUEE} \and 河岡 司\affiref{KUEE}}

\headauthor{渡部，河岡}
\headtitle{常識的判断のための概念間の関連度評価モデル}

\affilabel{KUEE}{同志社大学工学部知識工学科}
{Department of Knowledge Engineering \& Computer Sciences, Doshisha University}

\jabstract{
コンピュータに人間のような常識的判断を行わせるための主要素は，概念ベースおよび概念間の関連性に基づく概念連鎖機能であると考えられる．概念ベースは，自動学習などにより恒常的に拡張・精錬を行わなければならないために，その構造はできるだけ単純なものが望ましい．本論文では，概念間の関連度を評価するための新しい手法を提案している．従来の手法では，概念はその１次属性のベクトルモデルとして表現され，関連度はベクトル間の内積により求められている．そのような従来手法では，各１次属性をカテゴリーに変換しなければならないためシソーラスなどのカテゴリーデータベースが必要となる．提案手法では，関連度をカテゴリーを利用せず概念連鎖により求めている．約４万の概念よりなる概念ベースを用いた実験により，提案手法はベクトル内積を用いる方法に比べ正解率の面でやや優れる上に，概念知識の追加/変更が容易で利用を通じての質の向上が図れることを示した．}

\jkeywords{関連度，概念ベース，常識，概念連鎖}

\etitle{Measuring Degree of Association between Concepts \\
 for Commonsense Judgements}
\eauthor{Hirokazu Watabe \affiref{KUEE} \and Tsukasa Kawaoka \affiref{KUEE}} 

\eabstract{
It is thought that the main elements of commonsense judgement similar to human beings are a concept-base and the association mechanism based on the association between concepts. It is expected that the structure of the concept-base be as simple as possible since the concept-base has to be expanded and refined automatically by automated learning. This paper proposes a new method of measuring the degree of association between concepts. In the conventional method, a concept is expressed by first attributes vector model, and the degree of association between concepts is derived from an inner product of vectors. In this model, since each first attribute must be converted to its category, a category database such as a thesaurus is required. By the proposed method, the degree of association is derived using the chain of concepts without category. By experimental results using the concept-base, which consists of about 40,000 concepts, it is shown that the proposed method outputs the closer degree of association to that decided by human judgement than the conventional method. 
}

\ekeywords{degree of association, concept-base, commonsense, chain of concept}

\begin{document}
\maketitle



\section{はじめに}

人間はあいまいな情報を受け取り適宜に解釈して適切に会話を進めることができる．
これは，人間が長年にわたって蓄積してきた，言語やその基本となる語概念に関する「常識」を持っているからである．
すなわち，ある単語から概念を想起し，さらに，その概念に関係のある様々な概念を連想できる能力が重要な役割を果たしていると考えられる．

本研究の前提とする「常識的判断」とは，「女性−婦人」，「山−丘」などは同義・類義の関係，「山−川」，「夕焼け−赤い」などは密な関係，「山−机」，「電車−空」などは疎な関係であると判断するなど，語と語の意味的関係について，コンピュータにも人間の常識的な感覚に近い判断をさせることをねらうものである．
このような常識的判断を可能とするメカニズムは，利用者の意図を汲み取ることのできる人間的な情報処理システムの開発基盤として役立つと考えている．

我々が開発を進めている常識的判断システム全体は，日常的な事項，すなわち，大きさ，重さ，速さ，時間，場所等に関する基本的な知識\cite{Kikuyama,Obata}と感覚や感情に関する知識\cite{Baba,Hanada,Tsutiya}で構成する判断知識ベースサブシステムと本論文で対象とする語概念間の関連度を評価する概念連鎖メカニズムで構成している．
判断知識ベースを構成する知識は少数（約５千語）の代表的な語（代表語）の間の常識的な関係（事物の大小関係，夕焼け−赤いなど）を定義したものである．
常識的判断システムに入力される多くの語は代表語ではなく，知識ベースには陽に表現されていない未知語となるため概念連鎖メカニズムは，これらの未知語について，意味的関係やその強さの度合いを評価し，最も関連の強い代表語を決定する． 

本稿では，この概念連鎖メカニズムの基盤となる概念ベースの構造，すなわち，語とその意味を表す属性（関連の強い語）の集合の構成とそれを用いた概念間の関連度の定量化方式について提案している．
従来は一般に，概念間の類似性に重点が置かれ類似度として評価されているが，本稿では類似性のみならず「山と川」，「電車と駅」，「川と水」など概念間の幅広い関係の評価を対象とするため関連度として評価している．
例えば，類似性の評価において「車と馬」は乗り物という観点において類似しているという考え方がとられているが，本稿の関連度評価では，
両者の概念は乗り物という共通の属性をもっているに過ぎないと考え，
全体としての関連度はかなり低いものとなる．
当然，観点として乗り物が設定された場合の関連度は高くなる．
観点となる概念のもつ属性の範囲に限定した関連度を評価する\cite{Irie2}ことにより，類似や相対，反意などにも対応可能である．

概念間の類似度に関するテーマについては，幾つかの研究成果が報告されているが\cite{okada,oosuga,suzuki}，多くは，連想に関する理論，あるいは，自然言語処理における類似語の処理などの研究であり，本研究で対象とするような常識的判断のための概念ベースや概念関連度とは異なる．
概念ベースの構造や必要とされる正確さは目的により異なったものとなる．
我々の対象とする常識的判断システムの概念ベースは自動学習や利用者の教示による継続的な改善（成長）が前提となる．
常識的判断の適切さは概念ベースの内容と関連度計算方式に左右されるため，利用を通じた概念ベースの恒常的な成長の容易性は極めて重要な評価要因となる．

\cite{kasahara4}では，概念構造の定義と概念ベースの機械構築および概念類似度の計算方式について興味深い報告がなされている．
そこでは，一つの概念を，「意味特徴を表す属性」と「概念と属性の関連の深さを表す重み」で表現された$m$次元ベクトルとして取り扱い，２つの概念間の類似度は正規化された２ベクトルの内積として計算している．
このベクトル空間モデルでは，約４万の概念を約3千の独立性の高い属性で表現することによりベクトル表現のための直交性の問題に対処しているが，必ずしも直交性が保証されているとは言えない．
また，属性の重みの問題として，出現頻度に基づき重みが付与されているが，属性の追加／修正が発生した時，新しい属性の重みをどのように決定するのか，既に存在する属性の重みはどのように変更するのか，という問題が生じ，概念ベースの継続的な成長を前提とすることは難しい．

本稿では，これらの問題を考慮した上で，継続的な成長を容易とするような新たな概念ベースを構築し，常識的判断として適切な関連度を計算できるような関連度評価方式を提案し，実験により評価する．
以下，2章で，まず，概念連鎖メカニズムの実現に必要となる概念ベースの構造について述べ,
より単純な構造の概念ベースを提案する．
3章では，本稿の主題である概念関連度の定量化の問題を定式化し，概念の$n$次属性までの論理関係を考慮する新方式の提案を行う．
4章では，2,3章で提案した概念ベースと概念関連度計算方式の各組合わせについて評価実験を行い，人間の常識的判断により近いかという観点と，概念ベースの継続的な成長の容易性の観点において従来法との比較検討を行う．

\section{概念ベースの構造}

\subsection{概念ベース}\label{BasicCB}

本研究では\cite{kasahara1,kasahara4}で抽出された約４万の概念の利用を前提としており概念の定義は以下とする．

概念の定義：概念$A$はその属性$a_i$と重み$w_i$の対の集合とする．
\begin{eqnarray}
A &=& \{ (a_1, w_1), (a_2, w_2), \cdots, (a_m, w_m) \}
\end{eqnarray}
ここで，属性の直交性が仮定できるならば任意の概念は形式的に$m$次元属性空間のベクトルとして表現できる（ベクトルの大きさは1に正規化する）．
\begin{eqnarray}
A &=& (w_1, w_2, \cdots, w_m) \label{Vector} \\
  & & 0 \le w_i \le 1 \nonumber \\
  & & \sum_{i=1}^m w_i^2 = 1 \nonumber
\end{eqnarray}
ただし，$m$はすべての概念を定義するために必要な属性数である．

概念ベースはそれら概念の集合である．
以下に評価の前提とする３種の概念ベースについて述べる．
\begin{description}
	\item[基本概念ベース：] 約４万の概念Aとその属性$a_i$および重み$w_i$を複数の国語辞書などの語義文から自動的に獲得している．
辞書の見出し部の単語を概念とし，語義文に含まれる自立語を属性として抽出し，それらの重みは属性の出現頻度を基に付与している．
さらに，属性の自己参照による新たな属性の追加，及び不要な属性の統計的な除去からなる精錬を行うことによって概念ベースを機械構築している．
このようにして構築された概念ベースをその後に変更を加えた概念ベースと区別するため，基本概念ベースと呼び，そこで使われている属性を基本属性と呼ぶことにする．
（概念数：約４万，属性種別：約４万，一概念の属性数：平均約45，最大400）

	\item[圧縮概念ベース：] 基本概念ベースの各基本属性をALT-J/Eシソーラス\cite{ikehara}の約3千種の概念カテゴリーに分類圧縮し，これらを新属性として各属性の重みを正規化し直す．
この新属性の概念ベースを圧縮概念ベースと呼ぶ．
（概念数：約４万，属性種別：約３千，一概念の属性数：平均約70，最大533）

	\item[縮小概念ベース：] 基本概念ベースの各概念がもつ属性の中から重み順に30個を抽出することにより属性数を縮小し，属性の重みも正規化し直す．
これを縮小概念ベースと呼ぶ．
なお，属性数30という数は，\cite{irie}による最適な打ち切り属性数に関する実験結果によるものである．
 （概念数：約４万，属性種別：約４万，一概念の属性数：平均約26，最大30）
\end{description}

これらの３種の概念ベースは以下の特性を有している．
\begin{itemize}
\item 機械構築しているため概念の属性には少なからぬ雑音（属性としてふさわしくない単語）が含まれている．
\item 属性の重みは概念の語義文内での自立語の出現頻度に基づいており，概念と属性の意味的関連の強さを正しく表しているとは言えない．
\item 概念により，その属性数は異なっており属性数の不充分な概念も多々ある．
\end{itemize}

\subsection{概念ベースの利用形態} \label{DefCon}

３種の概念ベースのもつこれらの特性を考慮して，本研究では以下のような利用形態を評価する．
\begin{enumerate}
	\item  基本概念ベースでは属性の直交性を仮定できないので概念をベクトルとして表現することができず，関連度を２つの概念ベクトルの内積として定義できない．そのため，２つの概念の関連の強さを属性集合の近さ（一致する要素の数）で評価する．
	\item  約3千種の属性と重みで表現された圧縮概念ベースの場合には，カテゴリー圧縮により属性間の独立性がある程度高められていることから，各概念を約3千次元のベクトルとして扱うことができ，関連度は２つの概念ベクトルの内積として定義する．
	\item  概念の属性数を30個以下に縮小した縮小概念ベースにおいても属性の直交性を仮定できないので，２つの概念の関連の強さを集合としての近さで評価する．
\end{enumerate}

概念をベクトルとして取り扱う方式では，属性の重みは極めて重要な役割を果たすことから，新しい概念の追加や，既存の概念への属性の追加，修正に伴う重みの変更は困難となる．
また，属性圧縮により基本属性がもっていた詳細な意味情報が失われ，十分な評価精度が得られない恐れがある．
概念間の関連の度合いを属性集合の近さで評価する方式では，本稿で提案するような重みを使用しない評価方式や属性も概念としてとらえ，概念を属性の連鎖的関係で定義する方式も可能となる．
属性の重みを使用しない方式では，当然ながら，概念に対する新たな属性の追加時に重みの付与は不必要であり，また，明らかに不適切と思われる属性の削除も極めて容易となる．


\subsection{概念の連鎖的定義} \label{CBchain}

概念ベースにおける各概念は，ある単語の表記によってラベル付けできる．
また，基本概念ベースおよび縮小概念ベースでは，属性$a_i$もある単語表記である．
したがって，概念ベース中の任意の属性の単語表記が同じ概念ベース中の概念の単語表記中に存在すると仮定すると，属性$a_i$をその単語表記に対応する概念とみなすことで，属性の属性を取り出すことができる．
基本概念ベースと縮小概念ベースはこの条件を満たしているが，圧縮概念ベースはこの条件を満たしていない．

いま，単語表記を$Word_i$，概念の属性を$a_i$と表現すると，ある概念$A$は次式で定義される．
ただし，本節では重みは省略して記述する．
\begin{eqnarray}
\mbox{概念}A &=& Word_A \\
             &=& \{ a_1, a_2, \cdots, a_i, \cdots, a_N \} \\
             &=& \{ Word_1, Word_2, \cdots, Word_N \}
\end{eqnarray}
ここで，属性$a_i$を概念$A$の１次属性と呼ぶ．

$Word_i$はある概念と見なせるので，
\begin{eqnarray}
\mbox{1次属性}a_i &=& WORD_i \\
                  &=& \{ a_{i1}, a_{i2}, \cdots, a_{ij}, \cdots, a_{iM} \}
\end{eqnarray}
これらの属性$a_{ij}$（１次属性の１次属性）を概念$A$の２次属性と呼ぶ．

概念$A$を２次属性までの概念連鎖で定義すると以下のようにマトリックス状になる．

{\samepage
\begin{eqnarray}
  & & \left( \begin{array}{cccc}
         \hspace{0.2cm} a_1 \hspace{0.2cm}& a_2 \hspace{0.2cm}& \cdots \hspace{0.2cm}& a_{N} \hspace{0.1cm}
      \end{array} \right)	\nonumber \\
A &=& 
	\left[ \begin{array}{cccc}
		a_{11} & a_{21} & \cdots & a_{N1} \\
		a_{12} & a_{22} & \cdots & a_{N2} \\
		\vdots & \vdots & \vdots & \vdots \\
		a_{1M} & a_{2M} & \cdots & a_{NM} 
	\end{array} \right] 	\label{EMatrix}
\end{eqnarray}
}
ただし，式\ref{EMatrix}では，$a_{ij}$は概念$A$の２次属性であり，概念$A$の各１次属性の１次属性数は一定($M$)としている．

さらに，概念の2次属性もその1次属性の集合で表現でき，同様に概念$A$の$n$次属性まで定義可能である．
したがって，基本概念ベースおよび縮小概念ベースでは，概念$A$は$n$次までの属性の連鎖で定義されていることになる．

\section{概念関連度の評価モデル}


\subsection{概念の属性ベクトル空間モデル}

従来，概念関連度の評価は，２つの正規化された概念ベクトルの内積により行われている\cite{matsuzawa,kasahara4,salton,ishikawa,hokari}.
すなわち，式\ref{Vector}のように表現された概念ベクトル間の内積により計算できる．
しかし，この方式では各属性間の直交性を仮定しており，直交属性を選ぶことは容易ではなく\cite{takama}，また，適切な重みを設定することも非常に困難であると思われる．
\cite{kasahara4,ishikawa,hokari}ではALT-J/Eシソーラス\cite{ikehara}の約3千種の概念カテゴリーを属性として利用し，各属性の重みは，基本的には出現頻度により与えているが，十分な直交性を有しているか，また，適切な重みになっているか，あるいは，そのような適切な性質を持つような概念ベースへと自動的に精錬を行えるかが問題となる．

\subsection{概念の属性集合モデル}

\ref{CBchain}で示したように，本稿で提案する概念の定義では，任意の概念$A$はその属性の集合として定義されている．
また，各属性はある概念であるため，結果として任意の概念$A$は$n$次までの属性の連鎖で定義されている．
このような概念定義に対する関連度評価モデルを以下に述べる．

\subsubsection{1次属性集合の一致度}

各概念の属性には，その概念に関連する概念が並んでいるものと考えられるので，一致する属性数が多い程関連が強いと考えられる．
したがって，2つの概念$A$，$B$の関連度は，それぞれの1次属性同士の一致単語数を0から1の範囲に正規化したものとする．
すなわち，2つの概念$A$, $B$を１次属性$a_i, b_j$とその重み$u_i, v_j$を用いて，
\begin{eqnarray}
A &=& \{ (a_i, u_i) | i=1\sim L \} 	\label{ConA} \\
B &=& \{ (b_j, v_j) | j=1\sim M \}	\label{ConB}
\end{eqnarray}
と表現し，$a_i = b_j$なる$a_i$の個数を$s$個とするとき，概念$A$と概念$B$の一致度$Match(A,B)$を次式で定義する．
\begin{eqnarray}
Match(A,B) &=& (s/L + s/M)/2	\label{Ematch}
\end{eqnarray}
この式は，概念$A$から見たときの属性の一致割合$s/L$と概念$B$から見たときの一致割合$s/M$の平均を表しており，重み情報は無視している．
また，$L = M = N$（属性数が等しい）のとき，式\ref{Ematch}は，
\begin{eqnarray}
Match(A,B) &=& s/N	\label{EmatchN}
\end{eqnarray}
となる．

また，重み情報を利用する場合の一致度$MatchW(A,B)$を以下のように定義する．\begin{eqnarray}
	MatchW(A,B) &=& (s_A/n_A + s_B/n_B)/2 	\label{EmatchW} \\
	s_A &=& \sum_{a_i=b_j} u_i \nonumber \\
	s_B &=& \sum_{a_i=b_j} v_j \nonumber \\
	n_A &=& \sum_{i=1}^L u_i \nonumber \\
	n_B &=& \sum_{j=1}^M v_j \nonumber 
\end{eqnarray}
この式は，概念$A$から見たときの一致している属性の重みの割合$s_A/n_A$と概念$B$から見たときの一致している属性の重みの割合$s_B/n_B$の平均を表している．


\subsubsection{概念連鎖による関連度}

1次属性同士を比較する際に，単語の完全一致ではなく，その単語が表している概念としての一致度を利用することができる．
すなわち，1次属性同士の概念としての一致度は，それぞれの2次属性同士の一致単語数から導き，1次属性同士が単語としては一致していなくても，その一致度合いを見積もれるようにする方法である．

一致度を利用することにより，2つの概念間の関連度はそれぞれの1次属性同士の一致度の平均として定義できる．
ただし，一致度は0から1の範囲の実数であるため1次属性同士の対応関係が問題になってくる．
いま，ある1次属性$a_i$と相手のすべての1次属性$b_j(j=1, \cdots, M)$との一致度を計算したとき，$a_i$は一致度が最大の$b_j$に対応させるべきである．
しかし，同じことが他のすべての$a_i(i=1, \cdots, L)$にも言えるため，問題は複雑になる．
これは，1次属性同士を並べるときに，対応する1次属性間の一致度の合計が最大になるように並べ替える問題である．
このような並べ替え問題は，組み合わせ最適化問題の一種であり，要素数が多くなると組み合わせ爆発を起こすため，真の最適解を求めることはそれほど容易ではない．
しかし，\cite{ukita}で提案している「単純法」のように，単純な方法でも比較的最適解に近い値がでることから，本稿で行う実験では並び替え問題の部分には単純法を利用している．
単純法とは，最適化手法の欲張り法の一種で，一致度が最大のものを順に選択していく方法であり，その結果が最適解である保証はないが，比較的良好な解が得られるので，ここでの適用には十分であると判断している．
なお，より正確に一致度の合計が最大になるように並び替えたい場合は，遺伝的アルゴリズムなどを用いることができる\cite{ukita}．

以上の考察より，概念$A$と概念$B$との２次属性までの概念連鎖による関連度$Chain(A,B)$は以下に示すアルゴリズム(CNW)により評価する．

{\bf 概念連鎖による関連度評価アルゴリズム（CNW）}
\begin{enumerate}
\item 1次属性数の少ない方の概念を概念$A$とし（$L \le M$），概念$A$の1次属性の並びを固定する．
	\begin{eqnarray}
	A &=& (a_1, a_2, \cdots, a_L)
	\end{eqnarray}
\item 概念$B$の各1次属性を対応する概念$A$の各1次属性との一致度($Match$)の合計が最大になるように並べ替える．
ただし，対応にあふれた概念$B$の1次属性（$b_{x_j}, \ j=L+1, \cdots, M$)は無視する．
	\begin{eqnarray}
	B_x &=& (b_{x_1}, b_{x_2}, \cdots, b_{x_L})
	\end{eqnarray}
\item 概念$A$と概念$B$との関連度$Chain(A,B)$は，
	\begin{eqnarray}
	Chain(A,B) &=& (s/L+s/M)/2  \label{Echain} \\
	s &=& \sum_{i=1}^L Match(a_i, b_{x_i}) \nonumber
	\end{eqnarray}
である．
また，1次属性数が同じ場合（$L=M=N$）の関連度は，
	\begin{eqnarray}
	Chain(A,B) &=& s/N
	\end{eqnarray}
となる．
\end{enumerate}

アルゴリズムCNWでは重み情報を利用していない．
重み情報を利用した概念連鎖による関連度評価アルゴリズム(CW)は以下のようになる．

{\bf 重み付き概念連鎖による関連度評価アルゴリズム（CW）}
\begin{enumerate}
\item 1次属性数の少ない方の概念を概念$A$とし（$L \le M$），概念$A$の1次属性の並びを固定する．
	\begin{eqnarray}
	A &=& ((a_1, u_1), (a_2, u_2), \cdots, (a_L, u_L)) \nonumber \\
	\end{eqnarray}
\item 概念$B$の各1次属性を対応する概念$A$の各1次属性との重み付き一致度($MatchW$)の合計が最大になるように並べ替える．
ただし，対応にあふれた概念$B$の1次属性（$b_{x_j}, \ j=L+1, \cdots, M$)は無視する．
	\begin{eqnarray}
	B_x &=& ((b_{x_1}, v_{x_1}), (b_{x_2}, v_{x_2}), 
	\cdots, (b_{x_L}, v_{x_L})) \nonumber \\
	\end{eqnarray}
\item 概念$A$と概念$B$との関連度$ChainW(A,B)$は，
	\begin{eqnarray}
&&	ChainW(A,B) = (s_A/n_A + s_B/n_B)/2 \label{EchainW} \nonumber \\
	\\
&& \hspace*{1cm} s_A = \sum_{i=1}^L u_i MatchW(a_i, b_{x_i})  \nonumber \\
&& \hspace*{1cm} s_B = \sum_{i=1}^L v_{x_i} MatchW(a_i, b_{x_i}) \nonumber \\
&& \hspace*{1cm} n_A = \sum_{i=1}^{L} u_i \nonumber \\
&& \hspace*{1cm} n_B = \sum_{j=1}^{M} v_j \nonumber
	\end{eqnarray}
である．
ただし，$u_i, v_j$は，それぞれ属性$a_i, b_j$の重みである．
\end{enumerate}

アルゴリズムCWは，流れとしてはアルゴリズムCNWと同様であるが，1次属性同士の一致度の計算に重み付き一致度を用いる点と得られた一致度に重みを掛け合わせる点が異なる．

\small
\begin{figure}[tb]

	\begin{center}
	(a) １次属性

	\begin{tabular}{rrrrrrr} \hline
		机   & = & \{ & 学校, & 勉強, & 本棚 & \} \\
		椅子 & = & \{ & 勉強, & 教室, & 木   & \} \\ \hline
	\end{tabular}

	\vspace{0.2cm}

	(b) ２次属性

	\begin{tabular}{rrrrrrr} \hline
		学校 & = & \{ & 大学, & 生徒, & 木造 & \} \\
		勉強 & = & \{ & 勉学, & 鉛筆, & 成績 & \} \\
		本棚 & = & \{ & 勉学, & 書籍, & 壁   & \} \\
		教室 & = & \{ & 生徒, & 黒板, & 大学 & \} \\
		木   & = & \{ & 木造, & 樹木, & 曜日 & \} \\ \hline
	\end{tabular}

	\vspace{0.2cm}

	(c) 一致度マトリックス

	\begin{tabular}{r|rrr} \hline
		   & 学校      & 勉強    & 本棚    \\ \hline
	  勉強 &    0      & {\bf 1} &  1/3    \\
	  教室 & {\bf 2/3} &    0    &    0    \\
	  木   &  1/3      &    0    & {\bf 0} \\ \hline
	\end{tabular}
	
	\vspace{0.2cm}

	(d) 計算
	
	\begin{eqnarray}
		椅子_x &=& (教室, 勉強, 木) \nonumber \\
		s &=& 2/3 + 1 + 0 = 5/3 \nonumber \\
		Chain(机,椅子) &=& (5/3)/3 = 5/9 \nonumber
	\end{eqnarray}
	
	\end{center}
	
	\caption{関連度の計算例}
	\label{Eassoc}
\end{figure}
\normalsize

図\ref{Eassoc}に重みを利用しない場合の概念連鎖による関連度(CNW)の計算例を示す．
比較する対象概念を「机」と「椅子」とし，属性数を３個とした場合のそれぞれの(a)１次属性，および，１次属性の１次属性，すなわち，(b)２次属性の例である．
(c)一致度マトリックスは，概念「机」の各１次属性と概念「椅子」の各１次属性とのそれぞれの１次属性集合の一致度である．
たとえば，「学校」と「教室」の一致度は，「生徒」と「大学」が一致するので３個中２つが一致し，一致度は$2/3$である．
この一致度マトリックスから最大値を順に選んでいくと，太字で示した1, 2/3，0となり，「椅子」の１次属性の並びは(教室, 勉強, 木)となる．
したがって，一致度の合計は5/3となるので，関連度は5/9である．

\section{評価実験と考察}

関連度の性能は，使用する概念ベースと関連度計算方式の両方に左右されるが，本稿では概念ベース３通り（基本概念ベース，圧縮概念ベース，縮小概念ベース）に対して，以下に示す５通りの関連度計算方式の各組合せについて検討する．
\begin{description}
	\item[Match] 重み情報を利用しない１次属性同士の一致度 （式\ref{Ematch}）
	\item[MatchW] 重み情報を利用する１次属性同士の一致度 （式\ref{EmatchW}）
	\item[Chain] 重み情報を利用しない２次属性までの概念連鎖による関連度 （式\ref{Echain}）
	\item[ChainW] 重み情報を利用する２次属性までの概念連鎖による関連度 （式\ref{EchainW}）
	\item[Vector] ベクトル内積による関連度
\end{description}
ただし，使用する概念ベースによって利用できる関連度計算方式が限られてくるので，実際には表\ref{Case}に示す９通りについて評価する．
この中で，圧縮概念ベースに対するベクトル内積(Vector)が従来方式である．

\begin{table}[tb]
\caption[]{概念ベースと関連度計算方式}
\label{Case}
\begin{center}
\begin{tabular}{r|r|r|r}
     & 重み & １次属性             & ２次属性         \\ \hline
基本 & 利用 & 一致度(MatchW)       & 概念連鎖(ChainW) \\ \cline{2-4}
     & 無視 & 一致度(Match)        & 概念連鎖(Chain)  \\ \hline
圧縮 & 利用 & ベクトル内積(Vector) &                  \\ \hline
縮小 & 利用 & 一致度(MatchW)       & 概念連鎖(ChainW) \\ \cline{2-4}
     & 無視 & 一致度(Match)        & 概念連鎖(Chain)  \\
\end{tabular}
\end{center}
\end{table}


\subsection{評価法}

まず，以下のような４つの概念の組（サンプル概念）を準備する．

\begin{center}
(概念X $|$ 概念A \ 概念B \ 概念C)
\end{center}

ここで，概念Xは任意の概念（対象概念）であり，概念Aは概念Xと同義か類義の概念，概念Bは概念Xに密に関係する概念，概念Cは概念Xに疎な関係の概念である．
密な関係とは反意関係・対関係・上位下位関係・全体部分関係・兄弟関係など具体的な関係を定義できるものを指し，疎な関係とは具体的な関係を定義できないものを指す．
すなわち，対象概念Xに対してAが非常に関連が強く，Bが関連があり，Cはほとんど関連がない概念である．
$r_A$を概念Xと概念Aとの関連度，$r_B$を概念Xと概念Bとの関連度，$r_C$を概念Xと概念Cとの関連度とすると，
\begin{equation}
	r_A > r_B > r_C
\end{equation}
のとき，その関連度計算結果は正解であり，それ以外は不正解である．

次に，そのようなサンプル概念をどのように作成するのかが問題となるが，本研究では，人間の常識的判断に近いものほど良いと考えているので，サンプル概念の作成は人手によるものとした．
すなわち，被験者約30名に対して，サンプル概念(X, A, B, C)を20組以上作成してもらい，さらに，サンプル概念作成者以外の２人により各サンプル概念が正しいかどうか判断してもらい，１人でも正しいとは言えないと答えたサンプル概念は削除した．
したがって，３人中３人とも同じと判断したサンプル概念を抽出した．
以上のような過程を経て，合計559組のサンプル概念(da33-559)を準備した．
表\ref{Sample}に，準備したサンプル概念の一部を示す．

\begin{table}[tb]
\caption[]{サンプル概念（抜粋）}
\begin{center}
\begin{tabular}{r|rrr} \hline
概念X & 概念A & 概念B & 概念C \\ \hline
ご飯&	飯&	    米&	    青空\\
安易&	簡易&	気持ち&	経済\\
意図&	志向&	内心&	帰宅\\
飲料&	飲み物&	喉&     反省\\
羽	&   翼&     鳥&     返還\\
延期&	順延&	日程&	関連\\
演技&	芝居&	俳優&	灯油\\
演算&	計算&	処理&	芋\\
王女&	王妃&	王様&	一致\\
価格&	物価&	相場&	転職\\
河川&	川  &	対岸&	予想\\
火	&   炎	&   火事&	海\\
花	&   花弁&   花瓶&	弁別\\
過去&	以前&	歴史&	減額\\
会合&	集会&	集団&	現行犯\\
会話&	対話&	話	&   電車\\
回想&	回顧&	過去&	研修\\
海	&   海洋&   魚	&   机\\
絵画&	絵	&   画家&   文庫\\
獲得&	取得&	取捨&	類似\\ \hline
\end{tabular}
\end{center}
\label{Sample}
\end{table}

関連度評価方式の評価点は，全サンプル概念(559組)に対する正解率，すなわち，サンプル概念100組あたりのA,B,Cの順序正解個数とする．


\subsection{評価結果と考察}

以上で準備したサンプル概念を用いて，表\ref{Case}に示した９通りの方式（概念ベースと関連度計算方式）に対して，評価実験を行った．
実験結果を表\ref{Result1}に示す．

\begin{table}[tb]
\renewcommand{\arraystretch}{}
\caption[]{関連度評価方式の実験結果}
\label{Result1}
\begin{center}
\begin{tabular}{r|r|r}
概念ベース  & 関連度計算方式       & 正解率  \\ \hline
基本        & 一致度(Match)        &  75.1 \\ 
            & 一致度(MatchW)       &  77.8 \\ 
            & 概念連鎖(Chain)      &  79.8 \\
            & 概念連鎖(ChainW)     &  83.5 \\ \hline
圧縮        & ベクトル内積(Vector) &  79.4 \\ \hline
縮小        & 一致度(Match)        &  61.7 \\ 
            & 一致度(MatchW)       &  64.4 \\ 
            & 概念連鎖(Chain)      &  82.6 \\
            & 概念連鎖(ChainW)     &  84.3 \\ \hline
\end{tabular}
\end{center}
\end{table}

実験結果から，ChainW，Chain方式の正解率が従来のVector方式より高いことが分かる．
その中でも，基本概念ベースよりも縮小概念ベースを用いた方がより高い．
これは，基本概念ベースには多くの雑音（不適切な属性）が含まれているため，属性数を重みの大きい順に30個で打ち切ることにより，雑音をある程度除去できたことによる効果であると思われる．
ただし，MatchW，Match方式では，逆に，縮小概念ベースよりも基本概念ベースを用いた方が正解率が高い．
MatchW，Match方式は，１次属性のみしか用いないために，属性同士の一致確率が極めて低く，さらに属性数を30個に打ち切ってしまう縮小概念ベースでは，雑音の抑制効果よりも，属性同士の一致確率の減少がまさってしまうため，このような結果となったものと考えられる．
この点においても，概念を連鎖的に利用することにより，属性同士が完全に一致していなくても概念としての一致度を利用できるChainW，Chain方式が有効であることが確認できる．

圧縮概念ベースを用いたVector方式でも，属性数を30個で打ち切ると正解率が落ちる（正解率76.0\%）が，その落ちかたはMatchW，Match方式に比べて小さい．
これは，基本概念ベースおよび縮小概念ベースでは属性種別が約４万であるのに対し，圧縮概念ベースでは属性種別が約３千と少ないため属性同士の一致確率が比較的高いことによる効果と考えられる．
しかし，ChainW，Chain方式よりは劣っており，また，ChainW，Chain方式では縮小概念ベースを用いた方がより正解率が上がる．
したがって，関連度計算方式としてChainW，Chainを採用すれば，よりコンパクトな概念ベース（縮小概念ベース）で，より良い関連度を計算できることが分かる．

属性の重みを関連度計算に利用する場合と利用しない場合とを比較してみると，重みを利用した方がよりよい結果となっている．
このことは，辞書での出現頻度を基に付与した重み情報が，どの程度正しいかは不明ではあるが，有効であることを示している．
しかし，概念ベースを成長させて行くには，人間からの直接教示，電子新聞・書籍，インターネットを利用した文書収集などを通じて，概念や属性の追加・修正・削除を行っていく必要があり，その場合の適切な重み情報の付与は非常に困難である．
したがって，重み情報を利用しない計算方式であるChainの正解率が，ChainWに比べてそれほど劣っていないことは，注目に値する．
すなわち，単純に雑音的な属性は削除し良い属性は追加していくことで，概念ベースおよび関連度計算結果がより良いものになっていくであろうことが期待できる．
もちろん，適切な重み情報を付与することが可能ならば，ChainWによりより精度の高い関連度を求めることができる．

PentiumII 400MHzのパーソナルコンピュータで実行した場合，サンプル概念559組（1677回の関連度計算）に対する計算時間は，圧縮のVector，縮小のMatchおよびMatchWでは約12秒，縮小のChainおよびChainWでは約23秒である．
ChainやChainWでは，やや複雑な計算を行っているにもかかわらず倍程度の計算時間で済んでいる．
これは，関連度計算を行うためには，概念表記（単語）を基にその概念を概念ベースから検索する必要があり，概念ベースの概念数が約４万と多いために，概念の検索処理に多くの計算時間がかかるためである．
純粋な関連度計算の時間では，Vector, Match, MatchWの計算時間を１とすると，Chain，ChainWの計算時間は，縮小概念ベースの場合で30×30=900であるが，実際の利用においては検索処理は省略できないので，関連度計算時間は倍程度で済むようである．


\subsection{概念ベースへの属性追加実験}

従来の関連度計算方式Vectorでは，概念ベースを構築・拡張・精錬する際には，シソーラスなどの概念カテゴリーデータベースが必要であるのに対し，提案した関連度計算方式ChainW，Chainでは，シソーラスなどは不要である．
そのため，概念ベースの構造は単純なものとなり，拡張・精錬，すなわち概念ベースの成長が容易に行えるであろうことが予想される．
さらに，Chain方式では重み情報が不要なため，特に属性の追加は容易に行える．
そこでここでは，縮小概念ベース＋Chainに対して，概念への人手による適切な属性の追加実験を行い，概念ベースの成長の容易性と関連度性能の向上可能性を示す．

実験手順および結果は以下の通りである．
\begin{enumerate}
	\item 評価実験で用いたサンプル概念559組(da33-559)から，100組を抽出し(da33-100)評価実験を行う．
	その結果，正解率86\%，すなわち，不正解数は14であった．
	\item 不正解のサンプル概念組の各概念の関連度を見て，不当に低い概念に適切と思われる属性を人手により２〜６個追加する．
	\item 属性追加を行った概念ベースを用いて，サンプル概念100組(da33-100)に対して，評価実験を行う．
	その結果，正解率98\%となった．
	(属性追加を行っても正解とはならなかったサンプル概念が14組中２組あった．)
	\item 属性追加を行った概念ベースを用いて，サンプル概念559組(da33-559)に対して，評価実験を行う．
	その結果，正解率85.0\%となった．
	属性追加を行う前の概念ベースでは正解率82.6\%であり，正解数にすると462である．
	正解率85.0\%を正解数に換算すると475であり，正解数が13増えたことになる．
\end{enumerate}

Chain方式は，重みを計算には使用せずに概念の２次属性までを連鎖的に利用する方式である．
したがって，一つの概念Xの属性を修正すると，概念Xを属性として持つ多くの概念に影響がおよぶ可能性がある．
しかし，上の実験結果からは，そのような影響はプラス側にやや見られた程度であり，ほぼプラスマイナスゼロとみなせる．

以上，単純な属性追加実験によって，概念ベースを容易に成長させることが可能であることが分かった．

\section{おわりに}

コンピュータに常識的な判断能力をもたせるための第一歩として，本稿では，語と語の意味的関係の強さの評価に関し人間の常識的感覚による判断とできるだけ近い判断のできる概念連鎖メカニズムの実現手段を提案した．
具体的には，学習や利用者の教示により常に概念を更新できることを前提とする概念ベースの適切な構造とそれを用いた概念間の関連度の定量化方式を提案した．
従来の関連度定量化方式は，概念を属性の重みベクトルで表現し，概念の関連度は２つの重みベクトルの正規化されたベクトル内積により評価している．
ただし，ここでの属性はシソーラスなどを用いて，概念カテゴリーに置き換えておく必要があった．
本稿では，この従来方式と概念の関連度合いを属性集合の近さとして評価する提案方式を実験により比較した．

提案方式では，概念をその属性集合の２次元の連鎖的な集合（マトリックスで表現される）とみなし，２つの概念の対応する１次属性間での２次属性の一致度合いで評価している．
また，属性を概念カテゴリーに置き換える操作は不要であり，より単純な構造の概念ベースとなる．
実験は，機械構築した約4万語の概念ベースと複数の人間がそれぞれの常識的感覚に基づき作成した559組の評価サンプル概念を用いて行った．
結果として，提案方式は判断の的確性の点で，従来のベクトル方式よりも優れていることを示した．
また，提案方式で属性の重みを概念ベースの縮小化にだけ使い関連度計算では重みを使わない場合には，属性の追加・削減が容易となり，属性が学習や利用者の教示により継続的に改善され，判断の正確性は一層高いものとなることが期待できる．
これらの理由から，提案方式は常識的判断のための概念連鎖メカニズムの実現に適した方式であると言える．

\acknowledgment

本研究は文部省からの補助を受けた同志社大学の学術フロンティア研究プロジェクトにおける研究の一環として行った．


\bibliographystyle{jnlpbbl}
\bibliography{jpaper}

\begin{biography}

\biotitle{略歴}
\bioauthor{渡部 広一}{
1983年北海道大学工学部精密工学科卒業．
1985年同大学院工学研究科情報工学専攻修士課程修了．        
1987年同精密工学専攻博士後期課程中途退学. 
同年，京都大学工学部助手．                                            
1994年同志社大学工学部専任講師.
1998年同助教授．工学博士．                                
主に，CAD/CAM，進化的計算法，コンピュータビジョン，概念処理などの研究に従事.
言語処理学会，人工知能学会，情報処理学会，電子情報通信学会，システム制御情報学会，精密工学会各会員.
}
\bioauthor{河岡 司}{
1966年大阪大学工学部通信工学科卒業．
1968年同大学院修士課程修了．
同年，日本電信電話公社入社，情報通信網研究所知識処理研究部長，
NTTコミュニケーション科学研究所所長を経て，現在同志社大学工学部教授．
工学博士．
主にコンピュータネットワーク，知識情報処理の研究に従事．
言語処理学会，人工知能学会，電子情報通信学会，情報処理学会，IEEE(CS)各会員．
}

\bioreceived{受付}
\biorevised{再受付}
\bioaccepted{採録}

\end{biography}

\end{document}
