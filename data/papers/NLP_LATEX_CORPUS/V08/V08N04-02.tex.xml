<?xml version="1.0" ?>
<root>
  <title>統計的手法による分野非依存のテキスト分割</title>
  <author>内山将夫井佐原均</author>
  <jabstract>複数のトピックからなる文章を，それぞれのトピックに切り分けることをテキスト分割と呼ぶ．テキスト分割は，情報検索や要約のための基本技術として有用である．本稿では，分割確率最大化という観点からテキスト分割を定式化した．その定式化の特色の一つは，テキスト内の単語しか，確率推定に利用しないことである．そのため，提案手法は，任意の分野のテキストに対して適用できる．提案手法の有効性は二つの実験により確認された．まず，実験1では，公開データに対して提案手法を適用することにより，提案手法の分割精度が従来手法の分割精度よりも優れていることが示された．次に，実験2では，長い文書の元々の章や節の構造と提案手法による分割結果とを比較した結果，厳密な一致のみを正解とする場合，章には0.37,節には0.34の割合で一致し，±1行のずれを許容する場合，章には0.49,節には0.51の割合で一致した．これらのことは，提案手法が，テキスト分割に対して有効であることを示している．</jabstract>
  <jkeywords>テキスト分割，統計的手法</jkeywords>
  <subsection title="(W|S)の定義">区間S_iにn_i個の延べ単語があるとして，S_i中のj番目の単語をw^i_jとし，W_i=w^i_1w^i_2w^i_n_iとする．つまり，S_iとW_iとを一対一に対応させる．このようにすると，n=_i=1^mn_i，W=W_1W_2W_mである．このとき，ある区間に属する単語列は，その他の区間には独立に生起するとし，更に，同一区間に属する単語も，区間が与えられているという条件下では確率的に独立であるとすると，(W|S)&amp;=&amp;(W_1W_2W_m|S)&amp;=&amp;_i=1^m(W_i|S)&amp;=&amp;_i=1^m(W_i|S_i)&amp;=&amp;_i=1^m_j=1^n_i(w^i_j|S_i)eqnarrayである．この式の，2行目と3行目は，「ある区間に属する単語列は他の区間とは独立に生起する」という仮定から変形でき，最後の行は，「同一区間に属する単語は，その区間が与えられているという条件では，その他の単語と確率的に独立である」という仮定から変形できる．また，(W_i|S_i)は，区間S_iで単語列W_iが生起する確率であり，(w^i_j|S_i)は，区間S_iで単語w^i_jが生起する確率である．次に，W中における異なり単語の数をk，W_iにおいてw^i_jと同じ単語の数をf_i(w^i_j)とし，と定義する．ここで，()式は，ラプラス推定(Laplace'slaw)と呼ばれる確率推定式であるn_iと推定できるが，最尤推定の推定精度は，一般に，観測事象の数(この場合にはn_i)が大きくないと良くないことが知られており，観測事象の数が少ないときには，何らかのスムージングが必要である．ラプラス推定は，そのようなスムージング方法の一つである．たとえば，最尤推定によると，ある区間S_iに一回も出現しない単語の確率は，0n_i=0と推定されるが，ラプラス推定では，一回も出現しない単語についても，0+1n_i+k&gt;0の確率が割当てられる．．なお，f_i(w^i_j)は，厳密には，次式で定義される．f_i(w^i_j)g(w^i_j|w^i_1w^i_2w^i_n_i)eqnarrayg(w^i_j|w^i_1w^i_2w^i_n_i)_k=1^n_i(w^i_k,w^i_j).eqnarrayただし，については，単語aと単語bとが同じとき(a,b)=1，そうでないとき，(a,b)=0である．</subsection>
  <section title="はじめに">ある程度の長さの文章は，一般的に，複数のトピックからなる．そのような文章を切り分けて，それぞれの切り分けた部分が一つのトピックになるようにすることを，テキスト分割と呼ぶ．テキスト分割は，情報検索や要約などにおいて重要である．まず，情報検索においては，文書全体ではなく，ユーザの検索要求を満す部分(トピック)だけを検索した方が効果的である．また，要約においては，長い文書をトピックに分ければ，それぞれのトピックごとに要約を作成することにより，文書全体の要約を作成できるし，重要なトピックだけを選んで要約を作成することもできる．これらの目的のために，多くの手法が研究されている．これらの手法の主な共通点は，これらの手法が，分割対象のテキスト(および辞書やシソーラス)しか分割に利用しないことである．たとえば，は，テキスト内の単語分布の類似度しか分割に利用しない．言い換えれば，これらの手法は，その手法をテキスト分割に使用するにあたって，訓練データを必要としない．そのため，これらの手法は，訓練データの存在する分野に限られることなく，どんな分野の文章でも分割対象とすることができる．この点は重要である．なぜなら，情報検索や要約が対象とする文書は，分野を限定しない文書であるので，そのような文書に対応するためには，分野を限定しないテキスト分割の手法が必要であるからである．本稿で述べる手法も，これらの従来手法と同様に，訓練データを利用せずに，テキスト内の単語分布のみを利用してテキストを分割する．我々が，訓練データを利用しないテキスト分割手法を採用した理由は，我々が，テキスト分割の結果を利用して，長い文書を要約したり，講演のディクテーション結果を要約することを目的としているからである．そのためには，分野を限定しない(訓練データを利用しない)テキスト分割の方法が必要であるからである．本稿で述べる手法は，テキストの分割確率が最大となるような分割を選択するというものである．このようなアプローチは，分野を限定しないテキスト分割としては，新しいアプローチである．なお，従来の研究で，分野を限定しないテキスト分割の研究では，主に，語彙的な結束性を利用してテキストを分割している．その例としては，意味ネットワーク上での活性伝播に基づく結束性を利用するものや，単語分布の類似度(コサイン)を結束性としたものや，単語の繰り返し状況に基づいて結束性を計るものや，文間の類似度としてコサインを直接使うのではなくコサインの順位を結束性の指標とするものなどがある．なお，テキスト分割の方法としては，訓練データを利用しない(分野を限定しない)方法の他に，訓練データを利用する方法もある．そのような方法の応用としては，複数ニュースを個々のニュースに分割するものがある．この場合には，分野が明確であり，また，訓練データも多量にあるので，訓練データを利用したシステムにより，ニュースの境界を推定し分割する手法が主流である．しかし，そのような方法は，訓練データが利用できない分野については適用できないので，我々の目的である，テキスト分割の結果を利用して，長い文書を要約したり，講演のディクテーション結果を要約するためのテキスト分割手法としては適さない．以下，章では，テキスト分割のための統計的モデルを述べ，章で，最大確率の分割を選択するアルゴリズムを述べる．章では，まず，我々の手法を公開データに基づいて評価することにより，我々の手法が他の手法よりも優れた分割精度を持つことを示し，次に，我々の手法を長い文書に適用した場合の分割精度を述べる．章は考察，章は結論である．</section>
  <section title="テキスト分割のための統計的モデル">本章では，テキストの分割結果の確率を定義し，それを用いて最大確率であるような分割を定義する．そして，次章で，最大確率であるような分割を選ぶアルゴリズムを示す．本章では，テキストWが与えられたときに，その任意の分割Sに対して，条件付き確率(S|W)を定義する．(S|W)は，テキストWを条件とする分割Sの条件付き確率であるので，この値が最大の分割Sを選ぶことにより，Wが指定された場合の最大確率の分割Sを選ぶことができる．このような分割Sは，テキストWの本来の分割の推定として適当であると考えられる．まず，n個の延べ単語からなるテキストW=w_1w_2w_nが与えられたとき，m個の区間からなる分割S=S_1S_2_mの確率(S|W)は，である．ここで，(W|S)と(S)については，詳しくは，以下で定義するが，(W|S)は，分割Sが与えられたときに，テキストWが生起する確率であり，(S)は，分割Sの確率である．また，(W)は，テキストWの確率であるが，これは，Wが与えられているときには，定数であるから，最大確率の分割を求める際には無視できる．よって，最大確率の分割Sは，である．以下では，Sを最適分割と呼ぶことにする．次に，節で(W|S)を定義し，節で(S)を定義する．</section>
  <subsubsection title="分割確率最大化とMDL原理との関係">我々は，確率最大であるような分割を得るために，()式の右辺にあるを最大化しようとしているが，これは，を最小化しようとしていることと等価である．このことは，MDL原理の観点からは，分割Sが与えられたときのテキストWの記述長-(W|S)と，分割Sの記述長-(S)との和を最小化しようとしていることになる．なぜなら，一般に，ある事象Xの確率が(X)のときには，Xを記述(符号化)するために必要な最小記述長は-(X)であるからである．ただし，ここで，の底は2である．このように，最小記述長であるような分割を選択することと，最大確率であるような分割を選択することとは同等である．</subsubsection>
  <subsubsection title="記述長に基づく事前確率">以上の議論の逆から言えば，分割Sに対して，適当な記述長l(S)を割当てた場合には，その記述長を利用して，と定義できる(S)1となる．．なぜなら，l(S)=-(S)であるからである．つまり，分割Sの記述長を求めることにより，その事前確率を求めることができる．よって，以下では，分割Sの記述長を求めることにより，その事前確率を求めることにする．ここで，我々に，既に，分割対象のテキストが与えられているとすると，分割Sを指定するために必要な情報は，各区間の長さ，n_1,n_2,,n_mのみである．たとえば，我々に，既に，W=abcdefghiという長さが9のテキストが与えられていると仮定すると，そのテキストの分割を指定するためには，たとえば，2,3,3,1という4つの数字からなる数字列を指定すればよい．そうすれば，WをW=[ab][cde][fgh][i]のように4分割できる．つまり，我々は，m個の区間からなる分割を指定(記述)するためには，m個の数字を指定すれば良い．次に，これらの個々の数字は，1以上n以下のn個のうちの一つであることに注意すると，これらの個々の数字は，1/nの確率で選択されると考えることができるので，nの記述長で記述できる．よって，m個の数字を記述するためには，mnの記述長があれば良い．以上より，l(S)=mと計算できる．そのため，(S)はと定義できる．一般的にいって，(S)の値は，分割数が小さいほど大きな値を取る．一方，(W|S)の値は，分割数が大きいほど大きな値を取る．そのため，もし，分割を推定するのに，(W|S)だけを利用した場合には，推定される分割結果は，分割数が大きい分割，すなわち，細かすぎる区間からなる．それに対して，(S)と(W|S)の両方を利用した場合には，両者のバランスの取れた分割が得られる．</subsubsection>
  <section title="最適分割を選択するアルゴリズム">本章では，分割SのコストC(S)を，と定義し，このコストが最小となる分割S=_SC(S)を選択することにより，最大確率である分割Sを選択する．ここで，C(S)は以下のように展開できる．C(S)&amp;=&amp;-(W|S)(S)&amp;=&amp;-_i=1^m_j=1^n_i(w^i_j|S_i)+mn&amp;=&amp;_i=1^mc(w^i_1w^i_2w^i_n_i|n,k).eqnarrayただし，c(w^i_1w^i_2w^i_n_i|n,k)&amp;&amp;_j=1^n_in_i+kf_i(w^i_j)+1+n&amp;=&amp;_j=1^#(w^i_1w^i_2w^i_n_i)#(w^i_1w^i_2w^i_n_i)+kg(w^i_j|w^i_1w^i_2w^i_n_i)+1+n.eqnarrayここで，#()は，その引数である単語列の長さ(延べ単語数)である．なお，()式を，その最終行において，n_iやf_iを使わないで定義する理由は，次節で述べるアルゴリズムにおいて，()式を使うときの便宜を考えてのことである．次に，最小コスト分割(最大確率分割)であるSを求めるアルゴリズムを示す．</section>
  <subsection title="最小コスト分割を求めるアルゴリズム">まず，用語を定義する．延べ語数nのテキストW=w_1w_2_nにおいて，i番目の分割候補点g_iとは，単語w_iとw_i+1の間を言う．ただし，g_0はw_1の直前，g_nはw_nの直後である．このとき，分割候補点はg_0,g_1,,g_nのn+1個ある．また，分割候補点の集合をノード集合とするグラフを考えるとき，e_ij(0i&lt;jn)はg_iからg_jへの有向辺である．このように定義されたグラフの例を，図に示す．*1emこのとき，e_ijは，単語列w_i+1w_i+2w_jをカバーするという．e_ijは，テキストの，ある一区間w_i+1w_i+2w_jを表現している．そのため，e_ijのコストc_ijを，()式を利用することにより，次式で定義する．ただし，kは，W中の異なり単語数である．以上の準備の下で，最小コストを与える最適分割を求める手順は以下の通りである．ここで，Step2を効率的に解くアルゴリズムは良く知られている．なお，Step2は，全ての可能なパスの中での大域的な最小コストパスを求めるものであるが，そうする代りに，パスの長さを指定した最小コストパスを求めることもできる．そのようにして求められた最小コストパスは，区間数を指定した場合の最適分割に対応している．このようにして求めた最小コストパスについて，その各辺にカバーされる単語列を，それぞれ一つの区間とすると，それは最適分割である．たとえば，図で，e_01e_13e_35が最小コストパスであるとすると，最適分割は，[w_1][w_2w_3][w_4w_5]である．なお，実際にテキストを分割するときには，全ての分割候補点を考慮するのではなく，たとえば，文と文の間でのみテキストを分割したい場合がある．その場合には，分割位置として許される分割候補点の間にのみ有向辺を張るようにすれば良い．そして，そのグラフ上での最小コストパスを探索すれば良い．次節では，我々は，文間のみでテキストが分割されると仮定して議論している．</subsection>
  <subsection title="最小コスト分割よりも細かい分割をする際の問題点と解決策">前節で述べたように，グラフの最小コストパスを求めることにより，大域的な最小コストパスによる分割だけでなく，区間数を指定した最小コストパスによる分割を求めることもできる．しかし，予備実験の結果から，指定された区間数が，もし，大域的な最小コストパスにより求められる分割の区間数よりも，ある程度以上に大きいときには，1文や2文からなる小さい区間が生じやすいことが判った．このことは，大域的な最小コストパスによる分割のみが必要な場合，あるいは，大域的な最小コストパスによる分割よりも大雑把な分割が必要な場合には問題ではない．しかし，大域的な最小コストパスによる分割よりも細かい分割が必要なときには，問題である．そこで，我々は，大域的な最小コストパスよりも細かい分割が必要なときには，まず，文章全体を大域的な最小コストパスにより分割し，そのあとで，各々の区間を，その区間を一つの文章として，再帰的に分割することにした．このとき，各々の区間を分割するときの区間数は，その区間の長さの，全体の長さにおける割合に比例するようにした．たとえば，1000文からなる文章を20区間に分割したいときに，大域的な最小コストパスにより，200,400,300,100文からなる四つの区間が得られたときには，それぞれの区間を，4,8,6,2だけの区間に分割する．なお，分割数に余りがでるときには，その他の区間よりも大きい区間を，他よりも一つだけ余分に分割するようにした．たとえば，上述の文章を22に分割したいときには，それぞれを，4,8+1,6+1,2だけの区間に分割する．このようにすれば，1文や2文からなる小さい区間が生じにくいようにすることができる(W|S)+1(S)である．C(S)の第1項をデータのコストと呼び，第2項をモデルのコストと呼ぶことにする．一般に，データのコストは，分割が細かいほど小さくなり，モデルのコストは分割が細かいほど大きくなる．そして，最小コスト解は，これらのバランスがとれたところとなる．ところが分割を最小コスト解よりも細かくすると，モデルのコストがデータのコストよりも大幅に大きくなるため，分割の決定においてデータのコストが反映されにくくなり，妥当な分割が得られなくなる．一方，再帰的に分割したときには，再帰的な分割の対象となる各区間においては，()式のmもnも再帰的な分割をする前と比べて小さい値となるため，モデルのコストが小さくなる．そのため，モデルのコストとデータのコストのバランスが取れ，妥当な分割が得られやすくなる．以上をまとめると，．このプロセスは，必要な分割数が得られるまで再帰的に実行できるが，節で必要な，100程度までの分割数に対しては，1回だけの再帰で十分であった．なお，再帰的な分割の効果については，節で確認する．*5pt</subsection>
  <section title="実験">本章では，まず，実験1で，我々の手法を公開データに基づいて評価することにより，我々の手法が他の手法よりも優れた分割精度を持つことを示し，次に，実験2で，我々の手法を長い文書に適用した場合の分割精度を述べる．二つの実験の本稿全体における位置付けは以下の通りである．まず，実験1の目的は，提案手法と従来手法とを比較することにより，提案手法が，従来手法よりも，テキストを精度良く分割できることを示すことにある．そのため，もし，提案手法と従来手法とを比較したいだけならば，実験1のみで十分である．したがって，本稿の主要な目的である，提案手法の他の手法に対する優位性を示すためには，実験1だけで十分である．しかし，我々の最終的な目的は，テキスト分割の結果を，長い文書の要約や講演のディクテーション結果の要約に使うことであるので，その目的のために，提案手法が，どれほど役に立つかを調べたい．そのために，実験2においては，提案手法による分割が，どの程度，元の文書の章や節と一致するかを調べることにより，提案手法の，長い文書を要約するときへの応用の可能性を把握することを目的とする．そのため，実験2の位置付けは，今後我々の手法を実際の応用へと適用させるための前段階と考えている．我々は，将来的には，何らかのタスクに基づいて提案手法を評価することを考えている．</section>
  <subsection title="実験1：公開データによる評価">実験1で用いたデータは，により，各種のテキスト分割手法を比較するために用いられたデータであるchoif/software/C99-1.2-release.tgzより入手可能である．このパッケージを展開したときにできるnaacl00Exp/data/1,2,3/3-11,3-5,6-8,9-11/*を実験に用いた．．Choiは，彼の提案手法C99と，TextTiling，DotPlot,Segmenterを比較し，C99では，他の手法と比較し，誤り確率が半減されたと述べている．ただし，誤り確率とは，テキストを構成する単位(単語，文，パラグラフ等)について，任意に選んだr単位だけ離れた二つの単位が誤って分割される確率のことである．ここで，rは，正しい分割における各区間の長さの平均の半分が良いとされている．なお，実験1におけるrの単位は単語である．また，誤り確率が低いほど精度は良い．この実験データは，700個のテキストからなり，個々のテキストは，10個のテキスト断片を連結したものである．そして，それぞれのテキスト断片は，BrownCorpusからランダムサンプリングされたテキストの最初のs行である．個々のテキストは，sにより特徴付けられる．表には，実験データの諸元を示す．各テキストは，Choiのパッケージにあるライブラリを利用したstemmerにより正規化され，その正規化されたテキストが提案手法により分割された．ただし，分割可能な位置は，と同様に，文間のみである．その後，分割されたテキストの誤り確率は，Choiのパッケージにある評価プログラムにより計算された．その評価結果を表と表に示す．これらの表において，U00は，提案手法において，大域的な最小コスト分割を求めたときの評価結果であり，U00_(b)は，提案手法において，区間数を10に指定したときの評価結果である．また，C99は，Choiのアルゴリズムによる最適分割の評価結果であり，C99_(b)は，Choiのアルゴリズムにおいて区間数を10に指定した場合の評価結果であるの行にある数値は，のTable6のものと若干異なる．その理由は，元々の数値は500のサンプルテキストに基づいたものであるのに対して，この表のものは，700のサンプルに基づいて我々が再実験した結果だからである(Choi,personalcommunication)．なお，で使われた500サンプルにおけるC99_(b)の誤り確率は以下の表のものである．．また，二つの表において，**は，比較対象であるアルゴリズムの誤り確率がt検定により，有意水準1%で有意差があることを示す．なお，「3-11」などの列の数字は，それに該当するテキストにおける誤り確率の平均であり，「全体」は，全部のテキストについての誤り確率の平均である．これらの表から，提案手法が，C99あるいはC99_(b)と，同等あるいは，より精度良くテキストを分割できると言える．そして，C99あるいはC99_(b)は，分野非依存のテキスト分割手法のなかでは，その他の従来手法よりも精度良くテキストを分割できるので，我々の提案手法が，従来手法よりも精度良くテキストを分割できることが言える．</subsection>
  <subsection title="実験2：長い文書の章や節との一致度による評価">実験2では，比較的長い文章を分割し，その分割結果と元々の章や節による分割とを比較することにより，提案手法を評価した．実験に用いたデータは，文部省年報よりダウンロードできる．である．我々がこのデータを用いた理由は，それが公開されているということに加えて，SGMLでタグ付けされているため，付録に示す簡単なPerlスクリプトにより章(chapter)や節(section)を切り出せるためである．表には，実験に用いた文部省年報の諸元を示す．表で，章や節の数は，元のファイルでの章や節の数を数えたものであるが，ページ数は，我々が，元テキストをポストスクリプトファイルに変換して数えたものであるので，一応の目安と考えておくのが良い．表に示す文部省年報には，以下の前処理が加えられた．まず，付録のスクリプトを用いて，章や節を切り出した結果から，分割位置を示す記号を除いたテキストを得た．次に，そのテキストに対して，ChaSenversion2.25を適用し，その結果から，ChaSenの品詞体系における「名詞」「未知語」「記号-アルファベット」「接頭詞」に該当するもののみを抽出し，提案手法への入力とした．ただし，名詞のうちで，その下位分類が「数」「代名詞」「非自立」「特殊」「接続詞的」「動詞非自立的」に該当するものは除いた．また，平仮名だけからなる形態素も除いた．なお，このときの分割可能な位置はスクリプトの出力結果の各行の終りである．これは，段落の間で分割していることに相当する．表，表，表には，このようにして得られたテキストを，分割対象の章や節の数を指定して，分割したときの精度を示す．ここで，表のタイトルに付記している再帰的分割とは，節で述べた再帰的方法により分割した場合を示し，非再帰的分割とは，再帰的分割をしていない場合を示す．また，精度とは，元の章や節と一致した分割位置の数章や節の分割位置の総数displaymathである．ただし，章や節の数をnとすると，分割位置の数は，n-1である．なお，本実験で分割数を指定している理由は，本実験の目的が，指定された粒度の分割をどの程度の精度で実現できるかを調べることにあるからである．粒度を指定した分割は，長い文書から，必要に応じた長さの要約を得るときに重要である．これらの表において，「±0の精度」とは，システムによる分割位置が，元文書の分割位置と正確に同一な場合を一致としたときの精度である．また，「±1の精度」とは，正確に同一な場合に加えて，前後1行のずれまでを許容して一致としたときの精度である．なお，それぞれの精度を示す列において，括弧内の数値は，精度のベースラインである．ただし，本実験の場合には，サイズは行数でカウントする．まず，表における，章の数を分割数としたときの分割精度を見る．表では，±1の精度の平均が0.49であり，±0の精度の平均が0.37である．ここで，では，英文テキスト4文書について，彼の手法による分割結果が，平均0.25の精度で章の区切れと一致することを述べていて，では，ベースラインが0.005〜0.01のとき，F値再現率+適合率)は，のTable3から計算で求めた．が，0.31〜0.39である．これらの結果は，±0の精度に対応するが，テキストが違うため，直接比較することは不可能である．しかし，数値だけを比較するならば，我々の方法は，章の分割に関しては，これらの方法と比べて，少なくとも同等程度に章の区切れを再現していると考える．次に，節の数を指定したときの分割精度を表と表に示す．また，表には，最小コスト解による分割数と章や節の数とを示す．表は，再帰をせずに，分割数を指定して分割したときの精度を示している．このとき，±1の精度の平均が0.31であり，±0の精度の平均が0.14である．一方，再帰的分割をしたときには，表に示すように，±1の精度の平均が0.51であり，±0の精度の平均が0.34である．これから分かるように，最小コスト解よりも粒度の細かい分割が必要なときには，再帰的分割をした方が精度良く分割ができる．なお，では，ベースラインが0.035のときにF値が0.29であるので，再帰的分割による方法は，と比べて，少なくとも同等程度に節の区切れを再現していると考える．*20pt</subsection>
  <section title="考察と今後の課題">提案手法は，分割確率最大化という観点からテキスト分割を定式化した．これに類似の手法として，訓練データを利用したテキスト分割では，が隠れマルコフモデルに基づいて，複数ニュースを個々のニュースに分割しているが，訓練データを利用しないテキスト分割では，類似の研究はない．また，についても，彼等は，テキストの分割確率を直接扱っているのではなく，各単語を生起させるようなトピックを単語毎に求め，同一トピックの単語が連続する部分を同一トピックとする，という間接的アプローチをとっている．そのため，彼等のアプローチでは，たとえば，トピックの平均の長さなどを直接取り込むことが難しい．一方，我々のアプローチでは，このことは素直に表現できる．たとえば，と同様に，トピックの長さxが，平均長，標準偏差の正規分布N(x|,)に従うと仮定すると，単純な拡張としては，()式を，++=1として，以下のようにすれば，トピックの長さが平均と同じくなるような分割が優先される．c(w^i_1w^i_2w^i_n_i|n,k,,,,,)&amp;=&amp;_j=1^#(w^i_1w^i_2w^i_n_i)#(w^i_1w^i_2w^i_n_i)+kg(w^i_j|w^i_1w^i_2w^i_n_i)+1+n&amp;&amp;+1N(#(w^i_1w^i_2w^i_n_i)|,).eqnarray更に，彼等の手法と我々の手法との大きな違いは，彼等が単語の確率を訓練データから推定しているのに対して，我々は，単語の確率を分割対象のテキストから推定している点である．なお，訓練データが利用可能な場合に，彼等の手法と我々の手法とを比較することは興味深いであろう．その場合には，上式で示したような，トピックの長さをコスト関数として取り込むことや，種々の手がかり表現をコスト関数に取り込むことも検討したい．次に，提案手法のテキスト分割における特徴としては，節で述べたように，長い文章でも短かい文章でも，分割数が，大幅には変動しないというものがある．これは，短かい文章は，細かい粒度で分割し，長い文章は大雑把な粒度で分割するということである．この性質は，我々がテキスト分割をする目的が要約のため，という観点からは適した性質である．なぜなら，要約では，文章の長さに関わらず，それを適当に少ないトピックにまとめる必要があるので，分割の結果得られる区間数は，文章の長さに，それほど影響されない方が望ましいからである．しかし，応用によっては，任意に指定した粒度の分割が望ましい場合もあると考えられる．そのために，我々は，本稿では，大域的な最小コスト解よりも細かい分割が必要な場合には，再帰的な分割を適用し，それは有効ではあったが，より有効な分割方法を考えることは今後の課題としたい．そのための見込みのある方法の一つは，で提案されているように，分割したい粒度に応じて窓の大きさを設定し，その窓内を一つの文章としてテキストを分割することである．最後に，提案手法によると，テキストの分割の結果として，テキストの各区間における単語の確率(密度)が自然に求まる．このような密度は，重要単語の抽出や，重要説明箇所の特定に有用であることが知られている．提案手法を，このようなアプリケーションに対して適用することも興味深い．</section>
  <section title="おわりに">我々は，本稿において，分割確率最大化という観点から，テキスト内の情報のみを用いて，テキストを分割する手法を提案した．提案手法は，従来の手法と比べて，同等以上の精度でテキストを分割することができた．このことは提案手法がテキストの分割に有用であることを示している．我々は，今後，実際の応用におけるテキスト分割の有効性を調べることを考えている．</section>
</root>
