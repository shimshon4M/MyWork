



\documentstyle[epsf,jnlpbbl]{jnlp_j}

\setcounter{page}{127}
\setcounter{巻数}{8}
\setcounter{号数}{1}
\setcounter{年}{2001}
\setcounter{月}{1}
\受付{2000}{5}{26}
\再受付{2000}{8}{31}
\採録{2000}{10}{10}

\setcounter{secnumdepth}{2}

\title{最大エントロピーモデルに基づく形態素解析\\
  \q\p ---未知語の問題の解決策---}
\author{内元 清貴\affiref{CRL} 
  \and 関根 聡\affiref{NYU} \and 井佐原 均\affiref{CRL}}

\headauthor{内元 清貴・関根 聡・井佐原 均}
\headtitle{最大エントロピーモデルに基づく形態素解析---未知語の問題の解決策---}

\affilabel{CRL}{郵政省通信総合研究所}
{Communications Research Laboratory, Ministry of Posts and Telecommunications}
\affilabel{NYU}{ニューヨーク大学 コンピュータサイエンス学科}
{Computer Science Department, New York University}

\jabstract{
形態素解析は日本語解析の重要な基本技術の一つとして認識されている．
形態素解析の形態素とは，単語や接辞など，文法上，最小の単位となる要素
のことであり，形態素解析とは，与えられた文を形態素の並びに分解し，
それぞれの形態素に対し文法的属性(品詞や活用など)を決定する処理のことである．
近年，形態素解析において重要な課題となっているのは，
辞書に登録されていない，あるいは学習コーパスに現れないが
形態素となり得る単語(未知語)をどのように扱うかということである．
この未知語の問題に対処するため，これまで大きく二つの方法がとられてきた．
一つは未知語を自動獲得し辞書に登録する方法であり，
もう一つは未知語でも解析できるようなモデルを作成する方法である．
ここで，前者の方法で獲得した単語を辞書に登録し，
後者のモデルにその辞書を利用できるような仕組みを取り入れることができれば，
両者の利点を生かすことができると考えられる．
本論文では，最大エントロピー(ME)モデルに基づく形態素解析の手法を提案する．
この手法では，辞書の情報を学習する機構を容易に組み込めるだけでなく，
字種や字種変化などの情報を用いてコーパスから未知語の性質を学習することも
できる．我々はこの手法により未知語の問題が克服される可能性が高いと考えている．
京大コーパスを用いた実験では，再現率95.80\%，適合率95.09\%の精度が得られた．
}

\jkeywords{形態素解析，最大エントロピー(M.E.)モデル，未知語，辞書}

\etitle{Morphological Analysis Based on A Maximum\\
  Entropy Model --- An Approach to The Unknown\\
  Word Problem ---}
\eauthor{Kiyotaka Uchimoto\affiref{CRL} 
  \and Satoshi Sekine\affiref{NYU} \and Hitoshi Isahara\affiref{CRL}} 

\eabstract{
Morphological analysis is one of the basic techniques 
used in Japanese sentence analysis.
A morpheme is defined as the minimal grammatical unit 
such as a word or a suffix. 
Morphological analysis is the process segmenting a given sentence into 
a row of morphemes and assigning to each morpheme grammatical attributes 
such as a part-of-speech (POS) and an inflection type. 
Recently, one of the most important issues in morphological analysis 
has become how to deal with unknown words, or words which are not found 
in a dictionary or a training corpus. So far, there have been mainly 
two statistical approaches for coping with this issue. 
One is the method of acquiring unknown words from corpora 
and incorporating them into a dictionary. 
The other is the method of estimating a model 
which can recognize unknown words correctly. 
We would like to be able to make good use of both approaches.
If words acquired by the former method could be added to a dictionary 
and a model developed by the latter method 
could consult the amended dictionary, 
then the model could be the best statistical model 
which has the potential to overcome the unknown word problem. 
In this paper, we propose a method 
for Japanese morphological analysis based on a maximum entropy (M.E.) model. 
This method uses a model which can not only consult a dictionary 
with a large amount of lexical information but also 
recognizes unknown words by learning certain characteristics. 
We focused on the information such as what types of characters are used 
in a string in order to learn these characteristics. 
The model has the potential to overcome the unknown word problem.  
The recall and precision of the identification of a morpheme segment
and its major parts-of-speech were 95.80\% and 95.09\%, respectively, 
when using the Kyoto University corpus. 
}

\ekeywords{morphological analysis, maximum entropy (M.E.) model, 
  unknown words, dictionary}

\def\q{}
\def\p{}

\begin{document}
\thispagestyle{plain}
\maketitle


\section{はじめに}
\label{sec:introduction}

形態素解析は日本語解析の重要な基本技術の一つとして認識されている．
形態素解析の形態素とは，単語や接辞など，文法上，最小の単位となる要素
のことであり，形態素解析とは，与えられた文を形態素の並びに分解し，
それぞれの形態素に対し文法的属性(品詞や活用など)を決定する処理のことである．

近年，形態素解析において重要な課題となっているのは，
辞書に登録されていない，あるいは学習コーパスに現れないが
形態素となり得る単語(未知語)をどのように扱うかということである．
この未知語の問題に対処するため，これまで大きく二つの方法がとられてきた．
一つは未知語を自動獲得し辞書に登録する方法(例えば\cite{Mori:96}など)であり，
もう一つは未知語でも解析できるようなモデルを作成する方法(例えば
\cite{Kashioka:97,Nagata:99}など)である．
ここで，前者の方法で獲得した単語を辞書に登録し，
後者のモデルにその辞書を利用できるような仕組みを取り入れることができれば，
両者の利点を生かすことができると考えられる．
森らはn-gramモデルに外部辞書を追加する方法を提案している\cite{Mori:98}．
ある文字列が辞書に登録されている場合にその文字列が形態素となる確率を
割り増しするような方法である．
しかし，わずかな精度向上に留まっていることから，n-gramモデルでは辞書の情報を
利用する仕組みを容易に組み込むのは難しいのではないかと考えられる．
本論文では，最大エントロピー(ME)モデルに基づく形態素解析の手法を提案する．
この手法では，辞書の情報を学習する機構を容易に組み込めるだけでなく，
字種や字種変化などの情報を用いてコーパスから未知語の性質を学習することも
できる．
ここで辞書の情報とは，辞書に登録されている語が複数の品詞をとり得る場合に
どの品詞を選択するべきかといった情報を意味する．
京大コーパスを用いた実験では，再現率95.80\%，適合率95.09\%の精度が得られた．
本論文では，辞書の情報を用いない場合，未知語の性質を学習しない場合
についても実験し，それぞれの精度に及ぼす影響についても考察する．

\section{形態素モデル}
\label{sec:model}

この章では形態素としての尤もらしさを計算するモデルについて述べる．
我々はこのモデルをMEモデルとして実装した．

テストコーパスが与えられたとき，
そのコーパスの各文を形態素解析するという問題は
文を構成する各文字列に二つのタグのうち一つ，
つまり，形態素であるかないかを示す「1」か「0」を割り当てる問題
に置き換えることができる．
さらに，形態素である場合には文法的属性を付与するために「1」を
文法的属性の数だけ分割する．
すると，文法的属性の数が$n$個のとき，各文字列に「0」から「$n$」までのうち
いずれかのタグを割り当てる問題に置き換えることになる．
形態素解析の問題において，
この$n+1$個のタグはMEモデルを定式化するときに
「未来(futures)」空間を形成する．
ここで，未来空間とは学習モデルにおける分類先に対応する．
MEモデルでは他の類似したモデルと同様に，
可能性のある未来空間$F$における任意の$f$と可能性のある履歴空間$H$に
おけるすべての$h$に対して確率分布$P(f|h)$を計算することができる．
ここで，MEモデルにおける「履歴(history)」とは
未来空間においてどこに分類するかという判断を下す根拠となるデータのことである．
形態素解析の問題における確率分布は次の式で表すことができる．
\begin{eqnarray*}
  \label{eq:ex:p}
  P(f|h_t) \ = \ P(f|テストコーパスから関係tに関して導出可能な情報)
\end{eqnarray*}
これは，テストコーパスからある関係$t$に関して導出可能な情報が得られたときに
$f$の確率が求まることを示している．
MEモデルにおける確率分布$P(f|h)$の計算は
「素性(features)」の集合，つまり，未来を予測する助けとなる情報に依存する．
この情報は素性関数として定義され，
近年の計算言語学の研究で用いられてきた他の多くのMEモデルと同様に
我々のモデルでも，履歴と未来を引き数とし0か1を返す2値関数として定義する．
以下にその一例をあげる．
\begin{eqnarray}
  \label{eq:g}
  g(h,f) & = & 
  \left\{
  \begin{array}[c]{l}
    1\ :\ {\rm if\ has}(h,x) = {\rm true},\\
    \q\q x = {\rm ``品詞(-1)(Major):動詞''}\p \\  
    \q\p \&\ f = 1 \\
    0\ :\ {\rm otherwise.}
  \end{array}
  \right.
\end{eqnarray}
ここで，「has($h$, $x$)」は履歴$h$に素性$x$が観測されるときに真を返す
2値関数である．
我々の場合，素性としては辞書の情報
\footnote{\baselineskip=0.7\baselineskip
  今回の実験では既存の辞書の情報のみを用いたが，
  自動獲得した辞書の情報も利用可能であると考えている．}
とともに，未知語の性質を学習できるように，
着目している文字列の長さや文字種，その文字列が辞書にあるかどうか，
連接する形態素の文法的属性，文字種の変化などを用いる．
詳しくは\ref{sec:exp_discussion}~章で述べる．

素性集合と学習データが与えられたとき，
エントロピーを最大にするという操作によりモデルが生成される．
このモデルではすべての素性$g_i$に対しパラメータ$\alpha_i$が関係付けられ，
モデルは次のような条件付き確率として表される\cite{berger:cl96}．
\begin{eqnarray}
  \label{eq:p}
  P(f|h) & = & \frac{\prod_{i}\alpha_{i}^{g_{i}(h,f)}}{Z_{\lambda}(h)} \\
  Z_{\lambda}(h) & = & \sum_f \prod_{i}\alpha_{i}^{g_{i}(h,f)}
\end{eqnarray}
パラメータを推定する際には，
学習コーパスにおけるすべての素性$g_i$に対し，
MEモデルから計算される$g_i$の期待値が
$g_i$の経験的期待値と等しくなるようにする．
つまり，以下の式を成り立たせるようなパラメータを推定する．
\begin{eqnarray}
  \label{eq:constraint}
  \sum_{h,f}\tilde{P}(h,f)\cdot g_{i}(h,f) 
  \ = \ \sum_{h}\tilde{P}(h)\cdot \sum_{f}P_{ME}(f|h)\cdot g_{i}(h,f)
\end{eqnarray}
ここで，$\tilde{P}$は経験的確率分布であり，$P_{ME}$ はMEモデル
として推定される確率分布である．

形態素に付与するべき文法的属性が$n$個あると仮定する．
文法的属性としては品詞と文節区切りを考える．品詞が$m$個の場合，その各々に
ついてその品詞を付与した形態素の左側が文節区切りであるかないかを考慮し，
文法的属性の数は$n=2\times m$ とする．
文字列が与えられたとき，その文字列が形態素であり，
かつ$i$$(1\leq i \leq n)$番目の文法的属性を持つとしたときの
尤もらしさを確率値として求めるモデルを形態素モデルと呼ぶ．
このモデルは式(\ref{eq:p})を用いて表される．
ここで，$f$は0から$n$までの値をとる．

一文が与えられたとき，一文全体で確率の積が最大になるよう形態素に分割し
文法的属性を付与する．最適解の探索にはビタビアルゴリズムを用いる．
N-best解の探索には文献\cite{Nagata:94}の方法を用いる．

\section{実験と考察}
\label{sec:exp_discussion}

\subsection{実験の条件}
\label{sec:exp_condition}

品詞体系はJUMAN\cite{juman3.61}のものを仮定した．
品詞は細分類まで考慮すると全部で53種類ある．
これに文節区切りを考慮すると推定するべき文法的属性の数は倍の106種類となる．
活用型，活用形は品詞が決まれば表記からほぼ一意に決めることができるので，
モデルから確率的に推定することはしない．
したがって，式(\ref{eq:p})の$f$は$0$から$106$までの107個の値をとるものとする．

実験には，京大コーパス(Version 2)
\cite{kurohashi:nlp97} を用いた．
学習には1月1日と1月3日から8日までの7日分(7,958文)，
試験には1月9日の1日分(1,246文)を用いた．

一文が与えられると，
5文字以下のすべての文字列および5文字を越えるが辞書に登録されている文字列
に対して，その文字列が形態素であるかないか，形態素である場合には
その文法的属性が何かを推定する．
5文字以下のすべての文字列としたのは，
5文字を越えるような形態素は大抵，複合語あるいはカタカナ語であり，
辞書に登録されていなければ，ほとんどの場合形態素ではないためである．
複合語は辞書に登録されているもの以外は5文字以下の文字列に分割できる
と仮定する．また，カタカナ連続は辞書に登録されていない場合，
ひとまとまりにして「未定義語(大分類)，カタカナ(細分類)」
という品詞を持つものとして辞書に登録されていたものとして扱う．
ビタビアルゴリズムを用いて最適解を探索する際には，
JUMANで定義されている連接規則を満たさなければならないという制約を加えた．

\ref{sec:model} 章に述べたモデルでは，
各文字列に対し品詞を付与する際，すべての品詞候補(53種類)のうち
一文全体の確率を最大にするものが選ばれる．
このとき，必ずしも辞書に記述されている品詞が選ばれるとは限らない．
そこで，辞書に登録されている文字列については，
その文字列に付与可能な品詞がすべて辞書に記述されていると仮定し，
各文字列に対し品詞を付与する際には，辞書に記述されている品詞の中から
選択するという制約を加える．

\begin{table*}[htbp]
  \scriptsize
  \begin{center}
    \caption{学習に利用した素性}
    \label{table:feature}
    \leavevmode
    \begin{tabular}[c]{|c|l|p{4cm}|c|c|c|}
      \hline
      素性 & & & \multicolumn{3}{c|}{削除した時の精度}\\
      \cline{4-6}
      番号 & \multicolumn{1}{c|}{素性名} & \multicolumn{1}{c|}{素性値} 
      & 再現率 & 適合率 & F \\
      \hline
      \hline
      1 & 文字列(0) & (4,331個) & 93.66\% & 93.81\% & 93.73\\
      2 & 文字列(-1) & (4,331個) & ($-$2.14\%) & ($-$1.28\%) & ($-$1.71)\\
      \hline
      3 & 辞書(0)(Major) & 動詞\, 動詞\&連語\, 形容詞\, 形容詞\&連語 
      $\ldots$ (28個) & 94.64\% & 92.87\% & 93.75\\
      4 & 辞書(0)(Minor) & 普通名詞\, 普通名詞\&連語\, 副助詞 
      $\ldots$ (90個) & ($-$1.16\%) & ($-$2.22\%) & ($-$1.69)\\
      5 & 辞書(0)(Major\&Minor) & 名詞\&普通名詞\, 名詞\&普通名詞\&連語 
      $\ldots$ (103個) & & & \\
      \hline
      6 & 長さ(0) & 1 2 3 4 5 6以上 (6個) & 95.52\% & 94.11\% & 94.81\%\\
      7 & 長さ(-1) & 1 2 3 4 5 6以上 (6個) 
      & ($-$0.28\%) & ($-$0.98\%) & ($-$0.63)\\
      \hline
      8 & 文字種(0)(頭) & 漢字 平仮名 記号 数字 カタカナ アルファベット (6個)
      & 95.17\% & 93.89\% & 94.52\%\\
      9 & 文字種(0)(末尾) 
      & 漢字 平仮名 記号 数字 カタカナ アルファベット (6個) 
      & ($-$0.63\%) & ($-$1.20\%) & ($-$0.92)\\
      10 & 文字種(0)(変化) & 漢字$\rightarrow$平仮名\, 
      数字$\rightarrow$漢字\, カタカナ$\rightarrow$漢字 $\ldots$ (30個) 
      & & & \\
      11 & 文字種(-1)(末尾) & 漢字 平仮名 記号 数字 カタカナ アルファベット 
      & & & \\
      12 & 文字種(-1)(変化) & 漢字$\rightarrow$平仮名\, 
      数字$\rightarrow$漢字\, カタカナ$\rightarrow$漢字 $\ldots$ (30個) 
      & & & \\
      \hline
      13 & 品詞(-1)(Major) 
      & 動詞 形容詞 名詞 助動詞 接続詞 未定義語 $\ldots$ (15個) 
      & 95.60\% & 95.31\% & 95.45\%\\
      14 & 品詞(-1)(Minor) 
      & 普通名詞 サ変名詞 数詞 程度副詞  $\ldots$ (45個) 
      & ($-$0.20\%) & ($+$0.22\%) & ($+$0.01)\\
      15 & 品詞(-1)(Major\&Minor) 
      & 無\, 名詞\&普通名詞\, 名詞\&普通名詞\&連語 $\ldots$ (54個) 
      & & & \\
      \hline
      16 & 活用(-1)(Major) 
      & 母音動詞 子音動詞カ行 $\ldots$ (33個) 
      & 95.66\% & 95.00\% & 95.33\%\\
      17 & 活用(-1)(Minor) 
      & 語幹 基本形 未然形 意志形 命令形 $\ldots$ (60個) 
      & ($-$0.14\%) & ($-$0.09\%) & ($-$0.11)\\
      \hline
      18 & 文節区切り(-1) & 無 有 (2個) & 95.82\% & 95.25\% & 95.53\%\\
      19 & 文節区切り(-1) \& 
      & 名詞\&普通名詞\&区切り\, & ($+$0.02\%) & ($+$0.16\%) & ($+$0.09)\\
      & 品詞(-1)(Major\&Minor) & 
      名詞\&普通名詞\&区切りではない $\ldots$ (106個) & & & \\
      \hline
    \end{tabular}
  \end{center}
\end{table*}

\vspace{1pt}
次に，実験に用いた素性を表~\ref{table:feature} にあげる．
ここで素性とは，各素性名に対し素性値を展開したもののことである．
各々の素性は式(\ref{eq:p})の素性関数$g_{i}(h,f)$の $i$ に対応する．
素性番号は便宜上設けたものであり，各素性名に対応している．
例えば，素性番号，素性名，素性値がそれぞれ
「13」，「品詞(-1)(Major)」，「動詞」である素性および，
「3」，「辞書(0)(Major)」，「形容詞」である素性に対応する素性関数は
それぞれ，式(\ref{eq:g})および以下の式のように表わされる．
なお，式中では添字の $i$ は省略している．
\begin{eqnarray*}
  g(h,f) & = & 
  \left\{
  \begin{array}[c]{l}
    1\ :\ {\rm if\ has}(h,x) = {\rm true},\\
    \q\q x = {\rm ``辞書(0)(Major):形容詞''}\p \\  
    \q\p \&\ f = 1 \\
    0\ :\ {\rm otherwise.}
  \end{array}
  \right.
\end{eqnarray*}
これらの式および表~\ref{table:feature} で素性名に使われている
「(0)」「(-1)」という表記はそれぞれ，着目している文字列，
その文字列の左に連接する一形態素を意味する．
素性関数としては，素性とfutureの組が学習コーパスで3回以上観測されたもの
のみを用いた．結果として実験に用いた素性は8,525個であった．
以下で，表~\ref{table:feature} の各素性名，素性値について説明する．

\noindent
{\bf (文字列)} 学習コーパスに形態素として現れた文字列のうち，頻度5以上のもの\\
{\bf (長さ)} 文字列の長さ\\
{\bf (文字種)} 文字の種類．「(頭)」「(末尾)」はそれぞれ文字列の先頭と末尾の
文字を表す．文字列ではなく一文字の場合はともに同じ文字を指すものとする．
「文字種(0)(変化)」は先頭と末尾の文字の変化を表す．
「文字種(-1)(変化)」は左に連接する一形態素の末尾文字の文字種から
着目している文字列の先頭文字の文字種への変化を表す．
例えば，左に連接する一形態素が「先生」，着目している文字が
「に」の場合，素性値は「漢字$\rightarrow$平仮名」と表す．\\
{\bf (辞書)} JUMANの辞書を用いる．
この辞書に登録されている異なり形態素数は約20万個である．
Major，MinorはそれぞれJUMANの品詞大分類と細分類に対応する．
Major\&MinorはMajorとMinorの可能な組み合わせである．
着目している文字列が辞書に登録されている場合，
辞書に記述されている品詞の情報を素性として利用する．
複数の品詞を持つものとして登録されている場合には
それぞれを素性として用いたときに形態素モデルから推定される確率が
一文全体で最大となるものを採用する．
その文字列が，連語辞書に登録されている形態素列の一番左の形態素の文字列で
ある場合には，その文字列が連語の先頭の形態素であるという情報を付加した
ものを素性として利用する．
この場合，素性値としては「連語」という表記が付加されているものを用いる．
連語については文献\cite{Yamaji:96}に詳しい説明がある．

未知語の性質を学習するために，
学習コーパスにおいて各文字列に対し辞書引きをしたときに
一回しか引かれなかったものは辞書になかったものとして学習する．
今回の実験ではそのような語の数は20,317個であった．
ちなみに，辞書引きされた語の延べ数は1,964,829個，
異なり語の総数は60,908個であった．
このような学習方法をとることによって，
辞書が充実すればその情報を反映できるとともに，
辞書に依存し過ぎることなく未知語にも対処できると考えている．\\
{\bf (品詞)} Major，MinorはそれぞれJUMANの品詞大分類と細分類に対応\\
{\bf (活用)} Major，MinorはそれぞれJUMANの活用型，活用形に対応\\
{\bf (文節区切り)} 形態素の左側に文節区切りがあるかないか

\subsection{実験結果と考察}
\label{sec:exp_result}

パラメータの推定にはImproved Iterative Scaling (IIS)アルゴリズム
\cite{pietra95}
を用いた．
計算マシンとして Sun Enterprise 450 (400MHz，SunOS Release 5.6 Version) 
を用いたところ，推定に要した時間は二日程度であった．

形態素解析の結果を表~\ref{Result} に示す．
ここで，再現率は
コーパス中の全形態素に対して区切りと品詞(大分類のみ)を
正しく推定できたものの割合を，
適合率はシステムが推定した全形態素に対して区切りと品詞(大分類のみ)を
正しく推定できたものの割合を求めたものである．
表中のFというのはF-measureのことで，以下の定義式により計算した．
\begin{eqnarray*}
  {\rm F-measure} & = & \frac{2\times 再現率\times 適合率}{再現率+適合率}
\end{eqnarray*}

\noindent
表の各行にはそれぞれ，\ref{sec:exp_condition}~節で述べた手法
およびJUMANによる精度をあげた．JUMANは単独では辞書に登録されていない
カタカナ語に対し「未定義語」という品詞を付与するため，
それによる誤りが多くなる．
ルールベースの構文解析システムKNP\cite{KNP2.0b6}は，
JUMANに複数解の出力を許しその出力を入力とすると，
構文解析の過程で品詞の曖昧性を解消し，未定義語も何らかの品詞に置き換える
ことができる．そこで，JUMANとKNPで解析した結果も評価した．
表には+KNPと表記した．

\begin{table}[tbh]
  \small
  \begin{center}
    \caption{解析結果(形態素区切りと品詞大分類)}
    \label{Result} 
    \begin{tabular}{|c||c|c|c|}
      \hline
      & 再現率 & 適合率 & F\\
      \hline      
      本手法 & 95.80\% (29,986/31,302) & 95.09\% (29,986/31,467) & 95.44 \\
      JUMAN & 95.25\% (29,814/31,302) & 94.90\% (29,814/31,417) & 95.07 \\
      +KNP & 98.49\% (30,830/31,302) & 98.13\% (30,830/31,417) & 98.31 \\
      \hline
    \end{tabular}
  \end{center}
\end{table}

表~\ref{Result} にあげた形態素区切りと品詞大分類に対する推定精度は，
我々の手法ではJUMAN+KNPよりも3\%程度低かった．
その原因として学習コーパスの量，素性，コーパスにおける形態素の揺れなどが
考えられる．今回用いた学習コーパスは約8,000文と少なく，
素性については文献\cite{Uchimoto:eacl99}などで用いられているような
組み合せの素性に相当するものはあまり用いていない．
利用可能なマシンのメモリ容量の都合上，今回は学習コーパスの量，
素性の数ともにこれ以上増やすのは困難であったが，いずれ可能になるだろう．
次に形態素の揺れについてであるが，
これは実験に用いた京大コーパスがJUMAN+KNPの解析結果を人手で修正したものである
ということに起因していると思われる．
このことはJUMAN+KNPの出力の評価に有利に働いている．
例えば，最後が「者」で終わる形態素はテストコーパス中に153個あり，
すべてJUMAN+KNPの出力と同じであった．
このうち我々のシステムの誤りは3個(約2\%)であった．
コーパスには「生産(名詞)者(接尾辞)」と「消費者(名詞)」の違いなどの揺れがあり，
このように区切りに一貫性のない場合，過学習にならないように学習するのは
難しい．揺れに関してはコーパス全体を通して他にも同様な例がいくつかある．
例えば，「芸術家(名詞)」と「工芸(名詞)家(接尾辞)」，
「警視庁(名詞)」と「検察(名詞)庁(名詞)」，
「現実的(形容詞)」と「理想(名詞)的(接尾辞)」などがそうである．
この揺れの問題を解決するためには，コーパス修正の研究がより活発に行なわれる
必要がある．一つの方法として，我々のモデルを用いる方法が考えられる．
学習したモデルを用いて学習コーパス中の各形態素の確率を再推定し，
確率の低い部分に一貫性を欠いたものがある可能性が高いと推測する方法である．
今後，この方法を試してみたい．

\vspace{2pt}
\subsection{素性と精度}
\label{sec:features_and_accuracy}

辞書の情報，未知語の性質は，我々が実験で用いた素性に反映されている．
表~\ref{table:feature} にあげた素性のうち，
「文字列」「辞書」の素性が辞書の情報を
\footnote{
「文字列」は学習コーパスに5回以上出現した形態素の文字列であり，
  これを素性として用いることは，学習コーパスから辞書的な情報を得て利用
  していることに相当する．
}，「長さ」「文字種」の素性が未知語の性質を反映する．
表~\ref{table:feature} の右欄には，
それぞれの素性を削除したときの解析精度と削除したことによる精度の増減を示した．
ほとんどの素性が精度向上に貢献しており，特に辞書情報の貢献度が高いことが
分かる．

逆に辞書が解析結果に悪い影響を及ぼす例もある．
例えば，「／海／に／かけた／ロマンは／，／」
「／荒波／に／負け／ない心／と／」(「／」は形態素区切り)
といった形態素区切りが出力として得られることがある．
これは，漢字を使った表記「ロマン派」「内心」に加えて
平仮名を使った表記「ロマンは」と「ない心」も名詞として辞書に登録されて
いたために生じた誤りである．このような間違いをなくすためには，
不自然な表記を辞書に登録しないようにする，
あるいは，辞書の表記に使われる文字種の性質を学習する必要がある．

学習の際，一回しか辞書引きされなかった語は辞書に登録されていなかったもの
として扱った．このようにしたのは，
テストコーパスを解析するときには未知語が多くなると予想されるため，
学習の際にもそれと同じ状況に少しでも近付けようとしたためである．
ところが，実験後，学習コーパス，テストコーパスにおける未知語の割合を
調べたところ，辞書に登録されていなかった語の数(見出し語の異なり数)の
異なり形態素数に対する割合は，学習コーパスで26.6\% (3,859/14,493)，
テストコーパスで17.7\% (901/5,093)であり，テストコーパスにおける未知語の割合の
方が学習コーパスにおける割合より少ないことが分かった．
ちなみに，未知語の大部分は数詞およびカタカナで表記された名詞が占めていた．
そこで，辞書に登録されていた場合には辞書引きの頻度に関わりなくその情報を
すべて学習に用いることにすると，
精度は再現率95.78\%，適合率95.38\%，F-measure 95.58ポイントとなった．
これは表~\ref{table:feature} にあげた精度よりわずかに良い結果である．
今回の実験では学習コーパスより未知語の割合が少ないコーパスに対して
実験したためこのような結果となったが，本手法を学習コーパスよりも未知語の割合が
多い分野に適用するときには我々がとった学習手法は有効ではないかと考えている．
その有効性を調べることは今後の課題である．

\subsection{学習コーパスと精度}
\label{sec:training_corpus_and_accuracy}

この節では，学習コーパスと解析精度の関係について考察する．
図~\ref{fig:learning_curve} に学習コーパスとテストコーパスのそれぞれを
解析した場合の学習コーパスの量と解析精度の関係をあげる．
図の横軸は学習コーパスの文数，縦軸はF-measureを表す．
学習コーパスの解析には基本的に京大コーパスの1月1日の一日分を用いた．

学習曲線(図~\ref{fig:learning_curve})を見ると，
わずかではあるが増加する傾向にある．
したがって，学習コーパスの量が増えればもう少し精度の向上が期待できそうである．

\begin{figure*}[htbp]
  \begin{center}
    \leavevmode
    \atari(94.5,90)
    \caption{学習コーパスの量と精度の関係}
    \label{fig:learning_curve}
  \end{center}
\end{figure*}

\subsection{未知語と精度}
\label{sec:unknown_words_and_accuracy}

我々の手法は，未知語に対しても前後の形態素のつながりから
形態素と認定でき，適切な品詞を付与することができる．
例えば，「漱石」や「露伴」はJUMANの辞書には登録されていないため，
JUMAN+KNPでは「漱(名詞)石(名詞)」「露(副詞)伴(名詞)」のように
解析されるのに対し，我々のシステムではどちらも正しく名詞であると解析される．
この場合は，細分類も正しく人名であると解析できた．
このような固有名詞などは未知語になることが多い．
そこで，未知語(辞書にも素性にもなかった語)に対する再現率を調査した．
結果を表~\ref{Result2} にあげる．
表には品詞細分類まで正しい場合に正解とするという基準で求めた再現率もあげた．
この基準で求めた我々の手法の精度はJUMAN+KNPに比べて10\%程度良かった．
この結果は我々のモデルでは未知語，特に固有名詞や人名，組織名，地名に関する語に
対する学習が比較的にできていることを示していると考えて良いだろう．

\begin{table}[htbp]
  \small
  \begin{center}
    \caption{未知語に対する精度(再現率)}
    \label{Result2} 
    \begin{tabular}{|c|c|c|}
      \hline
      & 形態素区切り & 形態素区切り\\
      & と品詞大分類 & と品詞細分類\\
      \hline      
      本手法 & 77.96\% (849/1,089) & 39.67\% (432/1,089)\\
      本手法+NE & 79.52\% (866/1,089) & 42.15\% (459/1,089)\\
      JUMAN+KNP & 86.87\% (946/1,089) & 29.94\% (326/1,089)\\
      \hline
    \end{tabular}
  \end{center}
\end{table}

さらに，固有表現に関する情報を素性として利用した場合の実験を行なった．
ここで固有表現とは，人名，組織名，地名など特定の事物を示す表現のことである．
これらの表現は特に未知語になりやすい．
固有表現に関する情報は，固有表現にSGML形式のタグを付与したコーパスから
抽出した．このようなコーパスとしては，
CRL(郵政省通信総合研究所)固有表現データ，
IREX-NE予備試験トレーニングデータ，
IREX-NE予備試験データ，IREX-NE本試験逮捕トレーニングデータ
など(合計約12,000文)\cite{Uchimoto:jnlp2000a}がある．
これをJUMANを用いて形態素解析した結果から，固有表現を構成する形態素あるいは
固有表現の前後の形態素の文字列として5回以上出現したもの2,279個を抽出し，
素性として追加した．実験結果を表~\ref{Result2} の二行目(本手法+NE)にあげる．
未知語に対する再現率は表~\ref{Result2} に本手法としてあげた精度より約2\%
良くなっている．テストコーパスに対する精度は再現率95.93\%，適合率95.12\%，
F-measure 95.52ポイントであった．これは表~\ref{Result} にあげた本手法の精度
よりもわずかに良い．
これらの結果から，未知語になりやすい文字列を選択して素性として利用すると
全体の精度が良くなるだけでなく，未知語に対する再現率も良くなることが分かる．

\section{関連研究}
\label{sec:related_works}

実験で比較として用いたJUMANはルールベースのシステムであり，
形態素に品詞を付与するときにかかるコスト(品詞コスト)と
形態素を連接するときにかかるコスト(連接コスト)の和が一文全体で
最小となるように形態素区切りと品詞を決める．
それぞれのコストは予め人手により設定する必要がある．
一方，我々の手法は学習に基づくシステムであり，
JUMANの品詞コストと連接コストに相当するものを一つの確率値として表し，
その確率値を計算するためのモデルをコーパスから統計的に学習する．
大きな違いは，ルールベースと統計ベースという点だけでなく，
JUMANが未知語を一文字からなる名詞と既知語に分割して
出力するのに対し，我々の手法は，未知語に対しても前後の形態素のつながりから
形態素と認定でき，適切な品詞を付与することができる点にもある．

日本語では，我々の手法の他にも統計モデルに基づく方法が
これまでにいくつか提案されている．
HMM \cite{Takeuchi:97} や
可変記憶長マルコフモデル(Variable Memory Markov Model, VMM) 
\cite{Haruno:acl97,Kitauchi:99} に基づく方法では
F-measure で約96ポイントの精度が得られている．
これは我々の手法よりも良い精度であるが，これらの手法では
未知語に対する扱いがあまり考慮されていない．
未知語は一文字からなる名詞と既知語に分割して出力される．

春野らは着目している形態素と一つ前の形態素の情報，
つまり2gramの情報を用いたときに，94\%程度の再現率が得られ，
3gramあるいはそれ以上の情報を用いたときに96\%程度の再現率が得られたと
報告している \cite{Haruno:acl97}．
我々は2gramの情報のみで96\%程度の再現率を得ていることを考慮すると，
3gram以上の情報を用いることにより，より良い精度が得られることが期待できる．
3gram以上の情報を用いることは今後の課題である．

\ref{sec:introduction} 節で述べたように，
未知語の問題に対処するため，これまで大きく二つの方法がとられてきた．
一つは未知語を自動獲得し辞書に登録する方法(例えば\cite{Mori:96}など)であり，
もう一つは未知語でも解析できるようなモデルを作成する方法(例えば
\cite{Kashioka:97,Nagata:99}など)である．
永田は未知語のモデルを提案し，未知語に対し約40\%の再現率が得られたと
報告している \cite{Nagata:99}．
我々の方法では表~\ref{Result2} に示したように，最大で，
形態素区切りと品詞大分類を推定したときに79.52\%，
形態素区切りと品詞細分類を推定したときに42.15\%の精度を得ることができた．
我々の手法の精度は永田の手法に比べて品詞細分類を推定したときでもわずかに良い．
永田は我々とは異なるコーパス(EDRコーパス)を用いており，
直接精度を比較することは難しいが，京大コーパスとEDRコーパスでは
品詞体系と形態素の定義が似ていることから，この結果は我々の手法を評価する
のに十分な材料の一つになると考えている．

森らは辞書情報を用いるモデルを提案した \cite{Mori:98}．
彼らはEDRコーパスを用いたときにF-measureで約92ポイント，
京大コーパスを用いたときにF-measureで約95ポイントの精度を得ている．
彼らが辞書情報を用いて得た精度向上はF-measureで約0.2ポイントであるのに対し，
我々の手法では辞書情報を用いることにより，F-measureで約1.7ポイント精度が
向上した．森らが京大コーパスを用いて得た約95ポイントの精度は，我々の精度とほぼ
同程度である．しかし，彼らは学習コーパスに現れた形態素の文字列の情報をすべて
用いているため，我々の実験に比べて未知語が少ない状況で実験していたものと
思われる．

英語では，品詞タグ付けの手法として，
HMM \cite{Cutting:92}，可変記憶長マルコフモデル \cite{Schutze:94}，
決定木モデル \cite{Daelemans:96}，MEモデル \cite{ratnaparkhi:emnlp96}，
神経回路網モデル \cite{Schmid:94}，誤り主導の変換に基づく学習 \cite{Brill:95}
などに基づく方法やこれらのうちいくつかのモデルを組み合せた
方法 \cite{Marquez:97,Halteren:98} などがこれまでに提案されてきた．
それぞれ高い精度が得られているが，
これらの方法では大規模な語彙情報を利用することは難しい．
一方，我々の提案したモデルは，
辞書引きをする仕組みが素性の一つとして組み込まれているため，
大規模な語彙情報も辞書情報として利用することができる．
さらに，未知語の性質も学習することができる．
そのため，我々のモデルを英語の品詞タグ付けに用いればより良い精度が得られる
可能性が高いと考えている．

\section{まとめ}
\label{sec:conclusion}

本論文では次の二つの特徴をもつモデルを
MEモデルとして実装した形態素解析の手法を提案した．
(1)学習コーパスからだけでなく辞書から得られる情報も用いる．
(2)形態素となる文字列だけでなく形態素とはならない文字列の
性質も学習することによって，未知語も形態素として推定でき，
同時にその文法的属性も推定できる．
実験により，辞書の精度に及ぼす影響の大きさ，および，
我々の手法が，固有名詞，人名，組織名，地名など未知語になりやすいものに対して
比較的に推定精度がよいことが分かった．
さらに，固有表現を構成するような文字列を抽出し素性として利用すると，
精度，特に未知語に対する再現率が良くなる(約2\%)ことが分かった．

今後の課題としては以下の三点をあげておきたい．
\begin{enumerate}
\item 学習に用いる情報について．

一つ前の形態素の情報だけでなく，二つから四つくらい前の形態素の情報を
利用するとともに，組み合わせの素性を増やす．

\item コーパスについて．

コーパスの量を増やすとともに，
コーパス修正の研究を活発に進める．また，異なるコーパスについても実験する．
\item 辞書について．

今回実験に用いた辞書は既存の辞書であったが，
今後，自動獲得した辞書を利用したときに
どの程度精度の違いがあるかについて調査したい．
また，
文法体系が変わったときにその体系に合うように辞書情報を変換する技術を
開発したい．
\end{enumerate}

\begin{flushleft}
{\bf 謝辞}  
\end{flushleft}

本研究の評価にあたり，評価ツールを提供して下さった
京都大学の黒橋禎夫講師に心から感謝の意を表する．



\bibliographystyle{jnlpbbl}
\bibliography{jpaper}

\newpage

\begin{biography}
\biotitle{略歴}
\bioauthor{内元 清貴}{
1994年京都大学工学部卒業．
1996年同大学院修士課程修了．
同年郵政省通信総合研究所入所．研究官．
自然言語処理の研究に従事．
言語処理学会，情報処理学会，ACL，各会員．}
\bioauthor{関根 聡}{
1987年東京工業大学応用物理学科卒．同年松下電器東京研究所入社．
1990-1992年UMIST，CCL，Visiting Researcher．1992年MSc．
1994年からNew York University，Computer Science Department，
Assistant Research Scientist．1998年PhD．
同年からAssistant Research Professor．
自然言語処理の研究に従事．
情報処理学会，人工知能学会，言語処理学会，ACL会員．}
\bioauthor{井佐原 均}{
1978年京都大学工学部電気工学第二学科卒業．
1980年同大学院修士課程修了．博士（工学）．
同年通商産業省電子技術総合研究所入所．
1995年郵政省通信総合研究所
関西支所知的機能研究室室長．
現在，同研究所けいはんな情報通信融合研究センター感性情報処理研究室室長．
自然言語処理，機械翻訳の研究に従事．
言語処理学会，情報処理学会，人工知能学会，日本認知科学会，ACL，各会員．}

\bioreceived{受付}
\biorevised{再受付}
\bioaccepted{採録}

\end{biography}

\end{document}

