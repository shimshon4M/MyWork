<?xml version="1.0" ?>
<root>
  <title/>
  <author/>
  <jkeywords/>
  <section title="Introduction">Themeaningofawordisnotunique.Thismakesnaturallanguagesnothingbutaconvenientandflexibletooltouseforcommunication.However,itisabigprobleminnaturallanguageprocessing.Tounderstandapassagelike,weneedtoknowwhatplantmeans:alivingthingorfactory?Unlessthisproblemisresolved,wewillhavenobettersystems,nottomentionacompleteone,formachinetranslation,naturallanguageunderstanding,informationretrieval,etc.Inwhatfollowsï¼Œwepresentamethodanditsexperimentalresultforresolvinglexicalambiguitiesbasedonawordsimilaritymeasure.</section>
  <section title="Word Sense Disambiguation">Thehistorytellsusthatearlyworkinlexicalorwordsensedisambiguation(WSD)tookapureAIapproach(e.g.,Hirst1987;SmallandRieger1982).Manyresearchersmanuallysetupaknowledgebaseforeachwordanddescribeditssensesinvariouslinguisticusages.Then,somestartedtousemachine-readabledictionaries(MRDs)toselectthepropersenseofapolysemousword(e.g.,Guthrie,Guthrie,andAidinejad1991;Lesk1986;Walker1987).TheMRDsarewidelyavailable,buttheyhavedeficienciesforWSD.Forinstance,thedefinitionsofawordinthemareoftentooshortandtoouneventocovernecessarycollocations.Weencounteranotherdifficultyinretrievingnecessaryinformationfromtheon-linedictionariessincetheircontentsareusuallyexpressedfortheuseofhumanbeingsratherthanforthecomputers.TheresearchonWSDreliesoncorporainthemostofworkbeingdonenowadays.Thecorpus-basedapproachhasanadvantageinitsgeneralapplicability.Itisalsopossibletogetaroundcostlyandperhapserror-proneparsingorsemanticanalysisotherwiseneeded.Alongthisline,Yarowsky(Yarowsky1992)presentedastatisticalmethodofresolvinglexicalambiguitieswiththeuseofRoget'sThesaurusandalargecorpus.Dagan,Marcus,andMarkovitch(Dagan,Marcus,andMarkovitch1995)proposedamethodbasedontheadjacentwordofapolysemouswordtodisambiguatethewordsenses.Leacock,Chodorow,andMiller(Leacock,Chodorow,andMiller1998)determinedthesenseofawordintextusingacorpusandWordNet(WordNet2001)relations.Sch&quot;utze(Sch&quot;utze1998)proposedamethodforWSDbasedonclusteringoncontextualsimilarities.KarovandEdelman(KarovandEdelman1998)devisedadisambiguationmethodusingacorpusandMRD,andattemptedtodeterminethewordsensesbythedifferentcontextualusagesofthepolysemouswordbeinganalyzed.</section>
  <section title="Measurement of Similarity">Wordsimilaritybetweentwowordsw_1andw_2dependsonthedegreeoftheirco-occurrencesintext.Thecontextuallysimilarwordsarethewordsco-occurringfrequentlyinadistanceintext.Theyneednotbesynonyms,orbelongtothesamesyntacticorsemanticcategory.Forinstance,thewordsdoctorandhealtharenotsynonymsinthesauruses,buttheyarelikelytobecontextuallysimilarastheytendtoco-occurinatext.</section>
  <subsection title="Mutual Information">Thecontextuallysimilarwordmaybemeasuredbyusingmutualinformation.Themutualinformation,I,estimatesthestrengthofassociationbetweentwowordsw_1andw_2(ChurchandHanks1990):Here,Nin(1)isthesizeofthecorpususedintheestimation,f(w_1,w_2)isthefrequencyofco-occurrencesofw_1andw_2,andf(w_1)orf(w_2)isthefrequencyofeachword.Ifthereisastrongassociationbetweenw_1andw_2,thenI(w_1,w_2)0.Ifthereisaweakassociationbetweenw_1andw_2,thenI(w_1,w_2)0.IfI(w_1,w_2)0,thenw_1andw_2aresaidtobeincomplementarydistribution.Ifw_1andw_2arecontextuallysimilar,thenI(w,w_1)andI(w,w_2)havesimilarmutualinformationforsomewordwinlexicon.Wewillusethethirdwordwinthenextsectiontomeasurethesimilarityoftwowordsw_1andw_2.</subsection>
  <subsection title="Similarity Metric of Two Words">Mathematically,theerrorcanbeusedtoexpressthedissimilarityoftwowordsw_1andw_2.Forexample,|I(w,w_1)-I(w,w_2)|showstheabsoluteerrorinthatitindicateshowmuchthetwonumbersI(w,w_1)andI(w,w_2)differ.Theabsoluteerrordoesnotexpresstheprecisionofthedifference,however.So,wedefinetheproportionalerror:[|I(w,w_1)-I(w,w_2)|I(w,w_1)+I(w,w_2)]WeknowthatthebiggerthesimilarvaluesofI(w,w_1)andI(w,w_2),thestrongerthesimilarityofw_1andw_2.Intheproportionalerror,however,(I(w,w_1),I(w,w_2))=(2.7,3.2)and(I(w,w_1),I(w,w_2))=(0.27,0.32),forinstance,havethesameeffectonmeasuringthesimilarity,thoughinrealitythenumbersinthesecondpairarefarsmallerandthusfarlessimportant(sometimesharmful)inmeasuringthesimilarity.</subsection>
  <subsubsection title="3.2.1   Strong Bridge Set">Itisdesirabletomeasurethewordsimilaritycollectingthesetofwordwsothatitsmemberhasbiggermutualinformationwithw_1andw_2.Wedefinesuchawordsetas:[S=w|I(w,w_1)&gt;t_0,I(w,w_2)&gt;t_0,t_1&lt;I(w,w_1)I(w,w_2)&lt;t_2,wLexicon]andcallitthestrongbridgeset.Here,t_0,t_1,andt_2arethresholdvalues,t_0makeseliminatesmallmutualinformationvalues,andt_1andt_2makeitpossibletogetthewordwforwhichw_1andw_2havesimilarmutualinformation.Wesett_2=t_1^-1.Thesimilarityofw_1andw_2getsstrongeras|S|becomesbigger.</subsubsection>
  <subsubsection title="3.2.2   Definition of Similarity">Wewilldefineasimilaritymetricoftwowordsw_1andw_2usingtheproportionalerrorandthestrongbridgesetS.Thefunctionf(x)=e^-xin(2)canconvertdissimilaritytosimilarity.Thefunctionf(x)=_10(x)istomakethefunctionvaluesmall.Evidently,equation(2)issymmetric,i.e.,sim(w_1,w_2)=sim(w_2,w_1).Thisagreeswithanintuitionthatthesimilarityoftwowordsw_1andw_2isequaltothatofw_2andw_1.Inequation(2),thewordw_1ismostsimilartoitself.Itisprovenbyshowingsim(w_1,w_2)sim(w_1,w_1).Putitsimply,[S(w_1,w_2)=w|I(w,w_1)&gt;t_0,I(w,w_2)&gt;t_0,t_1&lt;I(w,w_1)I(w,w_2)&lt;t_2,wLexicon][w|I(w,w_1)&gt;t_0,wLexicon=S(w_1,w_1)]andthenweget:[sim(w_1,w_2)=_10_wS(w_1,w_2)e^-|I(w,w_1)-I(w,w_2)|I(w,w_1)+I(w,w_2)_10_wS(w_1,w_2)e^-0][_10_wS(w_1,w_1)e^-0=sim(w_1,w_1)]</subsubsection>
  <section title="A Method of Word Sense Disambiguation">WenowdeviseamethodofWSDbasedonequation(2).IttakesthestepsinFigure1.Wordsensedisambiguationstartswithobtainingcontext-similarityvector,V_s_1,V_s_2,,V_s_r,foreachsenseofapolysemousword,w,usingthewordsimilaritymetricinequation(2).Atthesametime,wegetthecontextrepresentation,V_CR,forthesameword,w,appearingintext.Wethencalculatethesimilarity,sense(V_s_m,V_CR),betweeneachvectorandthecontextrepresentation.Finally,wecomparethevaluesofsimilarities,sense(V_s_1,V_CR),sense(V_s_2,V_CR),,sense(V_s_r,V_CR),andselectthesense,s_m,withthehighestvalueasthemeaningofthepolysemouswordinquestion.</section>
  <subsection title="Context Representation and Context-similarity Vector">Thecontextrepresentation(CR)weusehereisthesequenceofcontentwords(nouns,verbs,adjectives)thatappearinadistancelwiththepolysemouswordinquestion.Itisdefinedasthevector:Lets_mbethemthsenseofapolysemouswordw.Usingequation(2),wecangetavectorofeachsenseofapolysemousword.Herew_1^m,w_2^m,,w_k^marethesetofthekmostcontextuallysimilarwordsofthemthsenseofapolysemouswordw.s_mandw_i^m(i=1,,k)havebigandsimilarmutualinformationwithsomeotherword.Wesetktobe6inthispaper.Whenthemthsenseofapolysemouswordwinthecorpusfacesthedatasparsenessproblem,weseekitssynonymsetintheWordNet,choosefromthesetawordthatdoesnothavetheproblem,anduseittocalculateV_s_m.Whennosuchwordisfound,weuseitscoordinateword(WordNet2001)thatdoesnothavetheproblemtocalculateV_s_m.</subsection>
  <subsection title="Distributional Matrix and the Meaning of Polysemous Word">Using(3)and(4),wewillgetadistributionalmatrixofmutualinformation.ThematrixM(V_s_m,V_CR)expressesadistributionofthetightdegreeofassociationbetweenthemthsenseofapolysemouswordwandthecontextV_CR.AndwecangetthedensityofthematrixM(V_s_m,V_CR).IfI(w_i^m,w_j)isonlyapurenumber,then(V_s_m,V_CR)isonlyameanvalueofsomenumbers.However,I(w_i^m,w_j)haslinguisticsignificance,i.e.,italsoexpressesthetightdegreeofassociationbetweentwowords.Consequently,(V_s_m,V_CR)isnotonlyameanvaluebutalsoexpressesthetightdegreeofassociationbetweenthemthsenseofapolysemouswordwandthecontextV_CR.Inthecontextofnaturallanguage,therearenoisywordsorirrelevantinformation(DaganandItai1994)inV_CRtoexpressthetopicinvolved.Sowesetthethresholdttomodify(6)sothatweselectthemostinformativecontextwords(Gale,ChurchandYarowsky1992)forwordsensedisambiguation.Wethrowawaythemutualinformationofsomewordsthathavelittlecontributionfordisambiguation.Consequently,wemakethedistancebetweendensitiesenlargedandthedifferenceofthesensesofapolysemouswordmoreevident.Theprocesswillraiseourconfidenceinidentifyingthewordsense,especially,inthecase(V_s_1,V_CR)(V_s_2,V_CR).InthematrixM(V_s_m,V_CR),wethrowawaythevaluesofmutualinformationindmostsmallvaluesifthevalueislessthant.Fortherest,wecalculatethedensityofmutualinformation.Wemarkthedensitywithsense(V_s_m,V_CR).Theparametersdandtareadjustable.Yarowsky(Yarowsky1993,1994a,1994b)suggeststhatsemanticortopic-basedambiguitiesrequirealargewindowof20-50words.Inourexperiment,weusewindowsofl=10,35,and50wordsbeforeandafterthepolysemouswordtobedisambiguated.Whenl=10,wedonotusetandd.Forl=35andl=50,wesetttobe[t=_i=1^k_j=1^nI(w_i^m,w_j)4kn]anddtobe9kand12k,respectively.Supposethewordappearinginatextisw,anditslexicalmeaningsares_1,s_2,,s_r.Wecalculatethedensitysense(V_s_1,V_CR),sense(V_s_2,V_CR),,sense(V_s_r,V_CR).Themeaningofthewordinconsiderationiss_mthatsatisfiesthemaximalvalue.</subsection>
  <section title="Experiment and Result">Totestourmethod,weuseEDREnglishCorpus(EDR1998)astrainingdataand10polysemouswordsforwhich682instanceswereselectedrandomlyfrom(Text11998;Text21998)andothermaterials.TheEDRcorpuscontains160,000sentenceswithannotatedmorphological,syntacticandsemanticinformation.Andwesett_0=0.5,t_1=0.75,andt_2=1.33.Wepredeterminedthemeaningofeachinstanceofthepolysemouswordbytwohumansubjects.Themeaningofwisdeterminedbythefollowingprocedure:ObtainthevectorsV_s_1,V_s_2,,V_s_rforthelexicalmeaningsofw.GetV_CRinconsiderationusingawindowoflwordseachbeforeandafterwinthetext.Calculatethedensitysense(V_s_1,V_CR),sense(V_s_2,V_CR),,sense(V_s_r,V_CR).Selects_mthatgotthemaximaldensityvaluetobethemeaningofw.</section>
  <subsection title="Examples">Considerthepolysemouswordinconsiderationtobecabinetinthefollowingtextandseehowitsmeaningisdetermined:Cabinetisgiventwonominalmeanings:administrativeorgan(s_1)andashelf(s_2).Usingequation(2),wegetforthemthecontext-similarityvectors:ThecontextrepresentationV_CR(l=35)fromthetextis:Thecalculationofthedistributionalmatrixproducesthedensity0.411462forsense(V_s_1,V_CR)and0.180325forsense(V_s_2,V_CR).So,wedecidethemeaningofthecabinettobes_1(administrativeorgan).Takeanotherexampleforthewordsentence(l=35)in:Again,sentencehastwomeanings:punishment(s_1)andgroupofwords(s_2).Thecontext-similarityvectorsandthecontextrepresentationare:Now,thedensityforsense(V_s_1,V_CR)becomes0.497328andsense(V_s_2,V_CR)becomes0.179224.Thus,wegetthemeaningofthesentenceass_1(punishment).</subsection>
  <subsection title="Experimental Results">Table1showsthe10polysemouswords,theirsenses,thenumberoftheirinstances,andtheinstancesidentifiedcorrectlyinourexperimentusingl=35(Experiment2).Table2showsthedisambiguationresult(successrate).Experiment1,Experiment2,andExperiment3areforl=10,35,and50,respectively.AsisseeninTable2,theoverallaveragesuccessratesare89.4%,91.9%,and91.3%,respectively,forExperiment1,Experiment2,andExperiment3.Thebestoverallresultisgotwhenl=35(Experiment2).Thebestsingleresult,97.6%,isgotforcabinetwhenl=50andtheworstisforslugwhenl=50.</subsection>
  <section title="Evaluation">Theupper-boundforthesuccessrateinourexperimentissupposedtobe100%,whichisthatofhumanbeings.Thelower-boundorthebase-linesuccessrateis80.5%,whichisthesumofbiggerinstancesineachworddividedbythetotalinstances,i.e.,549/682.Obviously,ourresultismuchbetterthanthatofthebase-line.Methodologically,ourapproachissimilartoDagan,Marcus,andMarkovitch(Dagan,MarcusandMarkovitch1995)andSch&quot;utze(Sch&quot;utze1998)inthatallusesomekindsofwordsimilaritymeasures.Dagan,Marcus,andMarkovitch(Dagan,MarcusandMarkovitch1995)usedAugmentedTWSmethodtodisambiguatethewordsenses.Itreliedontheadjacentwordonlyofapolysemouswordtodisambiguatethewordsensesandwasdoomedtofail,atleastfornoundisambiguation,sincenotenoughdisambiguatinginformationwouldbeprovidedbytheadjacentwordalone.Inthispaper,weconsideredtheglobalcontextusingawindowoflwordseachbeforeandaftertheambiguouswordinthetext,andtookmorewordsintoaccounttomakeuseofenoughdisambiguatinginformation.Sch&quot;utze(Sch&quot;utze1998)usedanideaofWordSpacethatwasahigh-dimensional,real-valuedspace.Thenhegotvectorsofhugedimensions,inwhichmanyirrelevantinformationtoexpressthetopicinvolvedwasput.Hereinourpaper,weusedthekmostsimilarwordsandgotbettervectors,capturingeffectiveinformationfordisambiguatingwordsense.Experimentally,ourresultisbetterthantheonesinotherstudies(e.g.,ChenandChang1998;Hiro,WuandFurugori1996;Li,SzpakowiczandMatwin1995).Butthisdoesnotnecessarilymeanthesuperiorityofourmethod:noexperimentsaredoneunderthesamesettingsandtheydifferinconditionsanddataused.Previously,Peng,Takakura,andFurugori(Peng,TakakuraandFurugori2001)experimentedaDagan-likewordsimilaritymeasure(Dagan,MarcusandMarkovitch1995)forthesamedataweusedinthepresentpaper.Thesimilaritymeasureusedwas:Thesuccessrategotfromthisexperimentwas91.5%.Percentage-wise,thisresultiscomparablewithours,butourpresentmethodissuperiorinthefollowingtwopoints.Firstandcomputationally,theequation(2)makesthecalculationtimelessthanthatofusingequation(7)since|S||Lexicon|.Secondandmethodologically,theequation(2)isfreefromadeficiencyseeninequation(7).Letusseethisinexamplesfor|Lexicon|=8.Therewouldbecasesinusingequation(7).Incase1,2.7in(b)isfarbiggerthan0.27in(a)andthesameistruefor3.2and0.32.Nevertheless,wegetsim(w_1,w_2)=sim(w_3,w_4).Incase2,thenumbersofthefirsttwoterms(2.7,3.2)in(a)areverybig,whilethoseoftherestareverysmall,andthenumbersofthefirstfivetermsin(b)areverybig,whilethoseoftherestareverysmall.Nevertheless,wegetsim(w_1,w_2)sim(w_3,w_4).Asisclearalready,theseproblemsareeliminatedinequation(2).Butthenaquestionremainsthatwhyourresultisnotmuchbetterthanthatofusingequation(7).Ananswertothisquestionisthatsim(w_1,w_2)andsim(w_1,w_3)sharethemutualinformationI(w,w_1)inequation(7),sounfavorableeffectofcases1and2istherenotinfullforce.Anotheristhatthenoiseeffectofcases1and2isnegligibleasweusedthesmallvalueofk(=6)inbothexperiments.Wearesurethatthedifferenceinperformancesbetweenequations(2)and(7)wouldbecomeclearerasthevalueofkbecomesbigger.</section>
  <section title="Conclusion">Weproposedamethodtocalculatethesimilarityoftwowords.Usingthesimilaritymetricinequation(2)wedesignedanapproachforWSDthatusesthecontext-similarityvectorforresolvingambiguitiesofpolysemouswordsintext.Ourmethodisintuitionalanditsperformanceisinanacceptablelevel.However,toimprovetheperformance,itmaybeworthwhiletoconsidersuchpointsas:Dynamicadjustmentofparametersdandtforextractingthetopiceffectivelyfromthecontext.Invertionofthedistributionalmatrixtodistributionalimageinthetwodimensionalcoordinates,andthedeterminationofthesenseaccordingtodistributionalcharacteroftheimage.Inourmethodandmanyothers,contextisthemeanstoidentifythemeaningofapolysemousword.Butsomewordsensesarenonspecifictotopicsandawordwithsuchasensecanappearfreelyinmanydomainsofdiscourse.Itisdesirabletogiveathoughttodealwiththemeaningofpolysemouswordsusedinthenontopicalsense(Leacock,ChodorowandMiller1998).However,withinarestriction,wecontendthatourmethodistheoreticallysoundandbetteringeneralityandsimplicity.document</section>
</root>
