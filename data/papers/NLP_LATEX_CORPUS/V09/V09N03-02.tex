\documentstyle[epsf,jnlpbbl]{jnlp_j_b5}
\setcounter{page}{21}
\setcounter{巻数}{9}
\setcounter{号数}{3}
\setcounter{年}{2002}
\setcounter{月}{7}
\受付{2001}{10}{19}
\再受付{2002}{1}{21}
\採録{2002}{4}{5}
\title{ドメイン固有の文字列情報の組み込みによる\\形態素解析処理の精度の向上}
\author{延澤 志保\affiref{NAK} \and 佐藤 健吾\affiref{NAK} \and 斎藤 博昭\affiref{NAK}}
\headauthor{延澤，佐藤，斎藤}	
\headtitle{ドメイン固有の文字列情報の組み込みによる形態素解析処理の精度の向上}
\affilabel{NAK}{慶應義塾大学大学院理工学研究科}{Department of Computer Science, Keio University}

\jabstract{
辞書ベースの自然言語処理システムでは辞書未登録語の問題が避けられない．
本稿では訓練コーパスから得た文字の共起情報を利用する手法で辞書未登録語の抽出を実現し，辞書ベースのシステムの精度を向上させた．
本稿では形態素解析ツールをアプリケーションとして採用し，処理時に統計情報を動的に利用することによって形態素の切り分けの精度を上げる手法と，統計情報を利用して事前に辞書登録文字列を選別し必要なコスト情報を補って辞書登録を行なう手法との 2 つのアプローチを提案し，さらにこの 2 つの手法を組み合わせてそれぞれの欠点を補う手法を提案する．どちらも元のツールの改変を行なうものではなく，統計情報の付加的な利用を半自動的に実現するもので，元のツールでは利用できない辞書未登録語の抽出に対象を絞ることで精度の向上を図る．
実験の結果，動的な統計情報の利用のシステムが未知語の認識に，辞書登録システムが切り分け精度の向上に有効であることが示され， 2 つのシステムを適切に組み合わせることによって訓練コーパスのデータで認識可能な辞書未登録語をほぼ完全に解決できた．さらに複合語の認識も高い精度で実現することができた．
}

\jkeywords{文字間共起情報，辞書登録，統計情報の動的な利用，文字列自動抽出}
\etitle{The Use of Domain-Specific Statistical Data\\for Japanese Morphological Analysis}
\eauthor{Shiho Nobesawa\affiref{NAK} \and Kengo Sato\affiref{NAK} \and Hiroaki Saito\affiref{NAK}}
\eabstract{
We propose two methods for the recognition of unknown strings in dictionary-based natural language processing systems.
One method is for the dynamic use of statistical information during processing, and the other is for obtaining meaningful strings which should be added to the dictionary.
Both methods are based on statistical information drawn from a training corpus, and there is no need for part-of-speech tagging or other preprocessing of the training corpus.
We applied our methods to a Japanese morphological analysis system and had good results in reduction of unknown words and over segmentation.
}
\ekeywords{Statistical information, letter-unit, automatic string extraction}
\newcommand{\mi}{}
\newcommand{\uk}{}
\newcommand{\idl}{}
\newcommand{\chk}{}
\newcommand{\frq}{}
\newcommand{\pos}{}
\begin{document}
\maketitle
\section{はじめに}
辞書ベースの自然言語処理ツールは高い精度が期待できる反面，辞書未登録語の問題があるため，統計情報を利用して辞書未登録語の抽出を行なう研究が盛んに行なわれている．
辞書未登録語はドメイン固有の語句と考えることができ，対象ドメインの統計情報の利用が有効である．
本稿ではドメイン固有の文字列の自動抽出で問題となるノイズを 2 方向のアプローチで解決する手法を提案する．
本手法は辞書ベースのツールに付加的な情報を半自動的に与えて辞書未登録語の抽出を行なうことで処理精度の向上を図るものである．
本稿では形態素解析ツールについて実験を行なったが，本手法は処理内容やツールに特化したものではなく，ツールの改変を伴うものではない．
\section{語句抽出}
現在研究されている語句抽出システムは，ほとんどが対象を名詞に準じた単語列に限定したものである．
これは，抽出の対象となる語句は未知語や専門用語が主であり，どちらも名詞がその大半を占めるためである．
未知語，専門用語，固有名詞などはドメイン固有の語句と言ってよいが，ドメイン固有の語句となりうるのは新語や複合語がほとんどで，例えば助詞のように新語の出現しないものや活用語のようにドメインによってはほとんど新語がないものなどは抽出対象となりにくく，名詞に準じる語句を抽出対象とすることでかなりの未知語，専門用語などを取得することが可能である．
また，対象とする品詞を限定することで抽出処理に必要なルールが削減され，ノイズの軽減に繋がるという利点がある．
\subsection{名詞句抽出}
抽出対象は主に，名詞の推定と名詞句の推定とに二分されており，特に名詞の推定では専門用語など複合語の推定を行なうものと固有名詞などの未知語を認識するものに分けられる．

名詞句の推定を目的とした研究としては， Argamon らの提案したサブパターン概念を利用する手法\cite{argamon98}などが挙げられる．

専門用語などドメイン固有の語句の抽出では， tf・idf モデルなど語句の出現頻度を利用する手法と $n$-gram など文字の共起頻度を利用する手法がある．
湯本らは unigram および隣接 bigram の出現頻度を利用して複合名詞の認識を行なう手法を提案した\cite{yumoto01ipsjnl}．
湯本らの抽出対象は専門用語であったが，彼らは専門用語として複合名詞のみを考え，名詞のみに着目した隣接 bigram を利用して複合名詞の推定を行なった．
専門用語となる語句の大部分を複合名詞が占めること，頻出複合名詞の構成要素である名詞は共起頻度が高く，また特定の単語同士の並びが多いことを考えると，この手法は効率的だと言える．
Frantzi らは英語を対象として C-value を利用して複合語を認識する手法を提案し，さらにこれにコーパスサイズや重みなどを利用してランク付けを行なった\cite{frantzi97}．

Cha らは未登録語発見のための形態素パターン辞書を利用して未登録語の認識およびタグ付けを行なう手法を提案した\cite{cha98}．
固有名詞の認識では，最大エントロピー法を利用して固有名詞の切り出しを行なう Borthwick の手法\cite{borthwick99}や，固有名詞の前後に出現しやすい語をトリガーワードとして固有名詞の認識を行なう手法\cite{kitani94,hisamitsu97}，トリガーワードと構文解析情報を利用する福本らの手法\cite{fukumoto98}などがある．
また Cucerzan らは文字の並びの情報を利用して少ない訓練データからの固有名詞の推定を可能にする手法を提案し，ルーマニア語や英語など複数の言語に対して有効であることを示した\cite{cucerzan99}．
\subsection{文字単位の統計情報を利用した辞書未登録語抽出}
文字の共起情報を利用する手法としては， $n$-gram の画期的な抽出法を提案しこれを利用して文中の文字列の塊を認識する長尾らの手法\cite{nagao94} などが挙げられる．
中渡瀬らは $n$-gram 統計を利用して辞書未登録語を自動獲得する手法を提案した\cite{nakawatase98}．
中渡瀬らは任意の文字列の頻度を正規化する手法を提案し，これを用いて語の境界を決定することで，辞書未登録語を獲得している．
中渡瀬らはこの手法で漢字未登録語の 43\% の取得に成功したと報告している．
中渡瀬らの手法では評価対象を漢字未登録語に限定しているが，これは漢字とその他の字種の出現頻度分布が異なるためで，正規化を行なう中渡瀬らの手法では漢字での精度が高いためである．

延澤らは品詞タグや文法などに頼らず機械的に取得可能な文字間の統計情報のみを利用して文の切り分けを行なう手法を提案している\cite{nobesawa96coling}．
延澤らはこの手法を利用してドメイン固有の文字列の自動抽出を試みており，口語文章のような非文を多く含むコーパスに対しても有効であることを示した\cite{nobesawa00coling,nobesawa01anlp}他，固有名詞抽出など抽出対象を絞った場合などについても有効であるとしている\cite{nobesawa99}．
文字列の抽出はその後の利用を見込んだものであるが，延澤らの手法では文字単位での処理を行なっているため抽出される文字列は単語，複合語，言い回しなどサイズがさまざまである．
自然言語処理においては一般に単語または形態素が処理単位とされている．
単語は多義性を持つものも多く，単語が最適な処理単位と言えるかは疑問が残る\cite{fung98}．
その意味で，特定の処理単位を設定することは処理の精度に悪影響を与えている可能性もあるが，さまざまな処理単位を同時に扱う手法は確立されておらず，処理単位の特定が必要であるのが現状である．
このため，さまざまな処理単位の文字列が同時に出力とされる延澤らの手法を用いて出力された文字列は，そのままでは他のツールでの利用が困難である．
そこで本稿では，延澤らの手法の問題点を克服し，この手法を利用して辞書未登録語を抽出することで辞書ベースのツールの精度の向上を図る．
\section{システム概要}
本稿で提案するシステムは，対象ドメインのコーパスからシンプルな手法でドメイン固有の語句を抽出する延澤らの手法\cite{nobesawa01anlp}を応用したものであり，辞書ベースの自然言語処理ツールの支援を目的として 2 方向からのアプローチを試みる．
\subsection{システム概要}
本稿では，辞書ベースの形態素解析ツールに対して統計情報を利用することでその精度の向上を図るため，以下の 2 つのアプローチを試みた．
\begin{itemize}
\item システム M: 形態素解析ツールへの組み込みのための統計情報利用システム
\begin{quote}
形態素解析中に統計情報を利用してドメイン固有の語句を認識するシステムを形態素解析ツールに組み込むことで形態素解析時の誤解析を削減．
\end{quote}
\item システム D: 統計情報を利用した辞書の作成システム
\begin{quote}
形態素解析の前処理として対象ドメイン固有の文字列の辞書登録を行なうことで形態素解析時の誤解析を削減．
\end{quote}
\end{itemize}
どちらのアプローチも対象ドメインの訓練コーパスから得た統計情報を利用することで頻出文字列の認識を実現し，これに起因する解析誤りの削減を図るものである．

本稿では形態素解析ツールとして日本語形態素解析ツール茶筌 ver.2.2.3\cite{chasen}を採用した．
また，統計情報としては文字間の共起情報を採用した．
\subsection{共起関係抽出}
文字間の共起情報が頻出文字列認識に有用であるとの延澤らの主張\cite{nobesawa01anlp}に基づき，本稿では対象ドメイン固有の頻出文字列の抽出に利用する統計情報として，文字間の共起情報を採用した．
そこで，前処理として訓練コーパス中の各二文字ペアの共起頻度を数え上げる．

本システムは訓練コーパスに全く制限を設けない．
品詞情報などの付加情報を一切利用しないため，形態素解析や構文解析，タグ付けなども必要としない．

訓練コーパス中の文字共起頻度の数え上げには d-bigram 確率モデル\cite{tsutsumi93}を利用した．
d-bigram とは距離を考慮した bigram モデルであり， {\tt abbc} という文字列の場合，隣接する ({\tt a}, {\tt b}) などだけでなく {\tt a} と {\tt c} のように離れて出現する二文字の共起関係も取得する．
この例では {\tt a} と {\tt c} は距離 3 となり ({\tt a}, {\tt c}; 3) のように表される．
隣接 bigram では視野が非常に狭く文脈情報が利用できないという欠点があり，特に文字レベルでの利用はノイズが大きい．
これに対し， d-bigram モデルは距離の情報を保有することでこの問題に対処しており，例えば 3 単語の並び (trigram) も十分に評価できることが示されている\cite{tsutsumi96}．
さらに，同じ文中であっても離れて出現する文字同士は近接して出現する文字同士に比べて関係が薄いと考えることができる\cite{church89acl}という主張に基づき， d-bigram の取得，利用に際して距離の上限および距離の影響力を設定することが可能である．
\section{システム M: 茶筌への組み込み}
本稿で提案するシステムは，日本語を対象とした形態素解析ツール・茶筌に統計情報を利用した文字列抽出モジュール (システム M) を組み込むことで統計情報の活用を図るものである．
これは茶筌に特化した手法ではなく，茶筌本体の構造を改変するものではない．
\subsection{茶筌での統計情報の利用}
茶筌は辞書ベースの形態素解析ツールであり，文単位で処理を行なう．
図 \ref{fig:flo-o} に茶筌による形態素解析の流れを示す．
入力であるテストコーパスは一文ずつ処理され，形態素解析結果が出力される．
形態素解析処理においては，事前に準備された辞書を利用する．
\begin{figure}[hbt]
\begin{center}
\epsfile{file=flo-o.eps,scale=0.21}
\caption{茶筌のみでの形態素解析}
\label{fig:flo-o}
\end{center}
\end{figure}

図 \ref{fig:flo-m} に本稿で提案する統計情報利用システムを茶筌に組み込んだ場合の形態素解析の流れを示す．
\begin{figure}[hbt]
\begin{center}
\epsfile{file=flo-m.eps,scale=0.21}
\caption{システム M を組み込んだ茶筌による形態素解析}
\label{fig:flo-m}
\end{center}
\end{figure}
本稿で提案するシステムではまず茶筌に有繋文字列抽出モジュールを組み込むことにより文字列の認識を行ない (\ref{sec:ukninshiki} 節)，抽出された文字列に専用の品詞名を付けることで辞書の見出し語と同等に扱うことができるようにする (\ref{sec:ukriyou} 節)．

認識する文字列は延澤らの提案した有繋文字列\cite{nobesawa96coling}と呼ばれるもので，文字間の共起情報のみから一塊と推測された文字列である．
本システムを組み込むことで，茶筌の持つ辞書の他に，その文中に含まれる有繋文字列を形態素の候補として利用することが可能となる．
辞書に掲載されている語句が有繋文字列として抽出された場合は，辞書の情報を優先する．
従って，辞書既登録語句は有繋文字列として抽出されることはない．
\subsection{形態素解析時における有繋文字列の認識}
\label{sec:ukninshiki}
文中の $i$ 番目の文字と $i+1$ 番目の文字の間の有繋評価値 $\uk(i)$ の算出式を式 (\ref{exp:uk}) に示す\cite{nobesawa96coling}．
ただし， $w_i$ は文 $w$ の $i$ 番目の文字， $d$ は 2 文字間の距離， $d_{max}$ は $d$ の最大値， $g(d)$ は距離の影響に対する重み付け関数であり，本稿では $d_{max} = 5$， $g(d) = d^{-2}$ とした\cite{sano96}．
\begin{eqnarray}\label{exp:uk}
\uk(i) = \sum_{d=1}^{d_{max}}\sum_{j=i-(d-1)}^{i} \mi_d(w_j,w_{(j+d)};d) \times g(d)
\end{eqnarray}
また， 2 文字間の相互情報量の計算式を d-bigram に対応するよう拡張したものとして式 (\ref{exp:mid}) を利用した\cite{nobesawa96coling}．
ただし， $x$, $y$ は各文字， $d$ は 2 文字間の距離， $P(x)$ は文字 $x$ が出現する確率， $P(x,y;d)$ は d-bigram ($x$, $y$; $d$) が起こる確率とする．
\begin{eqnarray}\label{exp:mid}
\mi_d(x,y;d) = log_2\frac{P(x,y;d)}{P(x)P(y)}
\end{eqnarray}

図 \ref{fig:mountain-valley} に有繋評価値を利用した文字列認識の例を示す\cite{nobesawa96coling}．
\begin{figure}[hbt]
\begin{center}
\epsfile{file=mountain-valley-half.eps,scale=0.21}
\caption{有繋評価値を利用した文字列認識}
\label{fig:mountain-valley}
\end{center}
\end{figure}
図の横軸が入力文，縦軸が有繋評価値を示す．
横軸のアルファベットは入力文中の各文字を示す．
文中の各隣接文字ペア間の有繋評価値は，隣接文字ペアの共起頻度が高いほど高くなる．
従って図の中で評価値を繋いだ線が山状になっている部分は共起する可能性の高い部分であり，一塊の文字列である可能性が高い．
これに着目し山状の部分を抽出することで，文中の文字列の認識を行なう．
\subsection{形態素解析時における有繋文字列の利用}
\label{sec:ukriyou}
形態素解析中 \ref{sec:ukninshiki} 節の手法で認識された有繋文字列は専用の品詞およびコストが設定され既存の辞書の登録語と同等として形態素解析処理に利用される．
有繋文字列は特定の品詞に対応するものではないが，個々の有繋文字列に対してその品詞の推定を行なうことはシステムの実時間性を損ねるため，品詞「有繋文字列」を新設しこれに対して予め品詞情報を設定しておく．
実際に認識される文字列は名詞またはそれに準じるものがほとんどであるため，品詞「有繋文字列」の接続はすべて名詞接続とした．

茶筌では各語句に形態素コストが設定されている．
有繋文字列は文字間の共起情報によって決定するものであり，一塊の文字列であると評価する際の評価値の高さがそれぞれ異なる．
そこで，評価値によって有繋文字列を 5 段階に分類し，段階ごとに形態素コストを設定することで，評価値の高いものを優先的に利用できるように設定する．
\subsection{統計情報を組み込んだ茶筌による形態素解析例}
図 \ref{fig:ex1newspaper} に，本システムを茶筌に組み込んだ場合の実行例を挙げる．
図 \ref{fig:ex1newspaper} の上段が茶筌のみで解析を行なった場合，下段がシステム M を組み込んで解析を行なった場合の切り分け結果である．
下線は辞書未登録語を，太字は有繋文字列として抽出された部分を示す．
\begin{figure}[hbt]
\begin{center}
\begin{tabular}{ll}
茶筌のみ  & 自然 / 言語 / 処理 / の / 分野 / で / は /\\
          & 統計 / 情報 / として / \underline{bigram} / な / ど /\\
          & n / - / \underline{gram} / が / よく / 用い / られる / ．\\
&\\
本システム& {\gt 自然言語} / 処理 / の / 分野 / で / は /\\
          & {\gt 統計情報} / として / {\bf bigram} / など /\\
          & {\bf n-gram} / が / よく / 用い / られる / ．\\
\end{tabular}
\caption{本システムによる切り分けの例}
\label{fig:ex1newspaper}
\end{center}
\end{figure}
図 \ref{fig:ex1newspaper} では辞書未登録語 2 文字列が本システムを利用することで有繋文字列として抽出されている．
「 bigram 」のように辞書未登録語がそのままの形で一語である場合，この部分の切り分け結果は正解と変わらないため他の部分の解析結果への影響がない場合が多いが，図 \ref{fig:ex1newspaper} の例のように他の部分へ影響を与える場合もある．
この例では本システムを利用し「 bigram 」の品詞が「有繋文字列」となったことで「など」が正しく認識されている．
「 n-gram 」は茶筌のみを利用した場合「 n (記号)」「 - (記号)」「 gram (未知語)」に分割された．
複数の字種から成る未知語の場合は字種ごとに区切られる場合がほとんどである．
システム M では字種情報を利用せずすべての字種の文字を同様に扱うため，字種の替わり目で誤分割されず，「 n-gram 」の認識に成功した．

またシステム M を利用することで「自然言語」「統計情報」などの複合語も多く認識された．
「自然言語」は，複合語「自然言語処理」の一部分であるが，「自然言語」自体一塊で複合語を形成し「自然言語処理」の構成要素となると考えられる．
\section{システム D: 辞書への組み込み}
訓練コーパスから取得した共起情報をそのまま利用する手法ではノイズの問題が防げない．
この問題を解決するため，本章では共起情報をそのまま利用するのではなく，共起情報を利用して辞書登録候補文字列を抽出しこれを事前に辞書に登録する手法を提案する．

図 \ref{fig:flo-d} に本章で提案する辞書作成システムを利用して事前に作成した有繋文字列辞書を茶筌の辞書に組み込んだ場合による形態素解析の流れを示す．
\begin{figure}[hbt]
\begin{center}
\epsfile{file=flo-d.eps,scale=0.21}
\caption{システム D を利用した茶筌による形態素解析}
\label{fig:flo-d}
\end{center}
\end{figure}
基本的な流れは図 \ref{fig:flo-o} と同じだが，利用する辞書は茶筌の基本辞書に有繋文字列辞書を組み込んだものとなっている．
この有繋文字列辞書は訓練コーパスから作成したものであり，この辞書を組み込むことによってドメイン固有の文字列を形態素解析処理で利用する．

本章で作成する辞書は茶筌の辞書の補完という位置付けであり，辞書既登録語は登録しない．
また，茶筌が元々持つ辞書の改変を行なうこともない．
\subsection{登録文字列の属性設定}
\label{sec:morphcost}
辞書登録文字列の属性は以下のように決定する．
\paragraph{品詞}
登録文字列それぞれに対して適切な品詞を人手で設定することは多大な労力を必要とするだけでなく，その適切さの評価や曖昧性の問題などが存在するため，本稿では登録文字列はすべて同じ品詞とした．
登録文字列に割り振る品詞として「有繋文字列」を新設した．
品詞「有繋文字列」と他の品詞との接続コストの設定は茶筌の既存の品詞「名詞」中の「一般」カテゴリに準拠することとした．
\paragraph{形態素コスト}
個々の登録文字列の形態素コストはその文字列の頻度情報などの情報に基づいて個々に設定することとする．
この関数で利用するパラメータは，本稿で提案する複数の有繋文字列辞書作成手法に依存するものとする．

形態素コスト $c_i$ の算出式を式 (\ref{exp:morphcost}) に示す．
ここで $i$ は文字列， $x_i$ は文字列 $i$ の情報を示す値であり， $x_i$ に適用する値を変化させることで各辞書の特徴を形態素コストに反映させる．
\begin{eqnarray}
\label{exp:morphcost}
c_i = \left[-\frac{c_{max}}{x_{max} - 1} \times x_i + \frac{c_{max} \times x_{max}}{x_{max} - x_{min}}\right]
\end{eqnarray}
形態素コストは「コーパス内に 1 回出現する文字列の形態素コストを 4,000 とする」とする茶筌の定義に基づき，下限 $c_{min}$ を 0，上限 $c_{max}$ を 4,000 または 8,000 とする．
$x_{min}$ および $x_{max}$ は各辞書で利用する $x_i$ によって決まる．
\subsection{登録文字列の選択}
\label{sec:jisho}
本稿では辞書に登録する文字列の選択手法を 4 種類用意し， 4 つの辞書を作成した (表 \ref{tab:jisho})．
\begin{table}[ht]
\begin{small}
\begin{center}
\caption{有繋文字列登録のための辞書一覧}
\label{tab:jisho}
\begin{tabular}{l|lll}
&作成基準            &選別&コスト計算の基準\\
\hline
辞書 \idl&一人の評価による選別&人手&文字列出現頻度\\
辞書 \chk&複数人による評価得点&人手&評価得点\\
辞書 \frq&出現頻度            &自動&文字列出現頻度\\
辞書 \pos&品詞情報            &自動&品詞出現頻度\\
\end{tabular}
\end{center}
\end{small}
\end{table}
本稿では辞書登録の対象を名詞に準じる文字列に絞る．
\subsubsection{辞書 \idl: 一人の評価による登録文字列選択}
\label{sec:whatisidl}
訓練コーパスから抽出された有繋文字列を一人の手によってすべての候補をチェックし，
そのままで辞書登録可能な有繋文字列，過接合有繋文字列から適切な部分を切り出した文字列の 2 種類の文字列を選択した．
過分割有繋文字列については，分割され削除されていた部分が容易に推測できる場合であっても，登録文字列としなかった．
登録文字列の切り出しの対象は，名詞，複合名詞，数式，数値 (単位も含む)，意味のある記号の羅列，英単語の羅列とし，
茶筌既登録語は登録文字列から除外した．

各登録文字列 $i$ の形態素コスト $c_i$ は，式 (\ref{exp:morphcost}) に $x_i$ として $i$ の候補文字列としての出現頻度 $f_i$ を適用して算出した．
他の選択手法と異なり完全に人手で確認しているためノイズの心配がないことから， $c_{max}$ は 4,000 とした．

ユーザ個人が一人で選択する場合，登録語とする基準をユーザ個人で設定できるため，複数人で選別を行なう場合のようなばらつきや基準の統一といった問題がない．
しかし，ドメインが大きくなれば登録候補語も増加するため，一個人がこの選別を行なうことは大きな労力となる．
\subsubsection{辞書 \chk: 複数人による評価を利用した登録文字列登録}
\label{sec:whatischk}
11 名の被験者に登録候補文字列のリストを提示し，登録すべきもの，登録すべきか迷うもの，登録すべきでないもの，判断できないものの  4 段階に分類してもらい，それぞれ 2，1，0 点として集計を行なった．
「判断不能」は評価から外すものとした．
評価得点が 0 となった文字列は，被験者全員が登録すべきでないと判断したものであるため，登録候補としない．

各登録文字列 $i$ の形態素コスト $c_i$ は，式 (\ref{exp:morphcost}) に $x_i$ として $i$ の評価得点 $s_i$， $c_{max} = 8,000$ を適用して算出した．

対象ドメインに詳しい複数の人間が選別を行なうことで，一人一人の労力の軽減が図れるだけでなく，適切な候補語選択がなされると考えられる．
しかし選択を行なう人の専門分野や考え方などの相違から，候補語の絞り込みが難しくなる場合もあり得る．
\subsubsection{辞書 \frq: 出現頻度による登録文字列選択}
\label{sec:whatisfrq}
自動的に選別を行なう場合の最もシンプルな手法は，登録候補になんらかの順位付けを行ないそれに従って登録文字列を決定するものである．
評価値は共起情報を基に算出するため，出現頻度の高い文字列は評価値も高くなる傾向があり，この二点は独立ではない．
従って，本稿では出現頻度のみを基準として辞書登録文字列の選択および形態素コストの設定を行なう．

各登録文字列 $i$ の形態素コスト $c_i$ は，式 (\ref{exp:morphcost}) に $x_i$ として $i$ の候補文字列としての出現頻度 $f_i$， $c_{max} = 8,000$ を適用して算出した．
出現頻度が 1 の文字列はノイズである可能性があるため登録文字列から外し，出現頻度 2 の時形態素コストは最大の 8,000 を採るように設定した．
\subsubsection{辞書 \pos: 品詞情報による登録文字列選択}
\label{sec:whatispos}
登録候補文字列を茶筌に掛けて形態素解析を施し，得られた品詞情報を利用して登録文字列を決定する．
各登録文字列 $i$ の形態素コスト $c_i$ は，式 (\ref{exp:morphcost}) に $x_i$ として $i$ に対応する品詞列の候補文字列としての出現頻度 $t_i$， $c_{max} = 8,000$ を適用して算出した．
\section{システム M+D: 辞書登録と切り分け処理の併用}
処理の段階で動的に有繋文字列を認識し利用するシステム M では，ノイズを完全に防ぐことは不可能である．
ノイズを抑えるためには，動的な処理でなく，事前に必要な有繋文字列を辞書登録してしまう方法が有効である．
辞書登録を行なうことでドメイン固有の文字列を辞書に反映させることが可能となるが，完全な辞書の作成は不可能であるという辞書ベースの手法の問題点の完全な解決にはならない．
また本稿で利用する d-bigram 確率モデルは bigram 情報の積み重ねであるため特に複合語やこれに類するものの認識において間に入る語句を柔軟に扱えるという利点があるが，辞書登録では d-bigram の持つ柔軟性が失われる．
これらの問題を解決するために，辞書登録と切り分け処理の併用が考えられる．
事前にドメイン固有文字列の辞書登録を行ない，さらに補助として組み込みの切り分けシステムを利用することで，頻出語句の認識が可能な上，ノイズの減少を図ることが可能となる．
図 \ref{fig:flo-md} に本稿で提案する統計情報利用システムと辞書作成システムを利用した有繋文字列辞書の両方を茶筌に組み込んだ場合による形態素解析の流れを示す．
\begin{figure}[hbt]
\begin{center}
\epsfile{file=flo-md.eps,scale=0.21}
\caption{システム M，D 両方を組み込んだ茶筌による形態素解析}
\label{fig:flo-md}
\end{center}
\end{figure}

表 \ref{tab:threshold} にシステム M を組み込んだ実験での閾値ごとの形態素コストを示す．
\begin{table}[hbt]
\begin{center}
\caption{システム M での閾値および形態素コスト}
\label{tab:threshold}
\begin{small}
\begin{tabular}{r|rr}
閾値&\multicolumn{2}{c}{形態素コスト}\\
    &実験 M & 実験 M+D \\
\hline
\hline
11& 1,000&10,000\\
 9& 3,000&15,000\\
 7& 4,000&20,000\\
 5& 5,000&25,000\\
 3&20,000&30,000\\
\end{tabular}
\end{small}
\end{center}
\end{table}
実験  M+D ではシステム  M をシステム D の補完の立場で利用するため実験 M に比べて形態素コストを大きく設定している．
\section{実験および考察}
本稿で報告する実験は，論文コーパスを茶筌を用いて形態素解析を行なったものである．
対象ドメインの持つ統計情報を利用することで形態素解析の精度の向上を図る．
\subsection{実験内容}
\label{sec:experiments}
本章では，本稿で提案している 2 つのシステムの利用実験について報告する．
システム M だけを利用した場合，システム D だけを利用した場合， M と D 両方併用した場合，どちらも利用しなかった場合の 4 種類の実験を行なった．
システム D を利用した実験では，それぞれ， \ref{sec:jisho} 節で提案した 4 種類の辞書を試みた．
以上， 10 種類の実験について報告する．
\subsubsection{コーパス}
本実験では，コーパスとして自然言語処理分野の論文を利用した．
訓練コーパス，テストコーパス共に，自然言語処理分野を専攻する学生 6 人の論文計 17 本を合わせて作成したものである．
本稿では，統計情報利用システムの組み込みで利用する統計情報を得るための訓練コーパスおよび辞書登録のための辞書登録文字列の抽出について，同一の訓練コーパスを利用した．
訓練コーパスに含まれる文の総数は 4,816，含まれる文字の数は 213,489 (平均 44.33 文字 / 文) である．
また本稿では，それぞれの実験における形態素解析対象として，同一のテストコーパスを利用した．
テストコーパスに含まれる文の総数は 1,149，含まれる文字の数は 55,755 (平均 48.52 文字 / 文) である．
\subsubsection{正解形態素解析，正解文字列}
\label{sec:seikaikeitaisokaiseki}
比較のため，テストコーパスを人手で形態素解析したものと辞書登録すべき文字列の正解リストを作成した．
\paragraph{正解形態素解析}
テストコーパスを茶筌に掛け，その結果を人手で修正したものである．
修正の対象は切り分け誤りおよびタグ付け誤りとした．
切り分け誤りは，明らかな間違いの他，複合語とすべき語が切り分けられている場合も含む．
複合語としたのは名詞の並びの他，接頭詞が付着するもの，英単語列などである．
そのほか，数式，数値は全体で一塊とした．
\paragraph{正解文字列}
正解形態素解析結果中，茶筌に登録されていない文字列を正解文字列とした．
ただし，本稿の手法では名詞に準じるものを対象として考えているため，動詞，形容詞などに相当する文字列は正解文字列から除外した．
\subsection{実験結果}
\label{sec:expresult}
表 \ref{tab:expresult} に各実験の結果をまとめる．
「茶筌のみ」とある実験は統計情報を利用しない場合のものであり，これと比較することで本稿で提案するシステムの有用性を示す．
未知語削減率は茶筌のみの場合に対する割合，形態素総数は正解形態素解析での形態素総数に対する割合である．
\begin{table}[ht]
\begin{small}
\begin{center}
\caption{実験結果}
\label{tab:expresult}
\begin{tabular}{l|rr|rr|rr}
実験            &\multicolumn{2}{c|}{完全チェック}&\multicolumn{2}{c|}{拡張チェック}&未知語&形態素\\
                &適合率&再現率&適合率&再現率&削減率&総数\\
\hline
\hline
茶筌のみ        &  --- &  --- &  ---  &  --- &  --- &118.8\%\\
\hline
システム M      &38.3\%&26.0\%&50.1\%&51.7\%&86.1\%& 98.6\%\\
\hline
システム D\idl  &77.0\%&38.9\%&98.1\%&47.8\%&71.4\%&110.2\%\\
システム D\chk  &67.2\%&23.5\%&96.9\%&34.1\%&66.3\%&112.6\%\\
システム D\frq  &55.8\%&17.4\%&81.2\%&27.6\%&58.6\%&112.5\%\\
システム D\pos  &68.2\%& 5.8\%&97.5\%& 8.7\%&65.0\%&117.1\%\\
\hline
システム M+D\idl&75.5\%&39.4\%&96.3\%&50.0\%&86.5\%&109.3\%\\
システム M+D\chk&63.1\%&23.5\%&91.9\%&37.3\%&86.1\%&110.6\%\\
システム M+D\frq&52.3\%&17.6\%&79.1\%&31.6\%&85.9\%&110.5\%\\
システム M+D\pos&47.7\%& 5.7\%&59.0\%&14.2\%&86.1\%&113.5\%\\
\end{tabular}
\end{center}
\end{small}
\end{table}

それぞれのシステムについての考察は \ref{sec:resultsystemm} 節以降で述べる．
\paragraph{適合率，再現率}
\label{sec:expresultprf}
表 \ref{tab:expresult} に示した適合率，再現率の算出には，正解形態素解析，正解文字列 (\ref{sec:seikaikeitaisokaiseki} 節) を利用した．
有繋文字列と認識された箇所に対する正解形態素解析中の正解文字列の出現箇所の割合を適合率，全ての正解文字列に対して正しく認識された文字列の割合を再現率とした．
正解文字列に完全にマッチしたものを完全適合率および完全再現率，正解文字列と同一ではないが誤りではない有繋文字列の場合を拡張適合率および拡張再現率とした
\footnote{拡張適合率，拡張再現率では，有繋文字列が含まれている場合のみを数えた．すなわち正解文字列「自然言語処理」を一塊として抽出するのに失敗した場合，「自然言語 (有繋文字列)」「処理 (名詞)」のように切り分け結果に有繋文字列が含まれ，かつ切り分け結果が誤りでないと判断された場合を成功，「自然言 (有繋文字列)」「語 (名詞)」「処理 (名詞)」のように有繋文字列を含んでいても切り分け誤りがある場合は失敗，また「自然 (名詞)」「言語 (名詞)」「処理 (名詞)」のように切り分けは誤りでないが有繋文字列を含んでいない場合も失敗とした．}
．

再現率の計算には正解文字列の総数を利用しているが，この正解文字列はテストコーパスから作成したものであり，訓練コーパス中に出現しない文字列も多く存在する．
再現率が低くなる理由の一つに，論文中に含まれる数式および数値が挙げられる
\footnote{正解文字列から数式，数値を除いた場合，システム M+D\idl の完全再現率は 46.9\%，拡張再現率は 56.7\% となる．}
．
表 \ref{tab:expresult} では数式の扱いが厳しく，完全に適合する数式でないと失敗としているが，実際には数式は必ずしも一定の形で現れるとは限らず，辞書登録の対象とするには無理がある．
その他の要因としては，論文ごとの表記の揺れや出現語句の相違，複合語の認識失敗がある．

表 \ref{tab:expresult} を見ると，適合率は完全にマッチした場合で最高 77.0\%，拡張適合率では最高 98.1\% と高い．
これは，有繋文字列が高い精度で抽出されたことを示す．
有繋文字列は辞書既登録語を含まないことから，本手法が辞書未登録語の抽出に有効であると言える．

どの辞書を利用した場合でも，適合率はシステム D がシステム M+D の結果を上回る．
これは辞書登録文字列の選択が適切になされたことを示す．
システム M+D ではシステム M の影響でノイズが増えたが，システム M だけの場合では 50.1\% であり，システム D がノイズの削減に有効であることが判る．
\paragraph{未知語削減率}
\label{sec:expresultunk}
表 \ref{tab:expresult} の未知語削減率は，茶筌のみでの実験結果中の未知語数に対してどれだけ未知語が削減されたかを示す値である．
統計情報を一切使わない場合 483 の文字列が未知語として出力された．
本稿のシステムでは，最悪の場合でもその 58.6\% の認識に成功している．

未知語認識の上限は 86\% 前後となっているが，これらは訓練コーパス中に出現しなかった未知語 67 個の抽出に失敗したものである．
表 \ref{tab:expresult} によると実験 M，実験 M+D では未知語削減率が約 86\% となっており，
システム M が訓練コーパスに出現する未知語を最大限に削減できることが判る．
それに対して辞書登録のみを利用した実験 D は低くなっており，辞書登録だけでは未知語を完全にカバーすることが困難であることを示している．
システム M+D はシステム M の持つノイズの問題を抑えながら最大限の未知語削減率を保っている．

訓練コーパス中に出現しない文字列の認識は， $n$-gram のように連続型のモデルでは不可能である．
それに対し，本手法で採用した d-bigram はギャップのある事象の共起情報を複数組み合わせて文字列認識を行なうため，訓練コーパスに出現しない文字列についても有繋性の推定が可能である\cite{nobesawa98ipsjnl}．
推定の精度は訓練コーパスや認識対象文字列に依存するため，これらの効果的な認識が今後の課題として挙げられる．
\paragraph{総形態素数}
\label{sec:expresultmorph}
表 \ref{tab:expresult} の総形態素数とは各実験での形態素の総数の正解形態素解析に含まれる総形態素数に対する割合を示す．
総形態素数の割合が高ければ過分割が多いことが，割合が低ければ過接合が多いことが推察される．

表 \ref{tab:expresult} の形態素総数を見ると，本稿のシステムを利用しない場合の形態素総数は正解形態素解析の 118.8\% であり，過分割が頻繁に起こって形態素数が 2 割近く増えていることが判る．
システム M だけの場合にはこれが 100\% を下回り，過分割がかなり抑えられている．
しかし，訓練コーパスとテストコーパスが独立であるため未知語や過分割を完全には除去できないことを考えると，この結果には過接合も多く含まれることが考えられる．
実際，システム M では過接合が多く完全適合率は他に比べて低くなっていた．
システム D だけの場合は形態素総数が正解の 110\% から 117\% となっており，茶筌のみの場合に比べて過分割が減少している．
システム M+D は適合率を十分高く保ったまま形態素総数を 110\% 前後にしており，システム D だけの場合よりもさらに正解に近づいている．
本稿はオープンコーパスでの実験を行なったため，特に数式や数値などの文字列で過分割は避けられない．
これらのことから，システム M と D を的確に組み合わせることで過分割を最大限減少させたと言える．
\subsection{実験 M: 統計情報を組み込んだ茶筌による形態素解析}
\label{sec:resultsystemm}
表 \ref{tab:expresult} によると
システム M の適合率は他に比べてかなり低いが，これは形態素解析時に有繋文字列とされた文字列が他に比べて多いことにも起因する．
システム M では有繋文字列の絞り込みを一切行なっていないため動詞句などノイズとなる文字列を多く含むことが適合率の低下の原因である．

システム M での形態素総数は正解形態素解析の 98.6\% と唯一 100\% を下回り，システム M を適用することで過分割を大幅に削減できることが判る．
しかし，
本稿の手法では正解形態素解析よりも形態素総数が多くなることは避けられない．
正解形態素解析よりも形態素数が少なくなったシステム M は過接合を多く含んでおり，他に比べて精度が良いとは言えない．

システム M は拡張再現率が他に比べて高い (表 \ref{tab:expresult})．
これはシステム M による動的な統計情報の利用が正解文字列の認識に有効であることを示している．
辞書登録の場合ノイズは減少するが，柔軟な対応が必要な文字列の認識が難しくなる．
システム M の利用はこの問題点の解消に有効である．

表 \ref{tab:paperresultpos} に論文コーパスを茶筌を利用して形態素解析した実験結果を示す．
\begin{table}[hbt]
\begin{small}
\begin{center}
\caption{論文コーパスでの実験結果}
\label{tab:paperresultpos}
\begin{tabular}{l|rrrrrrrrr|r}
&\hspace{-2pt}{\footnotesize 未知語}&\hspace{-2pt}{\footnotesize 有繋
文字列}&\hspace{-2pt}{\footnotesize 名詞}&\hspace{-2pt}{\footnotesize 
動詞}&\hspace{-2pt}{\footnotesize 副詞}&\hspace{-2pt}{\footnotesize 形
容詞}&\hspace{-2pt}{\footnotesize 助動詞}&\hspace{-2pt}{\footnotesize 
助詞}&\hspace{-2pt}{\footnotesize 記号}&{\footnotesize 総
形態素数}\\
\hline
\hline
\hspace{-3pt}{\footnotesize 茶筌のみ}&\hspace{-2pt}{\footnotesize 483}&\hspace{-2pt}{\footnotesize
---}&\hspace{-2pt}{\footnotesize 12,863}&\hspace{-2pt}{\footnotesize
3,892}&\hspace{-2pt}{\footnotesize 213}&\hspace{-2pt}{\footnotesize
364}&\hspace{-2pt}{\footnotesize 1,777}&\hspace{-2pt}{\footnotesize
8,499}&\hspace{-2pt}{\footnotesize 4,445}&\hspace{-2pt}{\footnotesize
33,812}\\
\hspace{-3pt}{\footnotesize システム M}&\hspace{-2pt}{\footnotesize
67}&\hspace{-2pt}{\footnotesize 2,218}&\hspace{-2pt}{\footnotesize
9,086}&\hspace{-2pt}{\footnotesize 3,468}&\hspace{-2pt}{\footnotesize
215}&\hspace{-2pt}{\footnotesize 321}&\hspace{-2pt}{\footnotesize
1,641}&\hspace{-2pt}{\footnotesize 7,346}&\hspace{-2pt}{\footnotesize
2,898}&\hspace{-2pt}{\footnotesize 28,048}\\

\end{tabular}
\end{center}
\end{small}
\end{table}
表 \ref{tab:paperresultpos} を見ると名詞と記号が減少していることが判る．
論文コーパスでは複合名詞の出現が多く，頻出する複合名詞が有繋文字列として一塊にされたことが，名詞の減少の大きな理由である．

記号の減少については，数式と英字文字列，頻出言い回しが大きな理由として挙げられる．
英字，その他の記号の減少の最大の理由は数式である．
数式中に含まれる文字列はほとんど未知語または記号として扱われるが「 P(x) 」など確率を示す関数などは有繋文字列として認識されており，結果的に記号タグの振られる文字列が減少した．
句読点と助詞，動詞の減少は，主に頻出言い回しに起因する．
頻出言い回しの抽出はシステム M では防ぐことができず，過接合の頻出の原因となっている．
しかし表 \ref{tab:paperresultpos} を見ると副詞など他の品詞の形態素数の変化は小さく，システム M の利用による誤解析は少ないことが判る．
\subsection{実験 D: 有繋文字列の辞書登録}
本稿では，訓練コーパスを入力として抽出された有繋文字列を辞書登録候補とし，\ref{sec:jisho} 節で提案した 4 通りの方法で辞書登録文字列の選択を行なった．
本稿ではそれぞれの登録候補選択手法について文字列ごとに形態素コストの設定を行なっている．

登録候補文字列を取得するため，訓練コーパスに対して実験 M と同じ方法で形態素解析を施した．
この結果有繋文字列として出力された文字列 5,402 が各辞書の登録候補文字列となっており，この集合から辞書登録文字列の絞り込みを行なう．
\subsubsection{実験 D\idl: 一人での評価による登録文字列選択}
\label{sec:didl}
抽出された有繋文字列から選択された登録候補文字列の異なり文字列数は 2,482 であった．
茶筌既登録語を除いた結果，登録文字列数は 1,738 となった．
辞書 \idl を利用した実験は適合率が高く，人手での選別でノイズをほぼ抑えることが可能であることが示された．
\subsubsection{実験 D\chk: 複数人による登録文字列選択}
\label{sec:dchk}
複数人で選択を行なった結果は機械的に選択した 2 辞書によるものと大差ないという結果になった．
システム M と組み合わせた場合には，機械的に作成した辞書に比べてノイズの抑制に効果があった．
辞書登録の候補とすべきかどうかの判断では被験者それぞれの考え方にかなりの違いがあったことが問題の一つであると考えられる．
\subsubsection{実験 D\frq: 出現頻度による登録文字列選択}
\label{sec:dfrq}
登録候補文字列の異なり数 2,807 のうち出現頻度が 1 のものは 2,152 であり， 76.7\% に達する．
これを登録候補から外すことで，登録文字列数は 655 となったが，これは 4 辞書中最少である．
辞書 \frq を用いた実験の結果を見ると，他に比べて未知語削減率が低いことが判る (表 \ref{tab:expresult})．
これは頻度だけでは未知語の認識には不十分であることを示す．
しかし辞書 \frq を利用した場合でも形態素総数の削減は辞書 \chk と同等，特にシステム M と組み合わせた場合には辞書 \idl を利用した場合に近い結果となっていることが判る．
これはこの辞書が過分割の削減に有効であることを示し，頻度情報が複合語の認識に有効であることが判る．
\subsubsection{実験 D\pos: 品詞による登録文字列選択}
\label{sec:dpos}
辞書 \pos を用いた実験の結果は，システム D，システム M+D のどちらでも非常に再現率が低くなっている．
これは，この辞書を用いた場合有繋文字列として形態素解析された文字列の数が少なかったためである．
しかし
辞書 \pos を利用した場合でも未知語数の削減は他と同等に実現しており，辞書 \pos は特に未知語の削減に対して有効であることが判る．
これは，辞書 \pos の作成では品詞の情報を利用することで未知語を含むものが優先的に登録されたためであると考えられる．
辞書登録すべき文字列とそうでない文字列との品詞情報には明らかな違いがあり，異なり品詞列のうち辞書登録の対象とすべき品詞列は約 1 割であることが判った．
\subsection{実験 M+D: 辞書登録と統計情報利用システム組み込みの併用}
形態素解析時の統計情報の利用を行なうシステム M は動的な文字列の切り分けを可能にし，柔軟な処理が可能となった反面，ノイズの問題が起こる．
それに対して訓練コーパスから得られた統計情報を元に辞書未登録文字列を抽出し辞書登録を行なうシステム D は，登録文字列の絞り込みを行なうためノイズの問題を軽減することが可能であるが，柔軟な処理には不向きである．
従って，システム D で認識しきれない部分をシステム M が補完する形で両者を組み合わせることで，訓練コーパスの情報を生かした処理が可能となり，精度の向上が期待できる．

訓練コーパスからの学習では形態素総数の削減は正解の 110\% 程度が限界だが，システム M+D では形態素総数は 110\% 前後となっており，特に人手による辞書作成の場合適合率が十分高いことを考えると，コーパスの情報を利用することでテストコーパスの形態素切り分けの精度を十分にあげたと言ってよい．

またシステム M+D ではシステム M に対して大幅に過接合を削減したにもかかわらず，未知語の削減率はシステム M と同等であり，本手法で未知語の削減率を保ったままシステム D と組み合わせることに成功した．
未知語の削減率は最高で 86.5\% となっているが，未知語の 14\% 程度は訓練コーパス中に出現しないものであり，このことを考えると，本稿で提案した手法で未知語を最大限取得することに成功したと言える．
さらにシステム M と D を組み合わせることで訓練コーパスに出現しなかった未知語文字列が訓練コーパスの情報を利用することで正しく一塊と認識された例もあった．
\section{結論}
辞書ベースの自然言語処理ツールでは辞書未登録語の問題が防げない．
そのため辞書未登録語の自動認識の研究が盛んに行われているが，辞書未登録語には未知語，複合語の二種類の問題があり，ほとんどの研究はそのどちらかに対象を絞ったものである．

本稿では辞書ベースの形態素解析ツール・茶筌を対象とし，未知語，複合語双方の解決を目的として，統計情報を形態素解析段階で動的に利用するための組み込みシステム (システム M) と，統計情報を利用した辞書作成のシステム (システム D) の 2 種類のアプローチを提案した．
本稿の手法は茶筌に特化したものではなく，辞書ベースのツールに対してそのシステムを改変することなく付加的な情報を半自動的に追加し辞書未登録語の問題の解決を図るものである．
本稿で提案した手法は文字の共起頻度を元にしたものであり，構文解析などの処理は一切必要とせず，ヒューリスティクスも一切利用していない非常にシンプルなものでありながら，システム M のみで 86.1\%，システム D のみで 71.4\%，両方の併用で 86.5\% の未知語の解決に成功した．
また抽出した辞書未登録語の適合率が最高で 98.1\% となり，複合語についても高い精度で認識することができた．

本稿ではオープンコーパスを用いて実験を行ない，本稿で提案した 2 種類のアプローチを適切に組み合わせることで辞書未登録語の削減を効果的に行なうことに成功した．
さらに精度を上げるためにはヒューリスティクスの利用が必要となる．
\bibliographystyle{jnlpbbl}
\bibliography{biblio.ja}
\begin{biography}
\biotitle{略歴}
\bioauthor{延澤 志保}{
1994 年慶應義塾大学理工学部数理科学科卒業．
2002 年同大学院理工学研究科計算機科学専攻博士課程修了．
工学博士．
現在，東京理科大学理工学部助手．
}
\bioauthor{佐藤 健吾}{
1995 年慶應義塾大学理工学部数理科学科卒業．
1997 年同大学院理工学研究科計算機科学専攻修士課程修了．
現在，慶應義塾大学理工学研究科博士課程に在籍中．
}
\bioauthor{斎藤 博昭}{
1983 年慶應義塾大学理工学部数理科学科卒業．
工学博士．
慶應義塾大学理工学部専任講師．
}

\bioreceived{受付}
\biorevised{再受付}
\bioaccepted{採録}

\end{biography}
\end{document}
