<?xml version="1.0" ?>
<root>
  <author>宇津呂武仁颯々野学内元清貴</author>
  <title>正誤判別規則学習を用いた複数の日本語固有表現抽出システムの出力の混合</title>
  <jabstract>本論文では，日本語固有表現抽出の問題において，複数のモデルの出力を混合する手法を提案する．一般に，複数のモデル・システムの出力の混合を行なう際には，まず，できるだけ振る舞いの異なる複数のモデル・システムを用意する必要がある．本論文では，最大エントロピー法に基づく統計的学習による固有表現抽出モデルにおいて，現在位置の形態素が，いくつの形態素から構成される固有表現の一部であるかを考慮して学習を行なう可変(文脈)長モデルと，常に現在位置の形態素の前後数形態素ずつまでを考慮して学習を行なう固定(文脈)長モデルとの間のモデルの挙動の違いに注目する．そして，複数のモデルの挙動の違いを調査し，なるべく挙動が異なり，かつ，適度な性能を保った複数のモデルの出力の混合を行なう．次に，混合の方式としては，複数のシステム・モデルの出力(および訓練データそのもの)を入力とする第二段目の学習器を用いて，複数のシステム・モデルの出力の混合を行なう規則を学習するという混合法(stacking法)を採用する．第二段目の学習器として決定リスト学習を用いて，固定長モデルおよび可変長モデルの出力を混合する実験を行なった結果，最大エントロピー法に基づく固有表現抽出モデルにおいてこれまで得られていた最高の性能を上回る性能が達成された．</jabstract>
  <jkeywords>日本語固有表現抽出，複数システム混合，stacking，可変文脈長，最大エントロピー法，決定リスト学習</jkeywords>
  <subsubsection title="">*モデルの学習学習時には，現在位置の形態素が固有表現を構成しない場合には，5グラムモデルと同じく，現在位置およびその左右の二個ずつの形態素を考慮して学習を行なう．一方，現在位置の形態素M_i^NEがm(ただし本論文では3以下)個の形態素からなる固有表現の一部であるときには，固有表現を構成する形態素およびその左右の二個ずつの形態素を考慮して学習を行なう．つまり，現在注目している固有表現の長さmに応じて，考慮する周囲の形態素の総数が可変となる．(左側文脈)&amp;(固有表現)&amp;(右側文脈)M_-2^LM_-1^L&amp;M_1^NEM_i^NEM_m(3)^NE&amp;M_1^RM_2^R&amp;&amp;				&amp;&amp;eqnarray*また，現在位置の形態素M_i^NEが4個以上の形態素から構成される固有表現の一部であるときには，本論文では，以下の手順で，固有表現を構成するとみなす形態素数を3に限定するという近似を行なう．現在位置の形態素が固有表現の先頭である場合は，	先頭から三形態素のみが固有表現を構成するとみなし，	四番目以降の形態素については右側文脈であるとみなす．(左側文脈)&amp;(固有表現)&amp;(右側文脈)M_-2^LM_-1^L&amp;M_1^NEM_2^NEM_3^NE&amp;M_4^NEM_?^?&amp;&amp;				&amp;*2cm&amp;eqnarray*現在位置の形態素が固有表現の末尾である場合は，	末尾の三形態素のみが固有表現を構成するとみなし，	末尾の三形態素以外については左側文脈であるとみなす．(左側文脈)&amp;(固有表現)&amp;(右側文脈)M_?^?M_m-3^NE&amp;M_m-2^NEM_m-1^NEM_m^NE&amp;M_1^RM_2^R&amp;&amp;				&amp;&amp;eqnarray*その他の場合は，現在位置の形態素およびその前後一形態素ずつのみが	固有表現を構成するとみなし，	それ以外の形態素については左側もしくは右側文脈であるとみなす．(左側文脈)&amp;(固有表現)&amp;(右側文脈)M_?^?M_?^?&amp;M_i-1^NEM_i^NEM_i+1^NE&amp;M_?^?M_?^?&amp;&amp;					&amp;&amp;eqnarray*例えば，以下のように，現在位置の形態素M_i^NEが4個の形態素から構成される固有表現の一部である場合を考える．(左側文脈)&amp;(固有表現)&amp;(右側文脈)M_-2^LM_-1^L&amp;M_1^NEM_2^NEM_3^NEM_4^NE&amp;M_1^RM_2^R&amp;&amp;&amp;&amp;eqnarray*この場合，固有表現を構成する末尾の形態素M_4^NEが，あたかも固有表現の直後の右側文脈に存在する形態素であるかのようにみなされ，以下のように近似されてモデル化される．(左側文脈)&amp;(固有表現)&amp;(右側文脈)M_-2^LM_-1^L&amp;M_1^NEM_2^NEM_3^NE&amp;M_4^NEM_1^R&amp;&amp;&amp;&amp;eqnarray*</subsubsection>
  <section title="はじめに">これまで，機械学習などの分野を中心として，複数のモデル・システムの出力を混合する手法がいくつか提案され，その効果が報告されている．それらの成果を背景として，近年，統計的手法に基づく自然言語処理においても，複数のモデル・システムの出力を混合する手法を様々な問題に適用することが試みられ，品詞付け~，名詞句等の句のまとめ上げ~，構文解析(前置詞句付加含む)~などへの適用事例が報告されている．一般に，複数のモデル・システムの出力を混合することの利点は，単一のモデル・システムでは，全ての現象に対して網羅的かつ高精度に対処できない場合でも，個々のモデル・システムがそれぞれ得意とする部分を選択的に組み合わせることで，全体として網羅的かつ高精度なモデル・システムを実現できるという点にある．本論文では，日本語固有表現抽出の問題に対して，複数のモデルの出力を混合する手法を適用し，個々の固有表現抽出モデルがそれぞれ得意とする部分を選択的に組み合わせることで，全体として網羅的かつ高精度なモデルを実現し，その効果を実験的に検証する．一般に，日本語固有表現抽出においては，前処理として形態素解析を行ない，形態素解析結果の形態素列に対して，人手で構築されたパターンマッチング規則や統計的学習によって得られた固有表現抽出規則を適用することにより，固有表現が抽出される~．特に，統計的学習によって得られた固有表現抽出規則を用いる場合には，形態素解析結果の形態素列に対して，一つもしくは複数の形態素をまとめ上げる処理を行ない，同時にまとめ上げられた形態素列がどの種類の固有表現を構成しているかを同定するという手順が一般的である~．このとき，実際のまとめ上げの処理は，現在注目している位置にある形態素およびその周囲の形態素の語彙・品詞・文字種などの属性を考慮しながら，現在位置の形態素が固有表現の一部となりうるかどうかを判定することの組合わせによって行なわれる．一方，一般に，複数のモデル・システムの出力を混合する過程は，大きく以下の二つの部分に分けて考えることができる．	できるだけ振る舞いの異なる複数のモデル・システムを用意する．	(通常，振る舞いの酷似した複数のモデル・システムを用意しても，	複数のモデル・システムの出力を混合することによる精度向上は望めないことが予測される．)		用意された複数のモデル・システムの出力を混合する方式を選択・設計し，	必要であれば学習等を行ない，与えられた現象に対して，	用意された複数のモデル・システムの出力を混合することを実現する．複数の日本語固有表現抽出モデルの出力を混合するにあたっても，これらの()および()の過程をどう実現するかを決める必要がある．本論文では，まず，()については，統計的学習を用いる固有表現抽出モデルをとりあげ，まとめ上げの処理を行なう際に，現在位置の周囲の形態素を何個まで考慮するかを区別することにより，振る舞いの異なる複数のモデルを学習する．そして，複数のモデルの振る舞いの違いを調査し，なるべく振る舞いが異なり，かつ，適度な性能を保った複数のモデルの混合を行なう．特に，これまでの研究事例~でやられたように，現在位置の形態素がどれだけの長さの固有表現を構成するのかを全く考慮せずに，常に現在位置の形態素の前後二形態素(または一形態素)ずつまでを考慮して学習を行なうモデル(固定長モデル，~節参照)だけではなく，現在位置の形態素が，いくつの形態素から構成される固有表現の一部であるかを考慮して学習を行なうモデル(可変長モデル~，~節参照)も用いて複数モデルの出力の混合を行なう．次に，()については，重み付多数決やモデルの切り替えなど，これまで自然言語処理の問題によく適用されてきた混合手法を原理的に包含し得る方法として，stacking法~と呼ばれる方法を用いる．stacking法とは，何らかの学習を用いた複数のシステム・モデルの出力(および訓練データそのもの)を入力とする第二段の学習器を用いて，複数のシステム・モデルの出力の混合を行なう規則を学習するという混合法である．本論文では，具体的には，複数のモデルによる固有表現抽出結果，およびそれぞれの固有表現がどのモデルにより抽出されたか，固有表現のタイプ，固有表現を構成する形態素の数と品詞などを素性として，各固有表現が正しいか誤っているかを判定する第二段の判定規則を学習し，この正誤判定規則を用いることにより複数モデルの出力の混合を行なう．以下では，まず，~節で，本論文の実験で使用したIREX(InformationRetrievalandExtractionExercise)ワークショップの日本語固有表現抽出タスクの固有表現データについて簡単に説明する．次に，~節では，個々の固有表現抽出モデルのベースとなる統計的固有表現抽出モデルについて述べる．本論文では，統計的固有表現抽出モデルとして，最大エントロピー法を用いた日本語固有表現抽出モデル~を採用する．最大エントロピー法は，自然言語処理の様々な問題に適用されその性能が実証されているが，日本語固有表現抽出においても高い性能を示しており，IREXワークショップの日本語固有表現抽出タスクにおいても，統計的手法に基づくシステムの中で最も高い成績を達成している~．~節では，複数のモデルの出力の正誤判別を行なう規則を学習することにより，複数モデル出力の混合を行なう手法を説明する．本論文では，正誤判別規則の学習モデルとしては，決定リスト学習を用い，その性能を実験的に評価する．以上の手法を用いて，~節で，複数の固有表現抽出結果の混合法の実験的評価を行ない，提案手法の有効性を示す．にも示されているように，固定長モデルに基づく単一の日本語固有表現抽出モデルの場合は，現在位置の形態素の前後二形態素ずつを考慮して学習を行なう場合が最も性能がよい．また，~節の結果からわかるように，この，常に前後二形態素ずつを考慮する固定長モデルの性能は，可変長モデルに基づく単一のモデルの性能をも上回っている(なお，では，最大エントロピー法を学習モデルとして可変長モデルを用いた場合には，常に前後二形態素ずつを考慮する固定長モデルよりも高い性能が得られると報告しているが，この実験結果には誤りがあり，本論文で示す実験結果の方が正しい．)．ところが，可変長モデルと，現在位置の形態素の前後二形態素ずつを考慮する固定長モデルとを比較すると，モデルが出力する固有表現の分布がある程度異なっており，実際，これらの二つのモデルの出力を用いて複数モデル出力の混合を行なうと，個々のモデルを上回る性能が達成された．~節では，これらの実験について詳細に述べ，本論文で提案する混合法が有効であることを示す．</section>
  <section title="日本語固有表現抽出">固有表現抽出は，情報検索・抽出，機械翻訳，自然言語理解など自然言語処理の応用的局面における基礎技術として重要な技術の一つである．英語においては，特に米国において，MUC(MessageUnderstandingConference，例えば，MUC-7~)コンテストにおける課題の一つとして固有表現抽出がとりあげられ，集中的に研究が行なわれてきた．また，最近では，日本語においても，MET(MultilingualEntityTask,例えば，MET-1~，MET-2~)やIREXワークショップ~などのコンテストにおいて，固有表現抽出が課題の一つに取り上げられている．</section>
  <subsection title="IREXワークショップの固有表現抽出タスク">IREXワークショップの固有表現抽出タスクでは，表~に示す八種類の固有表現の抽出が課題とされた．表~には，主催者側から提供された訓練データの主要部分を占めるCRL(郵政省通信総合研究所---現，独立行政法人通信総合研究所)固有表現データ(毎日新聞1,174記事の固有表現をタグ付け)，および本試験データのうちの一般ドメインのもの(毎日新聞71記事の固有表現をタグ付け)について，八種類の固有表現数を調査した結果を示す．</subsection>
  <subsection title="形態素と固有表現の対応パターン">次に，上記のIREXワークショップの固有表現抽出タスクの訓練データを形態素解析システムbreakfastの品詞タグの種類数は約300であり，新聞記事に対しては99.6%の品詞正解率である．で形態素解析し，その結果の形態素と固有表現の対応パターンを調査した結果を表~に示す．これからわかるように，半分近くの固有表現については，形態素と固有表現が一対一に対応しないことがわかる．また，そのうち，	一つの固有表現が複数の形態素から構成されている場合は	90%近く(7175/(7175+1022)=87.5%)を占めており，	これらの固有表現については，	各固有表現の区切り位置はいずれかの形態素の区切り位置と一致している，	すなわち，固有表現の開始位置は，先頭の構成要素となる形態素の開始位置と，	また，固有表現の終了位置は，末尾の構成要素となる形態素の終了位置と，	それぞれ一致する．図~にこのような場合の例を示す．また，表~の「その他」の場合の多くは，一つ以上の固有表現が一つの形態素の一部となる場合である．例えば，「訪米」という形態素に対して，その一部である「米」のみがLOCATION(地名)であるという例がこれに相当する．この「その他」の場合の固有表現については，その割合が少なく，また，先行研究において，ある程度の割合で抽出できることがわかっているので，本論文における考慮の対象には含めない．</subsection>
  <section title="最大エントロピー法を用いた固有表現抽出">本節では，まず，ベースモデルとなる，最大エントロピー法を用いた日本語固有表現抽出の手法~を定式化する．</section>
  <subsection title="問題設定">ここでの固有表現抽出の問題は，固有表現まとめ上げおよび固有表現タイプ分類の問題ととらえることができる．いま，以下に示すような形態素列が与えられているとする．[]ここで，現在の位置が形態素M_0のところであるとすると，日本語固有表現まとめ上げおよび固有表現タイプ分類の問題とは，この現在位置の形態素M_0に，まとめ上げ状態および固有表現タイプ(詳細は~節で述べる)を付与することである．本論文の統計的固有表現抽出においては，訓練データからの教師あり学習により固有表現抽出モデルを学習する．その際には，各固有表現がどの形態素から構成されているかという情報が利用可能で，そのような情報を用いて固有表現抽出モデルを学習する．例えば，以下の例では，現在の位置に相当する形態素M_i^NEがm個の形態素からなる固有表現の一部であるという情報が利用可能である．(左側文脈)&amp;&amp;(右側文脈)M_-k^LM_-1^L&amp;M_1^NEM_i^NEM_m^NE&amp;M_1^RM_l^R&amp;&amp;&amp;&amp;eqnarrayまた，次節で述べる最大エントロピー法を用いて固有表現抽出モデルを学習する際には，現在位置および周囲の形態素の素性(~節)を条件として，現在位置の形態素に固有表現まとめ上げ状態およびタイプ(~節)をクラスとして付与するための条件付確率モデルを最大エントロピー法により学習する．なお，通常，学習された確率モデルを適用して，形態素に固有表現まとめ上げ状態および固有表現タイプを付与することにより，固有表現の抽出を行なう場合は，一文全体で，固有表現まとめ上げ状態および固有表現タイプの確率を最大とする固有表現の組合わせを求める必要がある．本論文では，この最適解探索の方法としては，のものをそのまま用いている．</subsection>
  <subsection title="最大エントロピー法">最大エントロピー法は，文脈を規定する制約を素性として与え，与えられた素性のもとでエントロピーを最大化するという条件によって求められる確率モデルである．確率モデルの学習においてエントロピーを最大化することにより，与えられた制約を満たす最も一様なモデルが学習されるため，データの過疎性に強いという特徴を持つ．ここでは，与えられた訓練集合から，文脈x(X)においてクラスy(Y)を出力するプロセスの確率的振舞い，すなわち条件付確率分布p(yx)を最大エントロピー法に基づいて推定する方法の概略を説明する．まず，訓練集合中の事象(x,y)の観測値を大量に集め，freq(x,y)を事象(x,y)の訓練集合中での生起頻度として，訓練集合中の経験的確率分布p(x,y)を以下のように推定する．p(x,y)&amp;&amp;freq(x,y)_x,yfreq(x,y)eqnarray*次に，訓練集合中のどのような現象に注目して確率分布を推定するのかを表す二値の関数f(x,y)を導入し，これを素性関数と呼ぶ．具体的には，各素性関数f_iについて，この関数が真となる事象xおよびyの集合V_xiおよびV_yiが規定されていると考え，この集合にしたがって素性関数f_iが以下のように定義される．f_i(x,y)&amp;=&amp;	.eqnarray*また，一般に確率モデル学習の際には，大量の素性からなる素性の候補集合Fから，活性化された素性の部分集合S(F)が選択され，これらによって事象(x,y)および確率分布p(yx)が記述される．次に，実際に確率モデル学習を行う際には，活性化された素性集合S中の各素性f_iについて，学習すべき確率分布p(yx)による素性f_iの期待値(左辺)と経験的確率分布p(x,y)による素性f_iの期待値(右辺)が等しいとする以下の制約等式を課す．_x,yp(x)p(yx)f_i(x,y)&amp;=&amp;_x,yp(x,y)f_i(x,y)f_iSeqnarray*そして，これらの制約等式を満たす確率分布p(yx)のうちで，以下の条件付エントロピーH(p)を最大にする最も「一様な」モデルが，求めるべきモデルp_であるとする．H(p)&amp;&amp;-_x,yp(x)p(yx)p(yx)p_&amp;=&amp;_pC(S)H(p)eqnarray()式を満たす確率分布は必ず存在し，それは以下の確率分布p_(yx)で記述される．p_(yx)&amp;=&amp;	(_i_if_i(x,y))		_y(_i_if_i(x,y))eqnarray*ただし，_iは各素性f_iのパラメータである．また，実際にエントロピーを最大にする最適なパラメータ^_iを推定するには，ImprovedIterativeScaling(IIS)アルゴリズムと呼ばれるアルゴリズムが用いられる．table*</subsection>
  <subsection title="固有表現まとめ上げ状態の表現法">本論文では，固有表現まとめ上げの際のまとめ上げ状態の表現法として，日本語固有表現抽出の既存の手法において用いられた_encoding法を採用する．この方法では，各固有表現タイプについて，以下の四種類のまとめ上げ状態を設定する．また，固有表現を構成しない形態素のための状態として以下の状態を設定する．結果として，この表現法では，固有表現まとめ上げ状態として，48+1=33の状態を設定する．この方法により日本語固有表現のまとめ上げを行なう様子を表~に示す．</subsection>
  <subsection title="各形態素の素性">各形態素の素性としては，以下の三種類のものを用いる．語彙---訓練コーパス中で，固有表現の位置および周囲二形態素以内に5回以上出現した	2,052語彙		．品詞---形態素解析システムbreakfastの約300種類の品詞．文字種---平仮名・片仮名・漢字・数字・英語アルファベット・記号，およびそれらの組合わせ．</subsection>
  <subsection title="周囲の形態素のモデル化">次に，本論文では，現在位置の形態素に対して固有表現のまとめ上げ状態を付与する際に，周囲のどれだけの形態素を考慮するか，つまり周囲の形態素をどのようにモデル化するかについて，以下の二種類のモデルを用いる．</subsection>
  <subsubsection title="固定(文脈)長モデル">一つ目のモデルは，現在位置の形態素がどれだけの長さの固有表現を構成するのかを全く考慮せずに，固有表現まとめ上げ状態を付与するモデルである．これは，学習時においても，現在の形態素が，いくつの形態素からなる固有表現の一部であるか(~節，式()参照)といった情報を全く考慮せず学習を行なうモデルである．このモデルにおいては，以下に示すように，現在位置の形態素M_0の左側および右側の文脈中の形態素については，学習時においても適用時においても，常に固定された数の形態素だけを考慮する．[]本論文ではこのモデルのことを，固定長モデルと呼ぶ．本論文では特に，現在位置の形態素M_0の左側および右側の文脈中の形態素をいくつ考慮するかに応じて，左右二形態素ずつを考慮する5グラムモデル(左側文脈)&amp;(現在位置)&amp;(右側文脈)M_-2M_-1&amp;M_0&amp;M_1M_2eqnarray*左右三形態素ずつを考慮する7グラムモデル(左側文脈)&amp;(現在位置)&amp;(右側文脈)M_-3M_-2M_-1&amp;M_0&amp;M_1M_2M_3eqnarray*左右四形態素ずつを考慮する9グラムモデル(左側文脈)&amp;(現在位置)&amp;(右側文脈)M_-4M_-3M_-2M_-1&amp;M_0&amp;M_1M_2M_3M_4eqnarray*を用いる．</subsubsection>
  <subsubsection title="可変(文脈)長モデル">一方，もう一つのモデルは，学習時において，現在位置の形態素が，いくつの形態素から構成される固有表現の一部であるか(式()参照)を考慮して学習を行なうモデルで，これを可変長モデルと呼ぶことにする~．</subsubsection>
  <subsubsection title="周囲の形態素の素性">前節までで述べた固定長モデルおよび可変長モデルにおいて，特に現在位置の周囲の形態素の素性について，~節で述べた素性のうちの全部または一部のみを用いるモデルとして，以下の三種類のモデルを設定し，これらについて実験的評価を行なう．全素性を用いるモデル．周囲の形態素M_l(-3)およびM_r(3)については，	語彙素性および品詞素性のみを考慮するモデル．周囲の形態素M_l(-3)およびM_r(3)については，	語彙素性のみを考慮するモデル．なお，と同様に，周囲の複数の形態素の素性を結合した結合素性は用いていない．</subsubsection>
  <section title="正誤判別規則学習を用いた複数システム出力の混合"/>
  <subsection title="訓練・評価データセット">本論文の複数システム出力の混合法では，以下の三種類の訓練・評価データセットを用いる．TrI:個々の固有表現抽出モデルを学習するための訓練データセット．TrC:複数システムの出力の正誤判別規則を学習するための訓練データセット．Ts:複数システムの出力の正誤判別規則を評価するための評価データセット．</subsection>
  <subsection title="訓練および評価手続きの概要">まず，以下に，訓練データセットTrIおよびTrCを用いて，複数システムの出力の正誤判別規則を学習するため手続きの概要を示す．訓練データセットTrIを用いて，	個々の固有表現抽出モデルNEext_i(i=1,,n)を学習する．個々の固有表現抽出モデルNEext_i(i=1,,n)を，	それぞれ，訓練データセットTrCに適用し，	各固有表現抽出モデルNEext_iにつき，	抽出結果の固有表現リストNEList_i(TrC)を	それぞれ一つずつ得る．訓練データセット(テキスト)TrC中での各固有表現の出現位置の情報を用いて，	抽出結果の固有表現リストNEList_i(TrC)(i!=!1,,n)を，	複数システム間(i!=!1,,n)で整列し，	訓練データセットTrCの事象表現TrCevを作成する．訓練データセットTrCの事象表現TrCevを教師あり訓練データとして，	複数システムの出力の正誤判別規則NEext_cmbを学習する．次に，評価データセットTsに，学習された正誤判別規則NEext_cmbを適用する手順の概要を示す．	個々の固有表現抽出モデルNEext_i(i=1,,n)を，	それぞれ，評価データセットTsに適用し，	各固有表現抽出モデルNEext_iにつき，	抽出結果の固有表現リストNEList_i(Ts)を	それぞれ一つずつ得る．評価データセット(テキスト)Ts中での各固有表現の出現位置の情報を用いて，	抽出結果の固有表現リストNEList_i(Ts)(i!=!1,,n)を，	複数システム間(i!=!1,,n)で整列し，	評価データセットTsの事象表現Tsevを作成する．複数システムの出力の正誤判別規則NEext_cmbを	評価データセットTsの事象表現Tsevに適用し，性能を測定する．</subsection>
  <subsection title="データ構造">本節では，訓練データセットTrCの事象表現TrCev，あるいは，評価データセットTsの事象表現Tsevのデータ構造を説明し，複数システムの出力の正誤判別規則を学習する際の素性・クラスについて述べる．以下では，訓練データセットTrCの事象表現TrCevを例にして説明する．</subsection>
  <subsubsection title="事象">訓練データセットTrCの事象表現TrCevは，訓練データセット(テキスト)TrC中での各固有表現の出現位置の情報を用いて，抽出結果の固有表現リストNEList_i(TrC)(i!=!1,,n)を複数システム間(i!=!1,,n)で整列することにより作成される．ここで，整列結果の事象表現TrCevは，セグメントの列Seg_1,,Seg_Nで表現され，各セグメントSeg_jは，整列された固有表現の集合NE_1,,NE_m_jによって表現される．TrCev&amp;=&amp;Seg_1,,Seg_NSeg_j&amp;=&amp;NE_1,,NE_m_jeqnarray*ただし，この整列の際には，少なくとも一つの形態素を共有する複数の固有表現は，同じセグメントに含まれなければならない，という制約が課せられる．次に，各セグメントSeg_j中の固有表現の集合NE_1,,NE_m_jは，固有表現の事象表現の集合NEev_1,,NEev_l_jに変換され，これにより，各セグメントSeg_jは事象表現SegEv_jに変換される．SegEv_j&amp;=&amp;NEev_1,,NEev_l_jeqnarrayここで，各事象表現NEev_k_jは，以下の二種類のうちのどちらかに対応し，それぞれ異なったデータ構造を持つ．そのセグメント中で少なくとも一つのシステムにより出力された固有表現の事象表現．そのセグメント中で一つも固有表現を出力しなかった一つのシステムに関する情報を	表す事象表現．i)のタイプの事象表現NEev_k_jは以下のようなデータ構造を持つ．NEev_k_j&amp;=&amp;		systems=	p,,q,mlength=x,&amp;&amp;	NEtag=,		POS=,class_NE=+/-	eqnarrayここで，``systems''はこの固有表現を出力したシステムの指標のリストを，``mlength''はこの固有表現を構成する形態素の数を，``NEtag''はこの固有表現のタイプを，``POS''はこの固有表現を構成する形態素の数の品詞のリストを，それぞれ表す．また，``class_NE''は，正解データと比較して，この固有表現が正解であるか(``+'')，それとも，システムによる誤出力であるか(``-'')を示す．一方，ii)のタイプの事象表現NEev_k_jは，このセグメント中で，指標rを持つシステムが固有表現を出力しなかったことを示す，以下のようなデータ構造を持つ．NEev_k_j&amp;=&amp;		systems=	r,class_sys=		eqnarray</subsubsection>
  <subsubsection title="クラス">複数システムの出力の正誤判別を行なう規則は，式()で定義されるセグメントの事象表現SegEv_jを一つの事象単位として，学習および適用が行なわれる．ここで，正誤判別規則の学習および適用の際には，セグメントSegEv_j中の固有表現を各システムごとにまとめて，システム単位で正誤のクラスを参照する．そこで，式()で定義される一つのセグメントの事象表現SegEv_jに対して，各システムiごとにまとめた以下のクラス表現を設定し，正誤判別規則の学習および適用を行なう．class_sys^1&amp;=&amp;	.&amp;&amp;_sys^n&amp;=&amp;	.eqnarrayここで，一般に，一つのセグメント中で，各システムは一つも固有表現を出力しない場合もあれば，複数の固有表現を出力する場合もありえるので，各システムiのクラスclass_sys^iは上記のような表現になる．</subsubsection>
  <subsubsection title="複数システムの出力の正誤判別規則">次に，前節の事象のデータ構造を用いて，複数システムの出力の正誤判別を行なう規則について説明する．複数システムの出力の正誤判別を行なう規則は，式()で定義されるセグメントの事象表現SegEv_jを一つの事象単位として，各システムiごとに，式()で示すクラスclass_sys^iを判別するという形式をとる．この正誤判別規則の学習の際には，式()で定義されるセグメントの事象表現SegEv_jから，次節で説明する素性を抽出し，この素性を用いて各システムiごとのクラスclass_sys^iを判別する規則を学習する(~節)．この正誤判別規則の適用の際にも，事象表現SegEv_jから抽出される素性を用いて各システムiごとにクラスclass_sys^iを判別する(~節)．</subsubsection>
  <subsubsection title="素性">式()で定義されるセグメントの事象表現SegEv_jから抽出される一つの素性fは，システムの指標のリストp,,q，および，固有表現の素性表現Fの組systems!=!p,,q,Fの集合によって表現される．f&amp;=&amp;			systems!=!	p,,q,F,,		systems!=!	p',,q',F'	eqnarrayこのうち，一つの組systems!=!p,,q,Fは，指標p,,qに相当する(複数の)システムによって出力された一つの固有表現が，素性表現Fを持つことを表している．固有表現の素性表現Fは，集合mlength!=!,NEtag!=!,POS!=!の巾集合の任意の要素，あるいは，そのセグメント中で指標p,,qに相当する(複数の)システムが固有表現を出力しなかったことを表す集合の形式，のいずれかで表現される．F&amp;=&amp;.eqnarray正誤判別規則の学習時には，式()で定義されるセグメントの事象表現SegEv_jから，式()の形式のあらゆる可能な素性fのうち，以下の制約を含むいくつかの制約を満たすものだけが抽出される．詳細については，次節の例を参照．	システムの指標のリストp,,qについては，その固有表現を出力した全てのシステムの指標を記すこととし，部分リストの形式は許さない．一つのシステムが，一つのセグメント中で複数個の固有表現を出力した場合は，一つの素性f中で，それらの複数の固有表現のうちの一部のものだけの情報を記述することは許さない．それらの全ての固有表現について何らかの情報を記述するか，どの固有表現についての情報も記述しないかのどちらかである．</subsubsection>
  <subsubsection title="例">~節の手続きにしたがって，二つのシステムの固有表現抽出結果を整列し，その整列結果を事象表現に変換する例を表~に示す．また，~節および~節の手続きにしたがって，それらの事象表現からクラスおよび素性を抽出する例を表~に示す．表~では，形態素解析の結果の形態素列に対して，システム0およびシステム1の二つのシステムがそれぞれ単独で出力した固有表現を，「単独システムの固有表現出力」の欄に示す．それらの単独システムの固有表現出力を整列した結果は，SegEv_iSegEv_i+3の四つのセグメントに分割されており，これらのセグメントを事象表現に変換した結果が「事象表現」の欄に示されている．各セグメントの特徴を簡単にまとめると以下のようになる．SegEv_i	:	システム0が連続する二つの固有表現を出力したのに対して，	システム1はそれらをまとめて一つの固有表現として出力している．	正解データとの比較では，システム1の出力結果の方が正解である．	このセグメントの事象表現は，いずれかの単独システムから出力された	三つの固有表現の事象表現から構成されている．SegEv_i+1:	システム1のみが固有表現を出力したが，この固有表現は誤出力である．	このセグメントの事象表現は，システム0からの出力がなかったことを	表す事象表現と，	システム1が出力した一つの固有表現の事象表現から構成されている．SegEv_i+2:	システム0が一形態素から構成される一つの固有表現を出力したのに対して，	システム1はその形態素を含む三形態素から構成される一つの固有表現を出力した．	正解データとの比較では，システム1の出力結果の方が正解である．	このセグメントの事象表現は，各々の単独システムから出力された	二つの固有表現の事象表現から構成されている．SegEv_i+3:	システム0，システム1ともに二形態素から構成される同一の固有表現を出力した．	正解データとの比較では，この固有表現は正解である．	このセグメントの事象表現は，この一つ固有表現の事象表現から構成されている．次に，表~においては，まずクラスについては，これらの各セグメントの事象表現において，各システムが出力した固有表現のクラス(もしくは出力がなかったことを表す事象表現のクラス)をシステムごとにまとめたものになっている．一方，素性の方は，各セグメントについて，以下の制約を満たす可能な素性の一覧を表現したものになっている．SegEv_i	:	システム0は，このセグメント中で二つの固有表現を出力しているが，	この二つの固有表現のうちの一つだけの情報を記述した素性は許容しない．SegEv_i+1:	ある単独システムからの出力がなかったことだけを記述した素性は許容しない．	例えば，				systems!=!	0,	class_sys!=!		という素性は許容しない．SegEv_i+3:	システムの指標のリストにおいては，	このセグメントの固有表現を出力した二つのシステムの指標0および1の両方を	必ず記述する．</subsubsection>
  <subsection title="学習アルゴリズム">教師あり学習法としては，決定リスト学習を用いる．決定リストは，ある素性のもとでクラスを決定するという規則を優先度の高い順にリスト形式で並べたもので，適用時には優先度の高い規則から順に適用を試みていく．本論文では，各規則の優先度として，素性fの条件のもとでの，システムiのクラスclass_sys^iの条件付確率P(class_sys^i!=!c_if)を用い，この条件付確率順に決定リストを構成する．ただし，決定リストを構成する際には，素性fの条件のもとでの，システムiのクラスclass_sys^iの頻度freq(f,class_sys^i)に下限L_fを設け，freq(f,class_sys^i)&amp;&amp;L_feqnarrayの条件を満たす規則だけを用いて決定リストを構築する．頻度の下限L_fは，各規則の条件付確率P(class_sys^i!=!c_if)を推定する際に使用したデータセット以外のデータセットに対して，正誤判別規則の性能を最大にする値を用いる．</subsection>
  <subsection title="正誤判別規則の適用による複数システム出力の混合">学習された正誤判別規則を適用することにより複数システムの出力の混合を行なう場合は，式()と同じ形式のセグメントの事象表現SegEv_j&amp;=&amp;NEev_1,,NEev_l_jeqnarray*に対して，決定リストの形式の正誤判別規則が参照され，素性fの条件のもとでの，システムiのクラスclass_sys^iの条件付確率P(class_sys^i!=!c_if)の推定値を得る．そして，複数のシステムによって出力された単一の固有表現は，同一の正誤クラスを持つ．少なくとも一つの形態素を共有する複数の固有表現が，正のクラス(``+'')を持ってはならない．という二つの制約のもとで，全システムについての条件付確率P(class_sys^i!=!c_if)の積を最大化するクラス割当ての組合わせが求められ，これが，セグメント中で各システムi(i=1,,n)が出力した固有表現への正誤クラスの判別結果class_sys^1,,class_sys^nとなる．class_sys^1,,class_sys^n&amp;=&amp;	_c_i,f_i_i=1^nP(class_sys^i!=!c_if_i)eqnarray*</subsection>
  <section title="実験および評価">本節では，IREXワークショップの固有表現抽出タスクの訓練データおよび試験データを用いて，複数の固有表現抽出結果の混合法の実験的評価を行なった結果について述べる．以下では，訓練データとして用いているCRL固有表現データの一般ドメインのものをD_CRL，評価データとして用いている本試験データのうちの一般ドメインのものをD_formalと記す．ただし，いずれも，表~の「その他」のものは除いている．</section>
  <subsection title="各モデル単独の出力の比較">本節では，~節で述べた各モデル単独の性能について述べ，各モデルの出力を比較する．実験に用いたモデルは，~節の固定長モデルとしては，5グラムモデル，7グラムモデル，9グラムモデル，および，~節の可変長モデルである．また，7グラムモデル，9グラムモデル，および，可変長モデルについては，~節の三種類の素性の設定も区別して実験を行なった．まず，表~に，個々の固有表現抽出モデルを学習するための訓練データセットTrIをD_CRLとした場合の，本試験データD_formalに対する各モデルのF値(=1)を示す．この結果からわかるように，単独のモデルでは，5グラムモデルが最も高い性能を示す．また，7グラムモデルおよび9グラムモデルは，素性の設定に関わらず，ほぼ同等の性能を示している．次に，最も性能のよい5グラムモデルの出力と，他のモデルの出力との違いを調べるために，5グラムモデル以外の各モデルの出力について，5グラムモデルの出力との和集合を求め，本試験データD_formalの正解データに対する再現率を算出した．また，5グラムモデル以外の各モデルの誤出力と5グラムモデルの誤出力の間の重複率誤出力の重複率&amp;=&amp;		eqnarray*を求めた．これらの結果を表~に示す．特に，和の再現率が最も高く，誤出力の重複率が最も低い結果(この場合は，可変長モデル(形態素M_l(-3)，M_r(3)の素性=全て)との差分)を太字で示す．表~および表~の結果から分かるように，7グラムモデルおよび9グラムモデルは，5グラムモデルと比べて出力の和集合の再現率が低く，かつ誤出力の重複率も高いことから，相対的に5グラムモデルと似通ったモデルであると言える．一方，可変長モデルは，7グラムモデルおよび9グラムモデルと比べて，相対的に5グラムモデルとの類似性が小さいことがわかる．特に，誤出力の重複率が比較的小さい点が目立つ．</subsection>
  <subsection title="複数システムの出力の混合の性能評価"/>
  <subsubsection title="評価方法">次に，7グラムモデル，9グラムモデル，可変長モデルについて，それぞれ，~節の三種類の素性の設定を区別して，合計9種類のモデルを考え，その各々について，5グラムモデルの出力との間で混合を行ない，その性能を評価した．ただし，個々の固有表現抽出モデルを学習するための訓練データセットTrI，複数システムの出力の正誤判別規則を学習するための訓練データセットTrC，~節の()式の頻度閾値L_fの設定の組合わせとしては，以下の二通りについて評価を行なった．なお，複数システムの出力の正誤判別規則を評価するための評価データセットTsについては，いずれも，本試験データD_formalを用いた．このうち，設定(a)は，二つの訓練データセットTrIとTrCについて，重複のないデータセットを用いたものに相当する．ただし，利用可能なデータ量に限界があることから，混合のための正誤判別規則学習の訓練データセットTrCのサイズが小さくなっている．一方，設定(b)の方は，個々の固有表現抽出モデルを訓練データTrI自身に適用したインサイド適用の結果を利用した混合となるが，混合のための正誤判別規則学習の訓練データセットTrCのサイズは設定(a)よりもずっと大きいの全体を用いて学習された各固有表現抽出モデルを適用して得られたものを用いた．訓練データがD_CRLであるかD_CRL-D^200_CRLであるかの違いによる固有表現抽出モデルの性能の差はそれほど大きくないので，このことによる影響は小さいと考えられる．．</subsubsection>
  <subsubsection title="評価結果">評価結果を表~に示す．この結果から分かるように，設定(a)と(b)を比べると，一律に，設定(b)の方が高い性能が得られている．このことから，正誤判別規則の学習において，たとえ，インサイド適用の結果しか利用できなかったとしても，混合のための正誤判別規則学習の訓練データセットTrCのサイズはできるだけ大きい方がよいことがわかる．特に，設定(b)においては，どの混合結果においても5グラムモデル単独の性能を上回っていることから，混合規則学習のための十分な訓練データがあれば，混合により多少なりとも個々のモデルの出力の性能を向上できることが予想される．また，設定(b)の場合，7グラムモデル，9グラムモデルといった固定長モデルの出力と5グラムモデルの出力を混合した場合よりも，可変長モデルの出力と5グラムモデルの出力を混合した場合の方が圧倒的に高い性能向上を達成している．この結果は，表~の差分の傾向と合致しており，5グラムモデルとの類似性が相対的に小さい可変長モデルの出力との混合において，より高い性能向上が得られている．また，可変長モデル同士の間で，形態素M_l(-3)，M_r(3)の素性の設定が異なる場合を比較しても，この傾向が成り立っており，5グラムモデルとの類似性が小さいほど混合結果における性能向上は大きい．これらの結果から，出力の和の再現率が高く，誤出力の重複率が小さくなるような，なるべく類似性の小さい複数の日本語固有表現抽出モデルの出力を用意して，本論文の手法により出力の混合を行なえば，単独のモデルの出力の性能向上が期待できることがわかる．</subsubsection>
  <subsubsection title="固有表現の形態素長/種類ごとの分析">次に，5グラムモデルの出力と可変長モデルの出力の混合の場合について，固有表現を構成する形態素数ごと，および，固有表現の種類ごとに，単独モデルの出力および混合結果の性能(F値，再現率，適合率)を列挙したものを，それぞれ，表~，および，表~に示す．なお，表中で，固有表現を構成する形態素数ごと，あるいは，固有表現の種類ごとに，最も高いF値を達成した結果をそれぞれ太字で示す．表~から分かるように，どの可変長モデルの出力との混合においても，ほぼ全ての形態素長の固有表現において，5グラムモデル単独の出力の再現率・適合率をともに上回っている．特に，最高の性能を示している「5グラムモデル+可変長モデル(全て)」の結果においては，5グラムモデルからの性能向上の度合は，形態素長が長くなるほど大きいことから，可変長モデルでしか出力されなかった長い固有表現を，混合によってうまく抽出できていることがわかる．また，表~からは，どの可変長モデルの出力との混合においても，ほぼ全ての種類の固有表現において，5グラムモデルの出力の再現率・適合率とほぼ同等かそれ以上の性能が得られている．そのうち，TIME，MONEY，PERCENTの三種類については，他の種類と比較して，訓練データ・評価データともその頻度が小さく，また，5グラムモデルにおける性能もかなり高いことから，改善の余地があまりなかったと考えられる．ただし，その場合でも，混合結果においては，可変長モデルの低い性能の悪影響を受けることなく，5グラムモデルの高い性能が反映されている．table*scriptsizetable*</subsubsection>
  <subsubsection title="単独モデル・混合結果の出力のパターンの分析">scriptsizetable*5グラムモデルの出力と可変長モデルの出力の混合の場合について，各単独モデルの出力における固有表現の有無，および，混合結果における固有表現の有無と，正解データにおける固有表現の有無のパターンの割合を調査した結果を表~に示す．表中で，「有」「無」は，それぞれ，単独モデルの出力，混合結果，正解データに固有表現が存在する場合，および，存在しない場合を表す．例えば，「有」「有」「有」「有」のパターンは，両方の単独モデルの出力にその固有表現が存在し，混合結果においてもその固有表現が出力され，かつ，それが正解データにも存在する正解の固有表現である場合に相当する．また，割合(%)の計算においては，両方の単独モデルの出力の和における固有表現数を分母，それぞれのパターンに該当する固有表現数を分子として，割合(%)を計算している．さらに，混合における正誤判別結果が正解であるか否かについては，混合結果および正解データにおける出力の有無が一致する場合は正誤判別が正解，一致しない場合は正誤判別が誤りであるので，「正誤判別率」の欄にそれぞれの率を示した．形態素M_l(-3)，M_r(3)の素性の設定が異なる場合についてこの結果を比較すると，「5グラムモデル+可変長モデル(全て)」において判別正解率が高くなっているが，これは，「可変長モデル(全て)」の性能が極端に悪く，「可変長モデル(全て)」のみが出力した固有表現の多くが誤りであり，その判別が比較的容易であったからである．全体では，どの可変長モデルの出力との混合においても，5グラムモデルの出力を覆すことで正解となった場合(「無」「有」「有」「有」および「有」「無」「無」「無」)が数%あり，これが，5グラムモデルからの性能向上に寄与している．その一方で，判別誤りの内訳をみると，その多くは，誤出力の検出が十分できなかった場合で，ほとんどの場合，少なくとも5グラムモデルはその誤りの固有表現を出力している．このことから，より効果的な素性を用いる，あるいは，より高性能な学習器を用いるなどして，誤出力検出の精度を向上させることにより，適合率を向上できる余地があることがわかる．</subsubsection>
  <subsection title="最大エントロピー法による正誤判別規則学習">最後に，正誤判別規則学習の学習法の比較のために，最大エントロピー法を用いて正誤判別規則学習を行なった．まず，最大エントロピー法を適用するために，~節の()式の事象表現SegEv_jを，以下のように変換する．SegEv_j&amp;=&amp;NEListev_p,,q,,NEListev_p',,q'eqnarrayここで，各事象表現NEListev_p,,qは，システムの指標のリストごとに固有表現をまとめたもので，固有表現のリストの事象表現に相当する．~節の場合と同様に，以下の二種類のどちらかに対応し，それぞれ異なったデータ構造を持つ．そのセグメント中で少なくとも一つのシステムにより出力された	固有表現のリストの事象表現．そのセグメント中で一つも固有表現を出力しなかった一つのシステムに関する情報を	表す事象表現．i)のタイプの事象表現NEListev_p,,qは以下のようなデータ構造を持つ．NEListev_p,,q&amp;=&amp;		systems=	p,,q,mlengthList=y,,z,&amp;&amp;	NEtagList=,		POSList=,&amp;&amp;		classList_NE=+/-,,+/-	eqnarrayこのデータ構造は，~節の()式のデータ構造とほぼ同じであるが，固有表現のリストを表現するために，各素性に相当する情報が全てリスト表現になっている点が異なる．一方，ii)のタイプの事象表現NEList_rは，~節の()式と同じく，以下のデータ構造で表現される．NEListev_r&amp;=&amp;		systems=	r,class_sys=		eqnarrayこのような事象表現を用いて正誤判別規則の学習および適用を行なう際には，上述の()式の事象表現を事象の単位とし，~節の場合と同様に，各システムiごとにまとめた以下のクラス表現を設定し，各システムiごとにクラスの判別を行なうための正誤判別規則の学習および適用を行なう．class_sys^1&amp;=&amp;	.&amp;&amp;_sys^n&amp;=&amp;	.eqnarray*その際には，()式の固有表現のリストの事象表現NEListev_p,,qのmlengthList，NEtagList，POSList，および，()式の固有表現の事象表現NEList_rのclass_sysを，それぞれ文脈xとし，上式の，各システムごとにまとめた正誤のクラスのリストを付与するための条件付確率モデルを，最大エントロピーモデルとして学習する．この最大エントロピーモデルは，各システムiごとに個別にモデルの学習・適用を行なう．このような方法で，7グラムモデル，9グラムモデル，可変長モデルについて，それぞれ，~節の三種類の素性の設定を区別して，合計9種類のモデルを考え，その各々について，5グラムモデルの出力との間で混合を行ない，その性能を評価した．ただし，TrI=TrC=D_CRLとし，評価データセットTsは本試験データD_formalとした．最大エントロピーモデルの素性関数の頻度に下限を設け，評価データセットTsに対して最も高い性能が得られた場合の結果を表~(a)に示す．また，決定リスト学習との間で条件を揃えるために，~節の()式の形式の決定リスト学習の素性のうち，上述の実験結果(a)では用いていなかった結合素性を追加して最大エントロピーモデルの学習および適用を行なった結果を表~(b)に示す．この場合は，決定リスト学習における各規則の条件付確率P(class_sys^i!=!c_if)に下限を設け，評価データセットTsに対して最も高い性能が得られた場合の結果を示している．表~の(a)と(b)の結果を比較すると，結合素性を用いた場合の方が性能が悪くなっている．また，いくかの結果を除いて，5グラムモデルの性能からの向上はみられるものの，決定リスト学習による可変長モデルの出力との混合の場合のような高い性能向上は達成できていない．この理由の一つとしては，最大エントロピーモデルと決定リスト学習の間のモデルの形式の違いの影響が挙げられる．最大エントロピーモデルは，あらゆる素性とクラスとの相関をそれぞれ別個のパラメータとし，モデル内では全パラメータを考慮する形式のモデルになっている．一方，決定リスト学習は，各々のクラス決定において最も寄与する素性の組合わせのみを考慮し，他の素性は全く考慮しない．したがって，素性間で寄与する度合の差がわずかしかない場合でも，決定リスト学習では，最も寄与する素性の組合わせのみが考慮されるのに対して，最大エントロピーモデルでは，全素性の寄与を総合的に考慮する．本論文の正誤判別規則学習による混合の問題では，素性の種類が比較的少なく，特に高頻度な素性は，実際にクラス判別に寄与する度合に関係なく，どの事象においても常に一定の値以上の重みを持つと考えられる．そのような問題の場合には，最大エントロピーモデルのように全素性の寄与を総合的に考慮する学習法でなく，決定リスト学習のように各々のクラス決定に最も寄与する素性の組合わせのみを考慮する学習法が適していると考えられる．逆に，正誤判別規則学習による混合の前段階である，形態素への固有表現まとめ上げ状態付与の問題の場合には，に示されるように，決定リスト学習よりも最大エントロピーモデルの方が高い性能を示している．この問題の場合には，素性の種類が比較的多く，極端に高頻度な素性も少ないことから，最大エントロピーモデルのように全素性の寄与を総合的に考慮する学習法が適していると考えられる．</subsection>
  <section title="関連研究"/>
  <subsection title="複数モデルの出力の混合法">~節で述べたように，一般に，複数のモデル・システムの出力を混合する過程は，大きく以下の二つの部分に分けて考えることができる．	できるだけ振る舞いの異なる複数のモデル・システムを用意する．	用意された複数のモデル・システムの出力を混合する方式を選択・設計し，	必要であれば学習等を行ない，与えられた現象に対して，	用意された複数のモデル・システムの出力を混合することを実現する．ここで，これまで自然言語処理の問題に適用された混合手法においては，これらの()および()の過程について，大体以下のような手法が用いられていた．まず，()については，大きく分けて以下のような手法がある．学習モデルが異なる複数のシステム等	(原理的には，人手による規則に基づくシステムとデータからの学習に基づくシステム，	などの組合わせも可能)，	ある程度振る舞いの異なる既存のシステムを	用意する~．i)と似ているが，学習モデルは単一のものを用い，データの表現法	(具体的には，まとめ上げ問題におけるまとめ上げ状態の表現法)として複数のものを	設定することにより，複数の出力を得る~．単一の学習モデルを用いるが，訓練データのサンプリングを複数回行なうことにより	複数のモデルを学習するbagging法~を用いる~，	あるいは，単一の学習モデルを用い，誤り駆動型で訓練データ中の訓練事例の重みを操作しながら	学習と適用を繰り返すことにより，各サイクルの誤りに特化した複数のモデル	(およびそれらの重み)を学習する	boosting法~を用いる~．これに対して，本論文においては，振る舞いの異なる複数のモデルを得る方法として，学習モデルは単一のものを用い，固有表現まとめ上げの際に考慮する周囲の形態素の個数を区別することで複数のモデルを得るという方法をとった．この方法は，上記のうちでは，ii)でとられた方法と比較的似ている．次に，()については，大きく分けて以下のような手法がある．重み付多数決など，何らかの多数決を行なうもの~．複数のシステム・モデルの重みに応じて採用するシステムの切り替えを行なうもの~．原理的に，上記のi)およびii)を包含し得る方法として，	複数のシステム・モデルの出力(および訓練データそのもの)を入力とする第二段の	学習器を用いて，複数のシステム・モデルの出力の混合を行なうstacking法~，	あるいは，それと同等の方法に基づくもの~．これらの方法のうち，本論文では，原理的に，i)およびii)を包含し得るiii)のstacking法を用いている．特に，本論文では，個々のシステムの出力する重みの情報は利用せずstackingを行なっているので，規則に基づくシステムなどで重みを出力しない場合でも，そのまま本論文の手法を適用することができる．これに対して，重み付多数決や重みを用いたシステム切り替えの場合は，システム数が少なく(例えば，二種類のシステムの混合の場合)，かつ，個々のシステムが重みを出力しない場合などでは，適用が困難になると考えられる．また，通常のbagging法やboosting法を適用する場合でも，第一段としては何らかの学習モデルを採用する必要があるが，本論文の混合法にはそのような制約はないので，原理的には，第一段として任意のシステムを採用することが可能である．</subsection>
  <subsection title="Stacking法">次に，本節では，stacking法についての関連研究，および，stacking法と同等の手法を自然言語処理におけるシステム混合の問題に適用している研究事例について述べる．stacking法は，によってその枠組みが提案され，その後，機械学習の分野においていくつかの応用手法が提案されている~．例えば，は，回帰法を用いたstackingを提案している．は，第一段の学習器として，決定木学習，ナイーブベイズ，最近隣法を用い，第二段の学習器として，決定木学習，ナイーブベイズ，最近隣法，線形回帰法の一種を用いた実験を行ない，性能の比較をしている．一方，は，それまで提案されたstacking法を，n段の学習器の連鎖に拡張し，第k(1&lt;kn)段の学習器は，第一段から第k-1段までの全ての学習器の入出力データを素性として学習を行なうというカスケード法を提案している．特に，それまでのstacking法は，第一段の学習器の出力のみを入力素性として第二段の学習器の学習を行なうものがほとんどであったのに対して，カスケード法では，前段までの学習器の出力だけでなく，入力素性もあわせて利用する点が特徴的である．一方，自然言語処理におけるシステム混合の問題にstacking法と同等の手法を適用している研究事例としては，英語品詞付けにおいて，最大エントロピー法，変形に基づく学習，トライグラムモデル，メモリベース学習を第一段の学習器とし，決定木学習，メモリベース学習法などを第二段の学習器としてstackingを行なうもの~，英語名詞句まとめ上げにおいて，七種類の学習器を第一段に用い，決定木学習，メモリベース学習法を第二段の学習器としてstackingを行なうもの~などがある．これらの事例においては，いずれも，第一段の入力素性および出力を用いて第二段の学習器の学習を行なった結果も報告している．また，は，英語の固有表現抽出において，単一の最大エントロピーモデルの素性として，通常の固有表現まとめ上げ・タイプ分類に用いる素性とあわせて，他の既存のシステムの出力を素性として用いて，個々の単語に固有表現まとめ上げ状態・タイプ分類を付与するための分類器の学習を行なっている．一方，は，情報抽出におけるテンプレート・スロット埋め問題において，ナイーブベイズ法，帰納的論理プログラミング法などを第一段の学習器とし，回帰法を第二段の学習器としてstackingを行なっている．ここでは，第二段の学習器の入力は，第一段の学習器の出力のみとなっている．これらの事例と比較すると，本論文の日本語固有表現抽出の問題においては，第一段の学習器は，個々の形態素に固有表現まとめ上げ状態・タイプ分類を付与するための分類器の学習を行なっているのに対して，第二段の学習器は，個々のシステムの固有表現抽出結果，および，第一段の学習器の入力となった素性(の一部)を入力として，個々のシステムの固有表現抽出結果の正誤を判定するための分類器の学習を行なっている．このように，本論文のstacking法では，第一段と第二段の学習器の学習の単位が異なっている点が変則的である．ただし，このような構成をとることにより，第一段としては，任意の固有表現抽出システムを用いることが可能となっている．また，と比較すると，では，本論文の第二段に相当する学習器が，個々の単語に固有表現まとめ上げ状態・タイプ分類を付与するための分類器の学習を行なっている点が異なっている．</subsection>
  <subsection title="統計的手法に基づく日本語固有表現抽出">統計的手法に基づく日本語固有表現抽出の研究事例としては，我々がベースとした，最大エントロピー法を用いるものの他に，決定木学習を用いるもの~，最大エントロピー法を用いるもの~，決定リスト学習を用いるもの~，SVM(supportvectormachines)を用いるもの~などがある．これらは，いずれも，単一の学習モデルを用いている．決定リスト学習を用いる事例~では，可変長文脈素性を用いることにより，固定長モデルの性能の上回る結果が得られているが，ベースとなる決定リスト学習の性能は最大エントロピー法の性能よりも劣っている．その他の事例では，いずれも，固定長文脈素性を用いている．また，stacking法の研究事例においては，異なる数種類の学習器を第一段に用いるという構成が多く見られ，一定の効果が報告されているので，上記の複数の学習器を第一段としてstacking法を行なうことにより，精度の向上が期待できる可能性がある．その他には，で報告されているように，解析の方向を文頭から文末と文末から文頭の二通り設定し，解析済の固有表現のタグを素性として利用する方法により，振る舞いの異なった出力が得られる可能性があり，stacking法でその出力を利用することで，精度の向上が期待できる可能性がある．また，では，決定木学習により学習された可読性の高い規則や人手による付加制約等を適用して複数の固有表現候補を生成し，最長一致法により複数の候補の選別を行なっている．ここで，複数の候補の選別を行なう際に，本論文の混合法を適用することにより，誤出力の棄却まで含めたより一般的な選別が自然な形で実現できる可能性があると考えられる．</subsection>
  <section title="おわりに">本論文では，日本語固有表現抽出の問題において，複数のモデルの出力を混合する手法を提案した．まず，最大エントロピー法に基づく統計的学習による固有表現抽出モデルにおいて，現在位置の形態素が，いくつの形態素から構成される固有表現の一部であるかを考慮して学習を行なう可変長モデルと，常に現在位置の形態素の前後数形態素ずつまでを考慮して学習を行なう固定長モデルとの間のモデルの挙動の違いに注目し，なるべく挙動が異なり，かつ，適度な性能を保った複数のモデルの出力の混合を行なった．混合の方式としては，複数のシステム・モデルの出力(および訓練データそのもの)を入力とする第二段の学習器を用いて，複数のシステム・モデルの出力の混合を行なう規則を学習するという混合法(stacking法)を採用した．第二段の学習器として決定リスト学習を用いて，固定長モデルおよび可変長モデルの出力を混合する実験を行なった結果，最大エントロピー法に基づく固有表現抽出モデルにおいてこれまで得られていた最高の性能を上回る性能が達成された．今回の実験では，固定長モデル同士は出力される固有表現の分布がお互いに似通っており，可変長モデル同士も使用する素性の集合に包含関係があることから，出力する固有表現の傾向が大きく異なるモデルは，固定長モデルと可変長モデルの二種類だけであると仮定した．そのため，評価実験においても，二つのモデルの出力の混合の結果のみを報告したが，今後は、傾向の大きく異なる三種類以上のモデルの出力に対して，本論文の混合手法の有効性を評価したいと考えている．また，本論文の手法は，個々の単独システムに何らかの固有表現候補を出力させて，それらの固有表現候補を取捨選択するという方法であるので，再現率の観点からは，個々の単独システムの出力の和の再現率が上限となってしまう．したがって，本論文の方法によってより高い性能の固有表現抽出を実現するためには，個々の単独システムが少しでも多くの固有表現候補を出力することが不可欠である．今後は，既存のどの固有表現抽出モデルを用いても抽出が失敗する固有表現の特性を分析し，できるだけ網羅的に固有表現候補を出力し，その結果を本論文の混合法で利用する方式について検討を行なう予定である．その際，網羅的に固有表現候補を出力するためには，まず，何らかの方法によって，広範なテキストから固有表現候補を収集して蓄積する必要があるが，ここでは，新聞記事やWWW上のテキスト等の大規模テキストから未知語を獲得する，あるいは専門用語を抽出するなどの手法の適用が有効であると考えている．document</section>
</root>
