<?xml version="1.0" ?>
<root>
  <title>要約の内的(intrinsic)な評価法に関するいくつかの考察--第2回NTCIRワークショップ自動要約タスク(TSC)を基に--</title>
  <author>難波英嗣奥村学</author>
  <jabstract>システムの出力した要約そのものを評価する方法は，一般に内的な評価と呼ばれている．これまでの典型的な内的な評価の方法は，人手で作成した抜粋と要約システムの出力との一致度を，F-measure等の尺度を用いて測ることで行われてきた．しかし，F-measureは，テキスト中に類似の内容を含む文が複数存在する場合，どちらの文が正解として選択されるかにより，システムの評価が大きく変化する，という問題点がある．本研究では，この問題点を解消するいくつかの評価方法をとりあげ，その有用性に関する議論を行う．F-measureの問題点を解消する評価方法の1つにutilityに基づく評価があるが，この方法では評価に用いるデータ作成にコストがかかるという問題がある．本研究では，あるテキストに関する複数の要約率のデータを用いることで，疑似的にutilityに基づく評価を実現する方法を提案する．提案する評価方法を，第2回NTCIRワークショップ自動要約タスク(TSC)のデータに適用し，有用性に関する調査を行った結果，提案方法は，F-measureの問題点をある程度改善できることが確認された．次に，F-measureの問題点を解消する他の評価方法の一つであるcontent-basedな評価を取り上げる．content-basedな評価では，指定された要約率の正解要約を一つだけ用意すれば評価可能であるため，utilityに基づく評価に比べ，被験者への負荷が少ない．しかし，この評価方法で2つの要約を比較する場合，どの程度意味があるのかについては，これまで十分な議論がなされていない．そこで，pseudo-utilityに基づく評価と同様にTSCのデータを用い，content-basedな評価の結果を被験者による主観評価の結果と比較した結果，2つの要約がcontent-basedな評価値で0.2以上の開きがあれば，93%以上の割合で主観評価の結果と一致することが分かった．</jabstract>
  <jkeywords>pseudo-utilityに基づく評価，F-measure，content-basedな評価，テキスト自動要約，内的な評価，TSC，NTCIR</jkeywords>
  <subsection title="">*課題A-1課題A-1のシステムの提出結果は，重要文抽出に基づいて作成された要約であり，人間が選択した重要文との間の一致度を元に評価を行なう．評価尺度としては，以下の3つを用いる．(再現率=システムが選んだ文の内で正解の文の数人間が選んだ正解の文の総数)(精度=システムが選んだ文の内で正解の文の数システムが選んだ文の総数)(F-measure値=2*再現率*精度(再現率+精度))これらの値を要約率ごとに求めた後，平均したものを最終的な結果とする.また，ベースラインシステムとして，以下の2種類を用いる.Lead:本文の先頭から要約率として指定された文数だけ出力する．TF:本文の各文ごとに内容語のTFの和を計算し，このスコアの高い文を要約率として指定された文数だけ選択する．選択した文を元の文の出現順に戻して出力する.</subsection>
  <section title="序論">近年，テキスト自動要約の研究が活発化するとともに，要約の評価方法が研究分野内の重要な検討課題の一つとして認識されてきている．これまで提案されてきた要約の評価方法は，内的な(intrinsic)評価と外的な(extrinsic)評価の2種類に分けることができる．内的な評価とは，システムの出力した要約そのものを，主に内容と読みやすさの2つの側面から評価する方法である．一方，外的な評価とは，要約を利用して人間がタスクを行う場合の，タスクの達成率が間接的に要約の評価となるという考え方に基づいて評価を行う方法である．本研究では，近年活発にその評価方法が議論され，改良が試みられている内的な評価，特に内容に関する評価方法に焦点を当てる．これまでの要約の内容に関する評価は，人手で作成した抜粋と要約システムの出力との一致の度合を，F-measure等の尺度を用いて測るのが典型的な方法であった．しかし，Jingらは，要約のF-measureによる評価と外的な評価を分析し，F-measureには「テキスト中に類似の内容を含む文が複数存在する場合，どちらの文が正解として選択されるかにより，システムの評価は大きく変化する」という問題があることを指摘している．この問題点を解決する方法がこれまでにいくつか提案されている．Radevらは，文のutilityという概念を用いた評価方法を示している．文のutilityとは，そのテキストの話題に対する各文の適合度(重要度)を10段階で表したものであり，正解の文のutilityにどのくらい近いutilityの文を選択できるかで評価を行なう．しかし，このような適合性の評価は被験者への作業負荷が大きいという問題がある．Donawayらは，人間の作成した正解要約の単語頻度ベクトルとシステムの要約の単語頻度ベクトルの間のコサイン距離で評価するcontent-basedな評価を提案している．content-basedな評価では，指定された要約率の正解要約を一つだけ用意すれば評価可能であるため，utilityに基づく評価に比べ，被験者への負荷が少ない．しかし，この評価方法で2つの要約を比較する場合，どの程度意味があるのかについては，これまで十分な議論がなされていない．そこで，本研究では，まず，utilityに基づく評価の問題点を改良する新しい評価方法を提案する．一般に低い要約率の抜粋に含まれる文は高い要約率の抜粋中の文よりも重要であると考えられる．このような考えに基づけば，あるテキストに関して複数の要約率のデータが存在する場合，テキスト中の各文に重要度を割り振ることが可能であるため，utilityに基づく評価を疑似的に実現することができる．これまでの要約研究において，1テキストにつき複数の要約率で正解要約が作成されたデータは数多く存在する(例えば，)ことから，提案する評価方法に用いるデータの作成にかかる負荷は決して非現実的なものではなく，utilityを直接被験者が付与するより負荷は小さいと考えられる．本研究では，評価型ワークショップNTCIR2の要約サブタスクTSC(TextSummarizationChallenge)で作成された10%，30%，50%の3種類の要約率の正解データを用いて，提案方法により評価を行う．この評価結果をF-measureによる結果と比較し，提案方法がF-measureによる評価を改善できることを示す．次に，本研究では，content-basedな評価を取り上げる．同様にTSCのデータを用いて，人間の主観評価の結果と比較し，これまで十分議論されていないその有用性に関する議論を行う．本論文の構成は以下のとおりである．次節では，まず，これまで提案されてきた内的な評価方法，特にF-measureの問題点の解消方法について述べる．3節では，本研究で提案する評価方法について説明する．4節では，F-measureと提案する評価方法を比較し，結果を報告する．また，content-basedな評価に関する調査についても述べる．最後に結論と今後の課題について述べる．</section>
  <section title="関連研究">要約の内容に関する評価について，Jingらは，典型的な評価方法の1つであるF-measureをとりあげ，その問題点をいくつか指摘している．Jingらは，システムの要約と人間の被験者の作成した抜粋との比較による評価と，要約を利用して人間がタスクを行なう場合のタスクの達成率による評価の2つの評価方法を分析し，評価結果に影響を与える要因を同定することを試みているが，その結果少なくとも次の2つの点において，これまでの人間の抜粋を用いた評価方法は問題であるとの知見を得ている．問題点1(要約率の変化に伴う評価値の変化):人間の抜粋との比較による評価では，要約率を変化させると，システムの評価がかなり変化する．このため，特定の要約率でシステム間の性能の比較をする意味がどの程度あるのかは疑問が残る．問題点2(テキスト中の複数類似個所の選択問題):テキスト中に類似の内容を含む文が複数存在する場合，どちらの文が正解として選択されるかにより，システムの評価は大きく変化する．これまで，問題点1(要約率の変化に伴う評価値の変化)を解消するいくつかの方法が提案されている．Mittalらは，要約率の違いによるシステムの評価の違いに関して，さまざまな要約率における精度を求めた上で，情報検索の評価で用いられている11点平均精度(11pointaverageprecision)のように，複数の要約率での精度の平均として結果を示すべきであるとしている．また，コーパスとするテキスト集合の違いが精度に影響を与えることから，コーパスの要約のしやすさを計る指標として，ランダムに文を選択して要約を作成した場合の精度をベースラインとして示すべきであると主張している．そして，システムの性能を評価する場合，[p'=p-b1-b](ここで，p，b，p'はそれぞれシステム，ベースライン，補正後のシステムの精度)のように，ベースラインを用いて補正した精度を用いるべきであるとしている．一般に，F-measureで要約の精度を評価する場合，ベースライン値＝要約率と考えることができるため，要約率が大きくなるにつれ，F-measure値は大きくなる傾向にある．従って，ベースラインを用いて評価値を補正する上記の評価方法は，Jingらの指摘する問題点1の解消には有用であると考えられる．一方，被験者間の一致の度合をJとすると，Jは要約システムの精度の上限と考えられ，また，ランダムに選択した時の精度Rは下限と言える．そのため，Radevらも，Mittalらと同様に，システムの性能を計る値を示す際，普通に計算された値Sを単に用いるのではなく，これらの値で正規化した値[S'=S-RJ-R]を示すべきであるとしている．問題点2(テキスト中の複数類似個所の選択問題)を解消する方法もいくつか提案されている．Jingらは，人間が選択した重要文を用いて評価を行なう際，正解と一致した場合正解数1，一致しない場合0として再現率，精度を計算するのではなく，正解数を被験者間の一致の度合として計算する方法を提案している．たとえば，5人の被験者中3人，2人がそれぞれ一致して選択した文が存在する場合，これまでの評価方法では，前者をシステムが選択した場合正解数1(過半数以上の被験者が選択しているので)，後者では0となるが，提案する方法では，システムの正解数は，前者では3/5，後者では2/5となる．Radevらは，文のutilityという概念を用いた評価方法を示している．文のutilityは，文がそのテキストの話題に対してどの程度適合した内容であるかを示す尺度であり，[0-10]の値をとる．人間が選択した重要文を用いたこれまでの評価方法は，正解と一致した場合正解数1，一致しない場合0として再現率，精度を計算していたが，utilityに基づく評価値は，システムが選択した文に対して人間が割り当てたutilityの総和を，正解の文のutilityの総和で割った値として計算する．これまでの評価方法では，システムが選択した不正解の文は，全く評価が得られなかったのに対し，utilityに基づく評価の場合，Jingらの方法と同様に，たとえ不正解でもその文がある程度の重要度を持つ場合，その重要度に対する部分的な評価が得られる点が異なる．ただ一つ正解が存在し，それとまさに一致することを要求されていたこれまでの評価に比べ，正解の文のutilityにどのくらい近いutilityの文を選択できるかで評価を行なう．Donawayらは，2種類の評価方法を提案している．一つは，人間にも，システムにも，テキスト中の文にすべて順位をつけさせるようにして，その文の序列を比較して評価を行なう方法である．これは，これまでの方法がテキスト中の文を重要/非重要の2つに分類して評価に利用していたのに対し，テキスト中の文数に分類して利用することに相当する．Donawayらが提案するもう一つの評価尺度は，人間の作成した正解要約の単語頻度ベクトルとシステムの要約の単語頻度ベクトルの間のコサイン距離で評価する方法(以後，content-basedな評価)である．Donawayらは，この2種類の評価尺度にこれまでの評価方法である再現率に基づいた評価を加え，これらを実験により比較，検討している．正解の抜粋に含まれる個所が要約作成者毎に異なっていても，内容の類似した個所を抜き出しているのであれば，どの要約作成者の抜粋を用いても似たような評価値が得られる必要がある．Donawayらは，4人の要約作成者の作った抜粋を用いて，上で述べたいくつかの尺度で要約を評価し，尺度毎に評価値の相関を調べている．その結果，content-basedな評価が人間の要約との比較による評価方法としては，Jingらの指摘する問題点2に対する解決策ともなっており，もっとも優れていると結論づけている．</section>
  <section title="pseudo-utilityに基づく評価">本研究では，あるテキストに関する複数の要約率の正解データを用いることで，utilityに基づく評価を疑似的に実現する方法(以後，pseudo-utilityに基づく評価)を提案する．例えばあるテキストに，要約率がr_1%，r_2%，r_3%(r_1&lt;r_2,r_2&lt;r_3)の3種類の正解データが存在する場合，テキスト中の各文は(1)r_1%の要約に含まれる文，(2)r_1%には含まれないがr_2%には含まれる文，(3)r_2%には含まれないがr_3%には含まれる文，(4)r_3%に含まれない文の4種類に分けられるただし，r_1%の要約はr_2%の要約に(r_1&lt;r_2)，r_2%の要約はr_3%の要約に(r_2&lt;r_3)含まれていることが前提となる．．これらは，テキストの話題に対する各文の適合度が4段階で表されたデータと考えることができるため，4段階の疑似的なutilityに基づく評価が実現できる．表に示す例を用いて，pseudo-utilityの計算方法を説明する．表は，要約率10%，30%，50%の要約データを用いた場合について述べている．表では，S1--S10の10文からなるテキストについて，要約率毎に，要約作成者と2つの要約システム(System1と2)が選択した重要文を`+'で示している．また，ここでは各文の重要度wを`1/要約率'として計算する．表において，例えばSystem1の要約率50%の要約において，System1が重要文として選択した5文(S3，S4，S7，S9，S10)のうち3文(S4，S7，S10)が一致するため，F-measure値は0.6(3/5)となる．一方，System1が選択した5文(S3，S4，S7，S9，S10)の重要度はそれぞれ0，1/30，1/50，0，1/30であるため，重要度の総和は0+1/30+1/50+0+1/30=13/150となる．また，要約作成者は要約率50%ではS1，S4，S7，S8，S10の5文を選択している．この場合の重要度の総和は1/10+1/30+1/50+1/50+1/30=31/150となる．pseudo-utility値は，システムの選択した文の重要度の総和を要約作成者の選択した文の重要度の総和で割って正規化した値であり，この例の場合13/15031/150=0.419となる．表に，System1，2のF-measure値とpseudo-utility値を示す．表において，要約率10%におけるF-measure値とpseudo-utility値を比較すると，どちらのシステムも10%要約の正解であるS1ではなくS4を選択しているため，F-measure値は0になる．ここで，S4は30%要約の正解に含まれているため，S1よりも重要度は低いが，ある程度重要な情報を含んだ文であると考えられる．この例の場合，要約率10%ではF-measure値は0か1しか取り得ないが，pseudo-utilityに基づく評価では，このような文も評価の対象とすることで，より適切な評価が可能になる．また，要約率50%におけるSystem1とSystem2の結果を比較すると，どちらも選択した5文のうち3文が50%要約の正解データに含まれているため，F-measure値は共に0.6である．この3文のうちS4とS10はSystem1，2が共通して選択しているが，他の1文はSystem1はS7(重要度1/50)，System2はS1(重要度1/10)を選択している．2システムが同数の正解文を抽出している場合でも，特に重要と考えられる文(この場合S1)が抽出されている場合とそうでない場合との区別がF-measureではできない．一方，pseudo-utilityに基づく評価では，この場合System1，2それぞれにおいて0.419，0.806と評価値に開きがあり，この例では両者の区別がつけられることが分かる．</section>
  <section title="評価方法の分析">本研究では，pseudo-utilityに基づく評価の有効性を調べるために，TSCのデータを用いて評価を行う．また，TSCではcontent-basedな評価がシステムの評価方法の一つに採用されているが，この評価結果を用い，content-basedな評価の有効性についても検討する．本節では，まず，4.1節でTSCの課題および評価方法について説明する．次に，4.2節でTSCのデータを用いた本研究の分析について述べる．</section>
  <subsection title="TSCにおける評価">TSCとは，要約研究における資源の共有や日本語テキストの要約に関する共通の評価方法や評価基準の明確化を本格的に推進させるために行われた，第2回NTCIRワークショップのタスクである．TSCでは3種類の課題が設定されているが，本節ではそのうち内的(intrinsic)な評価を適用している2つの課題A-1「重要文抽出型要約」とA-2「人間の自由作成要約と比較可能な要約」について述べる．なお，結果に関する詳細およびこの他の課題については，を参照されたい．</subsection>
  <subsubsection title="課題">・課題A-1:重要文抽出型要約flushleft新聞30記事から，要約率10%，30%，50%で重要文を抽出する．・課題A-2:人間の自由作成要約と比較可能な要約flushleft新聞30記事を対象に，要約率20%，40%を越えない文字数で要約を作成する．なお，要約部分がplaintextであり，指定文字数以内に納まっていれば，どのような要約でも構わないため，課題A-1と同じシステムの出力からタグを取り除いて，plaintextにすれば，課題A-2にも参加できる．</subsubsection>
  <subsubsection title="要約対象テキスト">毎日新聞94年および98年から15記事づつ，計30記事が選ばれている．記事は94年から600，900，1200文字以上の3種類の長さの報道記事が，98年からは1200，2400文字以上の2種類の長さの社説が選ばれている．</subsubsection>
  <subsubsection title="評価方法"/>
  <subsubsection title="F-measureとpseudo-utilityに基づく評価の比較(課題A-1)">まず，実際にどの程度pseudo-utilityに基づく評価が有効に機能しているか，いくつかの事例にあたって調べてみた．図は，pseudo-utilityに基づく評価が有効に機能した典型例である．2文は，「アジアにおけるエイズ感染」に関する報道記事から，要約率10%(1文)で重要文を選択したシステムの出力結果と正解の要約である．この2文は，どちらも「アジアにおいてエイズ患者が急増している」ことを示した個所である．F-measureによる評価では，システムは正解文を選択していないので，F-measure値は0となる．一方，システムの選択した文は30%要約には含まれているため，pseudo-utility値は0.333(1/0.31/0.1)となる．一般に，報道記事1記事に含まれる文数は10文-20文が中心的であり，この場合，要約率10%の時は正解文が1-2文しかない．このような場合，システムがある程度重要な情報を含んだ文を抽出していても，最重要文が抽出されなければF-measureでは全く評価に反映されない．一方，pseudo-utilityに基づく評価では，図の例のようにある程度評価値に反映されるため，より適切なシステムの評価が行なえると考えることができる．別の例を図に示す．記事940715208において，要約率10%では正解要約文数は3文である．システムが出力した3文のうち第1文目が正解の要約に含まれているため，F-measure値は0.333となっている．一方，システムの出力した3文のうち，正解に含まれていない残りの2文の一方は30%の正解に，もう一方は50%の要約に含まれているため，pseudo-utility値は0.511(1/0.1+1/0.3+1/0.53/0.1)となっている．正解とシステムの出力を比較すると，正解の2文目にある「大学や教育施設一体となった動き」の具体例がシステムの要約の2文目と3文目になっていることがわかる．つまり，システムの抽出した2文は正解文(2文目)の部分情報となっている．このような個所をシステムが抽出できたことをpseudo-utilityでは適切に評価できていることは妥当であると思われる．これらの調査から，pseudo-utilityに基づく評価が，Jingらの指摘する問題点2(テキスト中の複数類似個所の選択問題)をある程度解消できていると考えられる．次に，F-measureとpseudo-utilityに基づく評価を適用した結果をシステム別にまとめた．結果を表および表に示す．課題A-1には7団体10システム参加しており，表中のI--IXは各システムのIDを，また，同団体の異なるシステムはダッシュで示してある．F-measureとpseudo-utilityに基づく評価の各システムの順位を比較すると，F-measureではそれぞれ1位，2位であるシステムII，Iが，pseudo-utilityに基づく評価では順位が逆転している．また，多くのシステムは順位が1位か2位程度変動しており，中でもシステムVは，F-measureでは9位なのがpseudoutilityでは5位になっている．そこで，これらの順位の変動が適正であるかどうかを調べるため，システムIとIIの出力結果を比較した．システムIとIIが出力したそれぞれ90個の要約(30テキスト×10%,30%,50%)のうち，システムIとIIでF-measure値は同じだがpseudo-utility値の異なる16組の要約について調査した．16組のうち，システムIIよりもIの方がpseudo-utility値が高くなる場合は10組，IIが高い場合が6組であった．表にシステムIとIIの出力例を示す．表は，記事980500136における要約率10%での例で，原文中の文ID，pseudo-utilityに基づく評価に用いた重要度，システムIとIIが選んだ文，および文の内容を示している．重要度1/10の文が要約率10%の正解である．システムIが選択した5文のうち要約率10%の正解に含まれるもの(重要度1/10)が2文(S44とS52)あるため，F-measure値は0.4になる．システムIはこの他に重要度1/30の文を1文(S30)，重要度1/50の文を2文(S3とS4)選択しており，結果として，このテキストにおいてはpseudo-utility値0.547を得ている．一方，システムIIもシステムIと同様に要約率10%の正解に含まれる文(重要度1/10)を2文(S26とS43)選択しているため，F-measure値ではシステムIと同じく0.4になる．システムIIが選んだ残りの3文のうち，重要度1/50の文の2文(S3とS4)はシステムIと共通であるが，残りの1文(S31)は重要度が0であり，pseudo-utility値はシステムIよりも低い0.480に留まっている(表)．この記事の主題は「定年制高齢者に多様な働き方を６５歳現役社会の道も開け」であり，S22(重要度1/10)はその問題提起になっている．システムIが選んだS50はS22の一つの解決方法であり，ある程度重要な情報を持った文であるため，システムIとIIでこの文が選択できたかどうかで，pseudo-utility値に差ができることは妥当であると考えられる．[t]table*</subsubsection>
  <subsubsection title="content-basedな評価の考察　-- 主観評価との比較 -- (課題A-2)">まず，content-basedな評価の比較対象となる主観評価の結果について簡単に述べる．次に，主観評価とcontent-basedな評価の結果を比較し，考察する．</subsubsection>
  <section title="結論と今後の課題">本研究では，要約の評価方法について，pseudo-utilityに基づく評価方法を提案し，F-measureとの比較を行った．また，content-basedな評価と被験者による主観評価との結果を比較し，結果について検討した．F-measureとpseudo-utilityに基づく評価の比較では，要約システムの出力をいくつか調べたところ，正解には含まれていないが正解文と類似する内容の文をシステムが抽出した場合，pseudo-utilityに基づく評価では評価値にそれが反映されていることが確認された．すなわち，pseudo-utilityに基づく評価は，F-measureがかかえる2つの問題点のうち「(2)テキスト中に類似の内容を含む文が複数存在する場合，どちらの文が正解として選択されるかにより，システムの評価が大きく変化する」が解消できていることがわかった．次に，content-basedな評価と被験者による主観評価との比較の結果，2つの要約が，content-based値で0.2以上の開きがあれば，93%以上の割合で人間の主観評価の結果と一致することがわかった．本研究では，複数の要約率のデータを用いることで，Radevらの提案するutilityに基づく評価を疑似的に実現できることを示した．本研究はTSCで作成された10%,30%,50%の3種類の要約データを用いたが，今後は，この他の要約率の組合せについても調べる必要がある．また，本研究ではpseudo-utilityに基づく評価において，文の重要度を`1/要約率'として計算したが，この他にも様々な重要度を設定することが可能である．重要度をどのように設定すればより良い評価が可能になるかについても調べる必要があると考えられる．本研究では扱っていないが，Jingらの指摘する問題点1(要約率の変化に伴う評価値の変化)を解消する評価方法についても今後検討していく必要がある．document</section>
</root>
