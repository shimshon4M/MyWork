<?xml version="1.0" ?>
<root>
  <title>ドメイン固有の文字列情報の組み込みによる形態素解析処理の精度の向上</title>
  <author>延澤志保佐藤健吾斎藤博昭</author>
  <jabstract>辞書ベースの自然言語処理システムでは辞書未登録語の問題が避けられない．本稿では訓練コーパスから得た文字の共起情報を利用する手法で辞書未登録語の抽出を実現し，辞書ベースのシステムの精度を向上させた．本稿では形態素解析ツールをアプリケーションとして採用し，処理時に統計情報を動的に利用することによって形態素の切り分けの精度を上げる手法と，統計情報を利用して事前に辞書登録文字列を選別し必要なコスト情報を補って辞書登録を行なう手法との2つのアプローチを提案し，さらにこの2つの手法を組み合わせてそれぞれの欠点を補う手法を提案する．どちらも元のツールの改変を行なうものではなく，統計情報の付加的な利用を半自動的に実現するもので，元のツールでは利用できない辞書未登録語の抽出に対象を絞ることで精度の向上を図る．実験の結果，動的な統計情報の利用のシステムが未知語の認識に，辞書登録システムが切り分け精度の向上に有効であることが示され，2つのシステムを適切に組み合わせることによって訓練コーパスのデータで認識可能な辞書未登録語をほぼ完全に解決できた．さらに複合語の認識も高い精度で実現することができた．</jabstract>
  <jkeywords>文字間共起情報，辞書登録，統計情報の動的な利用，文字列自動抽出</jkeywords>
  <subsubsection title="辞書 : 一人の評価による登録文字列選択">訓練コーパスから抽出された有繋文字列を一人の手によってすべての候補をチェックし，そのままで辞書登録可能な有繋文字列，過接合有繋文字列から適切な部分を切り出した文字列の2種類の文字列を選択した．過分割有繋文字列については，分割され削除されていた部分が容易に推測できる場合であっても，登録文字列としなかった．登録文字列の切り出しの対象は，名詞，複合名詞，数式，数値(単位も含む)，意味のある記号の羅列，英単語の羅列とし，茶筌既登録語は登録文字列から除外した．各登録文字列iの形態素コストc_iは，式()にx_iとしてiの候補文字列としての出現頻度f_iを適用して算出した．他の選択手法と異なり完全に人手で確認しているためノイズの心配がないことから，c_maxは4,000とした．ユーザ個人が一人で選択する場合，登録語とする基準をユーザ個人で設定できるため，複数人で選別を行なう場合のようなばらつきや基準の統一といった問題がない．しかし，ドメインが大きくなれば登録候補語も増加するため，一個人がこの選別を行なうことは大きな労力となる．</subsubsection>
  <section title="はじめに">辞書ベースの自然言語処理ツールは高い精度が期待できる反面，辞書未登録語の問題があるため，統計情報を利用して辞書未登録語の抽出を行なう研究が盛んに行なわれている．辞書未登録語はドメイン固有の語句と考えることができ，対象ドメインの統計情報の利用が有効である．本稿ではドメイン固有の文字列の自動抽出で問題となるノイズを2方向のアプローチで解決する手法を提案する．本手法は辞書ベースのツールに付加的な情報を半自動的に与えて辞書未登録語の抽出を行なうことで処理精度の向上を図るものである．本稿では形態素解析ツールについて実験を行なったが，本手法は処理内容やツールに特化したものではなく，ツールの改変を伴うものではない．</section>
  <section title="語句抽出">現在研究されている語句抽出システムは，ほとんどが対象を名詞に準じた単語列に限定したものである．これは，抽出の対象となる語句は未知語や専門用語が主であり，どちらも名詞がその大半を占めるためである．未知語，専門用語，固有名詞などはドメイン固有の語句と言ってよいが，ドメイン固有の語句となりうるのは新語や複合語がほとんどで，例えば助詞のように新語の出現しないものや活用語のようにドメインによってはほとんど新語がないものなどは抽出対象となりにくく，名詞に準じる語句を抽出対象とすることでかなりの未知語，専門用語などを取得することが可能である．また，対象とする品詞を限定することで抽出処理に必要なルールが削減され，ノイズの軽減に繋がるという利点がある．</section>
  <subsection title="名詞句抽出">抽出対象は主に，名詞の推定と名詞句の推定とに二分されており，特に名詞の推定では専門用語など複合語の推定を行なうものと固有名詞などの未知語を認識するものに分けられる．名詞句の推定を目的とした研究としては，Argamonらの提案したサブパターン概念を利用する手法などが挙げられる．専門用語などドメイン固有の語句の抽出では，tf・idfモデルなど語句の出現頻度を利用する手法とn-gramなど文字の共起頻度を利用する手法がある．湯本らはunigramおよび隣接bigramの出現頻度を利用して複合名詞の認識を行なう手法を提案した．湯本らの抽出対象は専門用語であったが，彼らは専門用語として複合名詞のみを考え，名詞のみに着目した隣接bigramを利用して複合名詞の推定を行なった．専門用語となる語句の大部分を複合名詞が占めること，頻出複合名詞の構成要素である名詞は共起頻度が高く，また特定の単語同士の並びが多いことを考えると，この手法は効率的だと言える．Frantziらは英語を対象としてC-valueを利用して複合語を認識する手法を提案し，さらにこれにコーパスサイズや重みなどを利用してランク付けを行なった．Chaらは未登録語発見のための形態素パターン辞書を利用して未登録語の認識およびタグ付けを行なう手法を提案した．固有名詞の認識では，最大エントロピー法を利用して固有名詞の切り出しを行なうBorthwickの手法や，固有名詞の前後に出現しやすい語をトリガーワードとして固有名詞の認識を行なう手法，トリガーワードと構文解析情報を利用する福本らの手法などがある．またCucerzanらは文字の並びの情報を利用して少ない訓練データからの固有名詞の推定を可能にする手法を提案し，ルーマニア語や英語など複数の言語に対して有効であることを示した．</subsection>
  <subsection title="文字単位の統計情報を利用した辞書未登録語抽出">文字の共起情報を利用する手法としては，n-gramの画期的な抽出法を提案しこれを利用して文中の文字列の塊を認識する長尾らの手法などが挙げられる．中渡瀬らはn-gram統計を利用して辞書未登録語を自動獲得する手法を提案した．中渡瀬らは任意の文字列の頻度を正規化する手法を提案し，これを用いて語の境界を決定することで，辞書未登録語を獲得している．中渡瀬らはこの手法で漢字未登録語の43%の取得に成功したと報告している．中渡瀬らの手法では評価対象を漢字未登録語に限定しているが，これは漢字とその他の字種の出現頻度分布が異なるためで，正規化を行なう中渡瀬らの手法では漢字での精度が高いためである．延澤らは品詞タグや文法などに頼らず機械的に取得可能な文字間の統計情報のみを利用して文の切り分けを行なう手法を提案している．延澤らはこの手法を利用してドメイン固有の文字列の自動抽出を試みており，口語文章のような非文を多く含むコーパスに対しても有効であることを示した他，固有名詞抽出など抽出対象を絞った場合などについても有効であるとしている．文字列の抽出はその後の利用を見込んだものであるが，延澤らの手法では文字単位での処理を行なっているため抽出される文字列は単語，複合語，言い回しなどサイズがさまざまである．自然言語処理においては一般に単語または形態素が処理単位とされている．単語は多義性を持つものも多く，単語が最適な処理単位と言えるかは疑問が残る．その意味で，特定の処理単位を設定することは処理の精度に悪影響を与えている可能性もあるが，さまざまな処理単位を同時に扱う手法は確立されておらず，処理単位の特定が必要であるのが現状である．このため，さまざまな処理単位の文字列が同時に出力とされる延澤らの手法を用いて出力された文字列は，そのままでは他のツールでの利用が困難である．そこで本稿では，延澤らの手法の問題点を克服し，この手法を利用して辞書未登録語を抽出することで辞書ベースのツールの精度の向上を図る．</subsection>
  <section title="システム概要">本稿で提案するシステムは，対象ドメインのコーパスからシンプルな手法でドメイン固有の語句を抽出する延澤らの手法を応用したものであり，辞書ベースの自然言語処理ツールの支援を目的として2方向からのアプローチを試みる．</section>
  <subsection title="共起関係抽出">文字間の共起情報が頻出文字列認識に有用であるとの延澤らの主張に基づき，本稿では対象ドメイン固有の頻出文字列の抽出に利用する統計情報として，文字間の共起情報を採用した．そこで，前処理として訓練コーパス中の各二文字ペアの共起頻度を数え上げる．本システムは訓練コーパスに全く制限を設けない．品詞情報などの付加情報を一切利用しないため，形態素解析や構文解析，タグ付けなども必要としない．訓練コーパス中の文字共起頻度の数え上げにはd-bigram確率モデルを利用した．d-bigramとは距離を考慮したbigramモデルであり，abbcという文字列の場合，隣接する(a,b)などだけでなくaとcのように離れて出現する二文字の共起関係も取得する．この例ではaとcは距離3となり(a,c;3)のように表される．隣接bigramでは視野が非常に狭く文脈情報が利用できないという欠点があり，特に文字レベルでの利用はノイズが大きい．これに対し，d-bigramモデルは距離の情報を保有することでこの問題に対処しており，例えば3単語の並び(trigram)も十分に評価できることが示されている．さらに，同じ文中であっても離れて出現する文字同士は近接して出現する文字同士に比べて関係が薄いと考えることができるという主張に基づき，d-bigramの取得，利用に際して距離の上限および距離の影響力を設定することが可能である．</subsection>
  <section title="システム M: 茶筌への組み込み">本稿で提案するシステムは，日本語を対象とした形態素解析ツール・茶筌に統計情報を利用した文字列抽出モジュール(システムM)を組み込むことで統計情報の活用を図るものである．これは茶筌に特化した手法ではなく，茶筌本体の構造を改変するものではない．</section>
  <subsection title="茶筌での統計情報の利用">茶筌は辞書ベースの形態素解析ツールであり，文単位で処理を行なう．図に茶筌による形態素解析の流れを示す．入力であるテストコーパスは一文ずつ処理され，形態素解析結果が出力される．形態素解析処理においては，事前に準備された辞書を利用する．図に本稿で提案する統計情報利用システムを茶筌に組み込んだ場合の形態素解析の流れを示す．本稿で提案するシステムではまず茶筌に有繋文字列抽出モジュールを組み込むことにより文字列の認識を行ない(節)，抽出された文字列に専用の品詞名を付けることで辞書の見出し語と同等に扱うことができるようにする(節)．認識する文字列は延澤らの提案した有繋文字列と呼ばれるもので，文字間の共起情報のみから一塊と推測された文字列である．本システムを組み込むことで，茶筌の持つ辞書の他に，その文中に含まれる有繋文字列を形態素の候補として利用することが可能となる．辞書に掲載されている語句が有繋文字列として抽出された場合は，辞書の情報を優先する．従って，辞書既登録語句は有繋文字列として抽出されることはない．</subsection>
  <subsection title="形態素解析時における有繋文字列の認識">文中のi番目の文字とi+1番目の文字の間の有繋評価値(i)の算出式を式()に示す．ただし，w_iは文wのi番目の文字，dは2文字間の距離，d_maxはdの最大値，g(d)は距離の影響に対する重み付け関数であり，本稿ではd_max=5，g(d)=d^-2とした．(i)=_d=1^d_max_j=i-(d-1)^i_d(w_j,w_(j+d);d)g(d)eqnarrayまた，2文字間の相互情報量の計算式をd-bigramに対応するよう拡張したものとして式()を利用した．ただし，x,yは各文字，dは2文字間の距離，P(x)は文字xが出現する確率，P(x,y;d)はd-bigram(x,y;d)が起こる確率とする．_d(x,y;d)=log_2P(x,y;d)P(x)P(y)eqnarray図に有繋評価値を利用した文字列認識の例を示す．図の横軸が入力文，縦軸が有繋評価値を示す．横軸のアルファベットは入力文中の各文字を示す．文中の各隣接文字ペア間の有繋評価値は，隣接文字ペアの共起頻度が高いほど高くなる．従って図の中で評価値を繋いだ線が山状になっている部分は共起する可能性の高い部分であり，一塊の文字列である可能性が高い．これに着目し山状の部分を抽出することで，文中の文字列の認識を行なう．</subsection>
  <subsection title="形態素解析時における有繋文字列の利用">形態素解析中節の手法で認識された有繋文字列は専用の品詞およびコストが設定され既存の辞書の登録語と同等として形態素解析処理に利用される．有繋文字列は特定の品詞に対応するものではないが，個々の有繋文字列に対してその品詞の推定を行なうことはシステムの実時間性を損ねるため，品詞「有繋文字列」を新設しこれに対して予め品詞情報を設定しておく．実際に認識される文字列は名詞またはそれに準じるものがほとんどであるため，品詞「有繋文字列」の接続はすべて名詞接続とした．茶筌では各語句に形態素コストが設定されている．有繋文字列は文字間の共起情報によって決定するものであり，一塊の文字列であると評価する際の評価値の高さがそれぞれ異なる．そこで，評価値によって有繋文字列を5段階に分類し，段階ごとに形態素コストを設定することで，評価値の高いものを優先的に利用できるように設定する．</subsection>
  <subsection title="統計情報を組み込んだ茶筌による形態素解析例">図に，本システムを茶筌に組み込んだ場合の実行例を挙げる．図の上段が茶筌のみで解析を行なった場合，下段がシステムMを組み込んで解析を行なった場合の切り分け結果である．下線は辞書未登録語を，太字は有繋文字列として抽出された部分を示す．図では辞書未登録語2文字列が本システムを利用することで有繋文字列として抽出されている．「bigram」のように辞書未登録語がそのままの形で一語である場合，この部分の切り分け結果は正解と変わらないため他の部分の解析結果への影響がない場合が多いが，図の例のように他の部分へ影響を与える場合もある．この例では本システムを利用し「bigram」の品詞が「有繋文字列」となったことで「など」が正しく認識されている．「n-gram」は茶筌のみを利用した場合「n(記号)」「-(記号)」「gram(未知語)」に分割された．複数の字種から成る未知語の場合は字種ごとに区切られる場合がほとんどである．システムMでは字種情報を利用せずすべての字種の文字を同様に扱うため，字種の替わり目で誤分割されず，「n-gram」の認識に成功した．またシステムMを利用することで「自然言語」「統計情報」などの複合語も多く認識された．「自然言語」は，複合語「自然言語処理」の一部分であるが，「自然言語」自体一塊で複合語を形成し「自然言語処理」の構成要素となると考えられる．</subsection>
  <section title="システム D: 辞書への組み込み">訓練コーパスから取得した共起情報をそのまま利用する手法ではノイズの問題が防げない．この問題を解決するため，本章では共起情報をそのまま利用するのではなく，共起情報を利用して辞書登録候補文字列を抽出しこれを事前に辞書に登録する手法を提案する．図に本章で提案する辞書作成システムを利用して事前に作成した有繋文字列辞書を茶筌の辞書に組み込んだ場合による形態素解析の流れを示す．基本的な流れは図と同じだが，利用する辞書は茶筌の基本辞書に有繋文字列辞書を組み込んだものとなっている．この有繋文字列辞書は訓練コーパスから作成したものであり，この辞書を組み込むことによってドメイン固有の文字列を形態素解析処理で利用する．本章で作成する辞書は茶筌の辞書の補完という位置付けであり，辞書既登録語は登録しない．また，茶筌が元々持つ辞書の改変を行なうこともない．</section>
  <subsection title="登録文字列の属性設定">辞書登録文字列の属性は以下のように決定する．品詞登録文字列それぞれに対して適切な品詞を人手で設定することは多大な労力を必要とするだけでなく，その適切さの評価や曖昧性の問題などが存在するため，本稿では登録文字列はすべて同じ品詞とした．登録文字列に割り振る品詞として「有繋文字列」を新設した．品詞「有繋文字列」と他の品詞との接続コストの設定は茶筌の既存の品詞「名詞」中の「一般」カテゴリに準拠することとした．形態素コスト個々の登録文字列の形態素コストはその文字列の頻度情報などの情報に基づいて個々に設定することとする．この関数で利用するパラメータは，本稿で提案する複数の有繋文字列辞書作成手法に依存するものとする．形態素コストc_iの算出式を式()に示す．ここでiは文字列，x_iは文字列iの情報を示す値であり，x_iに適用する値を変化させることで各辞書の特徴を形態素コストに反映させる．c_i=[-c_maxx_max-1x_i+c_maxx_maxx_max-x_min]eqnarray形態素コストは「コーパス内に1回出現する文字列の形態素コストを4,000とする」とする茶筌の定義に基づき，下限c_minを0，上限c_maxを4,000または8,000とする．x_minおよびx_maxは各辞書で利用するx_iによって決まる．</subsection>
  <subsection title="登録文字列の選択">本稿では辞書に登録する文字列の選択手法を4種類用意し，4つの辞書を作成した(表)．本稿では辞書登録の対象を名詞に準じる文字列に絞る．</subsection>
  <section title="システム M+D: 辞書登録と切り分け処理の併用">処理の段階で動的に有繋文字列を認識し利用するシステムMでは，ノイズを完全に防ぐことは不可能である．ノイズを抑えるためには，動的な処理でなく，事前に必要な有繋文字列を辞書登録してしまう方法が有効である．辞書登録を行なうことでドメイン固有の文字列を辞書に反映させることが可能となるが，完全な辞書の作成は不可能であるという辞書ベースの手法の問題点の完全な解決にはならない．また本稿で利用するd-bigram確率モデルはbigram情報の積み重ねであるため特に複合語やこれに類するものの認識において間に入る語句を柔軟に扱えるという利点があるが，辞書登録ではd-bigramの持つ柔軟性が失われる．これらの問題を解決するために，辞書登録と切り分け処理の併用が考えられる．事前にドメイン固有文字列の辞書登録を行ない，さらに補助として組み込みの切り分けシステムを利用することで，頻出語句の認識が可能な上，ノイズの減少を図ることが可能となる．図に本稿で提案する統計情報利用システムと辞書作成システムを利用した有繋文字列辞書の両方を茶筌に組み込んだ場合による形態素解析の流れを示す．表にシステムMを組み込んだ実験での閾値ごとの形態素コストを示す．実験M+DではシステムMをシステムDの補完の立場で利用するため実験Mに比べて形態素コストを大きく設定している．</section>
  <section title="実験および考察">本稿で報告する実験は，論文コーパスを茶筌を用いて形態素解析を行なったものである．対象ドメインの持つ統計情報を利用することで形態素解析の精度の向上を図る．</section>
  <subsection title="実験内容">本章では，本稿で提案している2つのシステムの利用実験について報告する．システムMだけを利用した場合，システムDだけを利用した場合，MとD両方併用した場合，どちらも利用しなかった場合の4種類の実験を行なった．システムDを利用した実験では，それぞれ，節で提案した4種類の辞書を試みた．以上，10種類の実験について報告する．</subsection>
  <subsubsection title="コーパス">本実験では，コーパスとして自然言語処理分野の論文を利用した．訓練コーパス，テストコーパス共に，自然言語処理分野を専攻する学生6人の論文計17本を合わせて作成したものである．本稿では，統計情報利用システムの組み込みで利用する統計情報を得るための訓練コーパスおよび辞書登録のための辞書登録文字列の抽出について，同一の訓練コーパスを利用した．訓練コーパスに含まれる文の総数は4,816，含まれる文字の数は213,489(平均44.33文字/文)である．また本稿では，それぞれの実験における形態素解析対象として，同一のテストコーパスを利用した．テストコーパスに含まれる文の総数は1,149，含まれる文字の数は55,755(平均48.52文字/文)である．</subsubsection>
  <subsubsection title="正解形態素解析，正解文字列">比較のため，テストコーパスを人手で形態素解析したものと辞書登録すべき文字列の正解リストを作成した．正解形態素解析テストコーパスを茶筌に掛け，その結果を人手で修正したものである．修正の対象は切り分け誤りおよびタグ付け誤りとした．切り分け誤りは，明らかな間違いの他，複合語とすべき語が切り分けられている場合も含む．複合語としたのは名詞の並びの他，接頭詞が付着するもの，英単語列などである．そのほか，数式，数値は全体で一塊とした．正解文字列正解形態素解析結果中，茶筌に登録されていない文字列を正解文字列とした．ただし，本稿の手法では名詞に準じるものを対象として考えているため，動詞，形容詞などに相当する文字列は正解文字列から除外した．</subsubsection>
  <subsection title="実験結果">表に各実験の結果をまとめる．「茶筌のみ」とある実験は統計情報を利用しない場合のものであり，これと比較することで本稿で提案するシステムの有用性を示す．未知語削減率は茶筌のみの場合に対する割合，形態素総数は正解形態素解析での形態素総数に対する割合である．それぞれのシステムについての考察は節以降で述べる．適合率，再現率表に示した適合率，再現率の算出には，正解形態素解析，正解文字列(節)を利用した．有繋文字列と認識された箇所に対する正解形態素解析中の正解文字列の出現箇所の割合を適合率，全ての正解文字列に対して正しく認識された文字列の割合を再現率とした．正解文字列に完全にマッチしたものを完全適合率および完全再現率，正解文字列と同一ではないが誤りではない有繋文字列の場合を拡張適合率および拡張再現率とした．再現率の計算には正解文字列の総数を利用しているが，この正解文字列はテストコーパスから作成したものであり，訓練コーパス中に出現しない文字列も多く存在する．再現率が低くなる理由の一つに，論文中に含まれる数式および数値が挙げられる．表では数式の扱いが厳しく，完全に適合する数式でないと失敗としているが，実際には数式は必ずしも一定の形で現れるとは限らず，辞書登録の対象とするには無理がある．その他の要因としては，論文ごとの表記の揺れや出現語句の相違，複合語の認識失敗がある．表を見ると，適合率は完全にマッチした場合で最高77.0%，拡張適合率では最高98.1%と高い．これは，有繋文字列が高い精度で抽出されたことを示す．有繋文字列は辞書既登録語を含まないことから，本手法が辞書未登録語の抽出に有効であると言える．どの辞書を利用した場合でも，適合率はシステムDがシステムM+Dの結果を上回る．これは辞書登録文字列の選択が適切になされたことを示す．システムM+DではシステムMの影響でノイズが増えたが，システムMだけの場合では50.1%であり，システムDがノイズの削減に有効であることが判る．未知語削減率表の未知語削減率は，茶筌のみでの実験結果中の未知語数に対してどれだけ未知語が削減されたかを示す値である．統計情報を一切使わない場合483の文字列が未知語として出力された．本稿のシステムでは，最悪の場合でもその58.6%の認識に成功している．未知語認識の上限は86%前後となっているが，これらは訓練コーパス中に出現しなかった未知語67個の抽出に失敗したものである．表によると実験M，実験M+Dでは未知語削減率が約86%となっており，システムMが訓練コーパスに出現する未知語を最大限に削減できることが判る．それに対して辞書登録のみを利用した実験Dは低くなっており，辞書登録だけでは未知語を完全にカバーすることが困難であることを示している．システムM+DはシステムMの持つノイズの問題を抑えながら最大限の未知語削減率を保っている．訓練コーパス中に出現しない文字列の認識は，n-gramのように連続型のモデルでは不可能である．それに対し，本手法で採用したd-bigramはギャップのある事象の共起情報を複数組み合わせて文字列認識を行なうため，訓練コーパスに出現しない文字列についても有繋性の推定が可能である．推定の精度は訓練コーパスや認識対象文字列に依存するため，これらの効果的な認識が今後の課題として挙げられる．総形態素数表の総形態素数とは各実験での形態素の総数の正解形態素解析に含まれる総形態素数に対する割合を示す．総形態素数の割合が高ければ過分割が多いことが，割合が低ければ過接合が多いことが推察される．表の形態素総数を見ると，本稿のシステムを利用しない場合の形態素総数は正解形態素解析の118.8%であり，過分割が頻繁に起こって形態素数が2割近く増えていることが判る．システムMだけの場合にはこれが100%を下回り，過分割がかなり抑えられている．しかし，訓練コーパスとテストコーパスが独立であるため未知語や過分割を完全には除去できないことを考えると，この結果には過接合も多く含まれることが考えられる．実際，システムMでは過接合が多く完全適合率は他に比べて低くなっていた．システムDだけの場合は形態素総数が正解の110%から117%となっており，茶筌のみの場合に比べて過分割が減少している．システムM+Dは適合率を十分高く保ったまま形態素総数を110%前後にしており，システムDだけの場合よりもさらに正解に近づいている．本稿はオープンコーパスでの実験を行なったため，特に数式や数値などの文字列で過分割は避けられない．これらのことから，システムMとDを的確に組み合わせることで過分割を最大限減少させたと言える．</subsection>
  <subsection title="実験 M: 統計情報を組み込んだ茶筌による形態素解析">表によるとシステムMの適合率は他に比べてかなり低いが，これは形態素解析時に有繋文字列とされた文字列が他に比べて多いことにも起因する．システムMでは有繋文字列の絞り込みを一切行なっていないため動詞句などノイズとなる文字列を多く含むことが適合率の低下の原因である．システムMでの形態素総数は正解形態素解析の98.6%と唯一100%を下回り，システムMを適用することで過分割を大幅に削減できることが判る．しかし，本稿の手法では正解形態素解析よりも形態素総数が多くなることは避けられない．正解形態素解析よりも形態素数が少なくなったシステムMは過接合を多く含んでおり，他に比べて精度が良いとは言えない．システムMは拡張再現率が他に比べて高い(表)．これはシステムMによる動的な統計情報の利用が正解文字列の認識に有効であることを示している．辞書登録の場合ノイズは減少するが，柔軟な対応が必要な文字列の認識が難しくなる．システムMの利用はこの問題点の解消に有効である．表に論文コーパスを茶筌を利用して形態素解析した実験結果を示す．表を見ると名詞と記号が減少していることが判る．論文コーパスでは複合名詞の出現が多く，頻出する複合名詞が有繋文字列として一塊にされたことが，名詞の減少の大きな理由である．記号の減少については，数式と英字文字列，頻出言い回しが大きな理由として挙げられる．英字，その他の記号の減少の最大の理由は数式である．数式中に含まれる文字列はほとんど未知語または記号として扱われるが「P(x)」など確率を示す関数などは有繋文字列として認識されており，結果的に記号タグの振られる文字列が減少した．句読点と助詞，動詞の減少は，主に頻出言い回しに起因する．頻出言い回しの抽出はシステムMでは防ぐことができず，過接合の頻出の原因となっている．しかし表を見ると副詞など他の品詞の形態素数の変化は小さく，システムMの利用による誤解析は少ないことが判る．</subsection>
  <subsection title="実験 D: 有繋文字列の辞書登録">本稿では，訓練コーパスを入力として抽出された有繋文字列を辞書登録候補とし，節で提案した4通りの方法で辞書登録文字列の選択を行なった．本稿ではそれぞれの登録候補選択手法について文字列ごとに形態素コストの設定を行なっている．登録候補文字列を取得するため，訓練コーパスに対して実験Mと同じ方法で形態素解析を施した．この結果有繋文字列として出力された文字列5,402が各辞書の登録候補文字列となっており，この集合から辞書登録文字列の絞り込みを行なう．</subsection>
  <subsection title="実験 M+D: 辞書登録と統計情報利用システム組み込みの併用">形態素解析時の統計情報の利用を行なうシステムMは動的な文字列の切り分けを可能にし，柔軟な処理が可能となった反面，ノイズの問題が起こる．それに対して訓練コーパスから得られた統計情報を元に辞書未登録文字列を抽出し辞書登録を行なうシステムDは，登録文字列の絞り込みを行なうためノイズの問題を軽減することが可能であるが，柔軟な処理には不向きである．従って，システムDで認識しきれない部分をシステムMが補完する形で両者を組み合わせることで，訓練コーパスの情報を生かした処理が可能となり，精度の向上が期待できる．訓練コーパスからの学習では形態素総数の削減は正解の110%程度が限界だが，システムM+Dでは形態素総数は110%前後となっており，特に人手による辞書作成の場合適合率が十分高いことを考えると，コーパスの情報を利用することでテストコーパスの形態素切り分けの精度を十分にあげたと言ってよい．またシステムM+DではシステムMに対して大幅に過接合を削減したにもかかわらず，未知語の削減率はシステムMと同等であり，本手法で未知語の削減率を保ったままシステムDと組み合わせることに成功した．未知語の削減率は最高で86.5%となっているが，未知語の14%程度は訓練コーパス中に出現しないものであり，このことを考えると，本稿で提案した手法で未知語を最大限取得することに成功したと言える．さらにシステムMとDを組み合わせることで訓練コーパスに出現しなかった未知語文字列が訓練コーパスの情報を利用することで正しく一塊と認識された例もあった．</subsection>
  <section title="結論">辞書ベースの自然言語処理ツールでは辞書未登録語の問題が防げない．そのため辞書未登録語の自動認識の研究が盛んに行われているが，辞書未登録語には未知語，複合語の二種類の問題があり，ほとんどの研究はそのどちらかに対象を絞ったものである．本稿では辞書ベースの形態素解析ツール・茶筌を対象とし，未知語，複合語双方の解決を目的として，統計情報を形態素解析段階で動的に利用するための組み込みシステム(システムM)と，統計情報を利用した辞書作成のシステム(システムD)の2種類のアプローチを提案した．本稿の手法は茶筌に特化したものではなく，辞書ベースのツールに対してそのシステムを改変することなく付加的な情報を半自動的に追加し辞書未登録語の問題の解決を図るものである．本稿で提案した手法は文字の共起頻度を元にしたものであり，構文解析などの処理は一切必要とせず，ヒューリスティクスも一切利用していない非常にシンプルなものでありながら，システムMのみで86.1%，システムDのみで71.4%，両方の併用で86.5%の未知語の解決に成功した．また抽出した辞書未登録語の適合率が最高で98.1%となり，複合語についても高い精度で認識することができた．本稿ではオープンコーパスを用いて実験を行ない，本稿で提案した2種類のアプローチを適切に組み合わせることで辞書未登録語の削減を効果的に行なうことに成功した．さらに精度を上げるためにはヒューリスティクスの利用が必要となる．document</section>
</root>
