    \documentclass[english]{jnlp_1.4}

\usepackage{jnlpbbl_1.3}
\usepackage[dvips]{graphicx}
\usepackage{hangcaption_jnlp}
\usepackage{udline}
\setulminsep{1.2ex}{0.2ex}
\newcommand{\url}[1]{}




\newcommand{\para}[2]{}
\newcommand{\citep}{}
\newcommand{\citet}{}
\usepackage{mygb4e}
\usepackage{xspace,relsize}
\newcommand{\eng}[1]{}
\newcommand{\erg}{}
\newcommand{\jacy}{}
\newcommand{\verbmobil}{}
    \newcommand{\logonX}{}
\newcommand{\hpsg}{}
\newcommand{\IWSLT}{}
\newcommand{\JE}{}
\newcommand{\EJ}{}






\Volume{17}
\Number{3}
\Month{April}
\Year{2010}

\received{2009}{4}{29}
\revised{2009}{11}{30}
\accepted{2009}{12}{16}

\setcounter{page}{101}

\etitle{Paraphrasing Training Data for Statistical Machine Translation}
\eauthor{Eric Nichols\affiref{1} \and Francis Bond\affiref{2} \and
	D. Scott Appling\affiref{3} \and Yuji Matsumoto\affiref{1}} 
\eabstract{
  Large amounts of data are essential for training statistical machine
  translation systems. In this paper we show how training data can be
  expanded by paraphrasing one side of a parallel corpus. The new data
  is made by parsing then generating using an open-source, precise
  HPSG-based grammar. This gives sentences with the same meaning, but
  with minor variations in lexical choice and word order.  In
  experiments paraphrasing the English in the Tanaka Corpus, a
  freely-available Japanese-English parallel corpus, we show
  consistent, statistically-significant gains on training data sets
  ranging from 10,000 to 147,000 sentence pairs in size as evaluated
  by the BLEU and METEOR automatic evaluation metrics.
}
\ekeywords{Natural Language Processing, Machine Translation, Paraphrasing, HPSG}

\headauthor{Nichols et al.}
\headtitle{Paraphrasing Training Data for SMT}

\affilabel{1}{}{Nara Institute of Science and Technology, Japan}
\affilabel{2}{}{Nanyang Technological University, Singapore}
\affilabel{3}{}{Georgia Institute of Technology, United States}


\begin{document}

\maketitle



\section{Introduction}

Data-driven machine translation systems such as EBMT and SMT learn how
to translate by analyzing aligned bilingual corpora. In general, the
more data available the higher the quality of the translation.
Unfortunately, there are limits to how much bilingual data exists. In
this paper, we propose a method for increasing the amount of parallel
text available for training by using a precise, wide-coverage grammar
to paraphrase the text in one language.

The novelty in this work is that we are using a hand-crafted grammar
to produce the paraphrases, thus adding a completely new source of
knowledge to this system. The paraphrases are both meaning-preserving
and grammatical, and thus are quite restricted. Possible changes
include: changes in word order (\para{Kim sometimes goes}{Kim goes
sometimes}), lexical substitution (\para{everyone}{everybody}),
contractions (\para{going to}{gonna}) and a limited number of
corrections (\textit{the the}\,$\rightarrow$\,\textit{the}). We give
an example of paraphrasing in (\ref{a}). The grammar treats all of
these sentences as semantically equivalent.

\begin{exe}
\ex \label{a}
この こと から 、 会社 に は 事故 の 責任 が 無い こと に なる 。
\begin{xlist}
  \ex It follows from this that the company is not responsible for the
  accident.\hfill (= original)
  \ex It follows that the company isn't responsible for the accident
from this.
\ex  It follows that the company is not responsible for the accident
from this.
\ex That the company isn't responsible for the accident follows from this.
\end{xlist}
\end{exe}

We next introduce some related work, then the resources we use in this
paper. This is followed by a description of the method and the
evaluation. Finally, we discuss the results and our future research
plans.


\section{Related Work}

Approaches to applying paraphrasing in MT can be roughly classified
into (1) paraphrasing to expand a machine translation system's
coverage, (2) paraphrasing to increase the amount of training or
development data, and (3) paraphrasing to increase the similarity
between the source and target languages.

In this section, we discuss representative works in each group and
compare them with our proposed approach. We limit discussion to {\it
applications} of paraphrasing to machine translation, excluding
general discussion of methods of acquiring paraphrases.


\subsection{Paraphrasing to expand translation coverage}

\citet{Callison-Burch:Koehn:Osborne:2006} use paraphrases to increase
the coverage of unknown source language words in an SMT system. They
automatically acquire source language paraphrases from a parallel
corpus by using target language phrases as pivots. An example of this
approach is given below. By using the German phrase \textit{unter
kontrolle} as a pivot, the English phrase \textit{under control} can
be paraphrased as \textit{in check}.

\begin{exe}
  \ex what is more, the relevant cost dynamic is completely \ul{under control}
  \trans im \"ubrigen ist die diesbez\"ugliche kostenentwicklung v\"ollig
  \ul{unter kontrolle}
\ex wir sind es den steuerzahlern die kosten schuldig  \ul{unter kontrolle}
zu haben
\trans we owe it to the taxpayers to keep the costs \ul{in check}
\end{exe}

New translations are constructed by identifying source language
unknown words, finding paraphrases of the unknown words in the
system's phrase table, and adding new translation pairs consisting of
the unknown word and the translation of its paraphrase. New entries to
the phrase table are given the original translation probabilities
multiplied by the probability of the source language paraphrases used
in their construction. \citet{Callison-Burch:Koehn:Osborne:2006}
showed improvements for sparse datasets for
Spanish$\rightarrow$English and French$\rightarrow$English systems
constructed on the Europarl Corpus.

\citet{Marton:Callison-Burch:Resnik:2009} also use paraphrases to
expand an SMT system's phrase table, but they use semantic similarity
distribution measures to acquire source language paraphrases from
monolingual corpora. They evaluate on Chinese$\rightarrow$English and
Spanish$\rightarrow$English translation tasks, also improving systems
trained on sparse datasets, but their approach degrades system
performance when trained on 80k or more of data.





\citet{Guzman:Garrido:2007} take a similar approach and learn new
translations for Spanish$\rightarrow$English from multi-lingual
corpora by using French as a pivot. New translations are added to an
existing SMT system's phrase table with probabilities estimated by
taking a weighted sum of the combination of paraphrases that produced
the new translation. They investigate different weights for the
composite paraphrases but do not present evaluations against a
baseline system.


The work by \citet{Callison-Burch:Koehn:Osborne:2006},
\citet{Marton:Callison-Burch:Resnik:2009}, and
\citet{Guzman:Garrido:2007} focus on integrating paraphrases acquired
from multi- and mono- lingual corpora into an existing SMT system's
phrase table with the primary goal of reducing the number of unknown
words the system encounters. In order to integrate external
paraphrases into an existing phrase table, a measure of translation
probability is necessary, and so they develop a series of heuristics
to score the artificial alignments produced by pairing paraphrases of
a source phrase found in a parallel corpus with the original phrase's
alignment in the phrase table.

In contrast, our system produces paraphrases of the sentences in a
parallel corpus, and does not alter the phrase table creation
process. Rather than learning paraphrases directly from corpora, as in
the case of the aforementioned works, our paraphrases are produced
from external lexico-syntactic knowledge in the form of an HPSG
grammar. English sentences are parsed into a semantic representation
that normalizes word order, spelling, and small number of lexical
items. Paraphrases are produced from the semantic representation using
the same grammar and parser, with paraphrases ranked by a maximum
entropy generation model trained on an HPSG treebank. Unlike methods
that learn paraphrases directly from corpora, our HPSG paraphrases are
limited to grammatical English, eliminating the problem of noisy data.


\subsection{Paraphrasing to increase translation data}


\citet{Nakov:2008} used paraphrases to increase training and parameter
tuning data for SMT system. He produced sentence-level paraphrases by
using a small set of rules to identify and transform noun phrases in
the parallel corpus (e.g., \para{$NP_1$ of $NP_2$}{$NP_2$ 's $NP_1$}).

Some examples from \citet{Nakov:2008} are:
\begin{exe}
  \ex of members of the Irish parliament
  \trans of Irish parliament members
  \trans of Irish parliament's members
\ex action at community level
\trans community level action
\end{exe}

These transformations were structural, not lexical, in nature and
limited in scope. \citet{Nakov:2008} found that noun phrase-based
paraphrases were most effective when applied to training data,
achieving a BLEU score gain of about 1 point for limited corpus sizes.


Paraphrases have also been used to enrich the data used for parameter
tuning in SMT systems.
\citet{Madnani:Ayan:Reznik:Dorr:2007} obtained English language
paraphrases by identifying paraphrases using a pivot language as in
\citet{Callison-Burch:Koehn:Osborne:2006} and produced sentence-level
English paraphrases by training an English$\rightarrow$English
hierarchical SMT system \citep{Chiang:Hiero:2005}. Experiments showed
that paraphrasing the tuning data used for MERT in a
Chinese$\rightarrow$English hierarchical SMT system performed
competitively with increasing human references. Paraphrasing data for
parameter tuning is a promising approach, however, evaluating our
paraphrasing method in tuning remains future work.



\subsection{Paraphrasing to increase linguistic similarity}

Another use of paraphrasing is to increase the similarity between
source and target languages in order to facilitate translation. The
approaches discussed here can be classified into methods that try to
simplify the source language vocabulary and those that reorder the
source language into a form closer to the word order of the target
language.



\subsubsection{Simplifying source language vocabulary}


\indent One of the earliest applications of paraphrasing to simplify
translation input is shown by the rule-based \JE~MT system, ALT
J/E. \citet{Shirai:Ikehara:Kawaoka:1993} simplified untranslatable
Japanese input into a ``pseudo-source language'' that, while
ungrammatical, was easier for ALT J/E to parse and translate.
\citet{Yamamoto:2001} adopted a similar approach with his ``Sandglass
Paradigm'' \--- normalizing input to a rule-based MT system before
expanding it again during the translation phase.
\citet{Watanabe:Shimohata:Sumita:2002} also used paraphrases to
normalize source language text system by detecting paraphrases in a
parallel corpus and replacing them with the most commonly occurring
variant. Paraphrases were automatically detected with a dynamic
programming algorithm, and the normalized data was used to train an
SMT system.

Our approach is similar in spirit to these normalization efforts,
however, instead of using simple heuristics or identifying paraphrases
in a corpus, we apply an external source of knowledge: a rich, lexical
grammar. In addition, instead of directly transforming system input,
our approach uses paraphrases to enrich the training data, making it
more robust by providing instances of lexical and syntactic variants.
 


\subsubsection{Reordering source language text}



Overcoming differences in word order is a challenge for translating
highly divergent language pairs like Japanese-English or
German-English. Recently there has been much work on improving SMT by
reordering the source language to closer resemble the word order of
the target language.


\citet{Niessen:Ney:2001} identify differences in question order and
long-distance verbal prefix scrambling as phenomena that cause
difficulties for German$\leftrightarrow$English statistical machine
translation and used shallow patterns to reorder ``harmonize word
order'' between the German and English.
\citet{Collins:Koehn:Kucerova:2005} made use of parses of source
sentences to develop a reordering heuristic as well.
\citet{Komachi:Matsumoto:Nagata:2006} proposed a reordering model that
took into account predicate-argument structure in Japanese and
followed a heuristic for reordering sentences in the training data as
a preprocessing step. The reordering produces sentences that are not
grammatical Japanese, however, they are closer in word order to
English, facilitating the SMT alignment process.
\citet{Katz-Brown:Collins:2008} found that for \JE~phrasal SMT a
na\"{i}ve reversal of Japanese source language word order outperformed a
dependency-based reordering model. \citet{Xu:Seneff:2008} use a
rule-based parser to parse English and then generate {\it
Zhonglish}\footnote{A term coined by the paper's authors.}: English
reordered to resemble Chinese, with some Chinese function words
added. The result is then translated using an SMT system.

Our approach also produces variants in word order, however, they are
not artificial reorderings to reduce word order differences. Rather,
these variants are all valid English as defined by the English HPSG
grammar. We make the SMT system's training data more robust and
representative of English by providing paraphrases that encapsulate
the possible positions of adjuncts, such as adverbial and preposition
phrases; relative clauses; and other linguistic phenomena in English 
with variable word order.



\section{Resources}

In this section we describe the major resources used. For the SMT
system we used the open-source Moses system. 
For paraphrasing we used the open-source English Resource Grammar. We
evaluated on the Tanaka Corpus. We chose the Tanaka corpus primarily
because of its unencumbered availability (it is in the public domain),
making our results easy to reproduce. In the spirit of open science,
we have made the paraphrased Tanaka Corpus data as well as the scripts
and Moses settings files necessary to reproduce our experiments
available online at \url{http://www3.ntu.edu.sg/home/fcbond/data/}.  A
summary of all tools used is given in Table~\ref{tab:tools}.

\begin{table}[p]
\input{09table01-02.txt}
\end{table}



\subsection{Moses}


Moses \citep{Moses:2007} is in the words of its creators ``a factored
phrase-based beam-search decoder for machine translation.'' It is
distributed as open-source software with a collection of utilities
that make it easy for users to construct their own SMT system when
used with tools for constructing word alignments and language models.
    For word alignments we used the giza-pp branch of GIZA++
\citep{Och:Ney:2003}.
To construct language models, we used the SRILM Toolkit
\citep{Stolcke:2002}.



\subsection{The english resource grammar}

The LinGO English Resource Grammar (\erg; \citep{Flickinger:2000}) is
a broad-coverage, linguistically precise \hpsg-based grammar of
English that has been under development at the Center for the Study of
Language and Information (CSLI) at Stanford University since 1993.
The \erg was originally developed within the \verbmobil\ machine
translation effort, but over the past few years has been ported to
additional domains and significantly extended. The grammar includes a
hand-built lexicon of around 43,000 lexemes. We are using the
    development release LinGO (Apr-08). Parsing was done with the
efficient, unification-based chart parser, PET \citep{Callmeier:2002},
and generation with the Linguistic Knowledge Base
\citep{Copestake:2002}.  The \erg and associated parsers and
generators are freely available from the Deep Linguistic Processing
with HPSG Initiative\footnote{DELPH-IN:
\url{http://www.delph-in.net/}}.

For the most part, we use the default settings and the language models
    trained in the \logonX project both for parsing and generation
\citep{velldal-oepen:2006:EMNLP}. However, we set the root condition,
which controls which sentences are treated as grammatical, to be
\textbf{robust} for parsing and \textbf{strict} for generation. This
means that robust rules (e.g. a rule that allows verbs to not agree in
number with their subject) will apply in parsing but not in
generation. The grammar will thus parse \eng{The dog bark} or \eng{The
dog barks} but only generate \eng{The dog barks}.




\subsection{The Tanaka Corpus}

The Tanaka corpus is an open corpus of Japanese-English sentence pairs
compiled by Professor Yasuhito Tanaka at Hyogo University and his
students \citep{Tanaka:2001} and released into the public domain.
Professor Tanaka's students were given the task of collecting 300
sentence pairs each. After several years, 212,000 sentence pairs had
been collected. The sentences were created by the students, often
derived from textbooks, e.g. books used by Japanese students of
English. Some are lines of songs, others are from popular books and
Biblical passages. The original collection contained large numbers of
errors, both in the Japanese and English. These are being corrected
and added to by volunteers as part of ongoing activity to provide
example sentences for the Japanese-Multilingual dictionary JMDict
\citep{Breen:2003}. Recently, translations into other languages, most
notably French, have been added by the TATOEBA
project.\footnote{\url{http://tatoeba.fr}} We give a typical example
sentence in (\ref{7836}).

\begin{figure}[t]
\begin{center}
\includegraphics{17-3ia6f2.eps}
\end{center}
\caption{Paraphrase process for the sentence ``\textit{Everybody often goes to the the movies.}'}
\label{fig:para}
\end{figure}

\begin{exe} \small
\ex \label{7836}
あの 木 の 枝 に 数 羽 の 鳥 が とまっ て いる 。
\trans ``Some birds are sitting on the branch of that tree.'' (en)
\trans ``Des oiseaux se reposent sur la branche de cet arbre.'' (fr)
\end{exe}

The version (2007-04-05) we use has 147,190 sentence pairs in the
training split, along with 4,500 sentence pairs reserved for
development and 4,500 sentence pairs for testing. After filtering out
long sentences ($> 40$ tokens) as part of the SMT cleaning process,
there were 147,007 sentences in the training set. The average number
of tokens per sentence is 11.6 for Japanese and 9.1 for English (with
the tokenization used in the SMT system).



\section{Method}

\subsection{Paraphrasing}

\begin{figure}[t]
\begin{center}
\includegraphics{17-3ia6f1.eps}
\end{center}
\caption{Semantic Representation of ``\textit{Everybody often goes to
    the the movies.}''}
\label{fig:mrs}
\end{figure}

We paraphrase by parsing a sentence to an abstract semantic
representation using the English Resource Grammar then generating from
the resultant semantic representation using the same grammar. The
semantic representation used is Minimal Recursion Semantics (MRS:
\citep{Copestake:Flickinger:Sag:Pollard:MRS}). We give an example of
the paraphrasing process in Figure~\ref{fig:para} that shows three
kinds of paraphrasing. The input sentence is ``\textit{Everybody often
goes to the the movies.}''. It is paraphrased to the MRS shown in
Figure~\ref{fig:mrs}. From that, six sentences are generated. The
paraphrased sentences show three changes. Firstly, the erroneous
\textit{the the} is corrected to \textit{the}; secondly,
\textit{everybody} is optionally paraphrased as \textit{everyone} and
finally the adverb \textit{often} appears in three positions
(pre-verb, post-verb, post-verb-phrase). We consider the first two to
be lexical paraphrases (changes in words) and the latter syntactic
paraphrases. Of course, for most sentences there is a combination of
lexical and syntactic paraphrases.

``Score'' in Figure~\ref{fig:para} gives a maximum entropy based
likelihood estimate to each of the paraphrases. Note that the highest
ranked paraphrase is not in this case the original sentence. The
paraphrase is quite conservative: sentence initial \textit{often} is
not generated, as that is given a different semantics (it is treated
as focused). There are no open class paraphrases like
\para{film}{movie}. Only a handful of closed class words are
substituted, typically those that get decomposed semantically, (e.g.,
\para{everybody}{every(x),person(x)}).

\begin{figure}[t]
\begin{center}
\includegraphics{17-3ia6f3.eps}
\end{center}
  \caption{Types of paraphrases (Lexical and Syntactic).}
  \label{fig:dist}
\end{figure}


We attempted to parse all sentences of the Tanaka Corpus with the ERG
and the PET parser.  We got one or more well-formed semantic
representation for 87.1\% of the sentences (the remainder were
rejected as ungrammatical).  We selected the top ranked representation
and attempted to generate from it, this time using the ERG and the LKB
generator.  We were able to generate one or more realizations for
83.4\% of the original sentences.  However, many of these gave only
one realization and it was identical to the input sentence. Only
53.4\% of the sentences had at least one distinct paraphrase; 31.2\%
had two, 21.2\% had three, dropping down to only 1.1\% with ten
distinct paraphrases.

We show the distribution of paraphrase types over all of the generated
paraphrases in Figure~\ref{fig:dist}.  Lexical paraphrases are
identified by comparing the set of lexical items in the input with
those in the output. If they are different, then there is a lexical
paraphrase (Lex $\ne$). Syntactic paraphrases are identified by
comparing the parse trees. Almost a quarter of the sentences generated
are the same as the input (Lex =, Syn =). Most variations include some
syntactic paraphrasing (Syn $\ne$: 42\%), purely lexical paraphrasing
is relatively uncommon (8\%).



\subsection{Corpus expansion}
\label{sec:corpus-expansion}

Typically when learning translation models, it is assumed that each
sentence pair in the parallel corpus is given the same weight. This
raises the question of how additional paraphrased data should be
weighed. A straight-forward approach would be to simply add each new
paraphrase directly to the corpus. However, as sentences can have
different numbers of paraphrases, we risk assigning a different weight
to each set of original sentence pair and derived paraphrase. A more
sophisticated approach would be to assure that each set maintains the
same overall weight by distributing it equally among each paraphrase,
or by using the paraphrase generation score from Figure~\ref{fig:para}
to give more likely paraphrases a higher weight. Here, we explore
several methods of assigning weights to the paraphrased data by
varying the number of times we add each new paraphrase to the parallel
corpus.

To make the enhanced training data, we add up to $n$ distinct
paraphrases to each unchanged Japanese sentence and original English
sentence. We convert all paraphrases to lowercase before checking for
uniqueness. If there were $m$ paraphrases, and $n \le m$ then we just
add in the top $n$ ranked paraphrases. If $n > m$ then we produced
three test sets:

\begin{itemize}
\item (d)istributed: rotate between the original sentence and each
  paraphrase until the data has been padded out
\item (f)irst: after all paraphrases are used, the first
  (original) sentence is repeated to pad out the data
\item (v)arying: add just the paraphrases without padding all entries to the same number of sentences
\end{itemize} 


\begin{table}[t]
  \caption{Paraphrase distributions ($n = 4, m = 2$).}
    \label{tab:dist}
\input{09table03.txt}
\end{table}

These variations are shown in Table~\ref{tab:dist}. Both ($d$ and $f$)
keep the distribution close to the original corpus. $d$ puts more
weight on the paraphrased sentences and $f$ puts more weight on the
original sentence. For $v$ the frequency is distorted; some sentences
will be repeated many times. For $n \le 2$, $d$ and $f$ are the same.



\section{Evaluation}
\label{sec:eval}

In this section, we investigate the effects of supplementing training
data with paraphrases on the Tanaka Corpus. We construct phrase-based
SMT systems using Moses for the 
\linebreak
\EJ~and \JE~language pairs, and evaluate
systems on various training corpus sizes.


\subsection{Moses baseline}

We replicated the baseline in the ACL 2007 Second Workshop on
Statistical Machine Translation. The baseline is a factorless Moses
system with a 5-gram language model. We followed the online
    tutorial\footnote{http://www.statmt.org/wmt07/baseline.html} as-is,
with the exception that we used external morphological analyzers to
tokenize our data instead of using the provided scripts. We used the
Tree Tagger \citep{schmid94probabilistic} for English and MeCab
\citep{mecab} with NAIST Jdic for Japanese. Part-of-speech information
was discarded after tokenization.

All data was tokenized, separating punctuation from words and
converted to lowercase prior to training and translation. Translations
were detokenized and recased prior to evaluation using the helper
scripts distributed as part of the baseline system for the ACL 2007
SMT Workshop.

Prior to evaluation we conducted Minimum Error Rate Training on each
system using the development data from the target corpus. We used the
MERT implementation distributed with Moses. All results reported in
this article are post-MERT BLEU scores.



\subsection{Data preparation}

In order to measure the effectiveness of our method, we evaluated the
\JE~and \EJ~language pairs over the Tanaka Corpus. Because our HPSG
parsers perform best on data that is split on the sentence level,
wherever possible we split the corpora into the finest possible
sentence pairs. We used the following algorithm to split the
evaluation data. However, most of the data in the Tanaka Corpus has
already been split at the sentence level as part of the JMDict
initiative.

\begin{itemize}
	\item For each sentence pair:
	\begin{itemize}
		\item split each sentence on sentence-final punctuation (.?!)
		\item rejoin split on common English titles (Mr./Ms./Mrs./Dr.)
		\item split sentence pairs with same number of source and target sentences
		      into new pairs
		\item treat sentence pairs with different number of source and target sentences
		      as a single pair
	\end{itemize}
\end{itemize}


\subsection{Results}

We evaluated the effects of adding paraphrases to various initial
training data sizes using BLEU and METEOR scores. We compared a
baseline of no-paraphrases-added ($d.0/f.0$) to systems with
progressively larger numbers of new paraphrased sentence pairs added
to each training data size. We tested three distributions ($d$, $f$
and $v$). $v$ always gave results below the baseline, so we do not
report them in more detail. We give several analyses for $d$ and $f$
below.


\subsubsection{Learning curves}

We give learning curves in Figures~\ref{fig:curve-EJ} and
\ref{fig:curve-JE}.
The average BLEU scores for {\it distributed} and {\it first}
paraphrase systems are plotted against training corpus sizes (10k,
25k, 50k, 100k, 125k, and a maximum size of 147k). The training data
axis is scaled logarithmically. Best fit lines for the baseline
($d.0/f.0$) and each of the paraphrases show that there is a
log-linear relationship in training data size and BLEU
score. Paraphrasing almost always outperforms the baseline for small
data sets (EJ: 10k-25k, JE: 10k-75k) and large data sets (EJ:
100k-147k, JE: 125k-147k).  The region in the middle (EJ: 50k-75k, JE:
100k) is anomalous; the paraphrased averages are below the
baseline. We suspect this may be caused by these data sizes containing
non-representative samples of data or paraphrases.


\begin{figure}[p]
\begin{center}
\includegraphics{17-3ia6f4.eps}
\end{center}
  \caption{Learning curve for \EJ\ paraphrase distribution averages.}
  \label{fig:curve-EJ}
\end{figure}
\begin{figure}[p]
\begin{center}
\includegraphics{17-3ia6f5.eps}
\end{center}
  \caption{Learning curve for \JE\ paraphrase distribution averages.}
  \label{fig:curve-JE}
\end{figure}

\subsubsection{BLEU score}

    BLEU scores were calculated using the multi-bleu.perl
implementation distributed with Moses.  We measured the statistical
significance of BLEU score differences with the bootstrap methods
outlined in \citet{koehn04statistical} using Jun-ya Norimatsu's
MIT-Licensed BLEU Kit.
BLEU scores for $d$ and $f$ are given in Table~\ref{tbl:bleu}, results
with an improvement of $p <= 0.05$ over the baseline are shown in {\bf
  bold}, and the best score for each data size is \ul{underlined}. We
observe a maximum gain of {\bf 0.67} BLEU points for \EJ~at (10k,
$d.4$) and a maximum gain of {\bf 0.63} for \JE\ at (50k,
$d.2/f.2$). Gains appear to peak at 8 paraphrases; $d.10$ and $f.10$
rarely achieve higher scores that can be achieved with fewer
paraphrases. The large number of statistically significant BLEU score
improvements reinforce our observations made from the learning curves
that paraphrasing is beneficial for small data sets and large data
sets. We also notice a trend that small numbers of heavily weighted
paraphrases like $d.4$ are more effective for small data sets, while
larger numbers of lightly-weighted paraphrases like $f.8$ are more
effective for large data sets.


\begin{table}[p]
\input{09table04.txt}
\end{table}


\subsubsection{Meteor score}

METEOR \citep{banerjee-lavie:2005:MTSumm} is an advanced MT evaluation
metric that uses stemming and WordNet synonym matching to relax
constraints for English n-gram matches to achieve higher levels of
correlation to human judgement than possible with simpler metrics like
BLEU and NIST. We calculated all METEOR scores using version 1.0 with
the following options: stemming, WordNet stemming, WordNet synonym
matching, and ``normalization''~\--- stripping of punctuation and
conversion to lower case. METEOR score for \JE~for $d$ and $f$ are
given in Table~\ref{tbl:meteor-JE}. The METEOR scores do not show
as consistent gains as BLEU scores do, but the 10k data set shows
great improvements for every paraphrase size. We also note a correlation
between statistical significant BLEU score gains and METEOR score
improvements; 14/20 paraphrase systems with statistically significant 
BLEU score gains have increases in METEOR scores.




\section{Discussion}

Overall, we show significant, consistent improvements on the Tanaka
Corpus. Paraphrased SMT systems show statistically significant
improvements over the baseline for the majority of the data sizes
tested. Furthermore, we observe a log-linear relationship between the
size of the system's training data and the BLEU score, with best-fit
lines demonstrating the superiority of the paraphrased system over the
baseline.

Table~\ref{tab:ex} shows some examples of how translation output
changes with the addition of various amounts of paraphrasing data for
the \JE~language pair. The example translations contain
difficult-to-learn phrasal translations, such as {\it raining on and
off} and {\it to the point}. As is to be expected from the BLEU
scores, the system $f.8$ often gives the best translation. We theorize
that the additional data provided by our paraphrases results in better
phrasal alignments, which, in turn, improves lexical selection and
allows the language model to produce more natural-sounding
translations.


Compared to \citet{Callison-Burch:Koehn:Osborne:2006},
\citet{Madnani:Ayan:Reznik:Dorr:2007}, or \citet{Nakov:2008} we are
very conservative in our paraphrasing, and this is probably why we get
a slightly lower improvement in quality.  We could do more extravagant
paraphrasing, but would have to retrain our HPSG parser's generation
model to effectively rank the new lexical paraphrases. At the moment,
it expects fully specified input MRSes. If we were going to allow
variation in noun phrase structure or open class lexical variation,
then the task could be re-framed as translating between English
sentence, and we could build an English$\rightarrow$English semantic
transfer system to produce richer paraphrases. An example of how to do
this (for bilingual transfer of Norwegian$\rightarrow$English) is
given in \citet{Oepen:Velldal:Loening:Meurer:Rosen:2007}.

Our syntactic reordering is not aimed at matching the target language
like \citet{Komachi:Matsumoto:Nagata:2006}, \citet{Xu:Seneff:2008}, or
\citet{Katz-Brown:Collins:2008}. We correspondingly get a smaller
improvement. On the other hand, because our English paraphrasing
method does not depended on a parallel corpus, we expect to get a
similar improvement even for different language pairs.  Also, our
improvement is still there after MERT, whereas the improvement of
\citet{Komachi:Matsumoto:Nagata:2006} did not make it through the
optimization.

\begin{table}[t]
\hangcaption{Example \JE~translations from SMT systems trained on 147k of data. The system with the highest BLEU score is {\protect\ul{underlined}}.}
\label{tab:ex}
\input{09table05.txt}
\end{table}

We have seen similar increases in SMT system performance for Japanese
and English data on the Basic Travel Expression Corpus that is used in
the International Workshop for Spoken Machine Translation's
translation task. We reported these results in
\citet{iwslt08:TP:bond}. Unfortunately, data usage restrictions
prevent us from reproducing the results here.




\section{Further Work}

There are three areas in which we think the current use of
paraphrasing could be improved: (1) increasing the coverage of the
grammar (2) adding new classes of paraphrase rules and (3) improving
the integration with the SMT process.

To increase the cover of the paraphrasing, we need to improve the
handling of unknown words. Currently, the grammar can parse unknown
words (which brings the coverage up to almost 95\%), but does not pass
enough information to the generator to then generate them. We can
overcome this with more powerful hybrid parsing, following
\citet{Adolphs:Oepen:Callmeier:Crysmann:Kiefer:2008}. A more
far-ranging increase would be to paraphrase the Japanese side as
well. We are working on this using Jacy, an HPSG-based Japanese
grammar similar to the \erg \citep{Bond:Kuribayashi:Hashimoto:2008}
and applying the grammatical error tools of
\citet{Goodman:Bond:2009acl} to improve the generation coverage of the
Japanese grammar.

Before we increase the types of paraphrases we first need to measure
which rules (e.g. lexical or syntactic) have the most effect. We then
intend to create English rewriting rules using the MRS transfer
    machinery from the \logonX project, which is already used in an open
source \JE~MT system \citep{Bond:Oepen:Siegel:Copestake:Flickinger:2005}.
For example, we can easily write noun phrase rewriting rules of the
type used by \citet{Nakov:2008}. For lexical substitution we will try
using WordNet, after first disambiguating the input.

Finally, we would like to enhance Moses (primarily GIZA++) so that
input sentences can be weighted. That way, if we have $n$ paraphrases
for one sentence and $m$ for another, each can just be entered with a
weight of $1/n$ and $1/m$ respectively. If we could do this, we could
then experiment with setting a probability based threshold on the
number of paraphrases, for example, to select all paraphrases within
$\beta$ of the probability of the original sentence, according to some
language model. In this way we could add only ``good'' paraphrases,
and as many as we deem good for each sentence.



\section{Conclusion}

Large amounts of training data are essential for training statistical
machine translation systems. In this paper we showed how training data
can be expanded by paraphrasing one side of a parallel corpus. The new
data is made by parsing then generating using a precise HPSG-based
grammar. This gives sentences with the same meaning, but with minor
variations in lexical choice and word order.  In experiments
paraphrasing the English in the Tanaka Corpus, we showed consistent,
statistically-significant gains on training data sets ranging from
10,000 to 147,000 sentence pairs in size as evaluated by the BLEU and
METEOR automatic evaluation metrics.




\acknowledgment

This work was done while the second author was a member of, and the
third author an intern at,
NICT, Japan.  We would like to thank the members of the Language
Infrastructure and Machine Translation Groups for their helpful
comments, especially Kiyotaka Uchimoto, Michael Paul and Kentaro
Torisawa.
 

\bibliographystyle{jnlpbbl_1.5}
\begin{thebibliography}{}

\bibitem[\protect\BCAY{Adolphs, Oepen, Callmeier, Crysmann, Flickinger, \BBA\
  Kiefer}{Adolphs et~al.}{2008}]{Adolphs:Oepen:Callmeier:Crysmann:Kiefer:2008}
Adolphs, P., Oepen, S., Callmeier, U., Crysmann, B., Flickinger, D., \BBA\
  Kiefer, B. \BBOP 2008\BBCP.
\newblock \BBOQ Some Fine Points of Hybrid Natural Language Parsing.\BBCQ\
\newblock In (ELRA), E. L. R.~A.\BED, {\Bem Proceedings of the Sixth
  International Language Resources and Evaluation (LREC'08)}, Marrakech,
  Morocco.

\bibitem[\protect\BCAY{Banerjee \BBA\ Lavie}{Banerjee \BBA\
  Lavie}{2005}]{banerjee-lavie:2005:MTSumm}
Banerjee, S.\BBACOMMA\ \BBA\ Lavie, A. \BBOP 2005\BBCP.
\newblock \BBOQ {METEOR}: An Automatic Metric for {MT} Evaluation with Improved
  Correlation with Human Judgments.\BBCQ\
\newblock In {\Bem Proceedings of the ACL Workshop on Intrinsic and Extrinsic
  Evaluation Measures for Machine Translation and/or Summarization},
  \mbox{\BPGS\ 65--72}, Ann Arbor, Michigan. Association for Computational
  Linguistics.

\bibitem[\protect\BCAY{Bond, Kuribayashi, \BBA\ Hashimoto}{Bond
  et~al.}{2008a}]{Bond:Kuribayashi:Hashimoto:2008}
Bond, F., Kuribayashi, T., \BBA\ Hashimoto, C. \BBOP 2008a\BBCP.
\newblock \BBOQ Construction of a Free {Japanese} Treebank Based on
  {HPSG}.\BBCQ\
\newblock In {\Bem 14th Annual Meeting of the Association for Natural Language
  Processing}, \mbox{\BPGS\ 241--244}, Tokyo.
\newblock (in Japanese).

\bibitem[\protect\BCAY{Bond, Nichols, Appling, \BBA\ Paul}{Bond
  et~al.}{2008b}]{iwslt08:TP:bond}
Bond, F., Nichols, E., Appling, D.~S., \BBA\ Paul, M. \BBOP 2008b\BBCP.
\newblock \BBOQ {Improving Statistical Machine Translation by Paraphrasing the
  Training Data}.\BBCQ\
\newblock In {\Bem Proceedings of IWSLT 2008}, \mbox{\BPGS\ 150--157}, Hawaii.

\bibitem[\protect\BCAY{Bond, Oepen, Siegel, Copestake, \BBA\ Flickinger}{Bond
  et~al.}{2005}]{Bond:Oepen:Siegel:Copestake:Flickinger:2005}
Bond, F., Oepen, S., Siegel, M., Copestake, A., \BBA\ Flickinger, D. \BBOP
  2005\BBCP.
\newblock \BBOQ Open Source Machine Translation with {DELPH-IN}.\BBCQ\
\newblock In {\Bem Open-Source {MT}: Workshop at {MT} Summit {X}}, \mbox{\BPGS\
  15--22}, Phuket.

\bibitem[\protect\BCAY{Breen}{Breen}{2003}]{Breen:2003}
Breen, J.~W. \BBOP 2003\BBCP.
\newblock \BBOQ Word Usage Examples in an Electronic Dictionary.\BBCQ\
\newblock In {\Bem Papillon (Multi-lingual Dictionary) Project Workshop},
  Sapporo.

\bibitem[\protect\BCAY{Callison-Burch, Koehn, \BBA\ Osborne}{Callison-Burch
  et~al.}{2006}]{Callison-Burch:Koehn:Osborne:2006}
Callison-Burch, C., Koehn, P., \BBA\ Osborne, M. \BBOP 2006\BBCP.
\newblock \BBOQ Improved Statistical Machine Translation Using
  Paraphrases.\BBCQ\
\newblock In {\Bem Human Language Technology Conference of the North American
  Chapter of the Association of Computational Linguistics}, \mbox{\BPGS\
  17--24}.

\bibitem[\protect\BCAY{Callmeier}{Callmeier}{2002}]{Callmeier:2002}
Callmeier, U. \BBOP 2002\BBCP.
\newblock \BBOQ Preprocessing and Encoding Techniques in {PET}.\BBCQ\
\newblock In Oepen, S., Flickinger, D., Tsujii, J., \BBA\ Uszkoreit, H.\BEDS,
  {\Bem Collaborative Language Engineering}, \BCH~6, \mbox{\BPGS\ 127--143}.
  CSLI Publications, Stanford.

\bibitem[\protect\BCAY{Chiang}{Chiang}{2005}]{Chiang:Hiero:2005}
Chiang, D. \BBOP 2005\BBCP.
\newblock \BBOQ A Hierarchical Phrase-Based Model for Statistical Machine
  Translation.\BBCQ\
\newblock In {\Bem ACL 2005 Proceedings}, \mbox{\BPGS\ 263--270}, Ann Arbor,
  Michigan. Association for Computational Linguistics.

\bibitem[\protect\BCAY{Collins, Koehn, \BBA\ K\v{u}cerov\'{a}}{Collins
  et~al.}{2005}]{Collins:Koehn:Kucerova:2005}
Collins, M., Koehn, P., \BBA\ K\v{u}cerov\'{a}, I. \BBOP 2005\BBCP.
\newblock \BBOQ Clause Restructuring for Statistical Machine Translation.\BBCQ\
\newblock In {\Bem Proceedings of the 43rd Annual Meeting of the ACL},
  \mbox{\BPGS\ 531--540}.

\bibitem[\protect\BCAY{Copestake}{Copestake}{2002}]{Copestake:2002}
Copestake, A. \BBOP 2002\BBCP.
\newblock {\Bem Implementing Typed Feature Structure Grammars}.
\newblock CSLI Publications.

\bibitem[\protect\BCAY{Copestake, Flickinger, Pollard, \BBA\ Sag}{Copestake
  et~al.}{2005}]{Copestake:Flickinger:Sag:Pollard:MRS}
Copestake, A., Flickinger, D., Pollard, C., \BBA\ Sag, I.~A. \BBOP 2005\BBCP.
\newblock \BBOQ {M}inimal {R}ecursion {S}emantics. {A}n Introduction.\BBCQ\
\newblock {\Bem Research on Language and Computation}, {\Bbf 3}  (4),
  \mbox{\BPGS\ 281--332}.

\bibitem[\protect\BCAY{Flickinger}{Flickinger}{2000}]{Flickinger:2000}
Flickinger, D. \BBOP 2000\BBCP.
\newblock \BBOQ On Building a More Efficient Grammar by Exploiting Types.\BBCQ\
\newblock {\Bem Natural Language Engineering}, {\Bbf 6}  (1), \mbox{\BPGS\
  15--28}.
\newblock (Special Issue on Efficient Processing with HPSG).

\bibitem[\protect\BCAY{Goodman \BBA\ Bond}{Goodman \BBA\
  Bond}{2009}]{Goodman:Bond:2009acl}
Goodman, M.~W.\BBACOMMA\ \BBA\ Bond, F. \BBOP 2009\BBCP.
\newblock \BBOQ Using Generation for Grammar Analysis and Error
  Detection.\BBCQ\
\newblock In {\Bem Joint conference of the 47th Annual Meeting of the
  Association for Computational Linguistics and the 4th International Joint
  Conference on Natural Language Processing of the Asian Federation of Natural
  Language Processing}, \mbox{\BPGS\ 109--112}, Singapore.

\bibitem[\protect\BCAY{Guzm\'{a}n~Herrera \BBA\
  Garrido~Luna}{Guzm\'{a}n~Herrera \BBA\
  Garrido~Luna}{2007}]{Guzman:Garrido:2007}
Guzm\'{a}n~Herrera, F.\BBACOMMA\ \BBA\ Garrido~Luna, L. \BBOP 2007\BBCP.
\newblock \BBOQ Using Translation Paraphrases from Trilingual Corpora to
  Improve Phrase-Based Statistical Machine Translation: A Preliminary
  Report.\BBCQ\
\newblock In {\Bem Proceedings of the Mexican International Conference on
  Artificial Intelligence}, \mbox{\BPGS\ 163--172}, Los Alamitos, CA, USA. IEEE
  Computer Society.

\bibitem[\protect\BCAY{Katz-Brown \BBA\ Collins}{Katz-Brown \BBA\
  Collins}{2008}]{Katz-Brown:Collins:2008}
Katz-Brown, J.\BBACOMMA\ \BBA\ Collins, M. \BBOP 2008\BBCP.
\newblock \BBOQ Syntactic Reordering in Preprocessing for
  Japanese$\rightarrow$English Translation: MIT System Description for NTCIR-7
  Patent Translation Task.\BBCQ\
\newblock In {\Bem Proceedings of the 7th NTCIR Workshop Meeting}, Tokyo,
  Japan.

\bibitem[\protect\BCAY{Koehn}{Koehn}{2004}]{koehn04statistical}
Koehn, P. \BBOP 2004\BBCP.
\newblock \BBOQ Statistical Significance Tests for Machine Translation
  Evaluation.\BBCQ\
\newblock In {\Bem Proceedings of EMNLP 2004}, Barcelona, Spain.

\bibitem[\protect\BCAY{Koehn, Shen, Federico, Bertoldi, Callison-Burch, Cowan,
  Dyer, Hoang, Bojar, Zens, Constantin, Herbst, Moran, \BBA\ Birch}{Koehn
  et~al.}{2007}]{Moses:2007}
Koehn, P., Shen, W., Federico, M., Bertoldi, N., Callison-Burch, C., Cowan, B.,
  Dyer, C., Hoang, H., Bojar, O., Zens, R., Constantin, A., Herbst, E., Moran,
  C., \BBA\ Birch, A. \BBOP 2007\BBCP.
\newblock \BBOQ Moses: Open Source Toolkit for Statistical Machine
  Translation.\BBCQ\
\newblock In {\Bem Proceedings of the ACL 2007 Demo and Poster Sessions},
  \mbox{\BPGS\ 177--180}, Prague.

\bibitem[\protect\BCAY{Komachi, Matsumoto, \BBA\ Nagata}{Komachi
  et~al.}{2006}]{Komachi:Matsumoto:Nagata:2006}
Komachi, M., Matsumoto, Y., \BBA\ Nagata, M. \BBOP 2006\BBCP.
\newblock \BBOQ Phrase Reordering for Statistical Machine Translation Based on
  Predicate-Argument Structure.\BBCQ\
\newblock In {\Bem Proceedings of IWSLT 2006}, Kyoto, Japan.

\bibitem[\protect\BCAY{Kudo, Yamamoto, \BBA\ Matsumoto}{Kudo
  et~al.}{2004}]{mecab}
Kudo, T., Yamamoto, K., \BBA\ Matsumoto, Y. \BBOP 2004\BBCP.
\newblock \BBOQ Applying {C}onditional {R}andom {F}ields to {J}apanese
  {M}orphological {A}nalysis.\BBCQ\
\newblock In {\Bem EMNLP 2004 Proceedings}, \mbox{\BPGS\ 230--237}, Barcelona,
  Spain. Association for Computational Linguistics.

\bibitem[\protect\BCAY{Madnani, Fazil~Ayan, Resnik, \BBA\ Dorr}{Madnani
  et~al.}{2007}]{Madnani:Ayan:Reznik:Dorr:2007}
Madnani, N., Fazil~Ayan, N., Resnik, P., \BBA\ Dorr, B. \BBOP 2007\BBCP.
\newblock \BBOQ Using Paraphrases for Parameter Tuning in Statistical Machine
  Translation.\BBCQ\
\newblock In {\Bem Proceedings of the Second Workshop on Statistical Machine
  Translation}, \mbox{\BPGS\ 120--127}, Prague, Czech Republic. Association for
  Computational Linguistics.

\bibitem[\protect\BCAY{Marton, Callison-Burch, \BBA\ Resnik}{Marton
  et~al.}{2009}]{Marton:Callison-Burch:Resnik:2009}
Marton, Y., Callison-Burch, C., \BBA\ Resnik, P. \BBOP 2009\BBCP.
\newblock \BBOQ Improved Statistical Machine Translation Using
  Monolingually-Derived Paraphrases.\BBCQ\
\newblock In {\Bem EMNLP 2009 Proceedings}, \mbox{\BPGS\ 381--390}, Singapore.
  Association for Computational Linguistics.

\bibitem[\protect\BCAY{Nakov}{Nakov}{2008}]{Nakov:2008}
Nakov, P. \BBOP 2008\BBCP.
\newblock \BBOQ Improved Statistical Machine Translation Using Monolingual
  Paraphrases.\BBCQ\
\newblock In {\Bem Proceedings of the European Conference on Artificial
  Intelligence (ECAI'08)}, Patras, Greece.

\bibitem[\protect\BCAY{Nie{\ss}en \BBA\ Ney}{Nie{\ss}en \BBA\
  Ney}{2001}]{Niessen:Ney:2001}
Nie{\ss}en, S.\BBACOMMA\ \BBA\ Ney, H. \BBOP 2001\BBCP.
\newblock \BBOQ Morpho-Syntactic Analysis for Reordering in Statistical Machine
  Translation.\BBCQ\
\newblock In {\Bem Proceedings of MT Summit VIII}, \mbox{\BPGS\ 247--252}.

\bibitem[\protect\BCAY{Och \BBA\ Ney}{Och \BBA\ Ney}{2003}]{Och:Ney:2003}
Och, F.~J.\BBACOMMA\ \BBA\ Ney, H. \BBOP 2003\BBCP.
\newblock \BBOQ A Systematic Comparison of Various Statistical Alignment
  Models.\BBCQ\
\newblock {\Bem Computational Linguistics}, {\Bbf 29}  (1), \mbox{\BPGS\
  19--51}.

\bibitem[\protect\BCAY{Oepen, Velldal, L{\o}ning, Meurer, \BBA\ Rosen}{Oepen
  et~al.}{2007}]{Oepen:Velldal:Loening:Meurer:Rosen:2007}
Oepen, S., Velldal, E., L{\o}ning, J.~T., Meurer, P., \BBA\ Rosen, V. \BBOP
  2007\BBCP.
\newblock \BBOQ Towards Hybrid Quality-Oriented Machine Translation ---{O}n
  Linguistics and Probabilities in {MT}---.\BBCQ\
\newblock In {\Bem TMI 2007 Proceedings}, Sk{\"o}vde.

\bibitem[\protect\BCAY{Schmid}{Schmid}{1994}]{schmid94probabilistic}
Schmid, H. \BBOP 1994\BBCP.
\newblock \BBOQ Probabilistic Part-of-Speech Tagging Using Decision
  Trees.\BBCQ\
\newblock In {\Bem International Conference on New Methods in Language
  Processing}, Manchester, UK.

\bibitem[\protect\BCAY{Shirai, Ikehara, \BBA\ Kawaoka}{Shirai
  et~al.}{1993}]{Shirai:Ikehara:Kawaoka:1993}
Shirai, S., Ikehara, S., \BBA\ Kawaoka, T. \BBOP 1993\BBCP.
\newblock \BBOQ Effects of Automatic Rewriting of Source Language within a
  Japanese to English MT System.\BBCQ\
\newblock In {\Bem Proceedings of the Fifth International Conference on
  Theoretical and Methodological Issues in Machine Translation}, \mbox{\BPGS\
  226--239}, Kyoto, Japan.

\bibitem[\protect\BCAY{Stolcke}{Stolcke}{2002}]{Stolcke:2002}
Stolcke, A. \BBOP 2002\BBCP.
\newblock \BBOQ SRILM---An Extensible Language Modeling Toolkit.\BBCQ\
\newblock In {\Bem International Conference on Spoken Language Processing},
  \lowercase{\BVOL}~2, \mbox{\BPGS\ 901--904}, Denver.

\bibitem[\protect\BCAY{Tanaka}{Tanaka}{2001}]{Tanaka:2001}
Tanaka, Y. \BBOP 2001\BBCP.
\newblock \BBOQ Compilation of a Multilingual Parallel Corpus.\BBCQ\
\newblock In {\Bem Proceedings of PACLING 2001}, \mbox{\BPGS\ 265--268},
  Kyushu.

\bibitem[\protect\BCAY{Velldal \BBA\ Oepen}{Velldal \BBA\
  Oepen}{2006}]{velldal-oepen:2006:EMNLP}
Velldal, E.\BBACOMMA\ \BBA\ Oepen, S. \BBOP 2006\BBCP.
\newblock \BBOQ Statistical Ranking in Tactical Generation.\BBCQ\
\newblock In {\Bem EMNLP 2006 Proceedings}, \mbox{\BPGS\ 517--525}, Sydney,
  Australia. Association for Computational Linguistics.

\bibitem[\protect\BCAY{Watanabe, Shimohata, \BBA\ Sumita}{Watanabe
  et~al.}{2002}]{Watanabe:Shimohata:Sumita:2002}
Watanabe, T., Shimohata, M., \BBA\ Sumita, E. \BBOP 2002\BBCP.
\newblock \BBOQ Statistical Machine Translation on Paraphrased Corpora.\BBCQ\
\newblock In {\Bem Proceedings of the Third International Conference on
  Language Resources and Evaluation}, \mbox{\BPGS\ 2074--2081}, Las Palmas,
  Spain.

\bibitem[\protect\BCAY{Xu \BBA\ Seneff}{Xu \BBA\ Seneff}{2008}]{Xu:Seneff:2008}
Xu, Y.\BBACOMMA\ \BBA\ Seneff, S. \BBOP 2008\BBCP.
\newblock \BBOQ Two-Stage Translation: A Combined Linguistic and Statistical
  Machine Translation Framework.\BBCQ\
\newblock In {\Bem AMTA 2008 Proceedings}, Honolulu, Hawaii.

\bibitem[\protect\BCAY{Yamamoto}{Yamamoto}{2001}]{Yamamoto:2001}
Yamamoto, K. \BBOP 2001\BBCP.
\newblock \BBOQ Paraphrasing Spoken Japanese for Untangling Bilingual
  Transfer.\BBCQ\
\newblock In {\Bem Proceedings of Natural Language Processing Pacific Rim
  Symposium 2001}, \mbox{\BPGS\ 203--210}.

\end{thebibliography}


\begin{biography}

\bioauthor[:]{Eric Nichols}{
    He is currently a Researcher at Nara Institute of Science and
    Technology, Japan. Eric received a BS in Computer Science and a BA
    in Japanese from the University of Maryland at College Park,
    United States. He received an ME and a Dr Eng in Information
    Science from NAIST and was a recipient of the Research Student
    Scholarship from MEXT, Japan. He has done joint research with NTT
    and NICT.
    Eric's research interests include machine translation, knowledge
    acquisition, and information credibility analysis.  }

  \bioauthor[:]{Francis Bond}{
    He is currently an Associate Professor at the Division of
    Linguistics and Multilingual Studies, Nanyang Technological
    University, Singapore.  Francis received a BA in 1988, a BEng
    (1st) in 1990 and later a PhD in 2001, all at the University of
    Queensland. He worked on machine translation and natural language
    understanding at NTT Corp., from 1991 to 2006. From 2006--2009 he
    worked at NICT, Japan, where his focus was on open source natural
    language processing.
    His main research interest is natural language understanding.  }

  \bioauthor[:]{D. Scott Appling}{
    He is currently a research scientist at the Georgia Tech Research
    Institute in Atlanta, Georgia, United States. Scott holds a BS in
    Computer Science with a Minor in Japanese language and an MS in
    Computer Science
    from the Georgia Institute of Technology. His research interests
    include natural language understanding, machine translation,
    applied machine learning, and narrative intelligence.  }

  \bioauthor[:]{Yuji Matsumoto}{
    He is currently a Professor of Information Science, Nara Institute
    of Science and Technology.  Yuji received his MS and PhD degrees
    in information science from Kyoto University in 1979 and in 1989.
    He joined the Machine Inference Section of Electrotechnical
    Laboratory in 1979.  He has then experienced an academic visitor
    at Imperial College of Science and Technology, a deputy chief of
    First Laboratory at ICOT, and an associate professor at Kyoto
    University.  His main research interests are natural language
    understanding and machine learning.  }

\end{biography}

\biodate



\end{document}
