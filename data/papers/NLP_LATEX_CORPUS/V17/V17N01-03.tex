    \documentclass[japanese]{jnlp_1.4}
\usepackage{jnlpbbl_1.2}
\usepackage[dvips]{graphicx}
\usepackage{amsmath}
\usepackage{hangcaption_jnlp}
\usepackage{udline}
\setulminsep{1.2ex}{0.2ex}
\let\underline
\newcommand{\Sze}{}

\usepackage{multirow}
\usepackage{threeparttable}
\usepackage{algorithmic}



\Volume{17}
\Number{1}
\Month{January}
\Year{2010}

\received{2009}{4}{22}
\revised{2009}{7}{2}
\rerevised{2009}{8}{25}
\accepted{2009}{9}{22}

\setcounter{page}{55}

\jtitle{形態論的制約を用いたオンライン未知語獲得}
\jauthor{村脇　有吾\affiref{GUKyoto} \and 黒橋　禎夫\affiref{GUKyoto}}
\jabstract{
日本語の形態素解析における未知語問題を解決するために，オンライン未知語獲
得という枠組みと，その具体的な実現手法を提案する．
オンライン未知語獲得では，形態素解析器と協調して動作する未知語獲得器が，
文が解析されるたびに未知語を検出し，その可能な解釈の候補を列挙し，最適な
候補を選択する．
このうち，列挙は日本語の持つ形態論的制約を利用し，選択は蓄積した複数用例
の比較により行う．
十分な用例の比較により曖昧性が解消されると，解析器の辞書を直接更新し，獲
得された未知語が以降の解析に反映される．
実験により，比較的少数の用例から高精度に未知語が獲得され，その結果形態素
解析の精度が改善することが示された．
}
\jkeywords{未知語，辞書，語彙獲得，形態素解析}

\etitle{Online Acquisition of Japanese Unknown Morphemes using Morphological Constraints}
\eauthor{Yugo Murawaki\affiref{GUKyoto} \and Sadao Kurohashi\affiref{GUKyoto}}
\eabstract{
To solve the unknown morpheme problem in Japanese morphological
analysis, we propose a novel framework of online unknown morpheme
acquisition and its implementation.
In online unknown morpheme acquisition, an unknown morpheme acquirer,
which works in concert with the morphological analyzer, detects unknown
morphemes from each segmented and POS-tagged sentence, enumerates its
possible interpretations, and selects the best candidate.
In enumeration, morphological constraints of the Japanese language are
utilized, and selection is done by comparing multiple examples kept in
storage.
When the number of examples being compared is large enough for
disambiguation, the acquirer directly updates the dictionary of the
analyzer, and the acquired morpheme will be used in subsequent analysis.
Experiments show that unknown morphemes are acquired from relatively
small numbers of examples with high accuracy and improve the quality of
morphological analysis.
}
\ekeywords{unknown morpheme, dictionary, lexicon acquisition,
morphological analysis}

\headauthor{村脇，黒橋}
\headtitle{形態論的制約を用いたオンライン未知語獲得}

\affilabel{GUKyoto}{京都大学大学院情報学研究科}{Graduate School of Informatics, Kyoto University}



\begin{document}
\maketitle


\section{はじめに}

形態素解析は，文を形態素列に分割し，各形態素に品詞をタグ付けするタスクで
ある．
形態素解析は自然言語処理における基盤技術であり，構文解析や情報検索といっ
た応用を実現するうえで高い精度の達成が不可欠となる．
日本語の形態素解析では，あらかじめ定義された辞書を用いる手法が高い精度を
達成している~\cite{Kurohashi1994full,浅原正幸:2002,Kudo2004full}．
この手法では，入力文は辞書引きにより得られた形態素のラティスに展開され，
ラティス中の最適なパスが出力として選択される．
しかし，辞書に基づく形態素解析には，辞書にない形態素（{\bf 未知語}）の解
析を誤りやすいという問題がある．
例えば，形態素解析器
    JUMAN\footnote{http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman.html}
は，デフォルトの辞書を用いると，未知の動詞「ググる」を誤って「ググ」
と「る」に分割する．
この未知語問題は，未知語を解析用の辞書に追加することで解決する．
しかし，人手による辞書登録はコストがかかるため，計算機による自動化が望ま
れる．

人手によらない未知語問題への解決策として，2通りの手法が提案されている．
ひとつ目の手法では，形態素解析における未知語モデルを改良す
る~\cite{Nagata1999full,内元清貴:2001,Asahara2004full,東藍:2006}．
日本語の形態素解析で広く用いられる未知語モデルは，字種に基づく簡単なヒュー
リスティクスだが，代わりに統計や機械学習に基づく未知語モデルを導入すると
未知語同定の精度が向上する．
二つ目の手法では，テキストから未知語を自動獲得し，形態素解析用の辞書
を拡張する~\cite{Mori1996full}．
二つの手法を比べると，前者は入力文中の個々の未知語を同定しようとする
のに対し，後者は同じ未知語のテキスト中での複数の使われ方を比較できるとい
う点で異なる．
複数の使われ方の比較は未知語の同定に効果的と考えられる．
例えば「ようつべ」（YouTube のスラング）という形態素を知らないまま，
「ようつべって…」という文を解釈したいとする．
このとき，「ようつべ」は，未知の名詞以外にも，未知の動詞「ようつべる」と
も解釈でき，いずれが正しいか判断しがたい．
同様に，別の文「ようつべとは，…」について，名詞「ようつべ」の他に，動詞
「ようつぶ」の命令形とも解釈できる．
しかし，両者を見比べると，2文とも名詞「ようつべ」で解釈できることから，
名詞という解釈がより自然だと推測できる．
従って，本論文では後者の手法を採用する．
ただし，両者は対立するものではなく，組み合わせることで，より高い解析
精度が得られるようになると期待できる．

未知語獲得の従来手法はバッチ処理であり，コーパスをソートしてすべての部分
文字列を調べる~\cite{Mori1996full}．
しかし，この手法は効率が悪い．
なぜなら高頻度の形態素のほとんどが解析用の辞書に登録済みであり，一般に出
現頻度でコーパスの90\%以上を網羅している．
こうした既知の形態素を改めて獲得しても無駄になる．
これに対し，提案手法では，辞書に登録されていない形態素のみを獲得対象とす
る．

従来研究は，資源の制約から，主に小規模な新聞記事を対象に行われ
てきたが，近年，ウェブの出現により大規模なテキストが入手可能となっている．
それに伴い，自然言語処理の様々な分野でデータの大規模化による性能向上が報
告されている\cite{Banko2001full,Brants2007full}．
しかし，未知語獲得は，データの大規模化が単純に解決する性質の問題ではない．
未知語の中には，「ブログ」のように高頻度ながら登録が漏れているものもある
が，大部分がいわゆるロングテールに属す低頻度の形態素である．
こうした形態素の出現するテキストには偏りがあるだけでなく，データを増やす
だけでは，次々と新たな未知語が出現してきりがない．
従って，とにかくデータを与えてそこから未知語を獲得するよりも，個々の未知
語候補に着目し，それが獲得されるまでデータを読み込む方が自然である．
そもそも，未知語の同定のために，何千，何万もの使われ方を調べる必要はなく，
直観的には，ほとんどの場合，10件程度を見比べればほぼ明らかではないかと思
われる．

本論文では，オンライン未知語獲得という枠組みと，その具体的な実現手法を提
案する．
オンライン未知語獲得では，バッチ処理ではなく，逐次的に入力されるテキスト
から未知語を獲得する．
形態素解析器自体は，通常通りテキストを文単位で解析し，形態素列を出力する．
異なる点は，解析の裏で未知語獲得器が動作することである．
具体的には，解析された文から未知語を抽出し，適当な時点で形態素解析器の辞
書を更新する．
これにより獲得された未知語が形態素解析に反映される．

オンライン未知語獲得では，獲得開始時に対象コーパスを決める必要がない．
そのため，例えば，クローラが毎日新たなページを取得するという設定でも，こ
の差分のみから未知語が獲得できる．

オンライン未知語獲得は，検出，列挙，選択のサブタスクにより実現される．
このうち，列挙は日本語の持つ形態論的制約を利用し，選択は蓄積した複数用例
の比較による．
実験により比較的少数の用例から高精度に未知語が獲得され，その結果形態素解
析の精度が改善することが示された．

本論文の構成は次の通りである．
\ref{sec:acquisition-task}章で未知語獲得タスクを整理し，
\ref{sec:online-acquisition}章でオンライン未知語獲得の枠組みを提案する．
\ref{sec:enumeration-and-selection}章では，オンライン未知語獲得の実現
手法のうち，列挙と選択を説明する．
\ref{sec:experiments}章で実験結果を報告し，\ref{sec:related-work}章で関
連研究，\ref{sec:conclusion}章で結論を述べる．



\section{未知語獲得タスク} \label{sec:acquisition-task}

未知語獲得とは，未知語について，テキスト中の一つ以上の{\bf
用例}から{\bf 辞書項目}を帰納的に生成するタスクである．
ここで，辞書項目は辞書の項目として記述される形態素であり，テキスト中に出
現したその形態素を用例とよぶ．
例えば，未知語「ググる」について，「なんとなく\underline{ググって}
みた．」や「\underline{ググら}ずに答える．」といったテキスト中の用例から
辞書項目を生成する．
ただし，個々の用例の解釈には曖昧性があり，そうした曖昧性を解消するこ
とによって辞書項目が生成される．

辞書項目の生成には{\bf 語幹}と{\bf 品詞}の同定が必要となる．
「ググる」の例にあるように，動詞や形容詞は文法的役割に応じて
形態変化を起こすが，この形態変化は活用という概念によって処理される．
活用する形態素は{\bf 語幹}と{\bf 語尾}からなる．
語幹は不変だが，語尾は活用に応じて変化する．
例えば，「ググって」は語幹「ググ」と語尾「って」からなる．
名詞は活用せず，語幹のみからなる．

品詞は形態素解析用に定義されたものに基づく．
ただし，既存の品詞は人手での付与が前提となっており，形態，構文，意味レベ
ルの情報が混在している．
未知語獲得タスクにおいて，いきなり意味レベルの情報を獲得するのは難しいた
め，本論文では，ひとまず形態レベルの情報の獲得を目指す．
そのために，品詞分類を整理する．
以下の説明は形態素解析器JUMANが採用する品詞体系に基づく．
品詞体系の設定方法には様々な流儀があるため一般化が難しいが，少なくとも
    ipadic\footnote{http://sourceforge.jp/projects/ipadic/}の品詞体系
でも同様の議論が成り立つことは容易に想像できる．

品詞は「品詞」，「品詞細分類」，「活用型」，「活用形」の4種類からなる．
「品詞」には「名詞」，「動詞」，「形容詞」などがある．
「名詞」の「品詞細分類」には，「普通名詞」や「サ変名詞」の他，固有名詞用の
「固有名詞」，「組織名」，「地名」，「人名」などがある．
しかし，固有名詞と普通名詞の識別は，形態レベルの文法的な情報のみでは
困難なので，本論文では，便宜的に固有名詞も「普通名詞」とみなす．

用言の「動詞」と「形容詞」には「品詞細分類」は設定されていない．
代わりに活用を扱うために{\bf 活用型}と{\bf 活用形}が与えられる．
活用型は活用のタイプに基づく分類であり，活用形は個々の具体的な活用形態を
指す．
例えば，「ググる」の活用型は「子音動詞ラ行」で，「ググって」の活用形は
「タ系連用テ形」，「ググら」は「未然形」となる．

未知語獲得タスクにおける品詞は，「品詞」，「品詞細分類」，「活用型」の適当
な組である．
簡単のために，名詞については「品詞細分類」，動詞と形容詞については「活用型」
で呼ぶ．
例えば，「ググる」の品詞は「子音動詞ラ行」となる．

\begin{table}[b]
\caption{獲得対象の品詞}
\label{tb:pos-list}
\input{04table01.txt}
\vspace{-1\baselineskip}
\end{table}

基本語彙は既に人手により辞書登録されているので，獲得対象をオープンクラス
の品詞に絞り込める．
つまり，「来る」などの不規則変化動詞や助詞，助動詞などの付属語は獲得対象
から除外される．
本論文では，名詞，動詞，および形容詞を獲得対象の品詞とする．
副詞もオープンクラスとみなせるが，今回は明示的な獲得対象としない．
副詞と名詞の識別も，形態レベルの情報だけでは困難だからである．
副詞の認識は今後の課題とする．
以上をまとめると，獲得対象の品詞は表\ref{tb:pos-list}の15種類となる．


形態素の単位認定基準，つまりある言葉が1形態素か否かは自明でない．
例えば，「ミンククジラ」のように構成的な名詞や「宣べ伝える」のような複合
動詞を1形態素とするか分割すべきか明らかでない．
実際，人手で整備された既存の形態素解析用の辞書も，単位に一貫性があるとは
言い難い．
他の単位認定基準としては，『現代日本語書き言葉均衡コーパス』が人間の
作業者向けに詳細な基準を設けている~\cite{BCCWJ2008}．
しかし，この基準は煩雑で，しかも意味レベルの情報も利用しているため，
プログラムに落とし込んで未知語の自動獲得に利用することは困難である．
本論文では，厳密な単位認定にはこだわらないとする．



\section{オンライン未知語獲得} \label{sec:online-acquisition}
\subsection{システム構成} \label{sec:online-acquisition-idea}

未知語獲得タスクに対して，我々はオンラインによる解法を提案する．
図\ref{fig:system}にオンライン未知語獲得のシステム構成を示す．
形態素解析器自体は，通常通り入力文に対して形態素列を出力する．
ただし，辞書として，人手で整備した基本語彙辞書の他に，自動獲得辞書も用い
る．
形態素解析の裏では未知語獲得器が動く．
獲得器は，形態素解析器が出力する形態素列を文ごとに受け取り，そこから未知
語を抽出する．
獲得器は，適当な時点で未知語を獲得し，形態素解析器の自動獲得辞書を更新す
る．
辞書更新により未知語獲得が以降の解析に反映される．

\begin{figure}[b]
\begin{center}
\includegraphics{17-1ia4f1.eps}
\end{center}
 \caption{オンライン未知語獲得システムの構成}
 \label{fig:system}
\end{figure}

獲得器には高い精度での未知語獲得が要求される．
獲得された未知語の辞書へのフィードバックに人手が介在しないが，誤獲得が解
析に悪影響を及ぼすことは避けたいからである．

獲得器は，未知語の用例を蓄積することで，それまでに解析されたテキストを獲
得に利用できる．
未解析のテキストは獲得に利用できないが，見方を変えれば，次に読むテキスト
をあらかじめ決める必要がないことを意味する．
従って，獲得の都合に応じて対象テキストを動的に変更するという応用も
可能である．

オンライン未知語獲得を実現するために，以下のサブタスクを設定する．
\begin{description}
\item [検出] 各文の形態素解析結果から未知語の用例を検出する．
\item [列挙] 検出された各未知語用例に対して，語幹と品詞からなる辞書項目
	    の候補を列挙する．
\item [選択] 各未知語用例に対して，最適な辞書項目の候補を選択する．選択
	    は，過去に検出された用例を蓄積しておき，それら複数用例の比較
	    により行う．比較される用例が増え，曖昧性が十分に解消できた時
	    点で獲得し，形態素解析器の辞書を更新する．
\end{description}

未知語「ググる」の獲得を例にシステムの挙動を説明する．
「ググる」は語幹「ググ」と品詞「子音動詞ラ行」からなる．
テキストを読み進めて，ある時点で文「なんとなくググってみた．」が入ってき
たとする．
獲得器は，まず，この文の「ググ」を手がかりに未知語用例を検出する．
次に，この用例に対して考えられる辞書項目の候補を列挙する．
辞書項目の候補としては，語幹「ググ」と品詞「子音動詞ラ行」以外にも，同じ
語幹で「子音動詞ワ行」，語幹「ググって」と品詞「子音動詞マ行」，語幹「な
んとなくググ」と品詞「子音動詞ラ行」なども考えられる．
こうした複数の候補の中から正しい候補を選択する必要があるが，この1用例だ
けを見ても正しい候補を判断しがたい．
そこで獲得器は判断を保留し，用例を記憶に蓄えておく．
さらにテキストを読み進めると，「ググらずに答える．」という文が入力される．
同様に検出と列挙を行ったのち，「ググってみた」の用例を記憶から取り出して，
「ググらず」と比較する．
すると，両者を共通に解釈できる辞書項目の候補は語幹「ググ」と品詞「子音動
詞ラ行」のみである．
このように複数の用例を比較して曖昧性を解消する．
比較する用例が増え，選択された候補が適当な終了条件を満たしたとき，その候
補を獲得する．
これにより，「ググる」が自動獲得辞書に追加される．

オンライン未知語獲得のサブタスクのうち，本論文では列挙と選択について詳述
する．
検出タスクについては簡単な手法を説明するにとどめる．


\subsection{未知語用例の検出} \label{sec:detection}

未知語検出は各文から未知語の用例を検出するタスクである．
文は，形態素解析結果に基づく形態素列，または文字列として表現される．
タスクの入力は，解析器が返す文の形態素列である．
一方出力は，未知語用例に対応する文の部分文字列であり，その範囲を
$[s_d, e_d]$ とする．
ただし，$[s_d, e_d]$ が未知語用例の語幹の範囲 $[s_u, e_u]$ と厳密に一致
する必要はない．
辞書項目，つまり語幹と品詞の組の候補の列挙は次の列挙タスクで行うが，どの
程度の正確さで検出が必要かは列挙のアルゴリズムに依存する．
\ref{sec:enumeration-method}節で述べる列挙アルゴリズムは，語幹の境界
候補の列挙を $s_d$ を基点に行うので，検出範囲は
$s_u \leq s_d \leq e_u$ を満たす必要がある．


日本語において未知語用例の検出は自明なタスクではない．
一番単純な検出手法として，既知語とテキストの文字列マッチングを行い，マッ
チしない箇所を検出するというものが考えられる．
しかし，日本語の単純な音韻体系がわざわいして，多くの未知語に対して無関係
な既知語がマッチし，検出漏れが起きる．
この現象は，形態素解析器が持つ文法知識を利用することである程度抑えられる．
形態素解析器は，入力文に対して，辞書引きと未知語処理により，出力すべき形
態素の候補を列挙する．
未知語処理により列挙される形態素候補を{\bf 未定義語}と呼ぶ．
JUMANでは，字種に基づく簡単なヒューリスティクスが採用されている．
例えば，カタカナの連続が一つの形態素候補とされる．
これにより，未知語「ググる」を含む入力文「ググってみた．」に対して，未定
義語「ググ」が形態素候補となり，これを含むパスが出力に選ばれる．
従って，形態素解析結果中の未定義語 $w_i$ を検出範囲 $[s_{w_i}, e_{w_i}]$
とする．
ただし，$s_{w_i}$ と $e_{w_i}$ は，形態素 $w_i$ の文字列表現における開
始・終了位置である．

形態素解析を用いる検出手法でも検出されない未知語用例が存在する．
例えば，「アブラハム」は「アブラ」（油）と「ハム」に分割され，「うざい」は
「う」（卯／雨／鵜）と「ざい」（剤／在／材／罪／剤）に分割される．
こうした過分割未知語の検出は今後の課題とする．


\section{辞書項目の列挙と選択} \label{sec:enumeration-and-selection}
\subsection{列挙タスクと選択タスク} \label{sec:enumeration-and-selection-tasks}

列挙は，検出された各用例に対して，文中の前後の文脈を利用して，考えられる
辞書項目の候補を列挙するタスクである．
辞書項目の候補は語幹と品詞からなる．
ここで，語幹の同定は前方境界と後方境界の二つの同定を意味する．
例えば，「なんとなくググってみた」の場合，「なく」と「ググ」の間に前方境
界が，「ググ」と「って」の間に後方境界が引かれる．
そこで，辞書項目の候補を前方境界，後方境界，品詞の組で表現する．
列挙される候補は，効率よく正解候補を選択するためには，なるべく数が少ない
ことが望ましい．

選択は，各未知語用例に対して，最適な辞書項目の候補を選択するタスクである．
この際，検出済みの未知語用例を蓄積することで，複数の用例が比較できる．
選択タスクの実現には，最適な候補を選択する基準と，最終的に獲得を判断する
ための終了条件が必要となる．


\subsection{形態論的制約の利用} \label{sec:enumeration-constraints}

辞書項目の列挙において，候補絞り込みの手がかりとして形態論的制約を利用す
る．
日本語は膠着語であり，形態素は，その文法的な役割に応じて，接尾辞，助動詞，
助詞などに後続される．
この際，用言は後続する形態素に応じて活用形を変える．
また，形態素同士の連接には品詞に応じて制約が働く．
例えば，助詞「を」は，「走る」の基本連用形「走り」に後続して「走りを」と
いう形は取り得るが，未然形「走ら」に後続して「走らを」とはならない．
このような連接に関する制限を形態論的制約と呼ぶ．


この形態論的制約を列挙に利用するために{\bf サフィックス}を導入する．
サフィックスとは，語幹に後続し得る文字列であり，自立語の語尾（あれ
ば）と後続する付属語列を連結したものである．
サフィックスの例を表\ref{tb:naming-conventions}に示す．
いま，ある文字列に対してあるサフィックスが後続したとする．
このとき，そのサフィックスの直前が自立語の語幹の後方境界の可能性が
ある．

\begin{table}[t]
    \caption{サフィックスの例}
    \label{tb:naming-conventions}
\input{04table02.txt}
\end{table}

サフィックスの集合は生テキストから収集される．
ここで，形態素解析が既知語について十分に高精度であることを利用する．
具体的には，テキストを形態素解析し，既知語に後続するサフィックスを収集す
る．
こうして集められたサフィックスを品詞ごとに集約する．
いま，サフィックスが十分に大きなコーパスから収集されたとき，ある品詞に属
す形態素の語幹に後続し得るサフィックスは，品詞に対応するサフィックス集合
中のいずれかに限定される．
従って，サフィックスを候補列挙に用いることで，後方境界と同時に品詞の候補
が列挙できる．
なおかつ，品詞候補を形態論的制約を満たすものに限定できる．
ただし，一般に，サフィックスは複数の品詞に後続し得る．
例えば，サフィックス「をも」は母音動詞にもサ変名詞にも後続できる．


サフィックスの収集には，Kawahara et~al.の手法により編纂されたウェブ
コーパスを用いる~\cite{Kawahara2006full}．
ただし，予備実験により，この大規模コーパスでもサフィックスの異なり数が収
束しないと判明した．
「させられかねなかっただろう」のような低頻度の長いサフィックスが存在する
からである．
そこで，サフィックスの最大長を5文字とし，それより長いサフィックスは
先頭の5文字で統合する．
実験では，約1億ページから約66万の異なるサフィックスを得た．
サフィックスあたりの品詞数は平均で1.33であった．


\subsection{サフィックスを用いた列挙手法} \label{sec:enumeration-method}

サフィックスを用いて辞書項目の列挙を行う．
まず，列挙に利用する文中の前後の文脈，つまり前方境界と後方境界の探索範囲
を文節を用いて限定する．
\pagebreak
文節については，構文解析器
    KNP\footnote{http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp.html}
が係り受け解析の前処理として文節まとめあげを行うので，その結果を利用する．
検出された未知語用例が属す文節，および最大で前後2文節を探索
範囲とする．
ただし，文頭，文末や句読点で探索を打ち切る．

後方境界と品詞の組の候補を図\ref{fig:suffix-match}のようにサフィックスを
用いて列挙する．
検出範囲の開始位置 $s_d$ から探索範囲の終端までの各位置で，サフィッ
クスのマッチングを行う．
サフィックスがマッチしたとき，サフィックス開始位置が後方境界の候補となり，
サフィックスに対応する1個以上の品詞が候補となる．

\begin{figure}[b]
\begin{center}
\includegraphics{17-1ia4f2.eps}
\end{center}
 \caption{候補の列挙}
 \label{fig:suffix-match}
\end{figure}

長さの異なる複数のサフィックスがマッチした場合，以下の規則で採用するサ
フィックスを選択する．
原則として長い候補を優先するが，サフィックスの終了位置が文節境界と一致し
なければならない．
ただし，サフィックスは最大5文字としているので，5文字のサフィックスがあれ
ば無条件で採用する．

また，サフィックス以外の手がかりとして，以下を前方境界と後方境界の候補列
挙に利用する．
\begin{itemize}
\item 文頭と文末\footnote{ただし，ウェブコーパスの場合は，HTMLから文抽出
      を行うため，文頭や文末は文抽出誤りの影響を受ける可能性があり，あま
      り信頼できない．}
\item 句読点や記号
\item 「御」などの接頭辞
\item 「首相」などの末尾要素
\item KNPにより与えられる文節境界
\end{itemize}
これらの手がかりにより列挙される候補のうち，後方境界については，特殊な品
詞 ``EOB'' を与える．
``EOB'' はサフィックスなしに語幹単独で出現し得ることを示す．
例えば，「グーグル」などの名詞には句読点などが直接後続し得る．
また，母音動詞は基本連用形（名詞化）が語幹と同形なので，語幹単独で出現し
得るとみなせる．
一方，「ググる」などの子音動詞ラ行は語幹単独では出現しない．
``EOB'' は選択タスクにおいて語幹単独で出現し得る品詞に展開され
る．




\subsection{用例の蓄積} \label{sec:selection-accumulation}

辞書項目の選択には，それまでに検出された複数の用例を利用する．
具体的には，新たに入ってきた用例について，その用例と同じ辞書項目を
表す可能性のある用例群を記憶から取り出して比較する．
ただし，真に同じ辞書項目を表す用例のみを取り出すのは難しいので，ひとまず
前方境界を共有する用例群を取り出し，後の処理で絞り込みを行う．
また，獲得に至らなかった用例は記憶に追加し，獲得時には使われた用例群を削
除する．

用例の効率的な管理のためにトライを利用する．
各用例の格納は，前方境界の候補数だけ行う．
トライのキーとして，各前方境界候補と，それより右で最左の後方境界候
補に挟まれた文字列を用いる．
例えば，図\ref{fig:suffix-match}の用例に対して，「ググ」と「何となくググ」
をキーとして2箇所に格納する．
用例取り出し時にはキーを使ってトライをたどり，途中のノード，およびキーの
末端ノードの子孫に格納された用例群を取り出す．



\subsection{辞書項目の選択} \label{sec:selection-method}

辞書項目の候補，つまり前方境界，後方境界および品詞の候補のうち，最適な候
補の選択を記憶から取り出された用例群の比較により行う．
図\ref{fig:selection}に選択の擬似コードを示す．
候補の絞り込みは前方境界，後方境界，品詞の順で行う．
また，語幹については短い候補（前方境界は右，後方境界は左）から順に調べる．

\begin{figure}[b]
\input{04fig03.txt}
 \caption{選択の擬似コード}
 \label{fig:selection}
\end{figure}

用例 $e$ の各前方境界候補に対して，まず記憶から前方境界 $f$ を共有する
用例群 $E$ を取り出す (retrieveExamples)．
次に，用例群の比較により，若干の後方境界候補の足切りを行う
(refineRearBoundaryCandidates)．
これにより，語幹の長さが0の候補や，後述の終了条件を満たさないことが明ら
かな候補を取り除く．
残った各後方境界候補 $r$ に対して，品詞の絞り込みを行う
(refinePOSCandidates)．
品詞候補が $p$ 一つに絞り込まれ，その候補が獲得の終了条件を満たすなら，
候補 $(f, r, p)$ を獲得する．

選択の方針は，単純に，多くの用例をうまく説明できる候補を選ぶというもので
あり，絞り込みは用例群の包含関係により行う．
refinePOSCandidates では，$(f, r)$ を共有する用例群中の被覆率が閾値以上
の品詞候補を選ぶ．
ただし，「普通名詞」，「サ変名詞」，「ナ形容詞」は区別が明確でなく，また，
「母音動詞」の「基本連用形」と「普通名詞」の区別は困難なため，これらの品
詞のみが候補として残った場合には「普通名詞」を採用する．

終了条件は，候補 $(f, r, p)$ を共有する用例群について，次の二つが満たさ
れる場合とする．
一つ目は前方境界の妥当性のチェックである．
具体的には，句読点などの明らかな境界マーカーから前方境界が得られた候補の
割合が閾値以上とする．
例えば，未知語「新撰組」に対して，形態素解析が「新」を接頭辞と解釈するた
め，常に「撰組」が辞書項目の候補となる．
選択アルゴリズムは短い候補を優先するので，「新撰組」よりも先に「撰組」が
調べられる．
しかし，「撰組」の直前に句読点等が来る用例はないので，「撰組」は獲得され
ない．
二つ目は活用型の異なり数が閾値以上という条件である．
これにより，品詞が偶発的に選択されたのではなく，実際に該当品詞として使わ
れていることを確認する\footnote{実験で
は異なり数の閾値を3とした．従って，獲得には最低3個の用例が必要となる．}．


\subsection{品詞分類手法の比較} \label{sec:selection-comparison}

品詞分類について先行研究との簡単な比較を示す．
Mori et~al.の後ろの「文字列」と福島・鍜治らの「後続するひらがな
n-gram」，および桑江らの「最長後続ひらがな列」は，本論文のサフィックスと同様の働き
をする~\cite{Mori1996full,福島健一:2007,鍜治伸裕:2009,桑江常則:2008}．

Mori et~al.は前後の文字列とその頻度をベクトルで表現し，語幹候補と品詞モ
デルとのベクトル間の距離の近さにより品詞を判定している．
しかし，同じ品詞に属す形態素が本当に似たベクトルを取るのだろうか．
直観的には，品詞は大雑把な分類であり，同じ品詞に属す形態素でも振る舞いに
ばらつきがありそうに思われる．
そこで，ウェブコーパスを対象に簡単な実験を行った．
まず，コーパスの形態素解析結果から既知語に後続するサフィックスを収
集する．
次に，サフィックスを各形態素ごとに集約し，形態素ごとの後続サフィック
スの頻度分布を求める．
同様にして，形態素が属す品詞ごとに，後続サフィックスの頻度分布を求める．
そして，各形態素と品詞との間で，後続サフィックスの頻度分布の近さを求
める．
ただし，頻度分布の近さの尺度として Skew divergence $s_\alpha$ を用
いる~\cite{Lee2001full}．
\pagebreak
\begin{align*}
 s_\alpha (q, r) & = D_{KL} (r || \alpha q + (1 - \alpha) r), \\
 D_{KL} (q||r) & = \sum_y q(y) (\log q(y) - \log r(y))
\end{align*}
ここで，$q$，$r$ はサフィックスの頻度分布とし，$\alpha = 0.99$
とする．

図\ref{fig:divergence}に，「子音動詞ラ行」の例を示す．
横軸は「子音動詞ラ行」の各形態素の絶対頻度を表し，縦軸は各形態素の，「母
音動詞」との近さと「子音動詞ラ行」との近さとの「差」を表す．
低頻度区間では，二つの近さの差が小さく，近さによる品詞判定では識別が難し
いと予想される形態素が目立つ．
それだけでなく，高頻度区間でも差が小さい形態素が散見される．
従って，出現頻度が大きくても，近さによる品詞判定が難しいと予想される場合
が存在する．


福島・鍜治らは品詞識別にSVMを用い，素性として後続するひらがなn-gram
を与える．
素性の値に福島らは頻度，鍜治らは出現したか否かの2値を使う．
SVMは識別器であり，品詞内の近さよりも品詞間の差異を学習すると期待される．

\begin{figure}[b]
\begin{center}
 \includegraphics{17-1ia4f4.eps}
\end{center}
 \caption{「子音動詞ラ行」の各形態素の，「母音動詞」との近さと「子音動詞
 ラ行」との近さとの「差」}
 \label{fig:divergence}
\end{figure}

一方，提案手法は，サフィックスの頻度には注目せず，個々のサフィックスを品
詞リストに写像する．
サフィックスは形態論的制約を満たすか否かの2値を表現しており，候補列挙の
時点で，制約を満たさない品詞は候補から除外される．
このように，品詞の絞り込みが各用例に対して行われるので，単純に多くの用例
を説明できる候補を選ぶだけで品詞分類が行える．

また，提案手法は一つの語幹に対応する品詞は一つという仮定を置いている．
これに対し，Mori et~al.と桑江らは，「楽し-い」と「楽し-む」のように，一
つの語幹が複数の品詞に属す可能性を明示的にモデル化している．
しかし，「楽し-い」と「楽し-む」のような派生関係にある形態素の品詞の衝突
は，基本語彙が登録済みのため，極めてまれと推測される．
無関係な形態素同士の偶発的な衝突については，提案手法はテキストを逐次的に
解析するため，同一ドメインのテキストを読んでいる場合，特に起きにくいと推
測される．



\subsection{獲得未知語の分割可能性} \label{sec:selection-decomposition}

獲得された未知語が実際には2個以上の形態素からなる可能性がある．
未知語は比較的少数の用例から獲得するため，未知語$B$が，観測された用例中
でたまたま$AB$という連続で現れていた場合，$AB$を1形態素として獲得してし
まう．
例えば，複合語「顆粒タイプ」が未知語「顆粒」よりも先に獲得されるかもしれ
ない．

この問題に対処するために，未知語獲得時に，獲得済みの形態素が獲得形態
素によって分割できるかを調べ，できる場合にはその形態素を辞書から削除す
る．
現在のところ，分割可能性の検査には形態素解析器を用いる．
これにより形態素解析器に記述された制約知識を利用する．
まず，分割対象形態素の候補列挙は単純な文字列マッチングにより行う．
次に，候補を一時的に辞書から取り除いた状態で，その候補の形態素解析を行い，
獲得形態素によって分割されなかった場合に候補を辞書に戻す．




\section{実験} \label{sec:experiments}

\subsection{実験設定} \label{sec:experiments-settings}

オンライン未知語獲得について，獲得される未知語の精度，および未知語獲得の
形態素解析への貢献を評価する．
基本語彙辞書として，形態素解析器JUMANのデフォルトの辞書を用いる．
この辞書は約3万の基本語彙を収録している．
表記ゆれを展開し，固有名詞を含めれば，語彙数は約12万となる．

獲得対象テキストとして，ドメインが限定されたコーパスを用いる．
話題を共有するテキストの方が，互いに無関係なテキストよりも未知語が集中的
に出現すると期待されるからである．
実験では，検索エンジン基盤TSUBAKI~\cite{Shinzato2008full}を用い，その検索結
果をドメイン限定コーパスとみなす．
各クエリに対して，システムは検索結果のページを順に読み，未知語を獲得する．
獲得は千ページ目で打ち切り，同じ千ページを拡張された語彙を用いて再解析す
る．
クエリとしては，「捕鯨問題」，「赤ちゃんポスト」，「ジャスラック」，「ツ
ンデレ」，および「アガリクス」を使用する．

獲得された未知語は，語幹と品詞の両方が正しい場合に正解とする．
ただし，\ref{sec:acquisition-task}章で述べたように，語幹の単位認定は難し
い．
実際，Nagataと内元らは，単位認定の不一致が報告されたエラーの原因の一つと
みなしている~\cite{Nagata1999full,内元清貴:2001}．
単位認定の不一致を回避するために，正解コーパスとの単純比較ではなく，人手
による判定を採用する．

未知語獲得の形態素解析への貢献の評価は次の手順で行う．
獲得対象テキストを基本語彙辞書と拡張された辞書の2通りで形態素解析す
る．
二つの解析結果を比較して，図\ref{fig:diff}のように単語分割の境界が一致し
ない箇所を抽出する．
これを``diff''ブロックとよぶ．
``diff''ブロックの正誤判定は，形態素への分割と，分割および品詞割り当ての2通
りにより行う．
ただし，形態素境界は，明らかに誤っていない場合に正解とする．
評価には，クエリごとに，再解析により解析結果が変化した文の中から無作為に
抽出した50文を用いる．


\begin{figure}[b]
\begin{center}
\includegraphics{17-1ia4f5.eps}
\end{center}
 \caption{``diff''ブロックの例}
 \label{fig:diff}
\end{figure}

品詞の評価については，「普通名詞」と「サ変名詞」という名詞の「品詞細分類」
を区別しない\footnote{また，「名詞形態指示詞」も名詞とみなす．名詞形態
指示詞の形態レベルの振る舞いは名詞と同様だからである．指示詞はクロー
ズドクラスだが，「コレ」のようなカタカナ指示詞は新聞記事には出現しないた
め，登録が漏れている．}．
また，JUMANが未知語に与える特殊な品詞「未定義語」は名詞とみなす．



\subsection{実験結果} \label{sec:experiments-results}

\begin{table}[b]
    \caption{クエリごとの統計}
    \label{tb:queries}
\input{04table03.txt}
\end{table}


表~\ref{tb:queries}にクエリごとの統計を示す．
再解析により変化した文の割合に大きなばらつきがある (0.43--9.26\%)．
基本語彙辞書はこれまで新聞記事を対象に整備されてきたため，新聞記事
と似ていないドメインほど未知語獲得の効果が大きい傾向がみられる．

獲得された未知語の精度は 97.3--98.5\% と高い．
しかも，獲得時点で利用した用例数の中央値は 4--7に過ぎない．
先行研究では出現回数が10回未満の候補を信用できないとして無視していたこと
を考えると非常に小さな値である~\cite{Mori1996full}．

\begin{figure}[b]
\vspace{-1\baselineskip}
\begin{center}
\includegraphics{17-1ia4f6.eps}
\end{center}
 \caption{クエリ「ジャスラック」における獲得された未知語の頻度と順位}
 \label{fig:frequency}
\end{figure}

図\ref{fig:frequency}に，獲得された未知語の頻度とその頻度の順位
との関係を示す．
ここで，頻度は，拡張された辞書を用いた再解析結果から数えたものである．
順位の下位区間における急な落ち込みは，用例数の不足により獲得されていない
未知語の影響と推測される．
図\ref{fig:process}に，獲得の経過を示す．
ここで，獲得未知語の累積出現数は，拡張された辞書を用いた再解析結果から数
えたものである．
終了時点での蓄積されている用例数と未知語の累積出現数の比較から，検出され
た未知語用例がすべて真の未知語と仮定すると，提案手法で検出される未知語の
うちおよそ半分が獲得されたと推定できる．


表~\ref{tb:examples}に獲得された未知語の例を示す．
予想される通り，獲得された未知語の大半が名詞 (94.1--100\%) やカタカナの
みからなる形態素 (67.9--79.4\%) である．
「タイーホ」や「ぱくる」など新聞記事にはあまり見られない俗語も獲得されている．
字種が混在する「ドジっ娘」や「シャ乱Q」は字種に基づく形態素解析の未知語
処理では正しく解析できない．
「すごい」に対する「スゴい」，「解かる」に対する「解る」のように，登録済
みの形態素の異表記もあった．

誤り例には，「パクられる」や「フラグが立つ」など明らかに構成的な表現を1
形態素と認識しているものがある．
ただし，これらは，さらに未知語獲得を進めて，それぞれ「パクる」や「フラグ」
が獲得された場合，分割可能性のチェックにより消される．
他には，副詞の「やっぱ」が名詞と誤認識された．

\begin{figure}[t]
\begin{center}
\includegraphics{17-1ia4f7.eps}
\end{center}
 \caption{クエリ「ジャスラック」における獲得の経過}
 \label{fig:process}
\end{figure}
\begin{table}[t]
    \caption{獲得未知語の例}
    \label{tb:examples}
\input{04table04.txt}
\end{table}

表~\ref{tb:change-seg}に``diff''ブロックの評価結果を示す．
ほとんどのブロックが拡張された語彙によって正しく解析されている（E
$\rightarrow$ C  および  C $\rightarrow$ C）．
一方，獲得による副作用は限定されている (C $\rightarrow$ E)．
従って，獲得された未知語が形態素解析の精度を改善することが示された．



\subsection{議論} \label{sec:experiments-discussion}

形態素解析において，カタカナ未知語が短いカタカナ形態素によって分割される
ことがある．
例えば，基本語彙辞書のみを用いると，未知語「アブラハム」は「アブラ」と
「ハム」に過分割される．
「アブラハム」は，\ref{sec:detection}節で述べた単純な検出手法では検出さ
れず，従って獲得もされない．
また，未知語の獲得によって新たな過分割が発生し得る．
例えば，「サー」の獲得によって，「サーバー」が「サー」と既知の「バー」に
よって過分割されるようになる．
このような過分割の問題は，本論文が利用した形態レベルの文法的振る舞いだけ
を調べても解決できない．
他の手がかり，例えば，「サーバー」が server という一つの外来語だから分割
できないといった知識が必要となる．

\begin{table}[t]
\caption{``diff''ブロックの評価}
\label{tb:change-seg}
\input{04table05.txt}
\vspace{-1\baselineskip}
\end{table}

提案手法では，カタカナ「イイ」のように語尾までカタカナで表記された用言は
誤って名詞と認識される．
現在の形態素解析は語尾のひらがな表記を前提としている．
この仮定は新聞記事に対しては妥当だが，ウェブテキストに対しては無効であり，
より柔軟な解析が必要になる．
ただし，こうした未知語の解析は元々誤っており，獲得によって形態素解析が悪
化するわけではない．

未知語問題への2通りの解決策のうち，未知語モデルによる手法は，その利点と
して，低頻度語の正しい同定が強調されてい
る~\cite{Nagata1999full,Asahara2004full}．
しかし，ウェブの出現により，ほとんど無尽蔵のテキストが入手できるようになっ
た現在，限られた情報のみを用いた同定は不可欠ではない．
仮に，解析対象のテキストが少量で，未知語獲得を行うには用例の出現回数が足
りないとしても，ウェブから解析対象テキストと関連するテキストを収集するこ
とで，用例の出現回数を増やすことができる．
ここで，バッチ処理~\cite{Mori1996full}と異なる，オンライン獲得
という特徴を生かせる．
すなわち，解析対象テキストから検出された用例にマークして，それらの用例
が獲得に使われたか追跡することで，未知語が十分に獲得された時点で処理を停
止させることができる．

最後に，残された課題を整理する．
\ref{sec:acquisition-task}章で整理したように，形態素に付与される様々
な情報のうち，本論文はひとまず形態レベルの情報の獲得を目指した．
形態レベルの手がかりでは得られない知識としては，名詞と副詞の区別の他に，
固有名詞と普通名詞の区別などがある．
特に名詞の細分類は固有表現認識や省略・照応解析に役立つと期待されるので，
テキストからの自動獲得を目指したい．

本論文では形態素の単位認定にこだわらなかったが，獲得された未知語の中には
構成的なものが含まれている．
参考までに，クエリ「ジャスラック」の獲得結果を調べたところ，判断に迷う場
合を含めると 10\%弱 (45/460) が複合語であった．
ただし，複合語の基準としては，JUMAN 4.0 から 5.0 への変更時に行った複合
語の整理\footnote{「日本語形態素解析システムJUMAN version 6.0」付録
E12.1参照．
    http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman.html}を参考に
した．
日本語の複合名詞は，文法的なマーカなしに構成要素が直接連結されるため，形
態レベルの手がかりでは構成要素に分割できない．
細粒度での単位認定を実現するには，他の手がかりを利用する必要がある．


形態レベルでの未知語獲得については，検出が大きな課題として残っている．
なかでもひらがな表記の未知語は曖昧性が高く，形態素解析器によってより短い
既知の形態素へ過分割されることが少なくない．
予備調査として，形態素解析結果のうち，$1+1$，$1+2$，$2+1$文字という
パターンのひらがな形態素のペアのみを対象に，未知語検出の再現率を求めたと
ころ，本論文の手法では31\%にとどまることが判明している．
ひらがな表記の未知語は数の上では少なく，頻度の上でも異なり数でも，未知語
の大半をカタカナ名詞が占める．
しかし，カタカナ名詞は形態素解析の未知語処理でほぼ問題なく同定できるのに
対し，ひらがな未知語の解析誤りは応用に大きな悪影響を及ぼしやすい．
例えば，「ようつべ」が「よ」，「うつ」，「べ」に誤って分解され，「うつ」
が動詞と解釈された場合，文節まとめあげにより「よ」と「うつ」，「うつ」と
「べ」の間に文節境界が引かれ，これに基づき見当違いな係り受け解析が行われ
てしまう．
提案手法の利点の一つは，既知の形態素を改めて獲得しないことによる効率の良
さだが，今後はこの利点を維持しつつ検出の再現率を上げていきたい．




\section{関連研究} \label{sec:related-work}

形態素解析における未知語の問題は，言語の類型論的特徴や文字の性質に依存す
る部分が少なくない．
フィン語やトルコ語は日本語と同様の膠着語で，自立語に複数の付属語が後続し
て語を形成する．
ただし，これらの言語は分かち書きするため，形態素解析は，分かち書きの単位
である語を形態素に分割するタスクとなる．
例えば，Morpho Challenge では，頻度つきの語のリストから教師なしで形態素
    を切り出すタスクが競われている (Kurimo, Creutz, Varjokallio, Arisoy, and Sara{\c{c}}lar 2006; Kurimo, Creutz, and Turunen 2007)．

日本語と同様に分かち書きしない言語としては，中国語やタイ語などがある
が，いずれも分析的であり，膠着語の日本語とは性質が異なる．
Peng et~al.は中国語の単語分割に新語検出を組み込む~\cite{Peng2004full}．
この手法では，テキストを一度解析した結果から単語分割の信頼度を元に新語を
検出し，それらを素性に組み込んだ状態で再解析を行う．

日本語については，未知語モデルを導入する手法がいくつか提案されている．
Nagataは，字種や単語長に基づく生成的な未知語モデルを単語分割に組み込
む~\cite{Nagata1999full}．
内元らは最大エントロピーモデルに基づく形態素解析の中で，字種やその遷移な
どの未知語同定に有効な情報を素性として利用する~\cite{内元清貴:2001}．
東らは最大エントロピーモデルに代えて条件付き確率場を採用する~\cite{東
藍:2006}．
Asahara et~al.は文字レベルのチャンキングにより未知語を同定しており，学習
器としてSupport Vector Machineを用いる~\cite{Asahara2004full}．

中川らは，単語分割されたテキストに対して，未知語のすべての出現を考慮して
品詞を推定する手法を提案している~\cite{中川哲治:2008}．
しかし，我々が想定するタスクでは，品詞推定と独立に単語分割が実現されると
いう仮定は現実的ではない．

テキストからの未知語の自動獲得については，Mori et~al.は，語幹の前後の文
字列とその頻度をベクトルで表現し，コーパス中の任意の部分文字列について，
品詞のモデルとのベクトルの距離により品詞らしさを判定す
る~\cite{Mori1996full}．
鍜治らはカタカナ用言についてテキストからの自動獲得を行っている~\cite{鍜
治伸裕:2009}．
彼らは，獲得対象を「ググる」などの語幹が自明なカタカナの用言に限定し，
品詞分類に特化した手法を提案する．
一般の未知語を獲得する場合には，あわせて語幹同定の問題も解く必要がある．



\section{結論} \label{sec:conclusion}

本論文では，オンライン未知語獲得という枠組みと，その具体的な実現方法を提
案した．
実験により，未知語が高精度に獲得され，その結果形態素解析の精度が向上する
ことが示された．
形態素解析自体は成熟した技術であり，構文解析や情報検索といった応用のため
の前処理となっている．
従って，応用処理の精度向上に提案手法を利用したいと考えている．


\bibliographystyle{jnlpbbl_1.4}
\begin{thebibliography}{}

\bibitem[\protect\BCAY{浅原\JBA 松本}{浅原\JBA 松本}{2002}]{浅原正幸:2002}
浅原正幸\JBA 松本裕治 \BBOP 2002\BBCP.
\newblock 形態素解析のための拡張統計モデル.\
\newblock \Jem{情報処理学会論文誌}, {\Bbf 43}  (3), \mbox{\BPGS\ 685--695}.

\bibitem[\protect\BCAY{Asahara \BBA\ Matsumoto}{Asahara \BBA\
  Matsumoto}{2004}]{Asahara2004full}
Asahara, M.\BBACOMMA\ \BBA\ Matsumoto, Y. \BBOP 2004\BBCP.
\newblock \BBOQ Japanese Unknown Word Identification by Character-based
  Chunking.\BBCQ\
\newblock In {\Bem Proceedings of the 20th International Conference on
  Computational Linguistics (COLING 2004)}, \mbox{\BPGS\ 459--465}.

\bibitem[\protect\BCAY{東\JBA 浅原\JBA 松本}{東 \Jetal }{2006}]{東藍:2006}
東藍\JBA 浅原正幸\JBA 松本裕治 \BBOP 2006\BBCP.
\newblock 条件付確率場による日本語未知語処理.\
\newblock \Jem{情報処理学会研究報告. 自然言語処理研究会報告}, {\Bbf 2006}
  (53), \mbox{\BPGS\ 67--74}.

\bibitem[\protect\BCAY{Banko \BBA\ Brill}{Banko \BBA\
  Brill}{2001}]{Banko2001full}
Banko, M.\BBACOMMA\ \BBA\ Brill, E. \BBOP 2001\BBCP.
\newblock \BBOQ Mitigating the Paucity-of-data Problem: Exploring the Effect of
  Training Corpus Size on Classifier Performance for Natural Language
  Processing.\BBCQ\
\newblock In {\Bem Proceedings of the First International Conference on Human
  Language Technology Research (HLT 2001)}, \mbox{\BPGS\ 1--5}.

\bibitem[\protect\BCAY{Brants, Popat, Xu, Och, \BBA\ Dean}{Brants
  et~al.}{2007}]{Brants2007full}
Brants, T., Popat, A.~C., Xu, P., Och, F.~J., \BBA\ Dean, J. \BBOP 2007\BBCP.
\newblock \BBOQ Large Language Models in Machine Translation.\BBCQ\
\newblock In {\Bem Proceedings of the 2007 Joint Conference on Empirical
  Methods in Natural Language Processing and Computational Natural Language
  Learning (EMNLP-CoNLL 2007)}, \mbox{\BPGS\ 858--867}.

\bibitem[\protect\BCAY{福島\JBA 鍜治\JBA 喜連川}{福島 \Jetal
  }{2007}]{福島健一:2007}
福島健一\JBA 鍜治伸裕\JBA 喜連川優 \BBOP 2007\BBCP.
\newblock 機械学習を用いたカタカナ用言の獲得.\
\newblock \Jem{言語処理学会第13回年次大会 発表論文集}, \mbox{\BPGS\ 815--818}.

\bibitem[\protect\BCAY{鍜治\JBA 福島\JBA 喜連川}{鍜治 \Jetal
  }{2009}]{鍜治伸裕:2009}
鍜治伸裕\JBA 福島健一\JBA 喜連川優 \BBOP 2009\BBCP.
\newblock 大規模ウェブテキストからの片仮名用言の自動獲得.\
\newblock \Jem{電子情報通信学会論文誌}, {\Bbf J92-D}  (3), \mbox{\BPGS\
  293--300}.

\bibitem[\protect\BCAY{Kawahara \BBA\ Kurohashi}{Kawahara \BBA\
  Kurohashi}{2006}]{Kawahara2006full}
Kawahara, D.\BBACOMMA\ \BBA\ Kurohashi, S. \BBOP 2006\BBCP.
\newblock \BBOQ Case Frame Compilation from the Web using High-Performance
  Computing.\BBCQ\
\newblock In {\Bem Proceedings of The 5th International Conference on Language
  Resources and Evaluation (LREC-06)}, \mbox{\BPGS\ 1344--1347}.

\bibitem[\protect\BCAY{Kudo, Yamamoto, \BBA\ Matsumoto}{Kudo
  et~al.}{2004}]{Kudo2004full}
Kudo, T., Yamamoto, K., \BBA\ Matsumoto, Y. \BBOP 2004\BBCP.
\newblock \BBOQ Applying Conditional Random Fields to {J}apanese Morphological
  Analysis.\BBCQ\
\newblock In {\Bem Proceedings of the 2004 Conference on Empirical Methods in
  Natural Language Processing (EMNLP 2004)}, \mbox{\BPGS\ 230--237}.

\bibitem[\protect\BCAY{Kurimo, Creutz, \BBA\ Turunen}{Kurimo
  et~al.}{2007}]{Kurimo2007}
Kurimo, M., Creutz, M., \BBA\ Turunen, V. \BBOP 2007\BBCP.
\newblock \BBOQ Overview of {M}orpho {C}hallenge in {CLEF} 2007.\BBCQ\
\newblock In {\Bem Working Notes of the CLEF 2007 Workshop}, \mbox{\BPGS\
  19--21}.

\bibitem[\protect\BCAY{Kurimo, Creutz, Varjokallio, Arisoy, \BBA\
      Saraclar}{Kurimo et~al.}{2006}]{Kurimo2006}
Kurimo, M., Creutz, M., Varjokallio, M., Arisoy, E., \BBA\ Sara{\c{c}}lar, M.
  \BBOP 2006\BBCP.
\newblock \BBOQ Unsupervised Segmentation of Words into Morphemes--{C}hallenge
  2005, an Introduction and Evaluation Report.\BBCQ\
\newblock In {\Bem Proceedings of the PASCAL Challenge Workshop on Unsupervised
  Segmentation of Words into Morphemes}.

\bibitem[\protect\BCAY{Kurohashi, Nakamura, Matsumoto, \BBA\ Nagao}{Kurohashi
  et~al.}{1994}]{Kurohashi1994full}
Kurohashi, S., Nakamura, T., Matsumoto, Y., \BBA\ Nagao, M. \BBOP 1994\BBCP.
\newblock \BBOQ Improvements of {J}apanese Morphological Analyzer
  {JUMAN}.\BBCQ\
\newblock In {\Bem Proceedings of the International Workshop on Sharable
  Natural Language Resources}, \mbox{\BPGS\ 22--38}.

\bibitem[\protect\BCAY{桑江\JBA 佐藤\JBA 藤田}{桑江 \Jetal
  }{2008}]{桑江常則:2008}
桑江常則\JBA 佐藤理史\JBA 藤田篤 \BBOP 2008\BBCP.
\newblock 後続ひらがな列に基づく語の活用型推定.\
\newblock \Jem{情報処理学会研究報告}, {\Bbf 2008-NL-186}  (186), \mbox{\BPGS\
  7--12}.

\bibitem[\protect\BCAY{Lee}{Lee}{2001}]{Lee2001full}
Lee, L. \BBOP 2001\BBCP.
\newblock \BBOQ On the Effectiveness of the Skew Divergence for Statistical
  Language Analysis.\BBCQ\
\newblock In {\Bem Proceedings of the Eighth Internatinoal Workshop of
  Artificial Intelligence and Statistics (AI \& Statistics 2001)}, \mbox{\BPGS\
  65--72}.

\bibitem[\protect\BCAY{Mori \BBA\ Nagao}{Mori \BBA\ Nagao}{1996}]{Mori1996full}
Mori, S.\BBACOMMA\ \BBA\ Nagao, M. \BBOP 1996\BBCP.
\newblock \BBOQ Word Extraction from Corpora and Its Part-of-Speech Estimation
  Using Distributional Analysis.\BBCQ\
\newblock In {\Bem Proceedings of the 16th Conference on Computational
  Linguistics}, \lowercase{\BVOL}~2, \mbox{\BPGS\ 1119--1122}.

\bibitem[\protect\BCAY{Nagata}{Nagata}{1999}]{Nagata1999full}
Nagata, M. \BBOP 1999\BBCP.
\newblock \BBOQ A Part of Speech Estimation Method for {J}apanese Unknown Words
  using a Statistical Model of Morphology and Context.\BBCQ\
    \newblock In {\Bem Proceedings of the 37th Annual Meeting of the Association
  for Computational Linguistics (ACL 1999)}, \mbox{\BPGS\ 277--284}.

\bibitem[\protect\BCAY{中川\JBA 松本}{中川\JBA 松本}{2008}]{中川哲治:2008}
中川哲治\JBA 松本裕治 \BBOP 2008\BBCP.
\newblock 大域的な情報を用いた未知語の品詞推定.\
\newblock \Jem{情報処理学会論文誌}, {\Bbf 49}  (3), \mbox{\BPGS\ 1437--1450}.

\bibitem[\protect\BCAY{小椋\JBA 小磯\JBA 冨士池\JBA 原}{小椋 \Jetal
  }{2008}]{BCCWJ2008}
小椋秀樹\JBA 小磯花絵\JBA 冨士池優美\JBA 原裕 \BBOP 2008\BBCP.
\newblock \Jem{『現代日本語書き言葉均衡コーパス』形態論情報規程集}.

\bibitem[\protect\BCAY{Peng, Feng, \BBA\ McCallum}{Peng
  et~al.}{2004}]{Peng2004full}
Peng, F., Feng, F., \BBA\ McCallum, A. \BBOP 2004\BBCP.
\newblock \BBOQ Chinese Segmentation and New Word Detection using Conditional
  Random Fields.\BBCQ\
    \newblock In {\Bem Proceedings of the 20th International Conference on
  Computational Linguistics (COLING 2004)}, \mbox{\BPGS\ 562--568}.

\bibitem[\protect\BCAY{Shinzato, Shibata, Kawahara, Hashimoto, \BBA\
  Kurohashi}{Shinzato et~al.}{2008}]{Shinzato2008full}
Shinzato, K., Shibata, T., Kawahara, D., Hashimoto, C., \BBA\ Kurohashi, S.
  \BBOP 2008\BBCP.
\newblock \BBOQ {TSUBAKI}: An Open Search Engine Infrastructure for Developing
  New Information Access Methodology.\BBCQ\
\newblock In {\Bem Proceedings of the 3rd International Joint Conference on
  Natural Language Processing (IJCNLP 2008)}, \mbox{\BPGS\ 189--196}.

\bibitem[\protect\BCAY{内元\JBA 関根\JBA 井佐原}{内元 \Jetal
  }{2001}]{内元清貴:2001}
内元清貴\JBA 関根聡\JBA 井佐原均 \BBOP 2001\BBCP.
\newblock 最大エントロピーモデルに基づく形態素解析 未知語の問題の解決策.\
\newblock \Jem{自然言語処理}, {\Bbf 8}  (1), \mbox{\BPGS\ 127--141}.

\end{thebibliography}

\begin{biography}
\bioauthor{村脇　有吾}{
2006年3月京都大学工学部情報学科卒業．2008年3月京都大学大学院情報学研究
科修士課程卒業．同年4月，同博士後期課程入学，現在に至る．
}
\bioauthor{黒橋　禎夫}{
1994年京都大学大学院工学研究科電気工学第二専攻博士課程修了．博士（工学）．2006年，京都大学大学院情報学研究科教授，現在に至る．
}

\end{biography}


\biodate




\end{document}

