    \documentclass[japanese]{jnlp_1.4}
\usepackage{jnlpbbl_1.3}
\usepackage[dvips]{graphicx}
\usepackage{amsmath}
\usepackage{hangcaption_jnlp}
\usepackage{udline}
\setulminsep{1.2ex}{0.2ex}
\let\underline


\def\Dder{}                            
\def\Ider{}              
\def\Nsum#1{}                      
\def\Bdma#1{}                
\def\Stri#1#2{} 
\def\Conc#1#2{}        
\def\argmax{}           
\def\max{}           
\def\argmin{}           
\def\min{}           
\def\MLE{}    
\def\hs{}
\def\BT{}
    \def\UW{}
\def\UX{}
\def\UU{}

\def\unit#1{}
\def\MC#1#2#3{}
\newcommand{\pair}[2]{}
\def\Bdca#1{}
\newcommand{\cfbox}[1]{}

\def\tabref#1{}
\def\figref#1{}
\def\equref#1{}
\def\secref#1{}
\def\subref#1{}
\def\appref#1{}




\Volume{17}
\Number{4}
\Month{July}
\Year{2010}


\received{2009}{4}{10}
\revised{2010}{2}{16}
\accepted{2010}{4}{18}

\setcounter{page}{131}

\jtitle{自動獲得した未知語の読み・文脈情報による仮名漢字変換}
\jauthor{笹田　鉄郎\affiref{Author_1} \and 森　　信介\affiref{Author_1}\affiref{Author_2}
\and 河原　達也\affiref{Author_1}\affiref{Author_2}}
\jabstract{
未知語の問題は，仮名漢字変換における重要な課題の1つである．
本論文では，内容の類似したテキストと音声から
未知語の読み・文脈情報をコーパスとして自動獲得し，
仮名漢字変換の精度向上に利用する手法を提案する．
まず，確率的な単語分割によって未知語の候補となる単語をテキストから抽出する．
次に，各未知語候補の読みを複数推定して列挙する．
その後，テキストに類似した内容の音声を認識させることによって正しい読みを選択する．
最後に，音声認識結果を学習コーパスとみなして仮名漢字変換のモデルを構築する．
自動収集されたニュース記事とニュース音声を用いた実験では，
獲得した未知語の読み・文脈情報を仮名漢字変換のための学習コーパスとして
用いることで，精度が向上することを確認した．
}
\jkeywords{未知語，音声認識，仮名漢字変換}

\etitle{Kana-Kanji Conversion by Using Unknown Word-Pronunciation Pairs with Contexts}
\eauthor{Tetsuro Sasada\affiref{Author_1} \and Shinsuke Mori\affiref{Author_1}\affiref{Author_2} \and Tatsuya Kawahara\affiref{Author_1}\affiref{Author_2}}
\eabstract{
One of the significant problems of kana-kanji conversion (KKC) systems is unknown
words.
In this paper,
for the purpose of improvement in KKC accuracy,
we propose a method for extracting unknown words, their
pronunciations and their contexts from similar sets of Japanese text data and
speech data.
Unknown word candidates are extracted from text data with
a stochastic segmentation model, and their possible pronunciation entries are
hypothesized. These entries are verified by conducting automatic speech recognition
(ASR) on audio data on similar topics.
As a result of ASR, we obtain a corpus for training a stochastic model for KKC.
In the experiment, we use automatically-collected news articles and broadcast TV news
covering similar topics.
We made experimental evaluations with our KKC back-end 
enhanced with these corpora
on other web news articles and
observed an improvement in the accuracy.
}
\ekeywords{unknown word, automatic speech recognition, kana-kanji conversion}

\headauthor{笹田，森，河原}
\headtitle{自動獲得した未知語の読み・文脈情報による仮名漢字変換}

\affilabel{Author_1}{京都大学情報学研究科}{Graduate School of Informatics, Kyoto University}
\affilabel{Author_2}{京都大学学術情報メディアセンター}{Academic Center for Computing and Media Studies, Kyoto University}



\begin{document}
\maketitle


\section{はじめに}
\label{sect_intro}

計算機の急速な普及に伴い，様々な自然言語処理システムが一般に用いられるように
なっている．
中でも，日本語の仮名漢字変換は最も多く利用されるシステムの1つである．
仮名漢字変換の使いやすさは変換精度に大きく依存するため
，常に高精度で変換を行うことが求められる．
近年では，変換精度の向上とシステム保守の効率化を両立させるために，
確率的言語モデルに基づく変換方式である統計的仮名漢字変換
\cite{確率的モデルによる仮名漢字変換}
が広まりつつある．

変換精度を向上させる上で問題となるのは，
多くの言語処理システムと同様，未知語の取り扱いである．
統計的仮名漢字変換では，文脈
情報を反映するための単語$n$-gramモデル，
入力である読みと出力である単語表記の対応を取るための仮
名漢字モデルの2つのモデルによって出力文候補の生成確率を計算し，候補を確率の降順に
提示するが，未知語（単語$n$-gramモデルの語彙に含まれない単語）を含む候補の生成はできな
い．
この問題に対処して
変換精度を向上させる一般的な方法は，
仮名漢字変換の利用対象分野における未知語の読み・文脈情報を用いたモデルの改善である．


仮名漢字変換の利用対象となる分野は多岐に渡っており，
未知語の読み・文脈情報を含む対象分野の
学習コーパスが
あらかじめ利用可能であるという状況は少な
い．このため，情報の付与されていない対象分野のテキストに必要な情報を付与して学習
コーパスを新たに作成するということが行われる．
しかしながら，未知語の中には，読みや単語境界をテキストの表層情報から推定することが困難
な単語が少なからず存在する．
このような場合には，対象分野の学習コーパスを作成するために
その分野についての知識を有する作業者が必要となるなど，コストの面で問題が多い．


上記の問題を解決するために，
本論文では，テキストと内容の類似した音声を認識することで
未知語の読み・文脈情報を単語とその読みの組として自動獲得し，
統計的仮名漢字変換の精度を向上させる手法を提案する．
以下に手法の概略を述べる．
まず，情報の付与されていない対象分野のテキストから，
未知語の出現を考慮した単語分割コーパスである疑似確率的単語分割コーパスを作成し，
未知語候補の抽出を行う．
次に，疑似確率的単語分割コーパスから音声認識のための言語モデルを構築するとともに，
未知語候補の読みを複数推定・列挙し，発音辞書を作成する．
その後，言語モデルと発音辞書を用いて
対象分野の音声を認識し，音声認識結果から単語と読みの組の列を獲得する．
最後に，獲得した単語と読みの組の列を統計的仮名漢字変換の学習コーパスに追加して言語
モデルと仮名漢字モデルを更新する．



実験では，
統計的仮名漢字変換のモデル構築に用いる
一般分野のコーパスに，獲得した未知語の読み・文脈情報を追加し，モデルを再構築することで
変換精度が向上することを確認した．
本論文で提案する枠組みは，
対象分野のテキストと音声の自動収集が可能であるという前提のもとで，
未知語に対して頑健なモデルを構築することができるため，
統計的仮名漢字変換の効率的かつ継続的な精度向上に有効である．




\section{単語$n$-gramモデルとその応用}
\label{sect2_LM}

確率的言語モデルとは，任意の記号列\footnote{ここで述べる「記号」は処理単位としての記号であり，文字や単語，品詞など様々な
ものを考えることができる．}に対して，その記号列が
ある自然言語から生成された確率を計算する枠組みを与えるためのモデルである
\cite{確率的言語モデル}
．
本節では，最も一般的な確率的言語モデルの1つである
単語$n$-gramモデルとその応用について述べる．


\subsection{単語$n$-gramモデル}
\label{subsect:W-ngram}

本項では，確率的言語モデルとして広く用いられる
単語$n$-gramモデルならびにモデルパラメータの推定について述べる．

単語$n$-gramモデルは，文を単語列$\Bdma{w} = \Conc{w}{h}$とみなし，
単語の生起を($n-1$)重マルコフ過程で近似したモデルである．すなわち，
単語$n$-gramモデルにおいて，
ある時点での単語$w_{i}$の生起は直前の$(n-1)$単語に依存する．
ここで，単語列$\Bdma{w}$の生成確率$M_{w,n}(\Bdma{w})$は以下の式で与えられる．
\[
  M_{w,n}(\Bdma{w}) = 
  \displaystyle{\prod_{i=1}^{h+1}P(w_{i}| \Bdma{w}_{i-n+1}^{i-1})}
\]
この式で，$w_{i}\;(i \leq 0)$と$w_{h+1}$はそれぞれ文頭と文末を表す特別な記号である．

言語モデル構築の際には，学習コーパス内で観測されたデータの生じる確率を最大にするよ
うに最尤推定法でモデルパラメータを決定することが一般的である．
最尤推定で単語$n$-gramモデルのパラメータ推定を行う場合は，あらかじめ単語分割されている
コーパス内に出現する単語$n$-gramの頻度を計数し，以下の式によって単語$n$-gramの確率を求
める．
\begin{align}
 \label{equation:para1}
& \qquad \qquad &  P(w_i) &= \cfrac{f(w_i)}{f( \cdot )} & (\text{if} \quad  n = 1) &\qquad \qquad & \\
 \label{equation:para2}
&&  P(w_{i}| \Bdma{w}_{i-n+1}^{i-1}) & = \cfrac{f(\Bdma{w}_{i-n+1}^i)}{f(\Bdma{w}_{i-n+1}^{i-1})} & (\text{if} \quad n > 1) && 
\end{align}
式(\ref{equation:para1})において，
$f(w_i)$はコーパス内の単語$w_i$の出現頻度（1-gram頻度）を表し，
$f( \cdot )$はコーパス内における全ての単語の出現頻度（0-gram頻度）を表す．
式(\ref{equation:para2})において，$f(\Bdma{w}_{i-n+1}^i)$はコーパス内における
連続する$n$単語の組の出現頻度（$n$-gram頻度）を表す．

ここで，未知語を含む単語列の生成確率を単語$n$-gramモデルで計算する場合を考える．
未知語を含む単語列の生成確率が0となることを防ぐ
ため，未知語を表す特別な記号$\UW$を用意して，モデル構築の際に他の語彙エントリと同様
に0より大きい確率を与えておく．
未知語を予測するには，まず単語$n$-gramモデルにより$\UW$ 
を予測し，さらにその表記（文字列）$\Bdma{x}$を以下の文字$n$-gramモデルにより予
測する．
\[
  M_{x,n}(\Bdma{x}) = \prod_{i=1}^{h'+1}P(x_{i}| \Bdma{x}_{i-n+1}^{i-1})
\]
ここで$x_{i}\;(i \leq 0)$と$x_{h'+1}$は，それぞれ語頭と語末
を表す特別な記号である．

本項で述べた$n$-gramモデルの応用として，
文献\cite{統計的言語モデルとN-best探索を用いた日本語形態素解析法}
では日本語や中国語のように分かち書きされない言語に対する
形態素解析器を提案している．
また，
文献\cite{N-gramモデルを用いた音声合成のための読み及びアクセントの同時推定}では，
文献\cite{統計的言語モデルとN-best探索を用いた日本語形態素解析法}で提案された手法
の拡張として，
式(\ref{equation:para1})(\ref{equation:para2})における$w_i$を単語，読み，アクセント，品
詞の4つ組に置き換えた$n$-gramモデルによってテキストの読みとアクセントの推定を行うシス
テムを提案している．



\subsection{統計的仮名漢字変換}
\label{subsect:KKC}

本項では，\cite{確率的モデルによる仮名漢字変換}で提案されている
確率的モデルを用いた統計的仮名漢字変換について述べる．

日本語の仮名漢字変換システムは，計算機のキーボードからの入力記号列\footnote{入力記号列の記号とは，キーボードから入力可能なラテン文字，記号，仮名文字を表
す．}
$\Bdma{z}$
を仮名漢字混じり文である文字列$\Bdma{x}$に変換する．
ここでは，出力を文字列$\Bdma{x}$とする代わりに単語列$\Bdma{w}$とし，
入力記号列$\Bdma{z}$に対応する
候補$\Bdma{w}$を以下に示す事後確率$P(\Bdma{w}|\Bdma{z})$が大
きいものから順に列挙する．
\begin{equation}
 P(\Bdma{w}|\Bdma{z}) = \cfrac{P(\Bdma{z}|\Bdma{w})P(\Bdma{w})}{P(\Bdma{z})} 
\end{equation}
最尤の変換結果$\hat{\Bdma{w}}$は，
$P(\Bdma{w}|\Bdma{z})$をベイズの定理により以下のように変形することで
求めることができる．
\begin{equation}
 \label{equation:KKC}
 \hat{\Bdma{w}} = \argmax_{\Bdma{w}} P(\Bdma{z}|\Bdma{w})P(\Bdma{w})
\end{equation}
式(\ref{equation:KKC})において，後半の$P(\Bdma{w})$は言語モデルであり，
\ref{subsect:W-ngram}節で述べた
単語$n$-gramモデルを用いることができる．
前半の$P(\Bdma{z}|\Bdma{w})$は
確率的仮名
漢字モデルと呼ばれ，単語列$\Bdma{w}$が与えられた際の入力記号列の生成確率を表す．
ここで述べている変換モデルでは出力を文字列$\Bdma{x}$ではなく単語列
$\Bdma{w}$とみなしているため，単語と入力記号列との対応関係がそれぞれ独立
であると仮定することで$P(\Bdma{z}|\Bdma{w})$は以下の式で表される．
\begin{equation}
  \label{equation:PMg}
  P(\Bdma{z}|\Bdma{w}) = \prod_{i=1}^{h} P(z_{i}|w_{i})
\end{equation}
ここで，部分入力記号列$z_{i}$は単語$w_{i}$に対応する入力記号列であり，
全体の入力記号列は$\Bdma{z} = \Conc{z}{h}$となる．

仮名漢字モデルのパラメータ推定には，単語ごとに入力記号列が付与されたコーパス
を用い，
式(\ref{equation:PMg})における確率$P(z_{i}|w_{i})$の値は，
以下の式によって計算される．
\begin{equation}
 \label{equ_param_PM}
  P(z_{i}|w_{i}) = \frac{f(z_{i},\;w_{i})}{f(w_{i})}
\end{equation}
ここで$f(z_{i},\;w_{i})$は単語と読みの組の出現頻度であり，
$f(w_{i})$は単語出現頻度である．



\subsection{確率的単語分割コーパス}
\label{subsection:SSC}

$n$-gramモデルの性能は
パラメータ学習のためのコーパスに大きく依存する．
しかし，決定的な単語分割を行うコーパスを単語$n$-gramモデルのパラメータ推定に用いる場合，
分割誤りによって未知語の出現頻度が0となっている可能性がある．
このようなコーパスから構築される単語$n$-gramモデルは
未知語に対する頑健性に欠けるため，
本項では，確率的単語分割コーパス
並びにその近似である疑似確率的単語分割コーパス
\cite{擬似確率的単語分割コーパスによる言語モデルの改良}
の枠組みを用いてこの問題に対処する方法を述べる．


\subsubsection{確率的単語分割コーパスを用いた$n$-gram確率の推定}
\label{subsubsect:WBP}

日本語の単語分割は，入力文における各文字間に単語境界があるかどうかを決定する問題と
みなせる．
入力となるコーパスを
長さ$n_r$の文字列$\Bdma{x}_1^{n_r} = \Conc{x}{n_r}$
としたとき，
確率的単語分割コーパスは
隣接する2文字$x_{i}$と$x_{i+1}$の間に単語境界確率$P_{i}$を与えたものとして
定義される．
ここでは，確率的単語分割コーパスを作成するために最大エントロピーモデルを用い
て単語境界確率の推定を行う
\cite{擬似確率的単語分割コーパスによる言語モデルの改良}．
単語境界をある文字列境界が単語境界であるか否かを決めるための素性として，
単語境界の周辺$x_{i-2}^{i+2}$の範囲の文字
$n$-gram ($n = 1, 2, 3$) と文字種の情報を用いる．

ここで，確率的単語分割コーパス内での単語の扱いについて述べる．
決定的に単語分割されたコーパスにおいて，単語0-gram頻度はコーパス中の全単語数，単語
1-gram頻度はそれぞれの単語の出現頻度である．確率的単語分割コーパスにおいては，単語
0-gram頻度$f(\cdot)$はコーパス中に現れる全ての部分文字列の期待頻度として，以下の式
で定義される．
\[
 f(\cdot) = 1 + \sum_{i=1}^{n_{r}-1} P_{i}
\]
また，確率的単語分割コーパス中のある1箇所に現れる単語$w$の期待頻度$f(w)$
は，文字列$x_{i+1} x_{i+2}\cdots x_{i+k}$が単語$w$である確率を以下に示す式から
計算することで得られる．
\[
     f(w) = 
     P_{i}\left[\prod_{j=1}^{k-1} (1-P_{i+j})\right]
     P_{i+k}
\]
これは$x_{i+1}$の左側（$i$番目の文字列境界）が単語境界，$x_{i+1} x_{i+2}\cdots x_{i+k}$
の間にある文字列境界が単語境界ではない，$x_{i+k}$の右側が単語境界である，というときに
文字列$x_{i+1} x_{i+2}\cdots x_{i+k}$が単語$w$である確率を示している．
確率的単語分割コーパス中における単語$w$とその期待頻度の扱いを
図\ref{figure_sect2_SSC_freq}に示す．
$f(w)$は1箇所の$w$に対する期待頻度なので，単語1-gram
期待頻度はコーパス中の全ての出現にわたる期待頻度の合計
となる．単語$n$-gram期待頻度 ($n\geq 2$) についても，
単語境界である確率$P_i$と単語境界ではない確率($1-P_i$)から同様に期待頻度の計算を行う．
単語$n$-gram確率は，式(\ref{equation:para1})(\ref{equation:para2})における
$n$-gram頻度を$n$-gram期待頻度として推定する．

以上に述べた確率的単語分割コーパスから構築される単語$n$-gramモデルは，テキスト中に出現
する全ての部分文字列が語彙となるため，未知語に対して頑健なモデルとなる．

\begin{figure}[t]
  \begin{center}
\includegraphics{17-4ia8f1.eps}
  \end{center}
  \caption{確率的単語分割コーパスにおける期待頻度}
  \label{figure_sect2_SSC_freq}
\end{figure}


\subsubsection{疑似確率的単語分割コーパス}
\label{subsubsection:P-SSC}

上述の確率的単語分割コーパスを用いて$n$-gram確率の推定を行う場合，単語の出現頻度を計算するために多くの計算時間が必要となる．

本節では，この問題に対処するために提案されている
疑似確率的単語分割コーパス
\cite{擬似確率的単語分割コーパスによる言語モデルの改良}
の枠組みについて述べる．
これにより，決定的に単語分割されたコーパスを用いて確率的単語分割コーパスに近い$n$-gram
確率を推定することができ，かつ未知語に対する頑健性を保持することができる．

疑似確率的単語分割コーパスは，確率的単語分割コーパスに対して以下の処理を最初の
文字から最後の文字まで($1 \leq i \leq n_{r}$)行うことで得られる．
\begin{enumerate}

\item 文字$x_{i}$を出力する．

\item 乱数$r_{i} (0 \leq r_{i} < 1)$を発生させ$P_{i}$と比較する．$r_{i} < P_{i}$の場合
      には単語境界記号（空白）を出力し，そうでない場合には何も出力しない．

\end{enumerate}
これにより，確率的単語分割コーパスの特徴をある程度反映し，かつ決定的に単語分割されたコ
ーパスを得ることができる．
この処理を1回行って得られるコーパスにおいて，文字列としての出現頻度が低い単語$n$-gram
の頻度は，確率的単語分割コーパスから期待頻度を計算した場合と大きく異なる可能性がある．
近似による誤差を減らすためには，上記の手続きを$M$回行って得られる単語分割コーパス全て
を単語$n$-gram頻度の計数の対象とすればよい．
このコーパスを
疑似確率的単語分割コーパスと呼び，$M$をその倍率と呼ぶ．



\section{未知語とその読み・文脈情報の自動獲得}
\label{sect3_Ext}

本節では，仮名漢字変換の対象となる分野のテキストと音声を用いて
未知語の読み・文脈情報を自動獲得し，
統計的仮名漢字変換で用いられる言語モデルならびに仮名漢字モデル
の性能を改善させる
手法について述べる．



\subsection{提案手法の概略}
\label{subsect:ext_overview}

\begin{figure}[b]
  \begin{center}
\includegraphics{17-4ia8f2.eps}
  \end{center}
  \caption{提案手法の概要図}
  \label{figure_sect4_overview}
\end{figure}

本項では提案手法の概略について述べる．
図\ref{figure_sect4_overview}に提案手法全体の概要を示す．
本研究では，人手によって読みと単語境界が付与されている一般分野のコーパス
$C_b$があらかじめ用意されているものとする．
また，以下では一般分野のコーパスから読みを取り除いたコーパスを
一般分野の単語分割コーパスと記述し，
その中に存在する単語を既知語，それ以外の単語を未知語と定義する．

提案手法では，以下に示す4段階の処理により，
未知語の読み・文脈情報を未知語を含む単語と読みの組の列として
音声認識結果から獲得\footnote{
音声認識には大語彙音声認識システムJulius
\cite{Julius.--.An.Open.Source.Real-Time.Large.Vocabulary.Recognition.Engine}
を用いる．}
し，統計的仮名漢字変換のモデルを更新する．
\begin{enumerate}
 \item 情報の付与されていない対象分野のテキストから疑似確率的単語分割コーパスを作成し，
       未知語の候補となる単語（以下，未知語候補と記述する）の抽出を行う（3.2項を参照）．
 \item 疑似確率的単語分割コーパスを用いて音声認識のための言語モデルを構築する．
       また，未知語候補の読みを複数推定し，音声認識のための発音辞書を作成する
       （3.3項参照）．
 \item 準備した言語モデル，発音辞書，音響モデルを用いて
       対象分野の音声を認識し，音声認識結果から単語と読みの組の列を獲得する
       （3.4項を参照）．
 \item 獲得した単語と読みの組の列を統計的仮名漢字変換の学習コーパスに追加して言語
       モデルと仮名漢字モデルを更新する（3.5項を参照）．
\end{enumerate}
以下では，これらの処理について詳細を述べる．


\subsection{疑似確率的単語分割コーパスを用いた未知語候補の抽出}
\label{subsect:ext_unk}

まず，獲得対象となる未知語候補を単語境界の付与されていない
対象分野のテキストから抽出する．
本項では，
\ref{subsection:SSC}項で述べた
疑似確率的単語分割コーパスを用いた未知語候補の抽出について述べる．

疑似確率的単語分割コーパスは決定的に単語分割されたコーパスの集合であるが，
全く同様の文であっても単語境界に揺れが存在するため，未知語の分割誤り
を抑制可能である．しかしながら，テキスト中に出現する全ての部分文字列が単語になり得ると
いう疑似確率的単語分割コーパスの性質上，低頻度の文字列は
単語として適切ではないものが多い．このため，出現頻度閾値を設定して適切な
未知語候補を抽出する．

以下では，
未知語候補「守屋」を抽出する場合を例にとり，その手続きを示す．
\begin{enumerate}
 \item 一般分野の単語分割コーパスから単語境界確率を推定するためのモデル
       （\ref{subsubsection:P-SSC}項を参照）を構築し，
       対象分野のテキストに単語境界確率を付与する．
{\tabcolsep = 1.5pt
       \vspace{10pt}
       \begin{center}
	\begin{tabular}{|ccccccccccccccccccccc|}\hline
	 $\cdots$&&昨&&日&&、&&\underline{守}&&\underline{屋}&&前&&次&&官&&が
	 &&$\cdots$ \\
	 $\cdots$&0.8&&0.1&&0.9&&0.9&&0.4&&0.7&&0.8&&0.3&&0.8&&0.8&$\cdots$ \\\hline
	\end{tabular}
       \end{center}
       \vspace{10pt}
}
 \item 単語境界確率と乱数の比較を行い，倍率$M$の疑似確率的単語分割コーパスを作成する．
       \vspace{10pt}
       \begin{center}
	\begin{tabular}{|c|c|}\hline
	 （試行1） &  $\cdots$ 昨日　、　\underline{守屋}　前　次官　が$\cdots$\\
	 （試行2） & $\cdots$ 昨日　、　\underline{守屋}　前次官　が$\cdots$\\
	 （試行3） & $\cdots$ 昨日　、　守　屋前　次官　が$\cdots$\\
	 $\vdots$ & $\vdots$\\
	 （試行$M$） & $\cdots$ 昨日　、　\underline{守屋}　前　次　官が$\cdots$
	 \\\hline
	\end{tabular}
       \end{center}
       \vspace{10pt}
 \item 作成した疑似確率的単語分割コーパス内に出現する単語のうち，
       頻度$F_{th}$以上の未知語（一般分野のコーパスに出現しない単語）
       を未知語候補として抽出する．
\end{enumerate}
次項では，未知語候補の音声認識を行うための言語モデルと発音辞書について述べる．


\subsection{未知語候補を含む言語モデルと発音辞書の作成}
\label{subsect:ucest}

音声認識システムを用いて未知語候補を正しい読みとともに認識するためには，
未知語候補が語彙に含まれる言語モデルと発音辞書が必要である．
本項では，未知語候補を考慮した言語モデルならびに発音辞書の作成方法について述べる．

まず，音声認識のための言語モデルを構築する．
大語彙連続音声認識システムを用いる場合には，対象分野のコーパスと一般分野のコーパスを用
いて対象分野に適合した言語モデルの構築を行うことが一般的である
\cite{Task.adaptation.in.stochastic.language.models.for.continuous.speech.recognition}
\cite{N-gram出現回数の混合によるタスク適応の性能解析}
．
本研究では，\ref{subsect:ext_unk}項で作成した疑似確率的単語分割コーパスを
一般分野の単語分割コーパスに追加し，言語モデルを構築する．

次に，未知語候補の読みを複数推定し，
既知語から作成された発音辞書に追加する．
読みの推定は，\ref{subsect:W-ngram}項の
$n$-gramモデルにおける単語
$w$を文字とその読みの組に置き換えた$n$-gramモデルによって行う．
以下では，未知語候補「守屋」を例にとって説明する．
\begin{enumerate}
 \item 単語を1文字ごとに分割し，それぞれの文字について単漢字辞書から得られる読みを列挙
       する．
       \vspace{10pt}
       \begin{center}
	\begin{tabular}{|ll|}\hline
	 守：&マモ，シュ，モリ\\
	 屋：&ヤ，オク\\\hline
	\end{tabular}
        \end{center}
        \vspace{10pt}
 \item 各文字の読みを組み合わせ，可能性のある単語の読みを列挙する．
       \vspace{10pt}
       \cfbox{マモヤ, マモオク, シュヤ, シュオク, モリヤ, モリオク}
       \vspace{10pt}
 \item 文字と読みの組を単位とする$n$-gramモデルにより，単語表記からの読みの生成確率
       を計算する．
       \vspace{10pt}
       \begin{center}
	\begin{tabular}{|rcr|}\hline
       $P(マモヤ|守屋)$ &$=$& $0.53$\\
       $P(モリヤ|守屋)$ &$=$& $0.22$\\
       $P(シュオク|守屋)$ &$=$& $0.05$\\
	 \multicolumn{3}{|c|}{$\vdots$}\\\hline
	\end{tabular}
        \end{center}
        \vspace{10pt}
 \item 読みが付与されている一般分野のコーパスから発音辞書を作成し，
       (3)で推定した未知語候補と読みの組の中から，確率の上位$L$個を追加する．
       この際，$L$個の未知語候補と読みの組の生成確率を反映させるため，
       単語の読みごとの確率を発音辞書に記述する\footnote{
	上位$L$個の確率の合計が1となるように正規化を行う．}．
       \vspace{10pt}
       \begin{center}
	\begin{tabular}{|rrr|rrr|}\hline
	 \multicolumn{3}{|c|}{既知語}&\multicolumn{3}{c|}{未知語候補}\\\hline
	 \multicolumn{3}{|c|}{$\vdots$}&\multicolumn{3}{c|}{$\vdots$}\\
       国会 & 1.00 & コッカイ&\underline{守屋} & 0.53 & マモヤ\\
       前 & 0.50 & ゼン & \underline{守屋} & 0.22 & モリヤ\\
       前 & 0.50 & マエ & \underline{守屋} & 0.05 & シュオク\\
	 \multicolumn{3}{|c|}{$\vdots$}&\multicolumn{3}{c|}{$\vdots$}\\\hline
	\end{tabular}
        \end{center}
       \vspace{10pt}
\end{enumerate}

上記の例における「守屋」の正しい読みは「モリヤ」であるが，(3)で述べた
$n$-gramモデルによって与えられる確率$P(モリヤ|守屋)$は最大とならないため，
確率の比較による正しい読みの選択は難しい．
次項では，本項で作成した言語モデルと発音辞書を用いた音声認識によって
未知語候補の正しい読みを選択する方法について述べる．


\subsection{未知語の読み・文脈情報の獲得}
\label{subsect:ASR}

前項の処理で発音辞書中に
列挙される未知語候補の読みの中に正しい読みが含まれている場合には，
音声認識によって未知語候補を含む単語と読みの組の列が得られる．
しかし，前項の処理で推定した読みの多くは誤った読みであるため，
音声認識の際に似た発音の単語を取り違え，
誤った読みの未知語候補を出力する可能性がある．
この問題に対処するため，ここでは
言語モデルならびに音響モデルの尤度を反映した事後確率から計算される信頼度
\cite{Confidence.measures.for.large.vocabulary.continuous.speech.recognition}
\footnote{ある単語を含む全ての
単語列候補
（音声認識結果）の相対的な尤度の比率を，その単語の
信頼度として表す．
なお，
本研究で用いる音声認識システムJuliusに
実装されている単語信頼度は，信頼度計算の対象となる単語を含む最尤パスの確率で全
体の確率の和を近似することによって計算される
\cite{2パス探索アルゴリズムにおける高速な単語事後確率に基づく信頼度算出法}
．}を用いて，認識結果における単語の文脈上の妥当性を判定する．
ある単語の信頼度$CM$は0から1の間の値で与えられ，
大きい値であるほど信頼性が高いとみなされる．

以下では，音声認識を用いて未知語の読み・文脈情報を
単語とその読みの列として獲得する手順を示す．
\begin{enumerate}
 \item  対象分野のテキストと同様の話題を扱った音声と，その音声に適合
       した音声認識用の音響モデルを用意する．
 \item (1)の音響モデルと，\ref{subsect:ucest}項の処理によって得られた
       言語モデルならびに発音辞書を用いて(1)の音声に対し音声認識を行い，単語，読み
       ，単語信頼度の3つ組の列を出力する．
       \vspace{-5pt}
       \begin{center}
	\begin{tabular}{|c|}\hline
	 $\cdots$　\underline{\mbox{守屋/モリヤ/0.7}}　前/ゼン/0.8　事務/ジム/0.8　次官/ジカ
	 ン/0.9　
	 $\cdots$\\
	 $\cdots$ 全体/ゼンタイ/0.4 の/ノ/0.7
	 守屋/シュヤ/0.05
	 が/ガ/0.8 狭/セマ/0.9 い/イ/0.9 
	 $\cdots$
	 \\\hline
	\end{tabular}
       \end{center}
       \vspace{10pt}
 \item 音声認識結果のうち，単語信頼度が$CM_{th}$以上の単語を抽出し，
       連続する単語とその読みの組の列を作成する．
       なお，単語信頼度が$CM_{th}$より小さい単語は抽出せず，それまでに抽出された単語と
       その読みの列を独立した文
       とみなす．
       \vspace{10pt}
       \begin{center}
	\begin{tabular}{|c|}\hline
	 $\cdots$　\underline{\mbox{守屋/モリヤ}}　前/ゼン　事務/ジム　次官/ジカン
	 $\cdots$\\
	 $\cdots$　全体/ゼンタイ　の/ノ\\
	 が/ガ　狭/セマ　い/イ　$\cdots$
	 \\\hline
	\end{tabular}
       \end{center}
       \vspace{10pt}
\end{enumerate}



\subsection{統計的仮名漢字変換のためのモデル構築}
\label{subsect:mkmodel}

仮名漢字変換のモデル性能を改善するには，対象分野の学習コーパスを大量に用意すること
が重要である．
人手によって十分な量のコーパスを作成することはコストの面で実用的ではないため，
まずテキストの読み推定を行うこと
によって対象分野のテキストに単語境界と読みを自動的に付与する．
ここでは，\ref{subsect:W-ngram}項の式(1)(2)において
単語$w$を単語と読みの組に置き換え，読み推定のための$n$-gramモデル
を一般分野のコーパス$C_b$から構築する．
この結果得られるコーパスを$C_n$とする．
一般的には，情報の付与されていない対象分野のテキストのみを大量に入手可能である，
という状況が多いため，上述の読み推定システム
や形態素解析器の利用によって大規模なコーパス$C_n$を作成し，$C_b$と$C_n$からモデルを構
築することによって変換精度を向上させることが可能である．
しかしながら$C_n$は一般分野のコーパス$C_b$から構築されるモデルを用いた
システムによって単語境界や読みを付与
されるため，
$C_b$の内部に出現しない未知語の情報をモデルに反映させることは難しい．
この問題を解決するため，提案手法では\ref{subsect:ASR}項の処理によって獲得される，
未知語を含む単語と読みの列をコーパス$C_r$とみなし，
$C_r$によって未知語の読み・文脈情報をモデルに反映させ，未知語の変換精度の向上を図る．



\section{評価}
\label{sect:eval}

本節では，\ref{sect3_Ext}節で述べた提案手法の評価実験について述べる．
まず，\ref{subsect:ext_unk}項〜\ref{subsect:ASR}項で
述べた手法に従って，未知語の読み・文脈情報を単語とその読みの組の列として獲得した．
その後，\ref{subsect:mkmodel}項で示した学習コーパスから
統計的仮名漢字変換の言語モデルならびに仮名漢字モデルを構築して精度評価を行い，
提案手法の有効性を検証した．


\subsection{実験で利用するテキストと音声}
\label{subsect:text_pre}

本項では，実験を行う際にあらかじめ準備するデータ，ならびに実験の過程で利用する
データについて述べる．


\subsubsection{テキスト}

本実験において利用するテキストコーパスを以下に示す．

一般分野のコーパス$C_b$には
現代日本語書き言葉均衡コーパス
(Balanced Corpus of Contemporary Written Japanese; BCCWJ)
\cite{『現代日本語書き言葉均衡コーパス』形態論情報規程集}
を用いた
．BCCWJはあらかじめ単語分割がされており，各単語に読みが付与されている\footnote{
本研究では，人手による修正が入ったコアデータのみを使用し，さらに
活用語を語幹と語尾に分割する等の変更を加えている．}．
ここで，BCCWJの内部に出現する全ての単語が既知語となる．

対象分野のテキストとして，2007年11月2日から2008年1月8日のうち68日間の
ウェブニュースを自動収集したものを用いた．
このウェブニューステキストには情報が付与されていないため，
このテキストに対して\ref{subsect:mkmodel}項で示した手法を
適用することで，単語分割と読みの付与を自動的に行い，コーパス$C_n$を作成した．
また，ウェブニュースのテキストは\ref{subsect:ext_unk}項で述べた疑似確率的単語分割コー
パスの作成に用いた．

後述する\ref{subsect:ex_ext}項の実験により，音声認識結果$C_r$として単語
と読みの組の列が獲得される．$C_r$は，$C_n$と同様に仮名漢字変換のためのモデル構築に用い
た．

テストセット$C_t$として，2008年1月9日，
2008年1月10日の2日間のウェブニュースを単語分割し，読みを付与したものを用いた．

\begin{table}[b]
  \caption{コーパスの一覧}
\input{08table01.txt}
  \label{table_corpus_suff}
\end{table}

以上に述べたテキストコーパスの文数，単語数，文字数を表\ref{table_corpus_suff}に示す．
なお，表\ref{table_corpus_suff}において，対象分野のテキストに対する自動読み推定結果，
ならびに音声認識結果の単語数は，各システムの出力結果から単語数を計数したものである．
また，音声認識結果の出力から文境界を同定することは困難であるため，単語数と文字数のみを
示す．

表\ref{table_unkr_test}に，
テストセットにおける未知の1-gram率（未知語率），未知の2-gram率を，単語を単位とする場合
と単語と読みの組を単位とする場合のそれぞれについて示す．

\begin{table}[t]
  \caption{テストセット$C_t$の未知$n$-gram率(\%)}
\input{08table02.txt}
  \label{table_unkr_test}
\end{table}



\subsubsection{音声}

読みを選択するために用いる音声として，収集したウェブニュース記事と同時期に当たる
2007年12月5日から2008年1月8日の間に放送された30分のニュース番組の
合計17時間の音声を用いた．
ここで，対象分野のテキストと音声の類似度として，音声の一部
の書き起こし（2008年1月7日，8日の2日分）に対するパープレキシティを示す．
後述する対象分野の疑似確率的単語分割コーパスから単語3-gramモデルを構築し
，書き起こしに対するパープレキシティを求めたところ，58.5となった．
これは，本実験で用いる疑似確率的単語分割コーパスから構築される音声認識用言語モデルは
認識対象となる音声に対して十分な単語予測性能を持っている（対象分野の音声と対象分野のテ
キストが十分に似ている）ことを示している．


\subsection{未知語とその読み・文脈情報の自動獲得}
\label{subsect:ex_ext}

本項では，
対象分野のテキストと対象分野の音声を用いた
未知語とその読み・文脈情報の自動獲得について述べる．
また，処理の途中段階で獲得した未知語とテストセット中の未知語を比較し，
各処理における未知語の検出精度を示す．


\subsubsection{未知語候補の抽出}

まず，
\ref{subsect:ext_unk}項で述べた手法に従って
対象分野のテキストから疑似確率的単語分割コーパスを作成し，
未知語候補の抽出を行った．
ここで，疑似確率的単語分割コーパスの倍率は$M=10$とした．
また，未知語候補を決定する際の閾値は，$F_{th} = 50$とした．

また，対象分野のテキストの規模と最終的に獲得可能な未知語の数との関係として，
表\ref{table_cov_cond}に，未知語候補のテストセット$C_t$中の未知語に対する再現率を
示す．表\ref{table_cov_cond}では，
利用するウェブニュースの日数と疑似確率的単語分割コーパスの倍率$M$
を変えることでテキストの規模を調節し，それぞれについて再現率を示した．
また，確率的単語分割コーパスを作成せず，決定的に単語分割を行った場合の再現率についても，
併せて表\ref{table_cov_cond}に示した．
$C_t$内の未知語の集合を$UW_t$，疑似確率的単語分割コーパス内の未知語候補の
集合を$UW_c$とし，コーパス$C$における単語$w$の出現頻度を$f(C,w)$とすると，
再現率は
\[
\cfrac{\displaystyle\sum_{w \in UW_t \cap UW_c} f(C_t, w)}{\displaystyle\sum_{w \in UW_t } f(C_t, w)}
\]
で表される．ここで，$\sum_{w \in UW_t } f(C_t, w) = 2,772$である（表
\ref{table_unkr_test}参照）．

\begin{table}[t]
  \caption{対象分野のテキストから抽出した未知語候補の再現率(\%)}
\input{08table03.txt}
  \label{table_cov_cond}
\end{table}

表\ref{table_cov_cond}から，未知語の抽出を行う場合には，決定的な単語分
割を行ったコーパスではなく，疑似確率的単語分割コーパスを利用することが有効であることが
わかる．



\subsubsection{未知語候補を含む言語モデルと発音辞書の作成}

\ref{subsect:ucest}項で述べた手法を用いて音声認識用の言語モデルと発音辞書を作
成した．
本実験で用いる音声認識システムJuliusは言語モデルとして順向き2-gramモデル，逆向き3-gram
モデルを必要とする．
ここでは，一般分野の単語分割コーパス(BCCWJ)と対象分野の疑似確率的単語分割コーパス
（ウェブニュース）から
単語表記を単位とする順向き2-gramモデルならびに逆向き3-gramモデルを構築した．

次に，抽出した未知語候補の読みを，文字と読みの組を単位とする2-gramモデルによって推定し，
生成確率の上位$L$個の単語と読みの組を既知語から作成される発音辞書に追加した．
本実験では$L=5$とした．


作成した発音辞書の詳細を表\ref{table_pron-dict}に示す\footnote{
片仮名のように文字ごとの読み候補が少ない場合，
もしくは単語長が短い場合など，5個まで読みの列挙を行うことができない未知語候補が存
在する．
このため，未知語候補のエントリ数は単語数の5倍未満になることがあり得る．}．
言語モデルにおける語彙の総数は表\ref{table_pron-dict}における既知語と
未知語候補の単語数を合計した数である20,712，
発音辞書のエントリ（単語と読みの組）の総数は26,880となった．

ここで，$L$の値の妥当性を検証するため，
$L$を変えた場合に得られる未知語候補と読みの組の，テスト
セット$C_t$内の未知語と読みの組に対する再現率を表\ref{table_cov_prondict}に示す．
コーパス$C$における単語と読みの組$u$の出現頻度を$f(C,u)$，
未知語候補と推定された読みの組の集合を$UU_e$，
テストセット$C_t$内の未知語と読みの組の集合を$UU_t$とすると，再現率は
\[
 \cfrac{\displaystyle{\sum_{u \in UU_t \cap UU_e} f(C_t, u)}}{\displaystyle{\sum_{u \in UU_t } f(C_t, u)}}
\]
で表される．ここで，$\sum_{u \in UU_t } f(C_t, u) = 2,925$である（表
\ref{table_unkr_test}参照）．
表\ref{table_cov_prondict}より，$L \geq 5$では再現率に大きな変化が見られないことか
ら，$L$の値を単純に大きくしても最終的に獲得可能な未知語と読みの組の量は変わらないことが
予想される．
また，$L$を大きくするに従って，誤った読みを持つエントリがより多く発音辞書に登録され，
認識誤りが増加する．本実験では以上の2点を考慮し，$L=5$とした．

\begin{table}[t]
  \caption{音声認識用の発音辞書}
\input{08table04.txt}
 \label{table_pron-dict}
\end{table}
\begin{table}[t]
  \caption{発音辞書に列挙された未知語候補と読みの組の再現率(\%)}
\input{08table05.txt}
  \label{table_cov_prondict}
\end{table}



\subsubsection{未知語の読み・文脈情報の獲得}

作成した言語モデルと発音辞書を利用し，
音声認識によって読みを選択し，音声認識結果から
未知語を含む単語と読みの組の列を獲得した．
音声認識システムには，Julius 3.5.3を用いた．
なお，Juliusの動作に必要となる音響モデルは，連続音声認識コンソーシアム2003年度版ソフト
ウェア\footnote{http://www.lang.astem.or.jp/CSRC/}
に同梱されている，
新聞記事読み上げ音声コーパス (JNAS) から学
習された3,000状態，64混合のPTM triphoneモデル
\cite{Phonetic.Tied-Mixture.モデルを用いた大語彙連続音声認識}
を用いた．


音声認識結果のうち，単語信頼度が$CM_{th}$を超えている単語のみ
を抽出し，単語と読みの組の列を単語境界と読みの付与されたコーパス($C_r$)の形で獲得した．
この際，単語信頼度の閾値は$CM_{th} = 0.1$とした．
また，獲得頻度の少ない未知語候補には音声認識誤りと考えられるものが多かったため，
上記の閾値による制限に加えて2回以上認識した未知語候補のみを獲得した\footnote{
アナウンサーの発話のように，音声が明瞭である部分の認識精度は80\%程度，記
者発表のように，背景に雑音が多く含まれる部分の認識精度は30\%程度であった．}．

$C_r$の単語数ならびに文字数は表\ref{table_corpus_suff}で示した通りである．
また，表\ref{table_unkr_rec}に音声認識結果$C_r$の未知語率を示す．ここでは，
テストセット$C_t$の未知語率（表\ref{table_unkr_test}参照）と同様に，
単語ならびに単語と読みの組を単位とした場合の未知$n$-gram率を示す．
なお，最終的に獲得された未知語候補と読みの組（異なり数）は872となった．


最後に，対象分野の音声の規模と獲得した未知語と読みの組の数との関係を調べるため，
使用するニュースの日数を変更した場合の$C_t$に対する再現率を表\ref{table_cov_rec}に示す．
$C_r$内の未知語と読みの組の集合を$UU_{r}$とすると，再現率は
\[
\cfrac{\displaystyle{\sum_{u \in UU_t \cap UU_{r}} f(C_t, u)}}
 {\displaystyle{\sum_{u \in UU_t } f(C_t, u)}}
\]
で表される．

\begin{table}[t]
  \caption{音声認識結果$C_r$の未知$n$-gram率(\%)}
\input{08table06.txt}
  \label{table_unkr_rec}
\end{table}
\begin{table}[t]
  \caption{音声認識結果内の未知語候補と読みの組の再現率(\%)}
\input{08table07.txt}
  \label{table_cov_rec}
\end{table}


\subsubsection{獲得した未知語と読み・文脈情報の再現率と適合率}

本実験の目的は，後述する仮名漢字変換の精度評価において，音声認識結果$C_r$から獲得した
未知語とその読み・文脈情報を利用してテストセット$C_t$を対象とした仮名漢字変換の変換精
度を向上させることにある．
$C_r$を用いて仮名漢字変換のモデルを構築する場合，$C_t$と
$C_r$に共通して出現する未知語と読みの組，または単語を単位とする未知の2-gram
が多いほど
仮名漢字変換の精度が向上する\footnote{前者は\ref{subsect:KKC}項で述べた
仮名漢字モデルの性能に影響し，後者は言語モデルの性能に影響する．}．
以下では，それぞれの再現率ならびに適合率を示す．


まず，$C_r$から獲得した未知語と読みの組の再現率，適合率を示す．
再現率
ならびに適合率はそれぞれ
\[
再現率   = \cfrac{\displaystyle{\sum_{u \in UU_t \cap UU_{r}} f(C_t, u)}}
 {\displaystyle{\sum_{u \in UU_t } f(C_t, u)}} \quad , \qquad
適合率 = \cfrac{\displaystyle{\sum_{u \in UU_t \cap UU_{r}} f(C_r, u)}}
 {\displaystyle{\sum_{u \in UU_t } f(C_r, u)}}
\]
で表される．計算の結果，再現率は31.6\%，適合率は38.2\%となった．

次に，未知の2-gramの再現率，適合率を示す．
コーパス$C$における単語2-gram ($w_{i-1}^i$) の出現頻度を$f(C,w_{i-1}^i)$，
テストセット$C_t$内の未知の単語2-gramの集合を$UB_t$，
音声認識結果$C_r$内の未知の単語2-gramの集合を$UB_r$とすると，再現率ならびに適合率は
\[
再現率  = \cfrac{\displaystyle{\sum_{w_{i-1}^i \in UB_t \cap UB_{r}} f(C_t, w_{i-1}^i)}}
 {\displaystyle{\sum_{w_{i-1}^i \in UB_t } f(C_t, w_{i-1}^i)}} \quad , \qquad
適合率 = \cfrac{\displaystyle{\sum_{w_{i-1}^i \in UB_t \cap UB_{r}} f(C_r, w_{i-1}^i)}}
 {\displaystyle{\sum_{w_{i-1}^i \in UB_t } f(C_r, w_{i-1}^i)}}
\]
で表される．計算の結果，再現率は31.9\%，適合率は25.5\%となった．



\subsection{統計的仮名漢字変換による精度評価}
\label{subsect:eval_KKC}

本項では，\ref{subsect:mkmodel}項で挙げた学習コーパスを用いて
統計的仮名漢字変換の精度評価を行い，提案手法の有効性を検証する．


\subsubsection{実験の条件}

本実験では，一般分野のコーパス$C_b$，対象分野のテキストの自動読み推定結果$C_n$，
音声認識結果$C_r$を用いて統計的仮名漢字変換のためのモデルを構築した．
各コーパスの規模は\ref{subsect:text_pre}の表\ref{table_corpus_suff}に示した通りである．

本実験では，3種類のコーパスを以下のように組み合わせて学習コーパスとし，言語モデル
（単語2-gramモデル）ならびに仮名漢字モデルを構築した．
\begin{enumerate}
 \item $C_b$: ベースライン
 \item $C_b + C_n$: テキストのみを用いた手法（既存手法）
 \item $C_b + C_n + C_r$: テキストと音声に共通して現れる
       未知語の読み・単語文脈を反映させる手法（提案手法）
\end{enumerate}
統計的仮名漢字変換システム全体の精度を評価する基
準として，文字単位の再現率と適合率を計算し，(1)--(3)について比較を行った．
\pagebreak
また，提案手法において未知語の読みと単語文脈を共に利用することの有効性を
検証するため
，(2)を基準として，$C_r$から言語モデル(LM)のみを更新した場合\footnote{
$C_b+C_n+C_r$から言語モデルを構築し，$C_b+C_n$から仮名漢字モデルを構築する．}と，
仮名漢字モデル(PM)のみを更新した場合\footnote{
	$C_b+C_n$から言語モデルを構築し，$C_b+C_n+C_r$から仮名漢字モデルを構築する．}についても変換精度の評価を行った．



本実験における評価指標として，文字単位の再現率と適合率を用いる．
それぞれの定義を以下に示す．
\begin{align*}
 \mbox{再現率} &= \cfrac{\mbox{正解文字数}}
                        {\mbox{テストセット中の文字数}}\\
 \mbox{適合率} &= \cfrac{\mbox{正解文字数}}
                        {\mbox{システムの出力した文字数}}\\
\end{align*}



\subsubsection{実験結果と考察}

(1)--(3)で示した学習コーパスから構築されるモデルによる
再現率，適合率を
表\ref{table_result}に示す．

\begin{table}[b]
  \caption{統計的仮名漢字変換による評価(\%)}
\input{08table08.txt}
  \label{table_result}
\end{table}

$C_b$を用いる場合（ベースライン）
の変換精度と$C_b, C_n$を用いる場合（既存手法）の変換精度を比較した結果
，再現率で8.94\%，適合率で11.68\%の精度向上が確認された．
ここで$C_n$と$C_t$は同分野のコーパスであり，
$C_b$は$C_n$に比較すると小規模なコーパスであるため，
この精度向上は単純に学習データの量を増やしたことに起因すると考えられる．

次に，$C_b, C_n$を用いる場合（既存手法）の変換精度と
$C_b, C_n, C_r$を用いる場合（提案手法）の変換精度
を比較した結果，仮名漢字変換の精度は再現率で0.36\%，適合率で0.48\%の改善
が見られた．既存手法において，
コーパス$C_n$は対象分野の未知語を考慮しない手法で読みを付与されているため，
未知語の正しい分割と読みの付与が行われず，$C_b$と$C_n$のみを用いて構築されるモデルでは
未知語の誤変換が発生する．
しかし，提案手法では\ref{subsect:ex_ext}項の実験で得られた$C_rを用いて$
未知語の読み・文脈情報をモデルに反映させることが可能である．
上記の精度増加は，
\ref{subsect:ex_ext}項で示した未知語の読み・文脈情報の獲得の実験で獲得した未知語と読みの
組，未知の2-gramの量に対応しており，より多くの未知語を獲得する
ほど変換精度が向上すると考えられる．

また，$C_r$を追加することによる精度向上の要因を明らかにするため，
$C_b$と$C_n$から構築したモデルによる変換精度
を基準に，$C_r$を利用して言語モデルと仮名漢字モデルを独立
に更新して精度を比較した．
言語モデルのみを更新した場合は，再現率，適合率ともに0.03\%の改善となり，
仮名漢字モデルのみを更新した場合は，再現率で0.17\%，適合率で0.27\%の改善となった．

言語モデルのみを更新する場合，
未知語と読み（仮名漢字変換における入力記号列）との対応付けを行うことが不可能であるため，
未知語周辺の文脈が変換精度の向上にほとんど寄与しない．
この際，
変換精度の向上に寄与する要素は$C_r$に現れる既知語周辺の文脈情報のみであり，かつ
$C_n$に比較して$C_r$の規模は非常に小さいために，精度がほぼ変化していないと考えられる．

仮名漢字モデルのみを更新する場合については，一定の精度向上が観察された．
しかしながら，
ある読みを持つ未知語に対し，同じ読みを持つ既知語，もしくは結合の結果同じ読みとなる既知
語の連続が存在するという状況では，
未知語を含む変換候補の言語モデル確率は既知語を含む変換候補の確率に比較して
小さくなる．
言語モデルと仮名漢字モデルの両方を更新する場合（提案手法）との精度の差は，
上述の言語モデル確率の差に起因する．

最後に，提案手法を用いることで未知語の変換誤りが改善した例を示す．
       \vspace{10pt}
       \begin{center}
	\begin{tabular}{|rl|}\hline
	 $C_b+C_n$: &前 事務 次官 の 森 や タケマサ\\
	 $C_b+C_n+C_r$: &前 事務 次官 の 守屋 武昌\\
	 \hline
	\end{tabular}
       \end{center}
       \vspace{10pt}
\ref{subsect:ext_unk}〜\ref{subsect:ASR}項において例として示した未知語（守屋）は，
本実験において実際に獲得された未知語の例であり，
音声認識結果$C_r$を用いることによって未知語の誤変換が改善されることを確認した．


以上の結果より，
テキストと音声から獲得される
未知語の読み・文脈情報は
統計的仮名漢字変換システムの精度向上に有効であることが確認された．




\section{関連研究}
\label{sect5_rwork}


第\ref{sect_intro}節で述べた通り，
人手によって任意の分野における未知語の情報を収集することはコストの面で現実的で
はない．このため，未知語に関する情報を自動獲得する研究が多く行われている．

まず，形態素解析など，自動単語分割を行うシステムにおいて
単語辞書に未知語を追加することを目的とした研究について述べる．

文献\cite{未知語の確率モデルと単語の出現頻度の期待値に基づくテキストからの語彙獲得}
では，
ある文の自動単語分割候補における$N$-bestの相対確率を，それぞれの候補において
出現する未知語の出現頻度の期待値として与える．その後，出現した未知語の中から
一定の閾値より大きい出現頻度の期待値を持つ未知語を獲得している．
また，単語分割の際には，未知語を構成する字種によって9種類の未知語タイプを定義し，
それぞれのタイプにおける単語長の分布を考慮した未知語モデルを用いることで，
未知語モデルの性能向上を図っている．

形態素解析のため，品詞を考慮して未知語を獲得する研究として，文献
\cite{Word.Extraction.from.Corpora.and.Its.Part-of-Speech.Estimation.Using.Distributional.Analysis}
では，コーパス中に出現する任意の部分文字列$\alpha$に注目し，
$\alpha$の前後の文字から，$\alpha$が未知語として出現する可能性の高い品詞
に属する確率を推定している．
その後，出現頻度が一定値以上かつ2文字以上の文字列$\alpha$を単語として抽出しておき，
形態素解析器にかけた結果に辞書未登録語が含まれている文字列$\alpha$を未知語として獲得し
ている．

日本語は分かち書きを行わない言語であるため，
自動単語分割器や形態素解析器において必須となる未知語の情報は正しい単語単位である．
このため，形態素解析器のための未知語獲得を行う研究では
未知語の読みには言及しないことが多い．
しかしながら，本研究では統計的仮名漢字変換の精度向上を目的としているため，
未知語の表記ならびにその読みに関する情報を同時に獲得することが望ましい．

文献\cite{自動未知語獲得による仮名漢字変換システムの精度向上}
では，仮名漢字変換を用いる際の入力とその変換結果から
未知語の獲得と言語モデルの更新を行う手法を提案している．
また，言語モデルの更新を繰り返すことで，
仮名漢字変換システムの精度が徐々に向上すると報告している．
ただし，ここで行われている実験はユーザによるシステムの利用を想定し
たシミュレーションであり，本論文で扱う自動獲得とは性質が異なる．

音声認識の分野においては，未知語を原因とする認識誤りの影響を抑制するため，
単語より小さい単位の語彙であるサブワードを擬似的な単語とし，
未知語をサブワードの連続として認識する手法が提案されている
\cite{単語N-gram言語モデルを用いた音声認識システムにおける未知語・冗長語の処理} \cite{Open.Vocabulary.Speech.Recognition.with.Flat.Hybrid.Models} \cite{New.Word.Acquisition.Using.Subword.Modeling}．
しかしながら，日本語の音声認識において
サブワードは基本的に仮名文字列から構成されるため，
サブワードをそのまま未知語獲得に用いても仮名漢字変換への寄与は低いと考えられる．


文献\cite{音声とテキストを用いた認識単語辞書の自動構築}では，
規則を用いてテキストから未知語の候補を抽出，
音声認識を用いて読みを自動的に獲得し，発音辞書に追加する手法が提案されている．
この手法は，テキストと音声から未知語と読みの情報を獲得する点で本研究と共通しているが，
未知語候補の抽出方法と獲得する情報の粒度が本研究と異なる．
本研究では，疑似確率的単語分割コーパスを用いることにより，一貫した単語単位で言語モデル
と発音辞書を作成する．また，音声認識結果から未知語の読みだけではなく文脈情報を獲得し，
統計的仮名漢字変換で利用する確率的言語モデル全体の性能向上を図っている．



\section{結論}
\label{sect6_conc}

本論文では，
類似した話題を扱っているテキストと音声から未知語の読み・文脈情報を
単語と読みの組の列として
自動獲得し，統計的仮名漢字変換の精度向上に利用する手法を提案した．

自動的に収集可能なニュース記事とニュース音声を用いた実験の結果，
音声認識結果から得られる単語と読みの組の列を学習コーパスとして
統計的仮名漢字変換のモデルを学習することにより，システム全体の
精度が向上することを確認した．

以上の結果から，テキストと音声を用いることにより，
仮名漢字変換システムの効率的かつ継続的な精度向上を行うことが可能であることが示された．



\bibliographystyle{jnlpbbl_1.5}
\begin{thebibliography}{}

\bibitem[\protect\BCAY{Bisani \BBA\ Ney}{Bisani \BBA\
  Ney}{2005}]{Open.Vocabulary.Speech.Recognition.with.Flat.Hybrid.Models}
Bisani, M.\BBACOMMA\ \BBA\ Ney, H. \BBOP 2005\BBCP.
\newblock \BBOQ Open Vocabulary Speech Recognition with Flat Hybrid
  Models.\BBCQ\
\newblock In {\Bem Proceedings of the Interspeech 2005}, \mbox{\BPGS\
  725--728}.

\bibitem[\protect\BCAY{Choueiter, Seneff, \BBA\ Glass}{Choueiter
  et~al.}{2007}]{New.Word.Acquisition.Using.Subword.Modeling}
Choueiter, F.~G., Seneff, S., \BBA\ Glass, J. \BBOP 2007\BBCP.
\newblock \BBOQ New Word Acquisition Using Subword Modeling.\BBCQ\
\newblock In {\Bem Proceedings of the Interspeech 2007}, \mbox{\BPGS\
  1765--1768}.

\bibitem[\protect\BCAY{伊藤\JBA 好田}{伊藤\JBA
  好田}{2000}]{N-gram出現回数の混合によるタスク適応の性能解析}
伊藤彰則\JBA 好田正紀 \BBOP 2000\BBCP.
\newblock N-gram出現回数の混合によるタスク適応の性能解析.\
\newblock \Jem{電子情報通信学会論文誌}, {\Bbf J83-D-II}  (11), \mbox{\BPGS\
  2418--2427}.

\bibitem[\protect\BCAY{甲斐\JBA 廣瀬\JBA 中川}{甲斐 \Jetal
  }{1999}]{単語N-gram言語モデルを用いた音声認識システムにおける未知語・冗長語
の処理}
甲斐充彦\JBA 廣瀬良文\JBA 中川聖一 \BBOP 1999\BBCP.
\newblock
  単語N-gram言語モデルを用いた音声認識システムにおける未知語$\cdot$冗長語の処
理.\
\newblock \Jem{情報処理学会論文誌}, {\Bbf 40}  (4), \mbox{\BPGS\ 1383--1394}.

\bibitem[\protect\BCAY{北}{北}{1999}]{確率的言語モデル}
北研二 \BBOP 1999\BBCP.
\newblock \Jem{確率的言語モデル}.
\newblock 言語と計算　4巻. 東京大学出版会.

\bibitem[\protect\BCAY{倉田\JBA 森\JBA 伊東\JBA 西村}{倉田 \Jetal
  }{2008}]{音声とテキストを用いた認識単語辞書の自動構築}
倉田岳人\JBA 森信介\JBA 伊東伸泰\JBA 西村雅史 \BBOP 2008\BBCP.
\newblock 音声とテキストを用いた認識単語辞書の自動構築.\
\newblock \Jem{情報処理学会論文誌}, {\Bbf 49}  (8), \mbox{\BPGS\ 2900--2909}.

\bibitem[\protect\BCAY{Lee, Kawahara, \BBA\ Shikano}{Lee
  et~al.}{2001}]{Julius.--.An.Open.Source.Real-Time.Large.Vocabulary.Recogniti
on.Engine}
Lee, A., Kawahara, T., \BBA\ Shikano, K. \BBOP 2001\BBCP.
\newblock \BBOQ Julius---an open source real-time large vocabulary recognition
  engine.\BBCQ\
\newblock In {\Bem Proceedings of the Eurospeech 2001}, \mbox{\BPGS\
  1691--1694}.

\bibitem[\protect\BCAY{李\JBA 河原\JBA 武田\JBA 鹿野}{李 \Jetal
  }{2000}]{Phonetic.Tied-Mixture.モデルを用いた大語彙連続音声認識}
李晃伸\JBA 河原達也\JBA 武田一哉\JBA 鹿野清宏 \BBOP 2000\BBCP.
\newblock Phonetic Tied-Mixture モデルを用いた大語彙連続音声認識.\
\newblock \Jem{電子情報通信学会論文誌}, {\Bbf J83-D-II}  (12), \mbox{\BPGS\
  2517--2525}.

\bibitem[\protect\BCAY{李\JBA 河原\JBA 鹿野}{李 \Jetal
  }{2003}]{2パス探索アルゴリズムにおける高速な単語事後確率に基づく信頼度算出法}
李晃伸\JBA 河原達也\JBA 鹿野清宏 \BBOP 2003\BBCP.
\newblock 2パス探索アルゴリズムにおける高速な単語事後確率に基づく信頼度算出法.\
\newblock \Jem{情報処理学会研究報告}, 2003-SLP-49-48, \mbox{\BPGS\ 281--286}.

\bibitem[\protect\BCAY{Matsunaga, Yamada, \BBA\ Shikano}{Matsunaga
  et~al.}{1992}]{Task.adaptation.in.stochastic.language.models.for.continuous.
speech.recognition}
Matsunaga, S., Yamada, T., \BBA\ Shikano, K. \BBOP 1992\BBCP.
\newblock \BBOQ Task adaptation in stochastic language models for continuous
  speech recognition.\BBCQ\
\newblock In {\Bem Proceedings of the ICASSP 1992}, \lowercase{\BVOL}~1,
  \mbox{\BPGS\ 165--168}.

\bibitem[\protect\BCAY{森\JBA 土屋\JBA 山地\JBA 長尾}{森 \Jetal
  }{1999}]{確率的モデルによる仮名漢字変換}
森信介\JBA 土屋雅稔\JBA 山地治\JBA 長尾真 \BBOP 1999\BBCP.
\newblock 確率的モデルによる仮名漢字変換.\
\newblock \Jem{情報処理学会論文誌}, {\Bbf 40}  (7), \mbox{\BPGS\ 2946--2953}.

\bibitem[\protect\BCAY{森\JBA 小田}{森\JBA
  小田}{2007}]{自動未知語獲得による仮名漢字変換システムの精度向上}
森信介\JBA 小田裕樹 \BBOP 2007\BBCP.
\newblock 自動未知語獲得による仮名漢字変換システムの精度向上.\
\newblock \Jem{言語処理学会第13回年次大会発表論文集}, \mbox{\BPGS\ 340--343}.

\bibitem[\protect\BCAY{森\JBA 小田}{森\JBA
  小田}{2009}]{擬似確率的単語分割コーパスによる言語モデルの改良}
森信介\JBA 小田裕樹 \BBOP 2009\BBCP.
\newblock 擬似確率的単語分割コーパスによる言語モデルの改良.\
\newblock \Jem{自然言語処理}, {\Bbf 16}  (5), \mbox{\BPGS\ 7--21}.

\bibitem[\protect\BCAY{Mori \BBA\ Nagao}{Mori \BBA\
  Nagao}{1996}]{Word.Extraction.from.Corpora.and.Its.Part-of-Speech.Estimation
.Using.Distributional.Analysis}
Mori, S.\BBACOMMA\ \BBA\ Nagao, M. \BBOP 1996\BBCP.
\newblock \BBOQ Word Extraction from Corpora and Its Part-of-Speech Estimation
  Using Distributional Analysis.\BBCQ\
\newblock In {\Bem Proceedings of the COLING 1996}, \mbox{\BPGS\ 1119--1122}.

\bibitem[\protect\BCAY{長野\JBA 森\JBA 西村}{長野 \Jetal
  }{2006}]{N-gramモデルを用いた音声合成のための読み及びアクセントの同時推定}
長野徹\JBA 森信介\JBA 西村雅史 \BBOP 2006\BBCP.
\newblock N-gramモデルを用いた音声合成のための読み及びアクセントの同時推定.\
\newblock \Jem{情報処理学会論文誌}, {\Bbf 47}  (6), \mbox{\BPGS\ 1793--1801}.

\bibitem[\protect\BCAY{永田}{永田}{1999a}]{統計的言語モデルとN-best探索を用い
た日本語形態素解析法}
永田昌明 \BBOP 1999a\BBCP.
\newblock 統計的言語モデルとN-best探索を用いた日本語形態素解析法.\
\newblock \Jem{情報処理学会論文誌}, {\Bbf 40}  (9), \mbox{\BPGS\ 3420--3431}.

\bibitem[\protect\BCAY{永田}{永田}{1999b}]{未知語の確率モデルと単語の出現頻度
の期待値に基づくテキストからの語彙獲得}
永田昌明 \BBOP 1999b\BBCP.
\newblock 未知語の$
  \!\!\!$確率モデルと単語の出現頻度の期待値に基づくテキストからの語彙獲得.\
\newblock \Jem{情報処理学会論文誌}, {\Bbf 40}  (9), \mbox{\BPGS\ 3373--3386}.

\bibitem[\protect\BCAY{小椋\JBA 小磯\JBA 冨士池\JBA 原}{小椋 \Jetal
  }{2008}]{『現代日本語書き言葉均衡コーパス』形態論情報規程集}
小椋秀樹\JBA 小磯花絵\JBA 冨士池優美\JBA 原裕 \BBOP 2008\BBCP.
\newblock \Jem{『現代日本語書き言葉均衡コーパス』形態論情報規程集}.

\bibitem[\protect\BCAY{Wessel, Schl{\"u}ter, \BBA\ Ney}{Wessel
  et~al.}{2001}]{Confidence.measures.for.large.vocabulary.continuous.speech.re
cognition}
Wessel, F., Schl{\"u}ter, R., \BBA\ Ney, H. \BBOP 2001\BBCP.
\newblock \BBOQ Confidence Measures for Large Vocabulary Continuous Speech
  Recognition.\BBCQ\
\newblock {\Bem IEEE Transactions on Speech and Audio Processing}, {\Bbf 9}
  (3), \mbox{\BPGS\ 288--298}.

\end{thebibliography}

\begin{biography}
\bioauthor{笹田　鉄郎}{
2007年京都大学工学部電気電子工学科卒業．
2009年同大学院情報学研究科修士課程修了．
同年，同大学院博士後期課程に入学，現在に至る．
}
\bioauthor{森　　信介}{
1993年京都大学工学部電気工学第二学科卒業．
1995年京都大学大学院工学研究科電気工学第二専攻修士課程修了．
1998年京都大学大学院工学研究科電子通信工学専攻博士後期課程修了．工学博士．
同年日本アイ・ビー・エム（株）入社．
2007年日本アイ・ビー・エム（株）退社．
同年より京都大学学術情報メディアセンター准教授．
現在に至る．
自然言語処理ならびに音声言語処理，特に確率的言語モデルに関する研究に従事．
1997年情報処理学会山下記念研究賞受賞．
2010年情報処理学会論文賞受賞．
}
\bioauthor{河原　達也}{
1987年京都大学工学部情報工学科卒業．
1989年同大学院修士課程修了．
1990年同博士後期課程退学．博士（工学）．
同年京都大学工学部助手．
1995年同助教授．
1998年同大学情報学研究科助教授．
2003年同大学学術情報メディアセンター教授．
現在に至る．
この間，
1995年から1996年まで　米国・ベル研究所客員研究員．
1998年から2006年まで　ATR 客員研究員．
1999年から2004年まで　国立国語研究所非常勤研究員．
2001年から2005年まで　科学技術振興事業団さきがけ研究21研究者．
2006年から　情報通信研究機構短時間研究員．
音声言語処理，特に音声認識及び対話システムに関する研究に従事．
1997年度日本音響学会粟屋潔学術奨励賞受賞．
2000年度情報処理学会坂井記念特別賞受賞．
情報処理学会連続音声認識コンソーシアム代表，
IEEE SPS Speech TC委員，
IEEE ASRU 2007 General Chair，
言語処理学会理事，を歴任．
情報処理学会音声言語情報処理研究会主査．
日本音響学会，情報処理学会　各代議員．
電子情報通信学会，人工知能学会，言語処理学会，IEEE 各会員.
}
\end{biography}





\biodate



\end{document}
