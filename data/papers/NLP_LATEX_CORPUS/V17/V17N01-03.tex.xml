<?xml version="1.0" ?>
<root>
  <jtitle>形態論的制約を用いたオンライン未知語獲得</jtitle>
  <jauthor>村脇有吾黒橋禎夫</jauthor>
  <jabstract>日本語の形態素解析における未知語問題を解決するために，オンライン未知語獲得という枠組みと，その具体的な実現手法を提案する．オンライン未知語獲得では，形態素解析器と協調して動作する未知語獲得器が，文が解析されるたびに未知語を検出し，その可能な解釈の候補を列挙し，最適な候補を選択する．このうち，列挙は日本語の持つ形態論的制約を利用し，選択は蓄積した複数用例の比較により行う．十分な用例の比較により曖昧性が解消されると，解析器の辞書を直接更新し，獲得された未知語が以降の解析に反映される．実験により，比較的少数の用例から高精度に未知語が獲得され，その結果形態素解析の精度が改善することが示された．</jabstract>
  <jkeywords>未知語，辞書，語彙獲得，形態素解析</jkeywords>
  <section title="はじめに">形態素解析は，文を形態素列に分割し，各形態素に品詞をタグ付けするタスクである．形態素解析は自然言語処理における基盤技術であり，構文解析や情報検索といった応用を実現するうえで高い精度の達成が不可欠となる．日本語の形態素解析では，あらかじめ定義された辞書を用いる手法が高い精度を達成している~．この手法では，入力文は辞書引きにより得られた形態素のラティスに展開され，ラティス中の最適なパスが出力として選択される．しかし，辞書に基づく形態素解析には，辞書にない形態素（未知語）の解析を誤りやすいという問題がある．例えば，形態素解析器JUMANは，デフォルトの辞書を用いると，未知の動詞「ググる」を誤って「ググ」と「る」に分割する．この未知語問題は，未知語を解析用の辞書に追加することで解決する．しかし，人手による辞書登録はコストがかかるため，計算機による自動化が望まれる．人手によらない未知語問題への解決策として，2通りの手法が提案されている．ひとつ目の手法では，形態素解析における未知語モデルを改良する~．日本語の形態素解析で広く用いられる未知語モデルは，字種に基づく簡単なヒューリスティクスだが，代わりに統計や機械学習に基づく未知語モデルを導入すると未知語同定の精度が向上する．二つ目の手法では，テキストから未知語を自動獲得し，形態素解析用の辞書を拡張する~．二つの手法を比べると，前者は入力文中の個々の未知語を同定しようとするのに対し，後者は同じ未知語のテキスト中での複数の使われ方を比較できるという点で異なる．複数の使われ方の比較は未知語の同定に効果的と考えられる．例えば「ようつべ」（YouTubeのスラング）という形態素を知らないまま，「ようつべって…」という文を解釈したいとする．このとき，「ようつべ」は，未知の名詞以外にも，未知の動詞「ようつべる」とも解釈でき，いずれが正しいか判断しがたい．同様に，別の文「ようつべとは，…」について，名詞「ようつべ」の他に，動詞「ようつぶ」の命令形とも解釈できる．しかし，両者を見比べると，2文とも名詞「ようつべ」で解釈できることから，名詞という解釈がより自然だと推測できる．従って，本論文では後者の手法を採用する．ただし，両者は対立するものではなく，組み合わせることで，より高い解析精度が得られるようになると期待できる．未知語獲得の従来手法はバッチ処理であり，コーパスをソートしてすべての部分文字列を調べる~．しかし，この手法は効率が悪い．なぜなら高頻度の形態素のほとんどが解析用の辞書に登録済みであり，一般に出現頻度でコーパスの90%以上を網羅している．こうした既知の形態素を改めて獲得しても無駄になる．これに対し，提案手法では，辞書に登録されていない形態素のみを獲得対象とする．従来研究は，資源の制約から，主に小規模な新聞記事を対象に行われてきたが，近年，ウェブの出現により大規模なテキストが入手可能となっている．それに伴い，自然言語処理の様々な分野でデータの大規模化による性能向上が報告されている．しかし，未知語獲得は，データの大規模化が単純に解決する性質の問題ではない．未知語の中には，「ブログ」のように高頻度ながら登録が漏れているものもあるが，大部分がいわゆるロングテールに属す低頻度の形態素である．こうした形態素の出現するテキストには偏りがあるだけでなく，データを増やすだけでは，次々と新たな未知語が出現してきりがない．従って，とにかくデータを与えてそこから未知語を獲得するよりも，個々の未知語候補に着目し，それが獲得されるまでデータを読み込む方が自然である．そもそも，未知語の同定のために，何千，何万もの使われ方を調べる必要はなく，直観的には，ほとんどの場合，10件程度を見比べればほぼ明らかではないかと思われる．本論文では，オンライン未知語獲得という枠組みと，その具体的な実現手法を提案する．オンライン未知語獲得では，バッチ処理ではなく，逐次的に入力されるテキストから未知語を獲得する．形態素解析器自体は，通常通りテキストを文単位で解析し，形態素列を出力する．異なる点は，解析の裏で未知語獲得器が動作することである．具体的には，解析された文から未知語を抽出し，適当な時点で形態素解析器の辞書を更新する．これにより獲得された未知語が形態素解析に反映される．オンライン未知語獲得では，獲得開始時に対象コーパスを決める必要がない．そのため，例えば，クローラが毎日新たなページを取得するという設定でも，この差分のみから未知語が獲得できる．オンライン未知語獲得は，検出，列挙，選択のサブタスクにより実現される．このうち，列挙は日本語の持つ形態論的制約を利用し，選択は蓄積した複数用例の比較による．実験により比較的少数の用例から高精度に未知語が獲得され，その結果形態素解析の精度が改善することが示された．本論文の構成は次の通りである．章で未知語獲得タスクを整理し，章でオンライン未知語獲得の枠組みを提案する．章では，オンライン未知語獲得の実現手法のうち，列挙と選択を説明する．章で実験結果を報告し，章で関連研究，章で結論を述べる．</section>
  <section title="未知語獲得タスク">未知語獲得とは，未知語について，テキスト中の一つ以上の例から辞書項目を帰納的に生成するタスクである．ここで，辞書項目は辞書の項目として記述される形態素であり，テキスト中に出現したその形態素を用例とよぶ．例えば，未知語「ググる」について，「なんとなくググってみた．」や「ググらずに答える．」といったテキスト中の用例から辞書項目を生成する．ただし，個々の用例の解釈には曖昧性があり，そうした曖昧性を解消することによって辞書項目が生成される．辞書項目の生成には語幹と品詞の同定が必要となる．「ググる」の例にあるように，動詞や形容詞は文法的役割に応じて形態変化を起こすが，この形態変化は活用という概念によって処理される．活用する形態素は語幹と語尾からなる．語幹は不変だが，語尾は活用に応じて変化する．例えば，「ググって」は語幹「ググ」と語尾「って」からなる．名詞は活用せず，語幹のみからなる．品詞は形態素解析用に定義されたものに基づく．ただし，既存の品詞は人手での付与が前提となっており，形態，構文，意味レベルの情報が混在している．未知語獲得タスクにおいて，いきなり意味レベルの情報を獲得するのは難しいため，本論文では，ひとまず形態レベルの情報の獲得を目指す．そのために，品詞分類を整理する．以下の説明は形態素解析器JUMANが採用する品詞体系に基づく．品詞体系の設定方法には様々な流儀があるため一般化が難しいが，少なくともipadicの品詞体系でも同様の議論が成り立つことは容易に想像できる．品詞は「品詞」，「品詞細分類」，「活用型」，「活用形」の4種類からなる．「品詞」には「名詞」，「動詞」，「形容詞」などがある．「名詞」の「品詞細分類」には，「普通名詞」や「サ変名詞」の他，固有名詞用の「固有名詞」，「組織名」，「地名」，「人名」などがある．しかし，固有名詞と普通名詞の識別は，形態レベルの文法的な情報のみでは困難なので，本論文では，便宜的に固有名詞も「普通名詞」とみなす．用言の「動詞」と「形容詞」には「品詞細分類」は設定されていない．代わりに活用を扱うために活用型と活用形が与えられる．活用型は活用のタイプに基づく分類であり，活用形は個々の具体的な活用形態を指す．例えば，「ググる」の活用型は「子音動詞ラ行」で，「ググって」の活用形は「タ系連用テ形」，「ググら」は「未然形」となる．未知語獲得タスクにおける品詞は，「品詞」，「品詞細分類」，「活用型」の適当な組である．簡単のために，名詞については「品詞細分類」，動詞と形容詞については「活用型」で呼ぶ．例えば，「ググる」の品詞は「子音動詞ラ行」となる．基本語彙は既に人手により辞書登録されているので，獲得対象をオープンクラスの品詞に絞り込める．つまり，「来る」などの不規則変化動詞や助詞，助動詞などの付属語は獲得対象から除外される．本論文では，名詞，動詞，および形容詞を獲得対象の品詞とする．副詞もオープンクラスとみなせるが，今回は明示的な獲得対象としない．副詞と名詞の識別も，形態レベルの情報だけでは困難だからである．副詞の認識は今後の課題とする．以上をまとめると，獲得対象の品詞は表の15種類となる．形態素の単位認定基準，つまりある言葉が1形態素か否かは自明でない．例えば，「ミンククジラ」のように構成的な名詞や「宣べ伝える」のような複合動詞を1形態素とするか分割すべきか明らかでない．実際，人手で整備された既存の形態素解析用の辞書も，単位に一貫性があるとは言い難い．他の単位認定基準としては，『現代日本語書き言葉均衡コーパス』が人間の作業者向けに詳細な基準を設けている~．しかし，この基準は煩雑で，しかも意味レベルの情報も利用しているため，プログラムに落とし込んで未知語の自動獲得に利用することは困難である．本論文では，厳密な単位認定にはこだわらないとする．</section>
  <section title="オンライン未知語獲得"/>
  <subsection title="システム構成">未知語獲得タスクに対して，我々はオンラインによる解法を提案する．図にオンライン未知語獲得のシステム構成を示す．形態素解析器自体は，通常通り入力文に対して形態素列を出力する．ただし，辞書として，人手で整備した基本語彙辞書の他に，自動獲得辞書も用いる．形態素解析の裏では未知語獲得器が動く．獲得器は，形態素解析器が出力する形態素列を文ごとに受け取り，そこから未知語を抽出する．獲得器は，適当な時点で未知語を獲得し，形態素解析器の自動獲得辞書を更新する．辞書更新により未知語獲得が以降の解析に反映される．獲得器には高い精度での未知語獲得が要求される．獲得された未知語の辞書へのフィードバックに人手が介在しないが，誤獲得が解析に悪影響を及ぼすことは避けたいからである．獲得器は，未知語の用例を蓄積することで，それまでに解析されたテキストを獲得に利用できる．未解析のテキストは獲得に利用できないが，見方を変えれば，次に読むテキストをあらかじめ決める必要がないことを意味する．従って，獲得の都合に応じて対象テキストを動的に変更するという応用も可能である．オンライン未知語獲得を実現するために，以下のサブタスクを設定する．未知語「ググる」の獲得を例にシステムの挙動を説明する．「ググる」は語幹「ググ」と品詞「子音動詞ラ行」からなる．テキストを読み進めて，ある時点で文「なんとなくググってみた．」が入ってきたとする．獲得器は，まず，この文の「ググ」を手がかりに未知語用例を検出する．次に，この用例に対して考えられる辞書項目の候補を列挙する．辞書項目の候補としては，語幹「ググ」と品詞「子音動詞ラ行」以外にも，同じ語幹で「子音動詞ワ行」，語幹「ググって」と品詞「子音動詞マ行」，語幹「なんとなくググ」と品詞「子音動詞ラ行」なども考えられる．こうした複数の候補の中から正しい候補を選択する必要があるが，この1用例だけを見ても正しい候補を判断しがたい．そこで獲得器は判断を保留し，用例を記憶に蓄えておく．さらにテキストを読み進めると，「ググらずに答える．」という文が入力される．同様に検出と列挙を行ったのち，「ググってみた」の用例を記憶から取り出して，「ググらず」と比較する．すると，両者を共通に解釈できる辞書項目の候補は語幹「ググ」と品詞「子音動詞ラ行」のみである．このように複数の用例を比較して曖昧性を解消する．比較する用例が増え，選択された候補が適当な終了条件を満たしたとき，その候補を獲得する．これにより，「ググる」が自動獲得辞書に追加される．オンライン未知語獲得のサブタスクのうち，本論文では列挙と選択について詳述する．検出タスクについては簡単な手法を説明するにとどめる．</subsection>
  <subsection title="未知語用例の検出">未知語検出は各文から未知語の用例を検出するタスクである．文は，形態素解析結果に基づく形態素列，または文字列として表現される．タスクの入力は，解析器が返す文の形態素列である．一方出力は，未知語用例に対応する文の部分文字列であり，その範囲を[s_d,e_d]とする．ただし，[s_d,e_d]が未知語用例の語幹の範囲[s_u,e_u]と厳密に一致する必要はない．辞書項目，つまり語幹と品詞の組の候補の列挙は次の列挙タスクで行うが，どの程度の正確さで検出が必要かは列挙のアルゴリズムに依存する．節で述べる列挙アルゴリズムは，語幹の境界候補の列挙をs_dを基点に行うので，検出範囲はs_us_de_uを満たす必要がある．日本語において未知語用例の検出は自明なタスクではない．一番単純な検出手法として，既知語とテキストの文字列マッチングを行い，マッチしない箇所を検出するというものが考えられる．しかし，日本語の単純な音韻体系がわざわいして，多くの未知語に対して無関係な既知語がマッチし，検出漏れが起きる．この現象は，形態素解析器が持つ文法知識を利用することである程度抑えられる．形態素解析器は，入力文に対して，辞書引きと未知語処理により，出力すべき形態素の候補を列挙する．未知語処理により列挙される形態素候補を未定義語と呼ぶ．JUMANでは，字種に基づく簡単なヒューリスティクスが採用されている．例えば，カタカナの連続が一つの形態素候補とされる．これにより，未知語「ググる」を含む入力文「ググってみた．」に対して，未定義語「ググ」が形態素候補となり，これを含むパスが出力に選ばれる．従って，形態素解析結果中の未定義語w_iを検出範囲[s_w_i,e_w_i]とする．ただし，s_w_iとe_w_iは，形態素w_iの文字列表現における開始・終了位置である．形態素解析を用いる検出手法でも検出されない未知語用例が存在する．例えば，「アブラハム」は「アブラ」（油）と「ハム」に分割され，「うざい」は「う」（卯／雨／鵜）と「ざい」（剤／在／材／罪／剤）に分割される．こうした過分割未知語の検出は今後の課題とする．</subsection>
  <section title="辞書項目の列挙と選択"/>
  <subsection title="列挙タスクと選択タスク">列挙は，検出された各用例に対して，文中の前後の文脈を利用して，考えられる辞書項目の候補を列挙するタスクである．辞書項目の候補は語幹と品詞からなる．ここで，語幹の同定は前方境界と後方境界の二つの同定を意味する．例えば，「なんとなくググってみた」の場合，「なく」と「ググ」の間に前方境界が，「ググ」と「って」の間に後方境界が引かれる．そこで，辞書項目の候補を前方境界，後方境界，品詞の組で表現する．列挙される候補は，効率よく正解候補を選択するためには，なるべく数が少ないことが望ましい．選択は，各未知語用例に対して，最適な辞書項目の候補を選択するタスクである．この際，検出済みの未知語用例を蓄積することで，複数の用例が比較できる．選択タスクの実現には，最適な候補を選択する基準と，最終的に獲得を判断するための終了条件が必要となる．</subsection>
  <subsection title="形態論的制約の利用">辞書項目の列挙において，候補絞り込みの手がかりとして形態論的制約を利用する．日本語は膠着語であり，形態素は，その文法的な役割に応じて，接尾辞，助動詞，助詞などに後続される．この際，用言は後続する形態素に応じて活用形を変える．また，形態素同士の連接には品詞に応じて制約が働く．例えば，助詞「を」は，「走る」の基本連用形「走り」に後続して「走りを」という形は取り得るが，未然形「走ら」に後続して「走らを」とはならない．このような連接に関する制限を形態論的制約と呼ぶ．この形態論的制約を列挙に利用するためにサフィックスを導入する．サフィックスとは，語幹に後続し得る文字列であり，自立語の語尾（あれば）と後続する付属語列を連結したものである．サフィックスの例を表に示す．いま，ある文字列に対してあるサフィックスが後続したとする．このとき，そのサフィックスの直前が自立語の語幹の後方境界の可能性がある．サフィックスの集合は生テキストから収集される．ここで，形態素解析が既知語について十分に高精度であることを利用する．具体的には，テキストを形態素解析し，既知語に後続するサフィックスを収集する．こうして集められたサフィックスを品詞ごとに集約する．いま，サフィックスが十分に大きなコーパスから収集されたとき，ある品詞に属す形態素の語幹に後続し得るサフィックスは，品詞に対応するサフィックス集合中のいずれかに限定される．従って，サフィックスを候補列挙に用いることで，後方境界と同時に品詞の候補が列挙できる．なおかつ，品詞候補を形態論的制約を満たすものに限定できる．ただし，一般に，サフィックスは複数の品詞に後続し得る．例えば，サフィックス「をも」は母音動詞にもサ変名詞にも後続できる．サフィックスの収集には，Kawaharaet~al.の手法により編纂されたウェブコーパスを用いる~．ただし，予備実験により，この大規模コーパスでもサフィックスの異なり数が収束しないと判明した．「させられかねなかっただろう」のような低頻度の長いサフィックスが存在するからである．そこで，サフィックスの最大長を5文字とし，それより長いサフィックスは先頭の5文字で統合する．実験では，約1億ページから約66万の異なるサフィックスを得た．サフィックスあたりの品詞数は平均で1.33であった．</subsection>
  <subsection title="サフィックスを用いた列挙手法">サフィックスを用いて辞書項目の列挙を行う．まず，列挙に利用する文中の前後の文脈，つまり前方境界と後方境界の探索範囲を文節を用いて限定する．文節については，構文解析器KNPが係り受け解析の前処理として文節まとめあげを行うので，その結果を利用する．検出された未知語用例が属す文節，および最大で前後2文節を探索範囲とする．ただし，文頭，文末や句読点で探索を打ち切る．後方境界と品詞の組の候補を図のようにサフィックスを用いて列挙する．検出範囲の開始位置s_dから探索範囲の終端までの各位置で，サフィックスのマッチングを行う．サフィックスがマッチしたとき，サフィックス開始位置が後方境界の候補となり，サフィックスに対応する1個以上の品詞が候補となる．長さの異なる複数のサフィックスがマッチした場合，以下の規則で採用するサフィックスを選択する．原則として長い候補を優先するが，サフィックスの終了位置が文節境界と一致しなければならない．ただし，サフィックスは最大5文字としているので，5文字のサフィックスがあれば無条件で採用する．また，サフィックス以外の手がかりとして，以下を前方境界と後方境界の候補列挙に利用する．文頭と文末句読点や記号「御」などの接頭辞「首相」などの末尾要素KNPにより与えられる文節境界これらの手がかりにより列挙される候補のうち，後方境界については，特殊な品詞``EOB''を与える．``EOB''はサフィックスなしに語幹単独で出現し得ることを示す．例えば，「グーグル」などの名詞には句読点などが直接後続し得る．また，母音動詞は基本連用形（名詞化）が語幹と同形なので，語幹単独で出現し得るとみなせる．一方，「ググる」などの子音動詞ラ行は語幹単独では出現しない．``EOB''は選択タスクにおいて語幹単独で出現し得る品詞に展開される．</subsection>
  <subsection title="用例の蓄積">辞書項目の選択には，それまでに検出された複数の用例を利用する．具体的には，新たに入ってきた用例について，その用例と同じ辞書項目を表す可能性のある用例群を記憶から取り出して比較する．ただし，真に同じ辞書項目を表す用例のみを取り出すのは難しいので，ひとまず前方境界を共有する用例群を取り出し，後の処理で絞り込みを行う．また，獲得に至らなかった用例は記憶に追加し，獲得時には使われた用例群を削除する．用例の効率的な管理のためにトライを利用する．各用例の格納は，前方境界の候補数だけ行う．トライのキーとして，各前方境界候補と，それより右で最左の後方境界候補に挟まれた文字列を用いる．例えば，図の用例に対して，「ググ」と「何となくググ」をキーとして2箇所に格納する．用例取り出し時にはキーを使ってトライをたどり，途中のノード，およびキーの末端ノードの子孫に格納された用例群を取り出す．</subsection>
  <subsection title="辞書項目の選択">辞書項目の候補，つまり前方境界，後方境界および品詞の候補のうち，最適な候補の選択を記憶から取り出された用例群の比較により行う．図に選択の擬似コードを示す．候補の絞り込みは前方境界，後方境界，品詞の順で行う．また，語幹については短い候補（前方境界は右，後方境界は左）から順に調べる．用例eの各前方境界候補に対して，まず記憶から前方境界fを共有する用例群Eを取り出す(retrieveExamples)．次に，用例群の比較により，若干の後方境界候補の足切りを行う(refineRearBoundaryCandidates)．これにより，語幹の長さが0の候補や，後述の終了条件を満たさないことが明らかな候補を取り除く．残った各後方境界候補rに対して，品詞の絞り込みを行う(refinePOSCandidates)．品詞候補がp一つに絞り込まれ，その候補が獲得の終了条件を満たすなら，候補(f,r,p)を獲得する．選択の方針は，単純に，多くの用例をうまく説明できる候補を選ぶというものであり，絞り込みは用例群の包含関係により行う．refinePOSCandidatesでは，(f,r)を共有する用例群中の被覆率が閾値以上の品詞候補を選ぶ．ただし，「普通名詞」，「サ変名詞」，「ナ形容詞」は区別が明確でなく，また，「母音動詞」の「基本連用形」と「普通名詞」の区別は困難なため，これらの品詞のみが候補として残った場合には「普通名詞」を採用する．終了条件は，候補(f,r,p)を共有する用例群について，次の二つが満たされる場合とする．一つ目は前方境界の妥当性のチェックである．具体的には，句読点などの明らかな境界マーカーから前方境界が得られた候補の割合が閾値以上とする．例えば，未知語「新撰組」に対して，形態素解析が「新」を接頭辞と解釈するため，常に「撰組」が辞書項目の候補となる．選択アルゴリズムは短い候補を優先するので，「新撰組」よりも先に「撰組」が調べられる．しかし，「撰組」の直前に句読点等が来る用例はないので，「撰組」は獲得されない．二つ目は活用型の異なり数が閾値以上という条件である．これにより，品詞が偶発的に選択されたのではなく，実際に該当品詞として使われていることを確認する．</subsection>
  <subsection title="品詞分類手法の比較">品詞分類について先行研究との簡単な比較を示す．Moriet~al.の後ろの「文字列」と福島・鍜治らの「後続するひらがなn-gram」，および桑江らの「最長後続ひらがな列」は，本論文のサフィックスと同様の働きをする~．Moriet~al.は前後の文字列とその頻度をベクトルで表現し，語幹候補と品詞モデルとのベクトル間の距離の近さにより品詞を判定している．しかし，同じ品詞に属す形態素が本当に似たベクトルを取るのだろうか．直観的には，品詞は大雑把な分類であり，同じ品詞に属す形態素でも振る舞いにばらつきがありそうに思われる．そこで，ウェブコーパスを対象に簡単な実験を行った．まず，コーパスの形態素解析結果から既知語に後続するサフィックスを収集する．次に，サフィックスを各形態素ごとに集約し，形態素ごとの後続サフィックスの頻度分布を求める．同様にして，形態素が属す品詞ごとに，後続サフィックスの頻度分布を求める．そして，各形態素と品詞との間で，後続サフィックスの頻度分布の近さを求める．ただし，頻度分布の近さの尺度としてSkewdivergences_を用いる~．s_(q,r)&amp;=D_KL(r||q+(1-)r),D_KL(q||r)&amp;=_yq(y)(q(y)-r(y))align*ここで，q，rはサフィックスの頻度分布とし，=0.99とする．図に，「子音動詞ラ行」の例を示す．横軸は「子音動詞ラ行」の各形態素の絶対頻度を表し，縦軸は各形態素の，「母音動詞」との近さと「子音動詞ラ行」との近さとの「差」を表す．低頻度区間では，二つの近さの差が小さく，近さによる品詞判定では識別が難しいと予想される形態素が目立つ．それだけでなく，高頻度区間でも差が小さい形態素が散見される．従って，出現頻度が大きくても，近さによる品詞判定が難しいと予想される場合が存在する．福島・鍜治らは品詞識別にSVMを用い，素性として後続するひらがなn-gramを与える．素性の値に福島らは頻度，鍜治らは出現したか否かの2値を使う．SVMは識別器であり，品詞内の近さよりも品詞間の差異を学習すると期待される．一方，提案手法は，サフィックスの頻度には注目せず，個々のサフィックスを品詞リストに写像する．サフィックスは形態論的制約を満たすか否かの2値を表現しており，候補列挙の時点で，制約を満たさない品詞は候補から除外される．このように，品詞の絞り込みが各用例に対して行われるので，単純に多くの用例を説明できる候補を選ぶだけで品詞分類が行える．また，提案手法は一つの語幹に対応する品詞は一つという仮定を置いている．これに対し，Moriet~al.と桑江らは，「楽し-い」と「楽し-む」のように，一つの語幹が複数の品詞に属す可能性を明示的にモデル化している．しかし，「楽し-い」と「楽し-む」のような派生関係にある形態素の品詞の衝突は，基本語彙が登録済みのため，極めてまれと推測される．無関係な形態素同士の偶発的な衝突については，提案手法はテキストを逐次的に解析するため，同一ドメインのテキストを読んでいる場合，特に起きにくいと推測される．</subsection>
  <subsection title="獲得未知語の分割可能性">獲得された未知語が実際には2個以上の形態素からなる可能性がある．未知語は比較的少数の用例から獲得するため，未知語Bが，観測された用例中でたまたまABという連続で現れていた場合，ABを1形態素として獲得してしまう．例えば，複合語「顆粒タイプ」が未知語「顆粒」よりも先に獲得されるかもしれない．この問題に対処するために，未知語獲得時に，獲得済みの形態素が獲得形態素によって分割できるかを調べ，できる場合にはその形態素を辞書から削除する．現在のところ，分割可能性の検査には形態素解析器を用いる．これにより形態素解析器に記述された制約知識を利用する．まず，分割対象形態素の候補列挙は単純な文字列マッチングにより行う．次に，候補を一時的に辞書から取り除いた状態で，その候補の形態素解析を行い，獲得形態素によって分割されなかった場合に候補を辞書に戻す．</subsection>
  <section title="実験"/>
  <subsection title="実験設定">オンライン未知語獲得について，獲得される未知語の精度，および未知語獲得の形態素解析への貢献を評価する．基本語彙辞書として，形態素解析器JUMANのデフォルトの辞書を用いる．この辞書は約3万の基本語彙を収録している．表記ゆれを展開し，固有名詞を含めれば，語彙数は約12万となる．獲得対象テキストとして，ドメインが限定されたコーパスを用いる．話題を共有するテキストの方が，互いに無関係なテキストよりも未知語が集中的に出現すると期待されるからである．実験では，検索エンジン基盤TSUBAKI~を用い，その検索結果をドメイン限定コーパスとみなす．各クエリに対して，システムは検索結果のページを順に読み，未知語を獲得する．獲得は千ページ目で打ち切り，同じ千ページを拡張された語彙を用いて再解析する．クエリとしては，「捕鯨問題」，「赤ちゃんポスト」，「ジャスラック」，「ツンデレ」，および「アガリクス」を使用する．獲得された未知語は，語幹と品詞の両方が正しい場合に正解とする．ただし，章で述べたように，語幹の単位認定は難しい．実際，Nagataと内元らは，単位認定の不一致が報告されたエラーの原因の一つとみなしている~．単位認定の不一致を回避するために，正解コーパスとの単純比較ではなく，人手による判定を採用する．未知語獲得の形態素解析への貢献の評価は次の手順で行う．獲得対象テキストを基本語彙辞書と拡張された辞書の2通りで形態素解析する．二つの解析結果を比較して，図のように単語分割の境界が一致しない箇所を抽出する．これを``diff''ブロックとよぶ．``diff''ブロックの正誤判定は，形態素への分割と，分割および品詞割り当ての2通りにより行う．ただし，形態素境界は，明らかに誤っていない場合に正解とする．評価には，クエリごとに，再解析により解析結果が変化した文の中から無作為に抽出した50文を用いる．品詞の評価については，「普通名詞」と「サ変名詞」という名詞の「品詞細分類」を区別しない．また，JUMANが未知語に与える特殊な品詞「未定義語」は名詞とみなす．</subsection>
  <subsection title="実験結果">表~にクエリごとの統計を示す．再解析により変化した文の割合に大きなばらつきがある(0.43--9.26%)．基本語彙辞書はこれまで新聞記事を対象に整備されてきたため，新聞記事と似ていないドメインほど未知語獲得の効果が大きい傾向がみられる．獲得された未知語の精度は97.3--98.5%と高い．しかも，獲得時点で利用した用例数の中央値は4--7に過ぎない．先行研究では出現回数が10回未満の候補を信用できないとして無視していたことを考えると非常に小さな値である~．図に，獲得された未知語の頻度とその頻度の順位との関係を示す．ここで，頻度は，拡張された辞書を用いた再解析結果から数えたものである．順位の下位区間における急な落ち込みは，用例数の不足により獲得されていない未知語の影響と推測される．図に，獲得の経過を示す．ここで，獲得未知語の累積出現数は，拡張された辞書を用いた再解析結果から数えたものである．終了時点での蓄積されている用例数と未知語の累積出現数の比較から，検出された未知語用例がすべて真の未知語と仮定すると，提案手法で検出される未知語のうちおよそ半分が獲得されたと推定できる．表~に獲得された未知語の例を示す．予想される通り，獲得された未知語の大半が名詞(94.1--100%)やカタカナのみからなる形態素(67.9--79.4%)である．「タイーホ」や「ぱくる」など新聞記事にはあまり見られない俗語も獲得されている．字種が混在する「ドジっ娘」や「シャ乱Q」は字種に基づく形態素解析の未知語処理では正しく解析できない．「すごい」に対する「スゴい」，「解かる」に対する「解る」のように，登録済みの形態素の異表記もあった．誤り例には，「パクられる」や「フラグが立つ」など明らかに構成的な表現を1形態素と認識しているものがある．ただし，これらは，さらに未知語獲得を進めて，それぞれ「パクる」や「フラグ」が獲得された場合，分割可能性のチェックにより消される．他には，副詞の「やっぱ」が名詞と誤認識された．表~に``diff''ブロックの評価結果を示す．ほとんどのブロックが拡張された語彙によって正しく解析されている（ECおよびCC）．一方，獲得による副作用は限定されている(CE)．従って，獲得された未知語が形態素解析の精度を改善することが示された．</subsection>
  <subsection title="議論">形態素解析において，カタカナ未知語が短いカタカナ形態素によって分割されることがある．例えば，基本語彙辞書のみを用いると，未知語「アブラハム」は「アブラ」と「ハム」に過分割される．「アブラハム」は，節で述べた単純な検出手法では検出されず，従って獲得もされない．また，未知語の獲得によって新たな過分割が発生し得る．例えば，「サー」の獲得によって，「サーバー」が「サー」と既知の「バー」によって過分割されるようになる．このような過分割の問題は，本論文が利用した形態レベルの文法的振る舞いだけを調べても解決できない．他の手がかり，例えば，「サーバー」がserverという一つの外来語だから分割できないといった知識が必要となる．提案手法では，カタカナ「イイ」のように語尾までカタカナで表記された用言は誤って名詞と認識される．現在の形態素解析は語尾のひらがな表記を前提としている．この仮定は新聞記事に対しては妥当だが，ウェブテキストに対しては無効であり，より柔軟な解析が必要になる．ただし，こうした未知語の解析は元々誤っており，獲得によって形態素解析が悪化するわけではない．未知語問題への2通りの解決策のうち，未知語モデルによる手法は，その利点として，低頻度語の正しい同定が強調されている~．しかし，ウェブの出現により，ほとんど無尽蔵のテキストが入手できるようになった現在，限られた情報のみを用いた同定は不可欠ではない．仮に，解析対象のテキストが少量で，未知語獲得を行うには用例の出現回数が足りないとしても，ウェブから解析対象テキストと関連するテキストを収集することで，用例の出現回数を増やすことができる．ここで，バッチ処理~と異なる，オンライン獲得という特徴を生かせる．すなわち，解析対象テキストから検出された用例にマークして，それらの用例が獲得に使われたか追跡することで，未知語が十分に獲得された時点で処理を停止させることができる．最後に，残された課題を整理する．章で整理したように，形態素に付与される様々な情報のうち，本論文はひとまず形態レベルの情報の獲得を目指した．形態レベルの手がかりでは得られない知識としては，名詞と副詞の区別の他に，固有名詞と普通名詞の区別などがある．特に名詞の細分類は固有表現認識や省略・照応解析に役立つと期待されるので，テキストからの自動獲得を目指したい．本論文では形態素の単位認定にこだわらなかったが，獲得された未知語の中には構成的なものが含まれている．参考までに，クエリ「ジャスラック」の獲得結果を調べたところ，判断に迷う場合を含めると10%弱(45/460)が複合語であった．ただし，複合語の基準としては，JUMAN4.0から5.0への変更時に行った複合語の整理を参考にした．日本語の複合名詞は，文法的なマーカなしに構成要素が直接連結されるため，形態レベルの手がかりでは構成要素に分割できない．細粒度での単位認定を実現するには，他の手がかりを利用する必要がある．形態レベルでの未知語獲得については，検出が大きな課題として残っている．なかでもひらがな表記の未知語は曖昧性が高く，形態素解析器によってより短い既知の形態素へ過分割されることが少なくない．予備調査として，形態素解析結果のうち，1+1，1+2，2+1文字というパターンのひらがな形態素のペアのみを対象に，未知語検出の再現率を求めたところ，本論文の手法では31%にとどまることが判明している．ひらがな表記の未知語は数の上では少なく，頻度の上でも異なり数でも，未知語の大半をカタカナ名詞が占める．しかし，カタカナ名詞は形態素解析の未知語処理でほぼ問題なく同定できるのに対し，ひらがな未知語の解析誤りは応用に大きな悪影響を及ぼしやすい．例えば，「ようつべ」が「よ」，「うつ」，「べ」に誤って分解され，「うつ」が動詞と解釈された場合，文節まとめあげにより「よ」と「うつ」，「うつ」と「べ」の間に文節境界が引かれ，これに基づき見当違いな係り受け解析が行われてしまう．提案手法の利点の一つは，既知の形態素を改めて獲得しないことによる効率の良さだが，今後はこの利点を維持しつつ検出の再現率を上げていきたい．</subsection>
  <section title="関連研究">形態素解析における未知語の問題は，言語の類型論的特徴や文字の性質に依存する部分が少なくない．フィン語やトルコ語は日本語と同様の膠着語で，自立語に複数の付属語が後続して語を形成する．ただし，これらの言語は分かち書きするため，形態素解析は，分かち書きの単位である語を形態素に分割するタスクとなる．例えば，MorphoChallengeでは，頻度つきの語のリストから教師なしで形態素を切り出すタスクが競われている(Kurimo,Creutz,Varjokallio,Arisoy,andSaraclar2006;Kurimo,Creutz,andTurunen2007)．日本語と同様に分かち書きしない言語としては，中国語やタイ語などがあるが，いずれも分析的であり，膠着語の日本語とは性質が異なる．Penget~al.は中国語の単語分割に新語検出を組み込む~．この手法では，テキストを一度解析した結果から単語分割の信頼度を元に新語を検出し，それらを素性に組み込んだ状態で再解析を行う．日本語については，未知語モデルを導入する手法がいくつか提案されている．Nagataは，字種や単語長に基づく生成的な未知語モデルを単語分割に組み込む~．内元らは最大エントロピーモデルに基づく形態素解析の中で，字種やその遷移などの未知語同定に有効な情報を素性として利用する~．東らは最大エントロピーモデルに代えて条件付き確率場を採用する~．Asaharaet~al.は文字レベルのチャンキングにより未知語を同定しており，学習器としてSupportVectorMachineを用いる~．中川らは，単語分割されたテキストに対して，未知語のすべての出現を考慮して品詞を推定する手法を提案している~．しかし，我々が想定するタスクでは，品詞推定と独立に単語分割が実現されるという仮定は現実的ではない．テキストからの未知語の自動獲得については，Moriet~al.は，語幹の前後の文字列とその頻度をベクトルで表現し，コーパス中の任意の部分文字列について，品詞のモデルとのベクトルの距離により品詞らしさを判定する~．鍜治らはカタカナ用言についてテキストからの自動獲得を行っている~．彼らは，獲得対象を「ググる」などの語幹が自明なカタカナの用言に限定し，品詞分類に特化した手法を提案する．一般の未知語を獲得する場合には，あわせて語幹同定の問題も解く必要がある．</section>
  <section title="結論">本論文では，オンライン未知語獲得という枠組みと，その具体的な実現方法を提案した．実験により，未知語が高精度に獲得され，その結果形態素解析の精度が向上することが示された．形態素解析自体は成熟した技術であり，構文解析や情報検索といった応用のための前処理となっている．従って，応用処理の精度向上に提案手法を利用したいと考えている．document</section>
</root>
