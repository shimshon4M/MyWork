<?xml version="1.0" ?>
<root>
  <jtitle>依存関係確率モデルを用いた統計的句アライメント</jtitle>
  <jauthor>中澤敏明黒橋禎夫</jauthor>
  <jabstract>語順や言語構造の大きく異なる言語対間の対訳文をアライメントする際に最も重要なことは，言語の構造情報を利用することと，一対多もしくは多対多の対応が生成できることである．本論文では両言語文の依存構造木上での単語や句の依存関係をモデル化した新しい句アライメント手法を提案する．依存関係モデルは木構造上でのreorderingモデルということができ，非局所的な語順変化を正確に扱うことができる．これは文を単語列として扱う既存の単語アライメント手法にはない利点である．また提案モデルはヒューリスティックなルールを一切用いずに，句となるべき単位の推定を自動的に行うことができる．アライメント実験では，既存の単語アライメント手法と比較して，提案手法にではアライメントの精度をF値で8.5ポイント向上させることができた．</jabstract>
  <jkeywords>機械翻訳，アライメント，木構造，依存関係確率</jkeywords>
  <subsection title="">*初期アライメント(E-step)依存関係確率は用いず，句翻訳確率のみから初期アライメントを生成する．全ての句候補同士の対応（もしくはNULL対応）に対して，句対応確率を式により計算する．これらの中から，句対応確率の相乗平均が高いものから順に，対応として採用する．この際，各単語は1度しか対応付かないようにする．つまりすでに採用されている対応と重なるような対応は採用しない．なお句候補の生成については後で述べる．初期アライメントが生成されたら，その状態でのアライメント確率を計算する．このときから依存関係確率も用い，式のように計算する．</subsection>
  <section title="はじめに">日本語と英語のように言語構造が著しく異なり，語順変化が大きな言語対において，対訳文をアライメントする際に重要なことは二つある．一つは構文解析や依存構造解析などの言語情報をアライメントに組み込み，語順変化を克服することであり，もう一つはアライメントの手法が1対1の単語対応だけでなく，1対多や多対多などの句対応を生成できることである．これは一方の言語では1語で表現されているものが，他方では2語以上で表現されることが少なくないからである．しかしながら，既存のアライメント手法の多くは文を単純に単語列としてしか扱っておらず，句対応は単語対応を行った後にヒューリスティックなルールにより生成するといった方法を取っている．QuirkらやCowanらはアライメントに構造情報を統合しようとしたが，前述の単語列アライメントを行った後に用いるに留まっている．単語列アライメント手法そのものの精度が高くないため，このような方法では十分な精度でアライメントが行えるとは言い難い．一方で，アライメントの最初から構造情報を利用する手法もいくつか提案されている．Wata-nabeらやMenezesとRichardsonは構文解析結果を利用したアライメント手法を提案しているが，対応の曖昧性解消の際にヒューリスティックなルールを用いている．YamadaとKnightやGildeaは木構造を利用した確率的なアライメント手法を提案している．これらの手法は一方の文の木構造に対して葉の並べ替え，部分木の挿入・削除といった操作を行って，他方の文構造を再現するものであるが，構文情報の利用が逆に強い制約となってしまい，文構造の再現が難しいことが問題となっている．YamadaとKnightはいったん木構造を崩すことによって，Gildeaは部分木を複製することによってこの問題に対処している．我々はこのような木構造に対する操作は不要であり，依存構造木中の部分木をそのままアライメントすればよいと考えた．またCherryとLinは原言語側の依存構造木を利用した識別モデルを提案している．しかしながらこの手法はアライメント単位が単語のみであり，一対一対応しか扱えないという欠点がある．phrase-basedSMTでいうところの“句”はただの単語列に過ぎないが，NakazawaとKurohashiは言語的な句をアライメントの最小単位とし，句の依存関係に着目したモデルを提案しているが，そこでは内容語は内容語のみ，機能語は機能語のみにしか対応しないという制約があり，また複数の機能語をひとまとまりに扱っているという問題もあり，これらがしばしば誤ったアライメントを生成している．本論文ではNakazawaとKurohashiの手法の問題点を改善し，単語や句の依存関係に注目した句アライメントモデルを提案する．提案手法のポイントは以下の3つである．両言語とも依存構造解析し，アライメントの最初から言語の構造情報を利用するアライメントの最小単位は単語だが，モデル学習時に句となるべき部分を自動的に推定し，句アライメントを行う各方向（原言語目的言語と目的言語原言語）の生成モデルを二つ同時に利用することにより，より高精度なアライメントを行う本モデルは二つの依存構造木において，一方の依存構造木で直接の親子関係にある一組の対応について，他方のそれぞれの対応先の依存関係をモデル化しており，単語列アライメントで扱うのが困難な距離の大きな語順変化にも対応することができる．言い替えれば，本モデルは木構造上でのreorderingモデルということができる．また本モデルはヒューリスティックなルールを用いずに，句となるべき部分を自動的に推定することができる．ここでいう句とは必ずしも言語的な句である必要はなく，任意の単語のまとまりである．ただし，Phrase-basedSMTにおける句の定義との重要な違いは，我々は木構造を扱っており，単語列としては連続でなくても，木構造上で連続ならば句として扱っているという点である．また我々のモデルはIBMモデルのような各方向の生成モデルを両方向分同時に用いてアライメントを行う．これはアライメントの良さを両方向から判断する方が自然であり，Liangらによる報告にもあるように，そうした方が精度よいアライメントが行えるからである．ただし，Liangらの手法がIBMモデルと同様に単語列を扱うものであるのに対し，提案手法は木構造を扱っているという重要な違いがある．またLiangらの手法では部分的に双方向のモデルを結合するに留まっており，アライメントの結果としては各方向それぞれ独立に生成されるが，我々の方法ではただ一つのアライメントを生成するという違いもある．最近の報告では生成モデルよりも識別モデルを用いた方がより高精度なアライメントが行えるという報告がなされているが，学習用にアライメントの正解セットを用意するコストがかかってしまう．そこで我々は教師なしでモデル学習が行える生成モデルを用いた．モデルは2つのステップを経て学習される．Step1では単語翻訳確率を学習し，Step2では句翻訳確率と依存関係確率が推定される．さらにStep2では単語対応が句対応に拡張される．各StepはEMアルゴリズムにより反復的に実行される．次章では我々の提案するアライメントモデルを，IBMモデルと比較しながら定義する．章ではモデルのトレーニングについて説明し，章では提案手法の有効性を示すために行った実験の結果と結果の考察を述べ，最後に結論と今後の課題を述べる．</section>
  <section title="提案モデル">以降の説明においては言語対として日本語と英語を用いるが，提案モデルはこの言語対に特別に設計されたものではなく，言語対によらないロバストなものである．提案モデルは依存構造木上で定義されるものであるので，まず対訳文を両言語とも依存構造解析し，単語の依存構造木に変換する．図の一番右に依存構造木の例を示す．単語は上から下に順に並んでおり，文のヘッドとなる単語は最も左側に位置している．アライメントの最小単位はこれら各単語であるが，モデル推定時に複数単語のかたまりを句として自動的に獲得する．これについては章で詳しく述べる．</section>
  <subsection title="提案モデル概観">本章では，広く知られており，かつ一般的に用いられている統計的なアライメント手法であるIBMモデルと比較しながら，我々が提案するモデルについて説明する．IBMモデルでは，与えられた日本語文fと英語文eからなる対訳文間の最も良いアライメントaは以下の式により獲得される：ここで，p(f|a,e)は語彙確率(lexiconprobability)と呼ばれ，p(a|e)はアライメント確率(alignmentprobability)と呼ばれている．fがn語(f_1,f_2,...,f_n)からなり，eがm語(e_1,e_2,...,e_m)とNULL(e_0)からなるとする．またアライメントaはfの各単語からeの単語への対応を表し，a_j=iはf_jがe_iに対応していることを示すとする．このような条件の下，上記二つの確率は以下のように展開される：p(f|a,e)=_j=1^Jp(f_j|e_a_j)p(a|e)=_i=1^Ip(j|e_i)gatherここでjはe_iに対応するfの単語の相対位置である．式は単語翻訳確率の積であり，式は相対位置確率の積となっている．ただし，ここで示した式は正確にIBMモデルを記述しているわけではなく，その意図を簡単に示したものである．また図の左側にIBMモデルによるアライメントの例を示す．IBMモデルは方向性があるため，アライメントに制限がある．これを解消するため，両方向による結果を最後に統合して最終的なアライメントとすることが多い．しかし日英のような言語構造の違いの大きい言語対においては，このような方法では十分な精度でのアライメントは行えない．提案モデルはIBMモデルを3つの点で改善する．一つ目は式において，単語ではなく句を考慮する．二つ目は式において，文中での単語の位置ではなく，依存関係を考慮する．最後に，提案モデルでは最も良いアライメントaを求める際に，片方向のモデルだけでなく，両方向のモデルを同時に利用する．つまり，式を以下のように変更する：我々のモデルでは句を扱っているため，上式を素直に計算できる．また式の上ではIBMモデルと同じ解が得られるはずであるが，それぞれの確率を近似するため，両方向を考慮した方がよりよい解が得られる．図の一番右に提案モデルによるアライメント例を示す．従来手法のアライメントと比べると，多対多対応が自然と獲得されていることがわかる．提案モデルはEMアルゴリズムにより学習される．目的関数として，与えられたデータに対する尤度を考える：この尤度を最大化するようなパラメータを求める．は各方向のモデルにおけるパラメータをまとめたものとする．E-stepでは現在のパラメータの下でのアライメントの事後確率を以下のように計算する：M-stepではパラメータの更新を行う：次節以降では，lexiconprobabilitiyとalignmentprobabilitiyを定義する．</subsection>
  <subsection title="句翻訳確率">fがN個の句(F_1,F_2,...,F_N)からなり，eがM個の句(E_1,E_2,...,E_M)とNULL(E_0)からなるとする．またアライメントA^feはfの各句からeの単句への対応を表し，A_j^fe=iは句F_jが句E_iに対応していることを示すとする．提案モデルでは，IBMモデルにおける単語翻訳確率p(f_j|e_i)の代わりに，句翻訳確率p(F_j|E_i)を考える．ただし，2語以上からなる句はNULL対応にはならないという制限を加える（その句に含まれる各単語がNULL対応になるものとする）．句翻訳確率を用いて，式を以下のように変更する：ここで，句F_jと句E_iが対応付いたと仮定すると，この句の対応に寄与する句翻訳確率は，双方向分の句翻訳確率を掛け合わせるため以下のようになる：この確率の積を句対応確率と呼ぶことにする．表の上部に図の例における句対応確率を示す．</subsection>
  <subsection title="依存関係確率">IBMモデルにおいて，単語の移動，すなわちreorderingモデルは，式に示したように，一つ前の単語のアライメントとの相対位置によって定義されている．これに対し提案モデルでは，単語の文内での位置ではなく，依存関係を考慮する．まずeのある単語e_pと，e_pに係る単語e_cについて考え，それらの可能なアライメントのうち，e_pが句E_Pに属し，e_cが句E_Cに属しており，E_CがE_Pに係っているものを考える．このような状況において，E_PとE_Cのfでの対応句F_A_P^efとF_A_C^efの関係をモデル化したものが依存関係確率である．図に例を示す．日英などのように語順の大きく異なる言語対であっても，文内の単語や句の依存関係は多くの場合保存され，F_A_C^efが直接F_A_P^efに係ることが多い．提案モデルはこのような傾向を考慮したものである．直接の親子関係にある2単語が属する2句の対応先の句の関係はrel(e_p,e_c)のように記述することにし，これはe_pが属する句の対応先の句F_A_P^efから，e_cが属する句の対応先の句F_A_C^efへの経路として定義される．経路は以下のような表記に従って示される：子ノードへ行く場合は`c'(childnode)親ノードへ行く場合は`p'(parentnode)2ノード以上離れている場合は，上記二つを並べて表記する例えば図において，``for''から``photodetector''への経路は`c'となり，``the''から``for''への経路は，2ノード離れているため`p;p'となる．句同士の依存関係を記述する際には，経路上にある全ての句は，2つ以上の単語からなる句も含めて，すべて1つのノードとして扱う．このため，図において``photogate''から``the''への経路は`p;c;c;c'となる．このrelを用いて，式を以下のように改善する：ここでD_e`-pcはeの木構造において直接の親子関係にある全ての単語の組み合わせである．またp_ef(rel(e_p,e_c))をef方向の依存関係確率と呼ぶ．p_efは木構造上でのreorderingモデルと考えることができる．relにはいくつか特別な値がある．まずE_CとE_Pが同じ場合，つまり，e_cとe_pが同じ句に属する場合，rel=となる．次にNULLアライメントに関してだが，これにはe_pがNULL対応の場合，e_cがNULL対応の場合，両方ともNULL対応の場合の3通りがあり，それぞれrelの値は`NULL_p'，`NULL_c'，`NULL_b'となる．例として表の下部に図の例における依存関係確率を各方向それぞれ示す．一般的に，構文解析などにおいても，親ノードとの関係だけでなく，さらにその親のノードとの関係を考慮することは自然であり，精度の向上につながる．提案モデルにおいても，直接の親子関係だけでなく，さらにその親ノードとの関係も考慮し，以下のように定式化する：ここでD_e`-gcはeの木構造において祖父と子の関係にある全ての単語の組み合わせである．p_ef`-pcは直接の親子関係にある2単語を見たときの依存関係確率であり，p_ef`-gcは親の親と子の関係にある2単語の場合の依存関係確率である．なお，逆方向（fからe）のモデルp(a|f)も全く同様に定義される．</subsection>
  <section title="トレーニング">提案モデルは2つのステップに分けて学習される．これはIBMモデルにおいて，完全に最適解が求まる簡単なモデルからスタートし，徐々により複雑なモデルに移行することに対応する．Step1では単語翻訳確率の推定が行われ，Step2では句翻訳確率と依存関係確率の推定が行われる．どちらのステップにおいてもモデルはEMアルゴリズムにより学習される．またステップ1においては句は扱わず，全て単語単位での学習となる．複数単語の塊＝句はStep2において自動的に獲得される．</section>
  <subsection title="Step 1">Step1では各方向独立に，単語翻訳確率を推定する．これはIBMModel1と全く同様の方法により行われる．Step1の推定の際には対応の単位は各ノード単体，つまり単語のみであり，句は考慮しない．句はStep2の推定から考慮し，句となるべき候補を動的に作り出すことにより実現する．これはStep1の段階で可能な句の候補全てを考慮すると，アライメント候補数が爆発し，扱えなくなるためである．fからeへのアライメントを考えると，fの各単語は，他の単語に関係なく，eの任意の単語，またはNULLに対応することができる．このことから，あるひとつの可能なアライメントaの確率は以下のように計算できる：p(a,f|e)&amp;=p(f|a,e)p(a|e)&amp;=_j=1^Jp(f_j|e_a_j)C(n,m)alignここでp(a|e)は全てのアライメントにおいて一定(uniform)であるとし，各文の単語数による関数C(n,m)と置く．さらに，全ての可能なアライメントを考慮すると，確率p(f|e)は以下のように計算できる．単語翻訳確率の初期値として一様な確率を与えておき，式とを計算して，正規化したアライメント回数p(a,f|e)p(f|e)をアライメントa内の全ての単語対応に与える．次に単語翻訳確率を最尤推定により求める．これを繰り返すことにより，単語翻訳確率を推定する．なおこの計算は効率的に行うことができ，近似することなく最適なパラメータが求められる．反対方向（eからfへのモデル）も同様に求めることができる．</subsection>
  <subsection title="Step 2">Step2では句翻訳確率と依存関係確率の両方を推定する．またfからe，eからfの二つのモデルを同時に用いて，一つの方向性のないアライメントを得る．Step1では計算を効率化することにより，近似を用いずにモデルの推定が完全に行えるが，Step2では可能なアライメントを全て考慮することは不可能である．そこで我々は最も良いアライメントを探索するために，まず句翻訳確率のみから初期アライメントを生成し，その後依存関係確率も考慮しつつ，山登り法によってアライメントを徐々に修正するという方法をとる．さらにStep2において新たな句候補の生成を行う．新たな句候補は山登り法によって求められた最も良いアライメントの状態から生成され，次のイタレーションから考慮される．つまり，Step2のイタレーションが進むに連れ，より大きな句の対応を発見することができる．全体として，Step2の1回のイタレーションは，E-stepでの“初期アライメント”の生成と“山登り法”により最適なアライメントの探索，E-stepとM-stepの間での新たな句候補の生成，M-stepでのパラメータの更新の4つの要素からなる．Step2での一回目のイタレーションでは，パラメータの初期値を以下のようにする．一回目のイタレーションにおいては全ての句は1単語からなるため（2単語以上からなる句候補が獲得されていないため）句翻訳確率については，Step1で求めた単語翻訳確率をそのまま用いる．依存関係確率は，Step1の最後のイタレーションで得られた最も良いアライメント結果において依存関係の生起回数を計数し，そこから求めた確率を用いる．</subsection>
  <section title="アライメント実験">提案手法の有効性を示すためにアライメント実験を行った．トレーニングコーパスとしてJST日英抄録コーパスを用いた．このコーパスは，科学技術振興機構所有の約200万件の日英抄録から，内山・井佐原の方法により，情報通信研究機構が作成したものであり，100万対訳文からなる．このうち475文に人手で正解のアライメントを付与し，正解データとした．ただし，正解データにはSure(S)アライメントのみが付与されており，Possible(P)アライメントはない．また評価の単位は日本語，英語とも単語とし，適合率・再現率・F値により精度を求めた．日本語文に対しては形態素解析器JUMANおよび依存構造解析器KNPを用い，英語文に対してはTsuruokaとTsujiiのPOSタガーでPOSタグを付与し，MSTパーサを用いて単語の依存構造木に変換する．またStep2のパラメータ推定の際に用いるアライメントの数はn=10とした．実験は2種類行った．一つ目は既存の単語列アライメント手法と比較することによって提案手法の有効性を示すための実験であり，二つ目は依存構造を利用することと，単語より大きな単位である句を扱うことの効果を示すための実験である．全ての実験において，各単語は原形に戻した状態でトレーニングを行った．</section>
  <subsection title="単語列アライメント手法との比較">比較実験として，単語列アライメント手法として広く利用されているIBMモデルを実装したアライメントツールであるGIZA++を用いてアライメントを行った．各モデルのイタレーション回数などのオプションはデフォルトの設定をそのまま利用した．さらに各方向のアライメント結果を三つの対称化手法により統合した．結果を表の下部3行に示す．利用した対称化手法は`intersection'，`grow-final-and'，`grow-diag-final-and'の3つである．一方，提案手法によるアライメント精度を表の上部に示す．まず`Step1'に示されているのは，Step1のイタレーションを5回行った後に学習されたパラメータ（単語翻訳確率）を用いたアライメントの精度である．なおここでのアライメントは，両方向のパラメータを用いて，章の初期アライメント生成手法と同様にアライメントを生成した結果である．`Step2-X'はStep2の各イタレーション終了時点でのアライメント精度である．`Step2-1'は句翻訳確率は`Step1'のものと同じだが，それに加えて`Step1'のアライメント結果から推定した依存関係確率を用いてアライメントを行っている．つまり，`Step1'と`Step2-1'とを比較することにより，依存関係確率を用いることによるアライメント精度の向上が見て取れる．以後Step2のイタレーションを行い，その都度アライメント精度を計測した．結果として，提案手法では単語列アライメント手法よりもF値で4.9ポイントのアライメント精度向上を達成した（Step2-7とgrow-diag-final-andとの比較による）．適合率だけを見ると`intersection'が最もよい値を示しているが再現率が極端に低くなっている．また再現率が最も高いのは`grow-diag-final-and'であるが，同程度の再現率を示している提案手法の結果を見ると，適合率では大きく上回っており，総合的に見て提案手法は単語列アライメント手法よりも優れているということができる．なおF値はStep2-7が最も高い値を示したが，Step2-5から2-7までは大差ないことと，RecallよりもPrecisionが高い方が翻訳での利用を考えた際には有利であり，イタレーションが進むに連れPrecisionが低下していくことを考慮して，Step2-5の結果に注目することにする．次に，機能語に関する簡単なルールを人手により作成し，最終的なアライメント結果の修正を行った．用いたルールは以下の3つである：英語の冠詞はその係り先のノード（普通は名詞）に併合する日本語の助詞と英語の`be'や`have'との間に対応がある場合，それらは棄却する日本語の‘する’，‘れる’，英語の`be'，`have'がNULLに対応している場合，その係り先の動詞や形容詞のノードに併合するこれらのルールをStep2-5の結果に適用することにより，F値は70.76に向上し，単語列アライメントよりも8.5ポイント高いF値を達成した（表のStep2-5+rule）．なお，以後の考察ではルールなしのStep2-5の結果を検討する．</subsection>
  <subsection title="依存構造と句を扱うことの有効性">依存構造木を用いることと，単語より大きな句という単位を用いることの有効性を示すための実験を行った．実験の条件として以下の4通りを採用した：依存構造木と句のどちらも用いる（結果の‘提案手法’）依存構造木のみを利用し，句は用いない依存構造木は利用せず，句のみを用いる依存構造木と句のどちらも利用しない（結果の‘ベースライン’）なお依存構造木を用いない実験においては，単語の依存関係ではなく相対位置の情報を用いた．例えば原言語側で連続している一組の対応のそれぞれの対応先が前，又は後ろに何単語（もしくは句）離れているかをモデル化した．実験結果を表に示す．全ての実験条件において，示した結果はStep2で5回イタレーションを行った後のアライメント結果での評価である．この結果から，句を扱うことは再現率の向上につながり，依存構造木を利用することは適合率の向上につながることがわかり，両方を用いることにより，適合率・再現率ともにバランスよく高い精度を達成することができると言える．</subsection>
  <section title="考察">章（表）から，単純な単語列アライメントモデルと比較して，提案モデルが十分に高精度なアライメントを行えていることがわかる．図および図で二つの手法のアライメント結果の比較を示す．灰色に塗られたマスはアライメントの正解であり，黒い四角（■）がある部分が出力である．図は単語列アライメントの結果の例である．文内に“非去勢マウス”と“去勢マウス”，``non-castratedmice''と``castratedmice''というように，同じ語が複数回出現しており，アライメントに曖昧性があるが，単語列アライメントモデルはこの曖昧性解消に失敗している．これは文を単純な単語列として見た場合，曖昧性を持つ語同士が互いに近くに位置しており，さらに曖昧性解消の手がかりとなる“同様にas”といった対応ともほぼ等距離にあるためである．一方で図に示すように，提案モデルではこれらの語を正しく対応付けることができており，文を木構造で見た場合の利点が生かされている．例えば英語側の木構造において，``as''に係っているのは``castratedmice''ではなく``non-castratedmice''であり，同様に日本語側の木構造においても，“同様に”に係っているのは“去勢マウス”ではなく“非去勢マウス”である．このような関係から，“非去勢マウスnon-castratedmice”，“去勢マウスcastratedmice”という正しい対応関係が獲得されている．章で述べたように，IBMモデルに代表されるような文を単純な単語列として扱う既存の統計的単語列アライメントモデルは，英語とフランス語などのように語順がほぼ同じであり，語順変化がある場合でも局所的である言語対においては十分頑健に働くが，語順が大きく変化する言語対においてはその精度に問題がある．例えば日本語と英語について考えてみると，日本語の文はSOVの語順であるのに対し，英語ではSVOの語順であり，このため語順の変化が大きくなりやすい．このような言語対においては言語の構造情報を利用することが自然であり，また有効であることが本実験により示されている．句を扱うことによる改善例としては，図において単語列アライメントモデルでは“受光素子photodetector”という句対応の獲得に失敗しているのに対し，図に示すように提案手法では正しく獲得されている．単語レベルでの対応を後から重ね合わせる手法では，どこまでが句となるべきかの境界判定ができないなどの欠点があり，このような句対応を精度良く発見することは難しい．これに対し提案手法ではパラメータの学習と同時に句を獲得することができる上に，木構造を利用しているため，単語列としては連続であっても意味上不連続であり，句となるべきではないといった境界判定が自然に行える．表に，得られた対応を大きさごとに計数した結果を示す（単語列アライメント手法はgrow-diag-final-and，提案手法はStep2の5回目のイタレーションの結果）．なお提案手法ではイタレーションが進むにつれて，1ずつ句の大きさが大きくなる．このため，5回目のイタレーションにおいては日英の句の合計が6の対応が最大となる．結果を見ると，単語列アライメント手法に比べて提案手法では得られた対応の個数がサイズがより大きなものへとシフトしていることが見て取れ，より大きなサイズの対応が獲得されていることがわかる．これが再現率の向上に大きく貢献しているといえる．さらにイタレーションごとの各言語のNULL対応ノード数と，平均フレーズサイズの推移を図に示す．平均フレーズサイズは，例えばある一つの対応に含まれる日本語の単語がj語，英語の単語がi語ならば(i+j)/2とした．ある程度までは平均フレーズサイズは上昇するが，6回目程度からはそれほど大きくは変化しておらず，アライメントが安定していることがわかる．翻訳での利用を考えた場合，適合率は高ければ高いほどもちろん翻訳の精度が向上すると考えられる．しかしながら再現率が低いと，例えばPhrase-basedSMTやHieroなどにおいてはフレーズテーブルのサイズが大きくなりすぎるという問題が起こり，用例ベース翻訳システムにおいては利用可能な用例の数が減ってしまうなど，再現率もおざなりにはできない．一方で再現率のみが高く，適合率が低くてもやはり質の良い翻訳は行えない．つまり両者のバランスを取り，どちらも向上させることが，翻訳の質の向上につながるはずであり，章で示したように，提案手法はこの観点からも有効であると言える．実際に提案手法によるアライメントによって翻訳精度が向上するかの調査は今後の課題である．提案手法におけるアライメント誤りの要因で最も大きいものは，構文解析の誤りによるものである．提案手法は構文解析結果に強く依存しており，構文解析が誤っていると容易にアライメントの誤りにつながってしまう．長期的には各言語の構文解析の精度が向上していくことも十分期待できるが，構文解析結果を修正しつつアライメントすることも考えられる．また山登り法によるアライメントの探索の際に局所解に陥ってしまうという問題もしばしば見受けられた．これはほとんどの場合，一方の言語で1文内に同じ語や句が複数回出現しているが，他方では省略されて1度しか出現していないなど，出現回数に差がある場合に起こる．初期アライメント生成時には周りのノードとの関係は一切見ていないため，このような省略がある場合にはどちらが正しい対応かを判断することができないため，ランダムにどちらかが選ばれる．このとき運悪く誤った方を選択してしまい，さらにその周囲に誤った対応がいくつかあると，お互いに足を引っ張り合い，局所解に陥ってしまう．このように，提案手法では必ずしも最もよいアライメントが得られるとは限らない．この問題を解決するためには，山登り法の初期値を複数用意しておき，探索を複数回行うといった方法を取ったり，アライメントの探索アルゴリズムをよりよいものに改良する必要があり，例えばBeliefPropagationを利用することなどが考えられる．さらに，機能語をどのように扱うかといった難しい問題も残されている．機能語は明確に対応する語を持たないことがしばしばある．例えば日本語における格助詞などや英語における冠詞などはその典型的な例である．このような語に対しては，アライメントの正解の基準と出力とが整合的でない場合が多く，これがアライメント精度の向上の障害になってしまう．章の最後に示したように，提案手法では簡単なルールを用いるだけで大幅な精度の向上を達成できる．これは木構造を利用していることの利点であるといえる．</section>
  <section title="結論">本稿では依存関係確率モデルを用いた統計的句アライメント手法を提案した．提案モデルは木構造上でのreorderingモデルということができ，シンプルなモデルながらも言語構造の違いを柔軟に吸収し，精度の高いアライメントを実現できた．実験結果から，語順の大きく異なる言語対に対しては既存の単語列アライメント手法では十分な精度を達成することは困難であり，構文解析などの言語情報を利用することが自然であり，高い効果を示すことが証明された．今回は日本語と英語間のアライメント実験のみしか行わなかったが，同様に語順に大きな違いのある日本語と中国語間での実験などを行い，提案手法が言語対によらずロバストな手法であることを示す必要がある．考察にも述べたとおり，提案手法は依存構造解析に大きく依存しており，依存構造解析誤りが容易にアライメントの誤りにつながってしまう．両言語の解析結果を照らしあわせて，文構造を修正しつつアライメントすることも可能なはずであり，現在検討中である．これが実現できれば，依存構造解析とアライメント双方の精度向上が可能となると考える．アライメントの精度のみを評価したが，この結果が翻訳の精度にどのように影響するかを調査することは今後の課題である．document</section>
</root>
