<?xml version="1.0" ?>
<root>
  <section title="Introduction">Data-drivenmachinetranslationsystemssuchasEBMTandSMTlearnhowtotranslatebyanalyzingalignedbilingualcorpora.Ingeneral,themoredataavailablethehigherthequalityofthetranslation.Unfortunately,therearelimitstohowmuchbilingualdataexists.Inthispaper,weproposeamethodforincreasingtheamountofparalleltextavailablefortrainingbyusingaprecise,wide-coveragegrammartoparaphrasethetextinonelanguage.Thenoveltyinthisworkisthatweareusingahand-craftedgrammartoproducetheparaphrases,thusaddingacompletelynewsourceofknowledgetothissystem.Theparaphrasesarebothmeaning-preservingandgrammatical,andthusarequiterestricted.Possiblechangesinclude:changesinwordorder(KimsometimesgoesKimgoessometimes),lexicalsubstitution(everyoneeverybody),contractions(goingtogonna)andalimitednumberofcorrections(thethe,,the).Wegiveanexampleofparaphrasingin().Thegrammartreatsallofthesesentencesassemanticallyequivalent.このことから、会社には事故の責任が無いことになる。Itfollowsfromthisthatthecompanyisnotresponsiblefortheaccident.(=original)Itfollowsthatthecompanyisn'tresponsiblefortheaccidentfromthis.Itfollowsthatthecompanyisnotresponsiblefortheaccidentfromthis.Thatthecompanyisn'tresponsiblefortheaccidentfollowsfromthis.xlistexeWenextintroducesomerelatedwork,thentheresourcesweuseinthispaper.Thisisfollowedbyadescriptionofthemethodandtheevaluation.Finally,wediscusstheresultsandourfutureresearchplans.</section>
  <section title="Related Work">ApproachestoapplyingparaphrasinginMTcanberoughlyclassifiedinto(1)paraphrasingtoexpandamachinetranslationsystem'scoverage,(2)paraphrasingtoincreasetheamountoftrainingordevelopmentdata,and(3)paraphrasingtoincreasethesimilaritybetweenthesourceandtargetlanguages.Inthissection,wediscussrepresentativeworksineachgroupandcomparethemwithourproposedapproach.Welimitdiscussiontoofparaphrasingtomachinetranslation,excludinggeneraldiscussionofmethodsofacquiringparaphrases.</section>
  <subsection title="Paraphrasing to expand translation coverage">Callison-Burch:Koehn:Osborne:2006useparaphrasestoincreasethecoverageofunknownsourcelanguagewordsinanSMTsystem.Theyautomaticallyacquiresourcelanguageparaphrasesfromaparallelcorpusbyusingtargetlanguagephrasesaspivots.Anexampleofthisapproachisgivenbelow.ByusingtheGermanphraseunterkontrolleasapivot,theEnglishphraseundercontrolcanbeparaphrasedasincheck.whatismore,therelevantcostdynamiciscompletelyundercontrolim&quot;ubrigenistdiediesbez&quot;uglichekostenentwicklungv&quot;olligunterkontrollewirsindesdensteuerzahlerndiekostenschuldigunterkontrollezuhabenweoweittothetaxpayerstokeepthecostsincheckexeNewtranslationsareconstructedbyidentifyingsourcelanguageunknownwords,findingparaphrasesoftheunknownwordsinthesystem'sphrasetable,andaddingnewtranslationpairsconsistingoftheunknownwordandthetranslationofitsparaphrase.Newentriestothephrasetablearegiventheoriginaltranslationprobabilitiesmultipliedbytheprobabilityofthesourcelanguageparaphrasesusedintheirconstruction.Callison-Burch:Koehn:Osborne:2006showedimprovementsforsparsedatasetsforSpanishEnglishandFrenchEnglishsystemsconstructedontheEuroparlCorpus.Marton:Callison-Burch:Resnik:2009alsouseparaphrasestoexpandanSMTsystem'sphrasetable,buttheyusesemanticsimilaritydistributionmeasurestoacquiresourcelanguageparaphrasesfrommonolingualcorpora.TheyevaluateonChineseEnglishandSpanishEnglishtranslationtasks,alsoimprovingsystemstrainedonsparsedatasets,buttheirapproachdegradessystemperformancewhentrainedon80kormoreofdata.Guzman:Garrido:2007takeasimilarapproachandlearnnewtranslationsforSpanishEnglishfrommulti-lingualcorporabyusingFrenchasapivot.NewtranslationsareaddedtoanexistingSMTsystem'sphrasetablewithprobabilitiesestimatedbytakingaweightedsumofthecombinationofparaphrasesthatproducedthenewtranslation.Theyinvestigatedifferentweightsforthecompositeparaphrasesbutdonotpresentevaluationsagainstabaselinesystem.TheworkbyCallison-Burch:Koehn:Osborne:2006,Marton:Callison-Burch:Resnik:2009,andGuzman:Garrido:2007focusonintegratingparaphrasesacquiredfrommulti-andmono-lingualcorporaintoanexistingSMTsystem'sphrasetablewiththeprimarygoalofreducingthenumberofunknownwordsthesystemencounters.Inordertointegrateexternalparaphrasesintoanexistingphrasetable,ameasureoftranslationprobabilityisnecessary,andsotheydevelopaseriesofheuristicstoscoretheartificialalignmentsproducedbypairingparaphrasesofasourcephrasefoundinaparallelcorpuswiththeoriginalphrase'salignmentinthephrasetable.Incontrast,oursystemproducesparaphrasesofthesentencesinaparallelcorpus,anddoesnotalterthephrasetablecreationprocess.Ratherthanlearningparaphrasesdirectlyfromcorpora,asinthecaseoftheaforementionedworks,ourparaphrasesareproducedfromexternallexico-syntacticknowledgeintheformofanHPSGgrammar.Englishsentencesareparsedintoasemanticrepresentationthatnormalizeswordorder,spelling,andsmallnumberoflexicalitems.Paraphrasesareproducedfromthesemanticrepresentationusingthesamegrammarandparser,withparaphrasesrankedbyamaximumentropygenerationmodeltrainedonanHPSGtreebank.Unlikemethodsthatlearnparaphrasesdirectlyfromcorpora,ourHPSGparaphrasesarelimitedtogrammaticalEnglish,eliminatingtheproblemofnoisydata.</subsection>
  <subsection title="Paraphrasing to increase translation data">Nakov:2008usedparaphrasestoincreasetrainingandparametertuningdataforSMTsystem.Heproducedsentence-levelparaphrasesbyusingasmallsetofrulestoidentifyandtransformnounphrasesintheparallelcorpus(e.g.,NP_1ofNP_2NP_2'sNP_1).SomeexamplesfromNakov:2008are:ofmembersoftheIrishparliamentofIrishparliamentmembersofIrishparliament'smembersactionatcommunitylevelcommunitylevelactionexeThesetransformationswerestructural,notlexical,innatureandlimitedinscope.Nakov:2008foundthatnounphrase-basedparaphrasesweremosteffectivewhenappliedtotrainingdata,achievingaBLEUscoregainofabout1pointforlimitedcorpussizes.ParaphraseshavealsobeenusedtoenrichthedatausedforparametertuninginSMTsystems.Madnani:Ayan:Reznik:Dorr:2007obtainedEnglishlanguageparaphrasesbyidentifyingparaphrasesusingapivotlanguageasinCallison-Burch:Koehn:Osborne:2006andproducedsentence-levelEnglishparaphrasesbytraininganEnglishEnglishhierarchicalSMTsystemChiang:Hiero:2005.ExperimentsshowedthatparaphrasingthetuningdatausedforMERTinaChineseEnglishhierarchicalSMTsystemperformedcompetitivelywithincreasinghumanreferences.Paraphrasingdataforparametertuningisapromisingapproach,however,evaluatingourparaphrasingmethodintuningremainsfuturework.</subsection>
  <subsection title="Paraphrasing to increase linguistic similarity">Anotheruseofparaphrasingistoincreasethesimilaritybetweensourceandtargetlanguagesinordertofacilitatetranslation.Theapproachesdiscussedherecanbeclassifiedintomethodsthattrytosimplifythesourcelanguagevocabularyandthosethatreorderthesourcelanguageintoaformclosertothewordorderofthetargetlanguage.</subsection>
  <subsubsection title="Simplifying source language vocabulary">Oneoftheearliestapplicationsofparaphrasingtosimplifytranslationinputisshownbytherule-based~MTsystem,ALTJ/E.Shirai:Ikehara:Kawaoka:1993simplifieduntranslatableJapaneseinputintoa``pseudo-sourcelanguage''that,whileungrammatical,waseasierforALTJ/Etoparseandtranslate.Yamamoto:2001adoptedasimilarapproachwithhis``SandglassParadigm''---normalizinginputtoarule-basedMTsystembeforeexpandingitagainduringthetranslationphase.Watanabe:Shimohata:Sumita:2002alsousedparaphrasestonormalizesourcelanguagetextsystembydetectingparaphrasesinaparallelcorpusandreplacingthemwiththemostcommonlyoccurringvariant.Paraphraseswereautomaticallydetectedwithadynamicprogrammingalgorithm,andthenormalizeddatawasusedtotrainanSMTsystem.Ourapproachissimilarinspirittothesenormalizationefforts,however,insteadofusingsimpleheuristicsoridentifyingparaphrasesinacorpus,weapplyanexternalsourceofknowledge:arich,lexicalgrammar.Inaddition,insteadofdirectlytransformingsysteminput,ourapproachusesparaphrasestoenrichthetrainingdata,makingitmorerobustbyprovidinginstancesoflexicalandsyntacticvariants.</subsubsection>
  <subsubsection title="Reordering source language text">OvercomingdifferencesinwordorderisachallengefortranslatinghighlydivergentlanguagepairslikeJapanese-EnglishorGerman-English.RecentlytherehasbeenmuchworkonimprovingSMTbyreorderingthesourcelanguagetocloserresemblethewordorderofthetargetlanguage.Niessen:Ney:2001identifydifferencesinquestionorderandlong-distanceverbalprefixscramblingasphenomenathatcausedifficultiesforGermanEnglishstatisticalmachinetranslationandusedshallowpatternstoreorder``harmonizewordorder''betweentheGermanandEnglish.Collins:Koehn:Kucerova:2005madeuseofparsesofsourcesentencestodevelopareorderingheuristicaswell.Komachi:Matsumoto:Nagata:2006proposedareorderingmodelthattookintoaccountpredicate-argumentstructureinJapaneseandfollowedaheuristicforreorderingsentencesinthetrainingdataasapreprocessingstep.ThereorderingproducessentencesthatarenotgrammaticalJapanese,however,theyarecloserinwordordertoEnglish,facilitatingtheSMTalignmentprocess.Katz-Brown:Collins:2008foundthatfor~phrasalSMTana&quot;ivereversalofJapanesesourcelanguagewordorderoutperformedadependency-basedreorderingmodel.Xu:Seneff:2008usearule-basedparsertoparseEnglishandthengenerate:EnglishreorderedtoresembleChinese,withsomeChinesefunctionwordsadded.TheresultisthentranslatedusinganSMTsystem.Ourapproachalsoproducesvariantsinwordorder,however,theyarenotartificialreorderingstoreducewordorderdifferences.Rather,thesevariantsareallvalidEnglishasdefinedbytheEnglishHPSGgrammar.WemaketheSMTsystem'strainingdatamorerobustandrepresentativeofEnglishbyprovidingparaphrasesthatencapsulatethepossiblepositionsofadjuncts,suchasadverbialandprepositionphrases;relativeclauses;andotherlinguisticphenomenainEnglishwithvariablewordorder.</subsubsection>
  <section title="Resources">Inthissectionwedescribethemajorresourcesused.FortheSMTsystemweusedtheopen-sourceMosessystem.Forparaphrasingweusedtheopen-sourceEnglishResourceGrammar.WeevaluatedontheTanakaCorpus.WechosetheTanakacorpusprimarilybecauseofitsunencumberedavailability(itisinthepublicdomain),makingourresultseasytoreproduce.Inthespiritofopenscience,wehavemadetheparaphrasedTanakaCorpusdataaswellasthescriptsandMosessettingsfilesnecessarytoreproduceourexperimentsavailableonlineathttp://www3.ntu.edu.sg/home/fcbond/data/.AsummaryofalltoolsusedisgiveninTable~.</section>
  <subsection title="Moses">MosesMoses:2007isinthewordsofitscreators``afactoredphrase-basedbeam-searchdecoderformachinetranslation.''Itisdistributedasopen-sourcesoftwarewithacollectionofutilitiesthatmakeiteasyforuserstoconstructtheirownSMTsystemwhenusedwithtoolsforconstructingwordalignmentsandlanguagemodels.Forwordalignmentsweusedthegiza-ppbranchofGIZA++Och:Ney:2003.Toconstructlanguagemodels,weusedtheSRILMToolkitStolcke:2002.</subsection>
  <subsection title="The english resource grammar">TheLinGOEnglishResourceGrammar(;Flickinger:2000)isabroad-coverage,linguisticallyprecise-basedgrammarofEnglishthathasbeenunderdevelopmentattheCenterfortheStudyofLanguageandInformation(CSLI)atStanfordUniversitysince1993.Thewasoriginallydevelopedwithinthemachinetranslationeffort,butoverthepastfewyearshasbeenportedtoadditionaldomainsandsignificantlyextended.Thegrammarincludesahand-builtlexiconofaround43,000lexemes.WeareusingthedevelopmentreleaseLinGO(Apr-08).Parsingwasdonewiththeefficient,unification-basedchartparser,PETCallmeier:2002,andgenerationwiththeLinguisticKnowledgeBaseCopestake:2002.TheandassociatedparsersandgeneratorsarefreelyavailablefromtheDeepLinguisticProcessingwithHPSGInitiative.Forthemostpart,weusethedefaultsettingsandthelanguagemodelstrainedintheprojectbothforparsingandgenerationvelldal-oepen:2006:EMNLP.However,wesettherootcondition,whichcontrolswhichsentencesaretreatedasgrammatical,toberobustforparsingandstrictforgeneration.Thismeansthatrobustrules(e.g.arulethatallowsverbstonotagreeinnumberwiththeirsubject)willapplyinparsingbutnotingeneration.ThegrammarwillthusparseThedogbarkorThedogbarksbutonlygenerateThedogbarks.</subsection>
  <subsection title="The Tanaka Corpus">TheTanakacorpusisanopencorpusofJapanese-EnglishsentencepairscompiledbyProfessorYasuhitoTanakaatHyogoUniversityandhisstudentsTanaka:2001andreleasedintothepublicdomain.ProfessorTanaka'sstudentsweregiventhetaskofcollecting300sentencepairseach.Afterseveralyears,212,000sentencepairshadbeencollected.Thesentenceswerecreatedbythestudents,oftenderivedfromtextbooks,e.g.booksusedbyJapanesestudentsofEnglish.Somearelinesofsongs,othersarefrompopularbooksandBiblicalpassages.Theoriginalcollectioncontainedlargenumbersoferrors,bothintheJapaneseandEnglish.ThesearebeingcorrectedandaddedtobyvolunteersaspartofongoingactivitytoprovideexamplesentencesfortheJapanese-MultilingualdictionaryJMDictBreen:2003.Recently,translationsintootherlanguages,mostnotablyFrench,havebeenaddedbytheTATOEBAproject.Wegiveatypicalexamplesentencein().あの木の枝に数羽の鳥がとまっている。``Somebirdsaresittingonthebranchofthattree.''(en)``Desoiseauxsereposentsurlabranchedecetarbre.''(fr)exeTheversion(2007-04-05)weusehas147,190sentencepairsinthetrainingsplit,alongwith4,500sentencepairsreservedfordevelopmentand4,500sentencepairsfortesting.Afterfilteringoutlongsentences(&gt;40tokens)aspartoftheSMTcleaningprocess,therewere147,007sentencesinthetrainingset.Theaveragenumberoftokenspersentenceis11.6forJapaneseand9.1forEnglish(withthetokenizationusedintheSMTsystem).</subsection>
  <section title="Method"/>
  <subsection title="Paraphrasing">WeparaphrasebyparsingasentencetoanabstractsemanticrepresentationusingtheEnglishResourceGrammarthengeneratingfromtheresultantsemanticrepresentationusingthesamegrammar.ThesemanticrepresentationusedisMinimalRecursionSemantics(MRS:Copestake:Flickinger:Sag:Pollard:MRS).WegiveanexampleoftheparaphrasingprocessinFigure~thatshowsthreekindsofparaphrasing.Theinputsentenceis``Everybodyoftengoestothethemovies.''.ItisparaphrasedtotheMRSshowninFigure~.Fromthat,sixsentencesaregenerated.Theparaphrasedsentencesshowthreechanges.Firstly,theerroneousthetheiscorrectedtothe;secondly,everybodyisoptionallyparaphrasedaseveryoneandfinallytheadverboftenappearsinthreepositions(pre-verb,post-verb,post-verb-phrase).Weconsiderthefirsttwotobelexicalparaphrases(changesinwords)andthelattersyntacticparaphrases.Ofcourse,formostsentencesthereisacombinationoflexicalandsyntacticparaphrases.``Score''inFigure~givesamaximumentropybasedlikelihoodestimatetoeachoftheparaphrases.Notethatthehighestrankedparaphraseisnotinthiscasetheoriginalsentence.Theparaphraseisquiteconservative:sentenceinitialoftenisnotgenerated,asthatisgivenadifferentsemantics(itistreatedasfocused).Therearenoopenclassparaphraseslikefilmmovie.Onlyahandfulofclosedclasswordsaresubstituted,typicallythosethatgetdecomposedsemantically,(e.g.,everybodyevery(x),person(x)).WeattemptedtoparseallsentencesoftheTanakaCorpuswiththeERGandthePETparser.Wegotoneormorewell-formedsemanticrepresentationfor87.1%ofthesentences(theremainderwererejectedasungrammatical).Weselectedthetoprankedrepresentationandattemptedtogeneratefromit,thistimeusingtheERGandtheLKBgenerator.Wewereabletogenerateoneormorerealizationsfor83.4%oftheoriginalsentences.However,manyofthesegaveonlyonerealizationanditwasidenticaltotheinputsentence.Only53.4%ofthesentenceshadatleastonedistinctparaphrase;31.2%hadtwo,21.2%hadthree,droppingdowntoonly1.1%withtendistinctparaphrases.WeshowthedistributionofparaphrasetypesoverallofthegeneratedparaphrasesinFigure~.Lexicalparaphrasesareidentifiedbycomparingthesetoflexicalitemsintheinputwiththoseintheoutput.Iftheyaredifferent,thenthereisalexicalparaphrase(Lex).Syntacticparaphrasesareidentifiedbycomparingtheparsetrees.Almostaquarterofthesentencesgeneratedarethesameastheinput(Lex=,Syn=).Mostvariationsincludesomesyntacticparaphrasing(Syn:42%),purelylexicalparaphrasingisrelativelyuncommon(8%).</subsection>
  <subsection title="Corpus expansion">Typicallywhenlearningtranslationmodels,itisassumedthateachsentencepairintheparallelcorpusisgiventhesameweight.Thisraisesthequestionofhowadditionalparaphraseddatashouldbeweighed.Astraight-forwardapproachwouldbetosimplyaddeachnewparaphrasedirectlytothecorpus.However,assentencescanhavedifferentnumbersofparaphrases,weriskassigningadifferentweighttoeachsetoforiginalsentencepairandderivedparaphrase.Amoresophisticatedapproachwouldbetoassurethateachsetmaintainsthesameoverallweightbydistributingitequallyamongeachparaphrase,orbyusingtheparaphrasegenerationscorefromFigure~togivemorelikelyparaphrasesahigherweight.Here,weexploreseveralmethodsofassigningweightstotheparaphraseddatabyvaryingthenumberoftimesweaddeachnewparaphrasetotheparallelcorpus.Tomaketheenhancedtrainingdata,weadduptondistinctparaphrasestoeachunchangedJapanesesentenceandoriginalEnglishsentence.Weconvertallparaphrasestolowercasebeforecheckingforuniqueness.Ifthereweremparaphrases,andnmthenwejustaddinthetopnrankedparaphrases.Ifn&gt;mthenweproducedthreetestsets:(d)istributed:rotatebetweentheoriginalsentenceandeachparaphraseuntilthedatahasbeenpaddedout(f)irst:afterallparaphrasesareused,thefirst(original)sentenceisrepeatedtopadoutthedata(v)arying:addjusttheparaphraseswithoutpaddingallentriestothesamenumberofsentencesThesevariationsareshowninTable~.Both(dandf)keepthedistributionclosetotheoriginalcorpus.dputsmoreweightontheparaphrasedsentencesandfputsmoreweightontheoriginalsentence.Forvthefrequencyisdistorted;somesentenceswillberepeatedmanytimes.Forn2,dandfarethesame.</subsection>
  <section title="Evaluation">Inthissection,weinvestigatetheeffectsofsupplementingtrainingdatawithparaphrasesontheTanakaCorpus.Weconstructphrase-basedSMTsystemsusingMosesforthe~and~languagepairs,andevaluatesystemsonvarioustrainingcorpussizes.</section>
  <subsection title="Moses baseline">WereplicatedthebaselineintheACL2007SecondWorkshoponStatisticalMachineTranslation.ThebaselineisafactorlessMosessystemwitha5-gramlanguagemodel.Wefollowedtheonlinetutorialas-is,withtheexceptionthatweusedexternalmorphologicalanalyzerstotokenizeourdatainsteadofusingtheprovidedscripts.WeusedtheTreeTaggerschmid94probabilisticforEnglishandMeCabmecabwithNAISTJdicforJapanese.Part-of-speechinformationwasdiscardedaftertokenization.Alldatawastokenized,separatingpunctuationfromwordsandconvertedtolowercasepriortotrainingandtranslation.TranslationsweredetokenizedandrecasedpriortoevaluationusingthehelperscriptsdistributedaspartofthebaselinesystemfortheACL2007SMTWorkshop.PriortoevaluationweconductedMinimumErrorRateTrainingoneachsystemusingthedevelopmentdatafromthetargetcorpus.WeusedtheMERTimplementationdistributedwithMoses.Allresultsreportedinthisarticlearepost-MERTBLEUscores.</subsection>
  <subsection title="Data preparation">Inordertomeasuretheeffectivenessofourmethod,weevaluatedthe~and~languagepairsovertheTanakaCorpus.BecauseourHPSGparsersperformbestondatathatissplitonthesentencelevel,whereverpossiblewesplitthecorporaintothefinestpossiblesentencepairs.Weusedthefollowingalgorithmtosplittheevaluationdata.However,mostofthedataintheTanakaCorpushasalreadybeensplitatthesentencelevelaspartoftheJMDictinitiative.	Foreachsentencepair:			spliteachsentenceonsentence-finalpunctuation(.?!)		rejoinsplitoncommonEnglishtitles(Mr./Ms./Mrs./Dr.)		splitsentencepairswithsamenumberofsourceandtargetsentences		intonewpairs		treatsentencepairswithdifferentnumberofsourceandtargetsentences		asasinglepair	</subsection>
  <subsection title="Results">WeevaluatedtheeffectsofaddingparaphrasestovariousinitialtrainingdatasizesusingBLEUandMETEORscores.Wecomparedabaselineofno-paraphrases-added(d.0/f.0)tosystemswithprogressivelylargernumbersofnewparaphrasedsentencepairsaddedtoeachtrainingdatasize.Wetestedthreedistributions(d,fandv).valwaysgaveresultsbelowthebaseline,sowedonotreporttheminmoredetail.Wegiveseveralanalysesfordandfbelow.</subsection>
  <subsubsection title="Learning curves">WegivelearningcurvesinFigures~and.TheaverageBLEUscoresfordistributedandfirstparaphrasesystemsareplottedagainsttrainingcorpussizes(10k,25k,50k,100k,125k,andamaximumsizeof147k).Thetrainingdataaxisisscaledlogarithmically.Bestfitlinesforthebaseline(d.0/f.0)andeachoftheparaphrasesshowthatthereisalog-linearrelationshipintrainingdatasizeandBLEUscore.Paraphrasingalmostalwaysoutperformsthebaselineforsmalldatasets(EJ:10k-25k,JE:10k-75k)andlargedatasets(EJ:100k-147k,JE:125k-147k).Theregioninthemiddle(EJ:50k-75k,JE:100k)isanomalous;theparaphrasedaveragesarebelowthebaseline.Wesuspectthismaybecausedbythesedatasizescontainingnon-representativesamplesofdataorparaphrases.</subsubsection>
  <subsubsection title="BLEU score">BLEUscoreswerecalculatedusingthemulti-bleu.perlimplementationdistributedwithMoses.WemeasuredthestatisticalsignificanceofBLEUscoredifferenceswiththebootstrapmethodsoutlinedinkoehn04statisticalusingJun-yaNorimatsu'sMIT-LicensedBLEUKit.BLEUscoresfordandfaregiveninTable~,resultswithanimprovementofp&lt;=0.05overthebaselineareshowninbold,andthebestscoreforeachdatasizeisunderlined.Weobserveamaximumgainof0.67BLEUpointsfor~at(10k,d.4)andamaximumgainof0.63forat(50k,d.2/f.2).Gainsappeartopeakat8paraphrases;d.10andf.10rarelyachievehigherscoresthatcanbeachievedwithfewerparaphrases.ThelargenumberofstatisticallysignificantBLEUscoreimprovementsreinforceourobservationsmadefromthelearningcurvesthatparaphrasingisbeneficialforsmalldatasetsandlargedatasets.Wealsonoticeatrendthatsmallnumbersofheavilyweightedparaphrasesliked.4aremoreeffectiveforsmalldatasets,whilelargernumbersoflightly-weightedparaphraseslikef.8aremoreeffectiveforlargedatasets.</subsubsection>
  <subsubsection title="Meteor score">METEORbanerjee-lavie:2005:MTSummisanadvancedMTevaluationmetricthatusesstemmingandWordNetsynonymmatchingtorelaxconstraintsforEnglishn-grammatchestoachievehigherlevelsofcorrelationtohumanjudgementthanpossiblewithsimplermetricslikeBLEUandNIST.WecalculatedallMETEORscoresusingversion1.0withthefollowingoptions:stemming,WordNetstemming,WordNetsynonymmatching,and``normalization''~---strippingofpunctuationandconversiontolowercase.METEORscorefor~fordandfaregiveninTable~.TheMETEORscoresdonotshowasconsistentgainsasBLEUscoresdo,butthe10kdatasetshowsgreatimprovementsforeveryparaphrasesize.WealsonoteacorrelationbetweenstatisticalsignificantBLEUscoregainsandMETEORscoreimprovements;14/20paraphrasesystemswithstatisticallysignificantBLEUscoregainshaveincreasesinMETEORscores.</subsubsection>
  <section title="Discussion">Overall,weshowsignificant,consistentimprovementsontheTanakaCorpus.ParaphrasedSMTsystemsshowstatisticallysignificantimprovementsoverthebaselineforthemajorityofthedatasizestested.Furthermore,weobservealog-linearrelationshipbetweenthesizeofthesystem'strainingdataandtheBLEUscore,withbest-fitlinesdemonstratingthesuperiorityoftheparaphrasedsystemoverthebaseline.Table~showssomeexamplesofhowtranslationoutputchangeswiththeadditionofvariousamountsofparaphrasingdataforthe~languagepair.Theexampletranslationscontaindifficult-to-learnphrasaltranslations,suchasrainingonandoffandtothepoint.AsistobeexpectedfromtheBLEUscores,thesystemf.8oftengivesthebesttranslation.Wetheorizethattheadditionaldataprovidedbyourparaphrasesresultsinbetterphrasalalignments,which,inturn,improveslexicalselectionandallowsthelanguagemodeltoproducemorenatural-soundingtranslations.ComparedtoCallison-Burch:Koehn:Osborne:2006,Madnani:Ayan:Reznik:Dorr:2007,orNakov:2008weareveryconservativeinourparaphrasing,andthisisprobablywhywegetaslightlylowerimprovementinquality.Wecoulddomoreextravagantparaphrasing,butwouldhavetoretrainourHPSGparser'sgenerationmodeltoeffectivelyrankthenewlexicalparaphrases.Atthemoment,itexpectsfullyspecifiedinputMRSes.Ifweweregoingtoallowvariationinnounphrasestructureoropenclasslexicalvariation,thenthetaskcouldbere-framedastranslatingbetweenEnglishsentence,andwecouldbuildanEnglishEnglishsemantictransfersystemtoproducericherparaphrases.Anexampleofhowtodothis(forbilingualtransferofNorwegianEnglish)isgiveninOepen:Velldal:Loening:Meurer:Rosen:2007.OursyntacticreorderingisnotaimedatmatchingthetargetlanguagelikeKomachi:Matsumoto:Nagata:2006,Xu:Seneff:2008,orKatz-Brown:Collins:2008.Wecorrespondinglygetasmallerimprovement.Ontheotherhand,becauseourEnglishparaphrasingmethoddoesnotdependedonaparallelcorpus,weexpecttogetasimilarimprovementevenfordifferentlanguagepairs.Also,ourimprovementisstillthereafterMERT,whereastheimprovementofKomachi:Matsumoto:Nagata:2006didnotmakeitthroughtheoptimization.WehaveseensimilarincreasesinSMTsystemperformanceforJapaneseandEnglishdataontheBasicTravelExpressionCorpusthatisusedintheInternationalWorkshopforSpokenMachineTranslation'stranslationtask.Wereportedtheseresultsiniwslt08:TP:bond.Unfortunately,datausagerestrictionspreventusfromreproducingtheresultshere.</section>
  <section title="Further Work">Therearethreeareasinwhichwethinkthecurrentuseofparaphrasingcouldbeimproved:(1)increasingthecoverageofthegrammar(2)addingnewclassesofparaphraserulesand(3)improvingtheintegrationwiththeSMTprocess.Toincreasethecoveroftheparaphrasing,weneedtoimprovethehandlingofunknownwords.Currently,thegrammarcanparseunknownwords(whichbringsthecoverageuptoalmost95%),butdoesnotpassenoughinformationtothegeneratortothengeneratethem.Wecanovercomethiswithmorepowerfulhybridparsing,followingAdolphs:Oepen:Callmeier:Crysmann:Kiefer:2008.Amorefar-rangingincreasewouldbetoparaphrasetheJapanesesideaswell.WeareworkingonthisusingJacy,anHPSG-basedJapanesegrammarsimilartotheBond:Kuribayashi:Hashimoto:2008andapplyingthegrammaticalerrortoolsofGoodman:Bond:2009acltoimprovethegenerationcoverageoftheJapanesegrammar.Beforeweincreasethetypesofparaphraseswefirstneedtomeasurewhichrules(e.g.lexicalorsyntactic)havethemosteffect.WethenintendtocreateEnglishrewritingrulesusingtheMRStransfermachineryfromtheproject,whichisalreadyusedinanopensource~MTsystemBond:Oepen:Siegel:Copestake:Flickinger:2005.Forexample,wecaneasilywritenounphraserewritingrulesofthetypeusedbyNakov:2008.ForlexicalsubstitutionwewilltryusingWordNet,afterfirstdisambiguatingtheinput.Finally,wewouldliketoenhanceMoses(primarilyGIZA++)sothatinputsentencescanbeweighted.Thatway,ifwehavenparaphrasesforonesentenceandmforanother,eachcanjustbeenteredwithaweightof1/nand1/mrespectively.Ifwecoulddothis,wecouldthenexperimentwithsettingaprobabilitybasedthresholdonthenumberofparaphrases,forexample,toselectallparaphraseswithinoftheprobabilityoftheoriginalsentence,accordingtosomelanguagemodel.Inthiswaywecouldaddonly``good''paraphrases,andasmanyaswedeemgoodforeachsentence.</section>
  <section title="Conclusion">Largeamountsoftrainingdataareessentialfortrainingstatisticalmachinetranslationsystems.Inthispaperweshowedhowtrainingdatacanbeexpandedbyparaphrasingonesideofaparallelcorpus.ThenewdataismadebyparsingthengeneratingusingapreciseHPSG-basedgrammar.Thisgivessentenceswiththesamemeaning,butwithminorvariationsinlexicalchoiceandwordorder.InexperimentsparaphrasingtheEnglishintheTanakaCorpus,weshowedconsistent,statistically-significantgainsontrainingdatasetsrangingfrom10,000to147,000sentencepairsinsizeasevaluatedbytheBLEUandMETEORautomaticevaluationmetrics.workwasdonewhilethesecondauthorwasamemberof,andthethirdauthoraninternat,NICT,Japan.WewouldliketothankthemembersoftheLanguageInfrastructureandMachineTranslationGroupsfortheirhelpfulcomments,especiallyKiyotakaUchimoto,MichaelPaulandKentaroTorisawa.document</section>
</root>
