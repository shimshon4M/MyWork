



\documentstyle[epsf,jnlpbbl]{jnlp_j_b5}

\setcounter{page}{49}
\setcounter{巻数}{2}
\setcounter{号数}{3}
\受付{1994}{11}{24}
\再受付{1995}{1}{30}
\採録{1995}{3}{1}
\setcounter{年}{1995}
\setcounter{月}{7}

\setcounter{secnumdepth}{2}

\def\atari(#1,#2){}
\def\bras#1{}
\def\pp#1{}
\def\nl{}

\title{動詞訳語選択のための「格フレーム木」の統計的な学習}
\author{田中 英輝\affiref{NHK}}

\headauthor{田中 英輝}
\headtitle{動詞訳語選択のための「格フレーム木」の統計的な学習}

\affilabel{NHK}{NHK放送技術研究所，先端制作技術研究部}
{NHK Science and Technical Research Laboratories, Program Production
  Technology Research Division} 

\jabstract{
  機械翻訳システムでは動詞の訳語を選択するために格フレームがよく利用さ
  れる．格フレームは従来主として人手で記述されていたが，一貫性を保って
  記述するのが難しいこと，格フレームを部分的に変更した場合に起こる影響
  が把握しにくいことなどの重大な問題があった．そこでこれらの問題を解決
  するため，本論文では格フレームを決定木の形で表し(これを格フレーム木
  と呼ぶ)，これを英日の対訳コーパスから統計的な帰納学習プログラムを利
  用して学習することを提案する．
  本論文ではまず，この提案によって上記の問題が軽減される根拠を述べた後，
  本論文で作成した英日対訳コーパスについて述べる．続いて7つの英語動詞
  について格フレーム木の獲得実験を2つ報告する．
  最初の実験は，格要素の制約として英語の単語を使う格フレーム木を学習し
  たものである．これにより得られた格フレーム木を観察したところ，人間の
  直観に近く，かつ直観を越えた非常に精密な訳し分けの情報が得られたこと
  が明らかになった．
  次に，この格フレームの一般性を高めるために，英語の単語の代わりに意味
  分類コードを制約として利用する手法を提案し，これに基づいて格フレーム
  木を学習する実験を行った．得られた格フレーム木で未学習のデータの動詞
  の訳語を決定する評価を行ったところ，2.4\%ないし32.2\%の誤訳率が達成
  された．この誤訳率と，先の英語単語を利用した格フレーム木での誤訳率と
  の差は13.6\%ないし55.3\%となり，意味分類コードが有効に機能したことが
  示された．}

\jkeywords{機械翻訳，機械学習，訳語選択，コーパス，決定木，統計}

\etitle{Statistical Learning of ``Case Frame Tree''\\
  for Translating English Verbs}

\eauthor{Hideki Tanaka \affiref{NHK}}

\eabstract{ Our English to Japanese machine translation system uses
  surface verbal case frames (case frames) to select a Japanese
  translation for an English verb.  The need to acquire and accumulate
  case frames leads directly to two problems:
\begin{itemize}
\item How to obtain transparency in the case frames?  Case frames are
  sometimes changed after they are written and it is hard to predict
  how the translation selection is affected by these changes.
\item How to keep consistency in the case elements and their
  restrictions?  These elements should be used consistently because
  the matching calculation between case frames and syntactic
  structures (parser output) expects fair use of these elements.
\end{itemize}
To solve these problems, we propose two methods.
\begin{itemize}
\item Use of decision tree for case frame representation (case frame
  tree).
\item Use of a statistical inductive learning algorithm to derive a
  case frame tree from a bilingual corpus.
\end{itemize}
The first method solves problem one: a change at any node in a case
frame tree will affect only the translations under the node which is
changed.  The second method solves problem two: the case elements and
their restrictions are evaluated on the same basis according to their
ability to distinguish the verb translations in the corpus.  We used
the learning algorithm C4.5, devised by Quinlan.
C4.5 takes as input a table listing attribute, value, and class. To
acquire a case frame tree, we replace attributes with case categories,
values with restrictions of the case categories, and classes with
Japanese translations of English verbs.  We termed such a table a {\em
  Primitive Case Frame Table (PCFT)}.
Before doing acquisition experiments on seven English verbs\mbox{} (``come'',
``get'', ``give'', ``go'', ``make'', ``run'', ``take''), we
constructed an English and Japanese bilingual corpus from the AP
(Associated Press) wirenews texts, a corpus that turned out to be
about 6,000 translation pairs with syntactic tags.
In the first experiment, we converted the corpus into the PCFT using
all case categories appearing in the corpus and word forms for their
restrictions.  The acquired case frame trees basically duplicated the
human work, but were far more precise in discriminating verb
translations appearing in the corpus.  Although the results indicate
the basic effectiveness of our approach, the acquired case frame trees
did not seem to have enough prediction power on open data since a lot
of the word forms could be unknown words.
To solve this problem, we generalized the word forms in the PCFT using
semantic codes (Ruigo-Kokugo-Jiten, consisting of 4 digits) and then
used C4.5.  The five-fold cross-validation method was used to ensure
the evaluation (error rate) precision.  The error rate on open data
for each verb was between 2.4\% and 32.2\%.  Comparison of these
figures with the baseline errors (error rates obtained by simply
putting out the most frequent translation of a verb) showed a gain of
between 13.6\% to 55.3\%, which indicates the basic effectiveness of
using semantic codes. 
To lower error rate, we are devising an algorithm that can integrate
word forms and semantic codes in an acquired case frame tree.  }

\ekeywords{Machine translation, Machine learning, Translation
  selection, Corpus, Decision tree, Statistics}

\begin{document}
\maketitle


\vspace*{-0.2mm}
\section{はじめに}

著者らは放送分野を対象とした英日機械翻訳システムを開発している
\cite{Aiz90,Tan93,TanAndHat94}．この中で最もコストがかかり手間を要する
のが辞書の作成である．著者らの経験によれば，この中で最も困難なのが動詞
の表層格フレーム(以下，格フレームと省略する)の記述である．これは英語の
動詞の日本語訳語を選択するために利用される情報で，動詞の取りうる文型と
その時の訳語を記述したものである．

従来，これらは冊子辞書や用例を参照しながら人手で収集・記述していた．し
かし，
\begin{enumerate}
\item 記述する表層格要素(以下，格要素と省略する)や，その制約を一貫して
  用いることが難しいこと
\item 格フレームの一部を変更した場合に，訳語選択に与える影響が把握しに
  くいこと等の問題があり，この収集・記述作業の効率は非常に悪かった．
\end{enumerate}
このため本論文ではこれらの問題の解決を目指し，格フレームの新たな表現手
法，および獲得手法を提案する．これは著者らの英日機械翻訳システムのみな
らず，動詞訳語選択に格フレームを利用するその他の機械翻訳システムの構築
にも応用できるものである．

本論文では上記の2問題を解決するために次の2点を提案する．
\begin{enumerate}
\item 動詞の翻訳のための格フレームを決定木の形で表現する．以下，本論文
  ではこの決定木を格フレーム木と呼ぶ．
\item 英日の対訳コーパスから，統計的な帰納学習アルゴリズムを用いて格フ
  レーム木を自動的に学習する\cite{TanAndEha93,Tan94a,Tan94b}．
\end{enumerate}
また，この提案に基づいて実際に対訳コーパスから格フレーム木を獲得する実
験を2種類行う．本論文で学習の対象としたのは訳語の数の多い英語の7つの動
詞(``come'', ``get'', ``give'', ``go'', ``make'', ``run'', ``take'')で
ある．

最初の獲得実験では格要素の制約として語形を利用した．この結果，人間の直
観に近く，かつ人手で獲得する場合より精密な訳し分けの情報が獲得されたこ
とを示す．また2番目の実験では，格フレーム木の一般性を確保することを目
的とし，意味コードを格要素の制約として用いた．この結果，未学習のデータ
を入力して動詞の訳語を決定する実験で2.4\%から32.2\%の誤訳率が達成され
た．これらの結果と，単純に最高頻度の訳語を出力した場合の誤訳率との差は
13.6\%から55.3\%となりかなりの改善が得られた．

実験に先だって著者らは英日の対訳コーパスを作成した．著者らの目的とする
格フレーム木は，放送ニュース文を対象とすることを想定している．このため，
学習には放送分野のコーパスを利用するのが望ましい．しかし，現在このよう
な英日対訳コーパスは入手可能でないため，AP (Associated Press)のニュー
ス英文を利用して作成した．本論文ではこの対訳コーパスの設計，作成過程お
よび特徴についても触れる．

著者らの研究は，コーパスから自然言語処理システムのルールを獲得する研究
である．大規模コーパスが入手可能になるにつれ，この種の研究は盛んになり
つつある．また，その獲得の目的とするルールもさまざまである．

これらの中で本論文に近い研究としては，\cite{UtuAndMat93}および，
\cite{Alm94}の研究が挙げられる．\cite{UtuAndMat93}では，自然言語処理一
般に利用することを目的とした日本語動詞の格フレームの獲得を試みている．
ここで提案されている手法は，タグ付けされていない対訳テキストから格フレー
ムが獲得できる点で著者らの手法より優れている．しかし，ここで利用されて
いる学習アルゴリズムは，格フレームの利用の仕方を考慮したものではない．
このため，著者らの目的である動詞の訳語選択にどの程度有効であるかは不明
である．

これに対して，著者らのアルゴリズムはエントロピーを基準にして，動詞の訳
語選択の性能を最大にするように格フレーム木の獲得を行う．この結果，訳語
選択に適した情報が獲得され，しかもその性能が統計的に把握できる利点を持っ
ている．

\cite{Alm94}では著者らと逆に日英機械翻訳システムで利用するための日本語
動詞の翻訳ルールを学習する手法を提案している．用いられている学習手法は
基本的には本論文と同じものである．ただし，この論文では動詞の翻訳のため
の規則を決定木で表現することの利点について触れていないが，これには大き
な利点があることを著者らは主張する．また，この論文では学習に利用した対
訳事例をどのような所に求めたかは明らかにされていない．しかし，これは獲
得される格フレーム木に大きな影響を与えるため著者らはこれを詳細に論ずる．
さらに，この論文では人手で作成したルールとの一致で評価を行っているが，
訳語選択の性能については触れられていない．これに対して著者らは動詞の誤
訳率で評価を行う．

本論文の構成は以下の通りである．2章では，人手で行っていた従来の格フレー
ムの獲得，記述の問題点を整理する．3章ではこの解決のため，先に述べた提
案を行うとともに，格フレーム木を英日対訳コーパスから学習する手法を説明
する．4章では，本論文で利用する英日対訳コーパスの作成について述べる．5
章では，このコーパスの語形を直接的に利用した格フレーム木の獲得実験を行
う．6章では，対訳コーパスを意味コードで一般化したデータを作成して格フ
レーム木の獲得実験を行う．7章では本論文のまとめを行い，今後の課題につ
いて述べる．

\begin{figure}
  \begin{center}
    \begin{tabular}{|ll|}
      \hline
      SN [man] take ON [boy]& 選ぶ\\
      SN [I] take ON [him] PN [to] PNc [BUILD]& 連れていく\\
      SN [HUMAN] take ON [CON] PN [to] PNc [BUILD]& 持っていく\\
      \hline
      \multicolumn{2}{r}{記号：格要素 {[制約]}}\\
      \end{tabular}
      \caption{``take''の格フレームの例}
    \end{center}
\end{figure}

\section{人手による格フレーム獲得の問題点}

著者らの英日機械翻訳システムで現在利用している格フレームの例(説明用)を
図~1に示す．これは，英語の動詞が持つ複数の日本語訳語から適切な訳語を選
択するために利用される．これらの格フレームは，それぞれの訳語ごとに記述
されている．著者らの格フレームに記述してある要素は次の4つである．
\begin{itemize}
\item 動詞が必須格として取りうる格要素
\item 格要素の制約(語形や意味分類のマーカー)
\item 動詞の日本語訳語
\item ウエイト(図~1では省略)
\end{itemize}
ウエイトとは，それぞれの格フレームに与えられた数値であり「使われにくさ」
の指標として使われる．この値が大きいほどその格フレームは利用されにくく
なる．格フレームは構文解析が終了してから利用される．すなわち入力英文の
構文構造と動詞が認定されてから利用される．この動詞の訳語を決定するには，
まず動詞の持つ格フレームすべて(辞書中に登録されているもの)と構文構造の
比較が行われ，それらの間の類似度が計算される．そして最も類似度の高い格
フレームの訳語が出力される．類似度は，構文構造と格フレームの間での格要
素と制約の一致度合，および格フレームのウエイトを総合して計算される．

従来，格フレームは人手で記述されていたが，困難かつコストのかかる作業と
なっていた．これは以下のような理由による．
\begin{enumerate}
\item[{\bf 問題1)}] {\bf 利用する情報の一貫性の確保}\\
  格要素として何を記述すべきか．必須格の定義は何か．格要素の制約は何を
  使用するか．これらは記述する時に必ず決めなければならないパラメータで
  ある．これらは何らかの規範として事前に決められ，作業者はそれに添って
  記述を行う．
  このとき，規範で決められた制約と格要素は一貫して利用しなくてはならな
  い．なぜなら，動詞の訳語を選択する時に行われる計算は，すべての格フレー
  ムが平等であることを前提としているからである．
  しかし，人手で一貫した方針を貫くのは困難であり不平等な記述になりがち
  である．

\item[{\bf 問題2)}] {\bf 格フレームの不透明性}\\
  格フレームは一旦記述し終わった後で部分的に変更されることがしばしばあ
  る．例えば，ある格フレームの格要素を削除，追加したり，格要素の制約を
  変更することなどである．しかし，この結果が動詞の訳語選択にどういう影
  響を与えるかを把握するのは難しい．これには構文構造との比較時に発生す
  る他の格フレームとの競合の状態の変化，すなわち類似度計算への影響を把
  握しなくてはならないからである．
  このような格フレームの変更結果の不透明性は，記述，保守管理の重大な障
  害となっている．

\item[{\bf 問題3)}] {\bf ウエイト決定の恣意性}\\
  ある格フレームのウエイト，すなわち「使われにくさ」を人間の内省で決め
  るのは非常に難しい．しかもウエイトは格要素と制約の一致度合と共に類似
  度計算に用いられるため，両者の評価の比重をどう設定するかなど問題は多
  い．
  実際には辞書の頻度情報などを参考に記述することが多いが，客観的なウエ
  イトを与えることは難しい．
\end{enumerate}
以上の3点の問題の内，問題3は著者らのシステムに固有の問題であるが，問題
1，2は格フレームを利用して動詞の訳語を決定する場合の共通する問題である
と考える．


\section{対訳コーパスからの格フレーム木の学習}

著者らは，2章で述べた問題の解決のため以下の2点を提案する．
\begin{itemize}
\item 格フレームを決定木の形で表現する(格フレーム木)
\item 英日の対訳コーパスから，統計的な帰納学習アルゴリズムを用いて格フ
  レーム木を自動的に学習する
\end{itemize}
また，本章ではこれらを実現する手法を具体的に述べる．

\subsection{格フレーム木}

\begin{figure}
  \begin{center}
    \unitlength=1mm
    \begin{picture}(60,35)
      \put( 0, 0){\framebox(60,35){}}
      \put(10, 5){\makebox(0,0){ 連れていく}}
      \put(30, 5){\makebox(0,0){ 選ぶ}}
      \put(45,10){\makebox(0,0){ 持っていく}}

      \put(10,10){\makebox(0,0){$\bullet$}}
      \put(30,10){\makebox(0,0){$\bullet$}}
      \put(45,15){\makebox(0,0){$\bullet$}}
      \put(20,20){\makebox(0,0){$\bullet$}}
      \put(30,30){\makebox(0,0){$\bullet$}}
      
      \put(30,30){\makebox(0,0)[b]{\raisebox{1ex}{ON}}}
      \put(20,20){\makebox(0,0)[rb]{PN }}
      \put(25,25){\makebox(0,0)[rb]{him }}
      \put(15,15){\makebox(0,0)[rb]{to }}
      \put(35,25){\makebox(0,0)[lb]{ box}}
      \put(25,15){\makebox(0,0)[lb]{ $\phi$}}

      \thicklines
      \put(10,10){\line( 1, 1){20}}
      \put(20,20){\line( 1,-1){10}}
      \put(30,30){\line( 1,-1){15}}
    \end{picture}
  \end{center}
  \caption{``take''の格フレーム木}
\end{figure}

格フレーム木は図~2に示すような構造をしている．各ノードには格要素，アー
クにはその制約，リーフには動詞の訳語が付与されている．一方，従来の形式
の格フレーム(図~1)を便宜的に線形格フレームと呼ぶ．格フレーム木は決定木
であるため，動詞の訳語選択を行うには，ルートからリーフに向かって構文構
造と比較するだけでよい．このため線形格フレームよりも効率良く動詞の訳語
選択を行うことができる．しかし，この構造を採用する最大の利点は，2章で
述べた問題2の「格フレームの不透明さ」を軽減できる点にある．これは，決
定木にすることによって，変更の影響が変更場所の下の訳語に限られるためで
ある．例えば，図~2の格要素PN (前置詞)を削除した場合には，下位の訳語
「連れていく」と「選ぶ」の選択が行われなくなることが容易にわかる．

\subsection{統計的な帰納学習}

2章で述べた問題1「利用する情報の一貫性の確保」は，統計的な帰納学習プロ
グラムを利用することで解決される．このようなプログラムとしては
CART~\cite{Brei84}，ID3~\cite{Qui86}，C4.5~\cite{Qui93}など，いろいろ
なものが既に提案されている．これらは共通して(属性，属性値，クラス)の形
式の表から決定木を学習する．著者らの目的とする格フレーム木を学習するに
は属性として格要素，属性値として格要素の制約，クラスとして動詞の訳語を
与える(表~1)．本稿ではこの表を原始格フレーム表と呼ぶ．またこの表の各行
を事例と呼ぶ．
\begin{table}
  \begin{center}
    \caption{原始格フレ−ム表}
    \begin{tabular}{ccccc|c}\hline\hline
      SN & V & ON & PN & PNc & 動詞の訳語\\\hline
      I & take & him & to & theater & 連れていく\\
      you & take & him & to & school & 連れていく\\
      you & take & him & to & park & 連れていく\\
      you & take & box & to & theater & 持っていく\\
      you & take & box & to & park & 持っていく\\
      I & take & box & to & school & 持っていく\\
      I & take & him & $ \phi $ & $ \phi $ &  選ぶ\\
      you & take & him & $ \phi $ & $ \phi $ &  選ぶ\\\hline
    \end{tabular}
  \end{center}
\end{table}

著者らの利用するプログラム (C4.5)では，原始格フレーム表の格要素が，動
詞の訳語の分類にどれだけ有効かを統計的に計算する\footnote{ID3， C4.5は
  基本的に同じアルゴリズムである．後者は前者の機能拡張版であるが，木の
  学習部分についてはほぼ同等である．CARTは2分木を学習するため著者らの
  目的に直接は利用できない．しかし木を作成していく過程はID3， C4.5と同
  じような統計的基準を採用している．}．すなわち，ある格要素の制約に従っ
て訳語を分類した場合に，どれだけ「きれいに」訳語が分類されるかをエント
ロピーを用いて計算し，これによって有効性を判定する．そして，有効性の高
い格要素から順番に上位ノードから下位ノードに配置して決定木を生成する
(アルゴリズムの概要は付録Aに示す．)．表~1にプログラムを適用すると，図~2
で示した格フレーム木が学習される．

このアルゴリズムでは，格要素およびその制約は，訳語の分類に対する有効性
という観点で一貫して評価されることになり，2章の問題1「利用する情報の一
貫性の確保」は解決される．

この手法で獲得された格フレーム木には，線形格フレームの個々のウエイトに
相当する指標は陽には現われない．しかし，個々の格要素の有効性の高い順に
格フレーム木が構成されるのでウエイトによる評価は必要なくなる．そのため
2章の問題3「ウエイト決定の恣意性」の問題も解決されると言える．また，こ
の手法を用いると格要素が必須格か自由格かという判定は，訳し分けの観点か
らプログラムで自動的に決定されることになる．例えば，図~2には主格
がなく不自然な印象を受ける．しかし，これは訳し分けのための有効性を主格
が持たなかった結果である．

従来，自動学習は手間の軽減を目的として利用されることが多かった．しかし，
有用な情報を目的に合わせて適切に配列する能力も大きな特徴である．本提案
はこの特徴を生かしており，動詞の訳し分けを行うのに必要な情報を単に自動
的に獲得するだけでなく，それらを，動詞の訳語を最適に選択できる決定木の
形で構成する\footnote{各ノードでの訳語選択を最適に行うように格フレーム
  木を構成する意味であり，全体としての訳語選択の性能の最適性を保証する
  ものではない．}．またこの決定木の表現は実用上の利便性が高いものである．

\subsection{格フレーム木の段階的な獲得}

統計的な帰納学習アルゴリズムを利用する上で重要なのは入力，すなわち，原
始格フレーム表である．この表は，訳し分けの対象とするドメインの事例の頻
度分布を反映して作成しなければならない．なぜなら格フレーム木は，原始格
フレーム表の事例の頻度を元に作成されるため，対象ドメインと原始格フレー
ム表で事例の分布が違った場合には，対象ドメインの動詞を有効に訳し分ける
条件が学習されないからである．

さらに，原始格フレーム表に記述する格要素と制約を決める必要があるが，こ
れには試行錯誤を要する．以上のことより，対象ドメインの英日対訳コーパス
を柔軟に変換して原始格フレーム表を作成する手法が有効と考えられる．しか
し，著者らの対象ドメインである放送ニュース分野の英日対訳コーパスの入手
は難しい．このため，著者らは英日の対訳コーパスを作成し，これを変換して
原始格フレーム表を作成する方針を採用した．この結果，格フレーム木獲得の
筋道は図~3に示すものとなった．

\begin{figure}
  
\begin{center}
\epsfile{file=fig3.eps,width=94.0mm}
\end{center}
  \caption{格フレーム木の段階的獲得}
\end{figure}

従来，線形格フレームは直接的に人手で記述していた．しかし，提案手法では，
対訳コーパスから人手を介しながら段階的に格フレーム木を獲得することにな
る．この手法で人間が介入する部分は，対訳コーパスの作成，および原始格フ
レーム表へ変換する部分だけである．対訳コーパス作成の部分では，英文への
統語ラベルの付与，日本語への翻訳等を行う．また，原始格フレーム表への変
換では，単に利用する格要素や制約を決定するだけである．このような人手の
作業は，線形格フレームをいきなり記述するより安定に行うことができる．そ
のため，格フレーム木に入る人間の恣意性を最小限に押さえることができると
期待できる．

\section{英日対訳コーパス}

\subsection{コーパスのソース}

先に述べたように，本手法では原始格フレーム表の事例の頻度分布が重要とな
る．このため英和辞典の例文などを利用することには問題がある．なぜなら高
頻度で出現する訳語もそうでない訳語も例文の数にあまり差がないからである．
そこで著者らの場合は，AP (Associated Press) 電ニュースをコーパス
のソースとして採用することにした．そしてこの英文から以下に示す条件を満
たすものを抽出して，日本語訳を付与することで対訳コーパスを作成した．

\subsection{対象動詞}
訳語の数が多く機械翻訳を行う場合に問題となる以下の動詞を対象とした．
\begin{quote}
  ``come'', ``get'', ``give'', ``go'', ``make'', ``run'',``take''
\end{quote}

\subsection{データ量}

上記の動詞を訳し分けるために必要な対訳コーパスの量を直接見積るのは困難
である．そこで，これらの動詞が出現する頻度調査を6ヶ月分のAP電について
行った．この結果，各月での動詞の出現頻度はほとんど一定していることが明
らかになった．もちろんこれは訳語の頻度が一定であることを保証するもので
はないが，一月単位が最低必要な量であろうと考えた．そこで1990年1月，
1991年1月の2ヶ月を英文抽出の母集団とした．

\subsection{作成手順}

作成は以下の手順で行った．図~4に作成したコーパスの一部を示す．
\begin{enumerate}
\item {\bf 英文の抽出}\\
  2ヶ月分のAP電から上記の7動詞を含む文を自動的に抽出した．このとき文の
  長さは15語以下とした．これは，文の長さをあまり長くすると解析が大変に
  なること，類似の文型の個数が減るため有効な原始格フレーム表が作成でき
  ないことによる．

\item {\bf 動詞の支配範囲の認定}\\
  対象動詞が直接支配している範囲を人手で認定した．図~4のENG行がこ
  れを示す．

\item {\bf 英語の格要素の認定}\\
  英語の文章を人手で解析して必要な情報を付与した．精度の高い構文解析器
  が利用できればこの作業はかなり自動化ができる．しかし，現状ではかなり
  の人手の介入が必要であるため，著者らは人手による解析を行うことにした．
  また，人手による精密な構文解析は手間が大きいので，原始格フレーム表を
  作成するのに最低限必要な解析を行うことにした．この解析で認定，付与し
  た情報は，
\begin{itemize}
\item[(I)] 文のタイプ，
\item[(II)] 主格，目的格などの統語単位，
\item[(III)] 統語単位中の主辞単語と前置詞，副詞小詞などの機能語である．
\end{itemize}
(I)，(II)には，それぞれラベルを設定した．文のタイプは11種類設定した．
そして英文中の動詞に文のタイプを示すラベルを付与した．統語単位は24種類
設定した．そして英文中の各統語単位に該当する範囲を\bras{ }で認定し，統
語単位を示すラベルを付与した\footnote{一つの英文内に同一の統語単位が複
  数回出現した場合，例えば副詞句(DD)が3回出現した場合，英語のケースデー
  タには出現順にDD, DD1, DD2と展開したラベルを付与している．}．また，
各統語単位の主辞は{[\ ]}で，機能語は\pp{\enskip}で認定した．ここで作成
したデータを英語のケースデータと呼ぶ．図~4のCASE行がこれにあたる．ラベ
ルの詳細を付録Bの表~6,~7に記す．

\item{\bf 日本語翻訳の付与}\\
  英語のケースデータの主辞と機能語に対して日本語の訳語を人手で付与した．
  図~4のJAP行がこれにあたる．また，JAP行でも主辞と機能語の指定が
  してある．対象の英文一つだけで翻訳できない場合は，前後の文脈を翻訳者
  に提供した．
\end{enumerate}

\begin{figure}[h]
  \begin{center}
    \begin{tabular}{|l@{ }p{0.8\textwidth}|}
      \hline
      19:& ``I just know I'm going to take those rubles and build another
      restaurant,'' he said.\\
      ENG:& I'm going to take those rubles\\
      CASE:& SN\bras{[I]}AX\bras{[be going
        to]}V\bras{[take]}ON\bras{those [ruble]}\\
      JAP:& SN\bras{[私]\pp{は}}AX\bras{[BE GOING TO]}V\bras{[受け取る]}
      ON\bras{[ルーブル]\pp{を}}\\
      &\\
      20:& ``I take everybody seriously,'' Graf said.\\
      ENG:& I take everybody seriously\\
      CASE:&
      SN\bras{[I]}V\bras{[take]}ON\bras{[everybody]}DD\bras{[seriously]}\\
      JAP:& SN\bras{[私]\pp{は}}V\bras{[受け止める]}ON\bras{[すべての人]
        \pp{を}}DD\bras{[真剣に]}\\
      \hline
      \multicolumn{2}{r}{\bras{ } 統語単位，{[ ]} 主辞，\pp{\enskip} 機能語}\\
    \end{tabular}
    \caption{英日対訳コーパス}
  \end{center}
\end{figure}

\begin{table}[h]
  \begin{center}
    \caption{英日対訳コ−パスの作成結果}
    \begin{tabular}{l|rrrrrrr}\hline\hline
      & \multicolumn{1}{c}{come} & \multicolumn{1}{c}{get} &
      \multicolumn{1}{c}{give} & \multicolumn{1}{c}{go} &
      \multicolumn{1}{c}{make} & \multicolumn{1}{c}{run} &
      \multicolumn{1}{c}{take}\\\hline
      英文数& 795 & 867 & 635 & 1204 & 1024 & 440 & 1062\\
      翻訳に文脈が必要であった場合& 3.4\% & 5.2\% & 4.1\% & 3.7\%
      & 6.6\% & 6.0\% & 4.0\%\\ 
      得られたデ−タ数& 782& 849& 637& 941& 1020& 303& 1067\\\hline
    \end{tabular}
  \end{center}
\end{table}


\subsection{コーパスの特徴}

対訳コーパスを作成するのに要した労力は6人月であった．表~2に作成し
たコーパスのデータを示す\footnote{その後も対訳データの作成作業を継続し
  ており，``call'', ``cut'', ``fall'', ``keep'', ``look'', ``put'',
  ``stand'', ``turn''についても対訳コーパスを作成している(総数約6,000)．
  さらに，``give''は490, ``make''は1,770，``take''は1,820データ追加され
  ている．}．
この表の2行目に示した数字は，翻訳を行う場合に前後の文脈が必要であった
比率を示す．すなわち，人間の翻訳者が一つの文だけを見て翻訳ができなかっ
た割合を示している．このような文章の多くは``it''などの代名詞を含んでお
り，翻訳のためにはそれらの指示内容が必要であった．

\section{語形を利用した実験}

\subsection{方針}

提案手法の基本的な能力を調査するために，以下の方針に従って原始格フレー
ム表を作成して格フレーム木獲得実験を行った\cite{TanAndEha93,Tan94a}．
\begin{itemize}
\item 文の種類に応じて格フレーム木を作成\\
  従来，動詞の訳語選択を行う場合，その動詞がどういう文型で使われていて
  も同じ格フレームを利用していた．しかし，同じ動詞でも平叙文，疑問文，
  関係節，不定詞句の中では共起する格要素は異なることが予想される．そこ
  で文型ごとに格フレーム木の作成を行う．

\item コーパスに出現する格要素すべてを利用\\
  これは著者らの使うアルゴリズムによって，どのような格要素が選択される
  かを調査するためである．

\item 格要素の制約としては，主辞の語形と機能語を利用\\
  これらは，それぞれの統語単位を代表する成分であるため，まずこれを利用
  する．
\end{itemize}
学習に利用したプログラムはC4.5(オプションなし，枝刈りなし)である．

\subsection{実験結果}

各動詞の各文型パターンについて格フレーム木の作成実験を行った．訳し分け
の対象にしたのは各動詞とも，頻度が10以上である．これは頻度が小さいと有
効なデータとならないからである．

\begin{table}
  \begin{center}
    \caption{入力諸元と出力 (平叙文)}
    \begin{tabular}{c|rrrrrrr}\hline\hline
      &
      \multicolumn{1}{c}{come}&
      \multicolumn{1}{c}{get} &
      \multicolumn{1}{c}{give}&
      \multicolumn{1}{c}{go}&
      \multicolumn{1}{c}{make}&
      \multicolumn{1}{c}{run}&
      \multicolumn{1}{c}{take}\\\hline
      (1)& 398& 274& 292& 225& 367& 68& 285\\
      (2)& 30& 28& 31& 20& 33& 15& 21\\
      (3)& 10& 9& 9& 8& 8& 3& 10\\\hline
      (4)& 6& 5& 5& 6& 6& 3& 5\\
      (5)& 10.1\%& 5.5\%& 13.0\%& 10.2\%& 6.2\%& 0.0\%& 6.0\%\\\hline
    \end{tabular}\par
    \medskip\null\hskip8em
    \begin{tabular}{r@{ }l}
      (1)& 学習に利用した事例数\\
      (2)& 原始格フレ−ム表に出現した格要素数(異なり)\\ 
      (3)& 動詞の訳語数\\
      (4)& 格フレ−ム木に出現した格要素数(異なり)\\ 
      (5)& 学習した事例を再分類した場合の誤訳率\\ 
    \end{tabular}
  \end{center}
\end{table}
表~3に，入力とした原始格フレーム表の諸元と結果の一覧を示す．ここでは，
平叙文のデータから作成した格フレーム木の結果のみを示した．

\begin{figure}
  \begin{center}
\vspace*{-4mm}
    \begin{minipage}{80mm}
      \large
      \def\baselinestretch{}
      \normalsize
      \begin{tabbing}
        D\bras{}\==over: 引き継ぐ (12.0)\\
        D\bras{}=up: 取る (3.0/1.0)\\
        D\bras{}=0:\\
        $|$\> ON\bras{}\==0: かかる (5.0/1.0) ← A\\
        $|$\> ON\bras{}=action: とる (8.0)\\
        $|$\> ON\bras{}=bronze: 獲得する (9.0)\\
        $|$\> ON\bras{}=hour: かかる (11.0/3.0) ← C\\
        $|$\> ON\bras{}=measures: とる (10.0)\\
        $|$\> ON\bras{}=part: 参加する (33.0/1.0) ← B\\
        $|$\> ON\bras{}=while: かかる (6.0)\\
        $|$\> ON\bras{}=place:\\
        $|$\>\>$|$ SN\bras{}=Sergei Shupletsov: 獲得する (1.0) ← D\\
        $|$\>\>$|$ SN\bras{}=attack: 行われる (4.0) ← D\\
        $|$\> ON\bras{}=time:\\
        $|$\>\>$|$ AX\bras{}=0: かかる (4.0/2.0)\\
        $|$\>\>$|$ AX\bras{}=may: 必要とする (1.0)\\
        $|$\>\>$|$ AX\bras{}=could: かかる (1.0)
      \end{tabbing}
    \end{minipage}
  \end{center}
  \caption{``take''の格フレーム木の一部}
\end{figure}

また図~5に獲得された``take''の格フレーム木の一部を示す．この図では左が
ルートノードである．また各行の右端の数字は，学習に利用した事例を格フレー
ム木で分類した場合に(そのリーフに分類された事例数/事例とリーフの訳語が
一致しなかった数[あれば])である．

以下に結果の特徴を記す．
\begin{enumerate}
\item {\bf 通常の辞書との類似}\\
  図~5に示すように，格フレーム木は直感的に理解しやすいものであっ
  た．これは，格フレーム木に使われた格要素の多くが，通常の辞書に使われ
  ていたことによる．
  例えば``take''の格フレーム木では，AX (助動詞相当語句)，D (副詞小詞)，
  ON (直接目的語名詞句)，SIN (主語の不定詞句)，SN (主語名詞句)が使われて
  いた．この中の，SN，ON，Dは通常の冊子辞書でも頻繁に使われている．
  さらにこの場合，ルートノード，すなわち訳し分けに最も有効な格要素はD
  となっていた．冊子辞書でも副詞小詞は重要な要素と見なされており，動詞
  と組み合わせて別項として記述されていることが多い．
  同様の特徴はその他の動詞にも見られた．表~3の第2行と第4行を比べ
  ると，入力に与えられた格要素の内，実際に格フレーム木で利用された格要
  素は少ないことがわかる．また，利用された格要素の多くは冊子辞書でもよ
  く利用されるものであった．それぞれの動詞の格フレーム木に利用された格
  要素を付録~Cの表~8に示す．

\item {\bf 訳し分けの精度}\\
  得られた格フレーム木は，直観では気付きにくい情報を学習していた．
  図~5のAに示した部分では，ONが何もないときの訳語は「かかる」と学
  習されている．``take''は通常他動詞として用いられるため，目的語がない
  というのは不自然な印象を受ける．しかし，対訳コーパスを調査してみると
  ``take awhile'', ``take long''といった時間表現があることがわかった．
  これらの表現では，目的語ではなく副詞を伴って「かかる」の意味になって
  おり，妥当な学習と言える．
  また図~5のBで示した部分では，``take part''で「参加する」と学習
  されており，``in''は冗長だとされている．これは通常の辞書の記述とは異
  なっている．しかし，これも対訳コーパス中に実際に``in''を伴わない用法
  が見つかり，むしろ望ましい学習と言える．このように格フレーム木の細部
  を見た場合には，コーパスの動詞を訳し分けるのに適した情報が学習されて
  いた．
  また，表~3の第5行目には誤訳率を記している．これは，学習に利用し
  た事例を格フレーム木で分類した場合に，誤った訳語が出力された割合であ
  る．学習データであっても誤訳率がゼロにならない理由は，
\begin{itemize}
\item[(I)] 格フレーム木を作成するアルゴリズムが，過剰学習を避けるため
  多少の誤訳を許すように設定されていること(付録A，脚注10, 11参照)
\item[(II)] 1文の範囲では訳し分けができない場合があることである．
\end{itemize}
以下具体的に説明する．(I)に該当する例は図~5のCの部分に見られる．
ここでは``take hour'' (11例)は「かかる」と訳すように学習されたが，3例は
誤っている．これらは「必要とする」と訳されなくてはならない．原始格フレー
ム表を調べてみると，``take hour''の11事例の主語はすべて異なっていた．
そこで，このノードの下をさらに主語で分岐すると上記の2つの訳語は正しく
分類することができる．ただしこの場合，主語の語形によって11の枝が生成さ
れ，リーフには1事例しか分類されない．このためこれは予測力の低い過剰学
習であると判定され，このような分岐は実行されない．

(II)に相当するのは同一の文でありながら動詞の訳が違う場合である．
``make''のコーパスには``I am going to make it.''という同一の文章で
``make''の訳語が「作る」と「成功する」になる2通りの場合が収録されてい
る\footnote{人手で翻訳した際には当然文脈を参照している．4.4節(4)，4.5
  節参照．}．これらの訳語はこの文の格要素だけで訳し分けることはできな
いため，誤りを含んだまま枝の生成は停止せざるを得ない．この誤りは文脈を
扱わない本手法の限界を示すものである．

\item {\bf 補償的な学習}\\
  図~5のDでは``take place''が「獲得する」と「行われる」に訳し分け
  られている．前者は``take third place''「3位を獲得する」というコーパ
  スの例文から学習されており，後者は成句表現を学習したものである．前者
  は通常の辞書には記述されていない．
  ここで興味深いのは学習された訳し分けの条件である．ここでは主語の性質
  によって訳し分けられている．すなわち，主語が「人間」の時は「獲得する」
  となり，「動作名詞」の時は「行われる」となっている．これは言語学的に
  納得できる条件である．
  しかし，この訳し分けは``place''の前の修飾語の有無で行うことも可能で
  ある．前者は``third''という修飾を受けているが，後者は成句表現である
  ため，修飾は基本的に受けないからである．
  この実験の原始格フレーム表には修飾語を利用していないため，このような
  学習は起こりえない．そこで，これに替わる条件を学習しているのである．
  このような学習を本論文では「補償的な学習」と呼ぶ．
  この「補償的な学習」は，その他の格フレーム木でも数多く見られた．例え
  ば図~6 (``come''の格フレーム木の一部)では，主語が``it''の場
  合に，その内容を参照することなく，前置詞を条件として動詞の訳し分けが
  行われている．また，この条件で事例の分類を行った結果，15事例の内12事
  例で訳語が正しく選択されている．
  補償的な学習は，与えられた情報の中で最適な訳し分け条件を見つけ出すア
  ルゴリズムの性質を反映したものであり，ここで述べたように人手で見つけ
  にくい有効な格要素を発見する上で有用である．
  しかし，この性質は格フレーム木のノードとして本来使うべき格要素を選ば
  ずに，たまたま原始格フレーム表の訳語をきれいに分類する格要素を選択す
  ることにつながる場合もある．
  このため，必ずしも言語学的な直観にあわない格要素が格フレーム木に含ま
  れる場合もあり，その正しさは人間が判定する必要がある．

\item {\bf 文型による格フレーム木の違い}\\
  平叙文に比べてその他の文型の事例数が少ないため同列の比較はできないが，
  文型による格フレーム木の違いはかなり顕著であった．例えば比較的事例数
  の多かった「to不定詞として用いられた``take''と``make''」の格フレーム木
  に使われた格要素はON (直接目的語名詞句)だけであった．また，これらの格
  フレーム木には平叙文で重要であったD (副詞小詞)は出現しなかった．これ
  らのことは，格フレーム木は文型に合わせて作成する必要があることを示唆
  している．
\end{enumerate}

\begin{figure}
  \begin{center}
    \begin{minipage}{80mm}
      \large
      \def\baselinestretch{}
      \normalsize
      \begin{tabbing}
        D\=\bras{}=0:\\
        $|$\> PN\=1\bras{}c=0:\\
        $|$\>\>$|$ SN\=\bras{}=it:\\
        $|$\>\>$|$\>$|$ PN\bras{}=0: 来る (5.0/2.0)\\ 
        $|$\>\>$|$\>$|$ PN\bras{}=as: なる (2.0)\\ 
        $|$\>\>$|$\>$|$ PN\bras{}=at a time of: 起きる (1.0)\\ 
        $|$\>\>$|$\>$|$ PN\bras{}=during: 来る (1.0)\\ 
        $|$\>\>$|$\>$|$ PN\bras{}=like: 起きる (1.0)\\ 
        $|$\>\>$|$\>$|$ PN\bras{}=on: なる (1.0)\\ 
        $|$\>\>$|$\>$|$ PN\bras{}=to: なる (4.0/1.0)
      \end{tabbing}
    \end{minipage}
  \end{center}
\caption{``come''の格フレーム木の一部}
\end{figure}

\subsection{議論1}

本章では，語形を利用した格フレーム木の学習を行った結果，大局的には人間
の直観に近い，わかりやすい格フレーム木が獲得された．また，細かく見ると
人手で獲得するよりも精密な条件が抽出される場合や，人手では気がつきにく
い条件が抽出されることがわかった．

ここで問題になるのは格フレーム木の一般性である．本手法では，格要素の制
約として語形を使ったため，未学習の事例の動詞を訳し分ける性能には疑問が
ある．なぜなら，未学習の事例を格フレーム木のルートからリーフへ照合する
過程で，制約(アークのラベル)が未知の語形になり，格フレーム木をたどれな
くなるからである．そこで，次章ではこの問題を解決する手法を提案して実験
を行う\cite{Tan94b}．

\section{意味コードを用いた実験}

\subsection{意味コードの利用}

著者らが利用している学習プログラムでは，事例と格フレーム木との照合中に，
あるノードで行き詰まった場合には訳語を予測して出力するようになっている．
このプログラムでは格フレーム木の作成と同時に各学習事例がどのリーフに分
類されるかを計算しており，リーフにはその頻度が付与されるようになってい
る(図~5の右端の数字)．そして訳語の予測にはこの頻度を利用している．

今，図~5の格フレーム木で，``D=0''かつ ``ON=place''かつ ``SN=war''とい
う事例を分類しようとすると，SNの制約(war)が未知語であるため訳語が決定
できない．このように行き詰まった場合には，先に述べた頻度を利用して，そ
のノードの下で最も頻度の高い訳語を予測値として出力する．今の場合，行き
詰まったノードSN\footnote{2つのSNがあるが(図~5のD)これは同じノードであ
  る．}の下で最高の頻度であった「行われる」が出力される．このような予
測機能のおかげで未知語は表面上は問題にはならない．しかし，この「局所的
な多数決原理」のヒューリスティクスがどれだけ有効であるかは不明である．

未知語になる可能性が高いのはオープンクラスの語彙，特に名詞である．これ
を軽減する手法としては，名詞を意味コード\footnote{名詞をある体系に従っ
  て分類し，それに与えたコード．例えば，類語国語辞典\cite{類語85}や分
  類語彙表\cite{分類64}などの分類番号．}で置換することが考えられる．こ
れによって膨大な数の名詞を一定の分類数で押さえることができるからである．
そこで本章では，名詞を意味コードで置換した格フレーム木の獲得を行い，こ
れが未学習の事例の動詞の訳し分けにどの程度有効かを評価する．

語形の代わりに意味コードを利用した格フレーム木を獲得するには，原始格フ
レーム表の語形を意味コードで置き換えて学習すればよい．これには英語の語
形がどの意味で使われたかを決定する必要があるが，意味的な曖昧性があるた
め，英語の語形だけを見たのでは自動的には決定できない．
そこで対訳コーパスに意味コードを付与して，これを原始格フレーム表に変換
することにした．対訳コーパスには日本語の訳語があり，これが英語の語形の
意味を表しているため，比較的容易に英語の語形の意味を決定できるからであ
る．

\subsection{コーパスへの意味コードの付与}

本論文で利用した意味コードは類語国語辞典\cite{類語85}のコードである．
これは，基本的には3桁の10進分類であり，補助的に4桁目が利用されている．
4桁すべてを利用すると2,794個の分類となる．

意味コードを付与したのは名詞を主辞に取る格要素である:  SN (主語名詞句),
ON (直接目的語名詞句), CN (補語名詞句), QN (間接目的語名詞句), PNc (前
置詞句本体)．

意味コードの付与は，あらかじめ作成してあるテーブル，(英単語，日本語訳
語，意味コード[，意味コード])の形式，を利用して半自動的に行った
\cite{TanAndEha91}．具体的には対訳コーパスの英語と日本語の対応する格要
素の主辞をこのテーブルで参照してコードを付与した\footnote{類語国語辞典
  は(日本語,意味コード[,意味コード])の形式のデータである．著者らは，こ
  れを英和辞書と照合して(英単語,日本語訳語,意味コード[,意味コード])の
  形式のデータを既に作成している．英語と日本語では，単語の持つ意味の広
  がりに差がある．このため，英単語と類語国語辞典のデータを組み合わせる
  際に，意味コードの中に不適切なコードが発生するが，これは人手で排除し
  ている．}．意味コードが自動的に付与できない場合には，人手で付与でき
るものは付与し，それでもわからない名詞については不明を意味するコードを
付与した．これはほとんど固有名詞で，人名，地名の判別ができない単語であっ
た．図~7のCODE行に意味コードを付与したデータを示す．この図では2桁の意
味コードを与えているが，任意の桁の意味コードを付与できる．

\begin{figure}
  \begin{center}
    \begin{tabular}{|l@{ }l|}
      \hline
      ENG:& my truth gave me all the strength I needed\\
      CASE:&
      SN\bras{[truth]}V\bras{[give]}QN\bras{[me]}ON\bras{[strength]}\\
      JAP:& SN\bras{[誠実さ]\pp{は}}V\bras{[与える]}QN\bras{[私]\pp{に}}
      ON\bras{[強さ]\pp{を}}\\
      CODE:& SN\bras{[83]}V\bras{[give]}ON\bras{[83]}QN\bras{[50]}\\
      \hline
    \end{tabular}
  \end{center}
\caption{意味コードを付与した英日対訳コーパス}
\end{figure}

\subsection{実験方法}

\begin{table}
  \begin{center}
    \caption{実験方法}
    \begin{tabular}{c|c}\hline\hline
      実験 & 制約\\\hline
      1 & 語形\\
      2 & 4桁のコ−ド\\
      3 & 3桁のコ−ド\\
      4 & 2桁のコ−ド\\
      5 & 1桁のコ−ド\\\hline
      \multicolumn{2}{c}{}\\
      \multicolumn{2}{c}{出現格要素をすべて利用}
    \end{tabular}
  \end{center}
\end{table}

\begin{table}
  \begin{center}
    \caption{入力デ−タ諸元}
    \begin{tabular}{l|rrrrrrr}\hline\hline
      & \multicolumn{1}{c}{come}& \multicolumn{1}{c}{get}&
      \multicolumn{1}{c}{give}& \multicolumn{1}{c}{go}&
      \multicolumn{1}{c}{make}& \multicolumn{1}{c}{run}&
      \multicolumn{1}{c}{take}\\\hline
      データ数& 398& 274& 292& 225& 367& 68& 285\\
      訳語異なり数& 10& 9& 9& 8& 8& 3& 10\\
      基準誤訳率& 66.3\%& 73.3\%& 64.3\%& 52.4\%& 27.7\%& 32.3\%& 82.8\%\\
      基準誤訳を率を与える訳語& 来る& なる& 与える& 行く& する& 走る& かかる\\
      \hline
    \end{tabular}
  \end{center}
\end{table}
5章の実験で用いた平叙文のコーパスに対して，表~4に示す5つの実験を行った．
原始格フレーム表に記述する格要素は5章と同じく，コーパスに現われたすべ
ての格要素である．
これらの実験の違いは，格要素に与えた制約である．実験1では，制約として
語形を与えた．この実験は評価方法を除けば5章の実験と基本的に同じである．
実験2--5は，意味コードの粗さと動詞の訳し分けの精度の関係を調べることを
目的とし，4桁から1桁の意味コードを制約として与えた．

実験に用いたデータの諸元を表~5に記す．
この表の第3行の基準誤訳率とは，各動詞での最高頻度の訳語を入力に関わら
ず出力した場合に発生する誤訳率である\footnote{言い換えると6.1節の「局
  所的な多数決原理」をルートノードに適用して全事例を分類した場合の誤訳
  率である．}．``take''は訳語の分布が平坦であるため，高い基準誤訳率に
なっている．また第4行には基準誤訳率を与える訳語を記した．

格フレーム木の評価は，学習に利用した事例を入力した場合と，学習に利用し
ていない事例を入力した場合の誤訳率で行った．誤訳率は，事例と学習プログ
ラム出力の訳語が一致しなかったものの相対頻度である．このとき，誤訳率の
精度を確保するために原始格フレーム表の事例を5分割してクロスバリデーショ
ン法で評価した．この手法では，評価データでの誤訳率の計算は以下のように
行われる．
\begin{itemize}
\item 原始格フレーム表を5つに分割して80\%の事例で格フレーム木を学習する．
\item 残りの20\%の事例をこの格フレーム木で分類し，誤訳率を計算する．
\item この操作を5回，データをシフトしながら行い，訳語の平均誤訳率を算出する．
\end{itemize}
学習データ上での誤訳率も同様に算出した．(以下，誤訳率は平均誤訳率のこ
とである)格フレーム木の獲得に用いたプログラムは5章と同じC4.5 (オプショ
ンなし，枝刈りなし)である．

\subsection{結果}


\begin{figure}
\begin{center}
  
\epsfile{file=fig8.eps,width=137.0mm}
\end{center}
  \caption{誤訳率の推移}
\end{figure}

実験1--5の学習データ，評価データでの誤訳率を図~8に示す．このデータから
次の特徴が読み取れる．
\begin{itemize}
\item 実験1: 殆どの動詞で評価データでの誤訳率は最大である．
\item 実験1--5: 学習データ，評価データの誤訳率ともほぼ下に凸の曲線を
  描いた．極小値を与える分類コードの桁数は動詞によって異なった．
\item いずれの実験でも，評価データ上の誤訳率は基準誤訳率より低い．実験
  で得られた意味コード(2桁)付きの格フレーム木を図~9に示す．
\end{itemize}

\begin{figure}[h]
  \begin{center}
    \begin{minipage}{80mm}
      \large
      \def\baselinestretch{}
      \normalsize
      \begin{tabbing}
        ON\bras{}\==10: 与える (4.0)\\
        ON\bras{}=12: 言う (3.0/1.0)\\
        ON\bras{}=15: 与える (7.0)\\
        ON\bras{}=26: 与える (3.0)\\
        ON\bras{}=27: 伝える (4.0/1.0)\\
        ON\bras{}=34: する (32.0/1.0) ← A\\
        ON\bras{}=36: 与える (3.0)\\
        ON\bras{}=41: する (7.0/1.0)\\
        ON\bras{}=46: もたらす (3.0/1.0)\\
        ON\bras{}=47: 与える (4.0/1.0)\\
        ON\bras{}=81: 示す (4.0/2.0)\\
        ON\bras{}=93: 与える (3.0)\\
        ON\bras{}=97: 与える (2.0)\\
        ON\bras{}=99: 与える (2.0)\\
        ON\bras{}=0:\\
        $|$\>D\bras{}=0: 与える (2.0)\\
        $|$\>D\bras{}=up: 諦める (11.0)
      \end{tabbing}
    \end{minipage}
  \end{center}
  \caption{``give''の意味コード付きの格フレーム木(部分)}
\end{figure}

\subsection{議論2}

本章の実験によって，殆どの動詞で意味コードを抽象化すると評価データ上で
の誤訳率が一旦減少し，また上昇することが確認された．この理由は，意味コー
ドを利用すれば未知語が減るものの，意味コードが粗くなりすぎると格フレー
ム木の分類能力が低下するためだと思われる．

意味コードが有効に働いたのは，図~9のAで示すような，事例が集中して，
かつ訳語が正しく決定された部分である(意味分類コード34は「陳述」を表す)．
また，最小の誤訳率を与える意味コードの粒度は動詞によって異なっていた．
従来，固定的な意味コードを格フレームに与える場合が多かったが，そこには
再考の余地があることをこの結果は示唆している．

付録Cの表~9に格フレーム木に採用された格要素を記した．5章の実験と同じよ
うに，多くの格要素は冊子辞書に利用されているものであり，得られた格フレー
ム木は言語学的な直観に合うものであった．

本実験での各動詞の評価データ上での最小の誤訳率は2.4\%から32.2\%となっ
た．これと基準誤訳率の差は13.6\%から55.3\%となり，かなりの改善が得られ
ている．

すべての動詞で十分な精度が得られたわけではないが，``take''のように基準
誤訳率が82.8\%もあるような動詞に対して誤訳率27.5\%が得られたことは，本
手法の基本的な有効性を示したものと考えられる．

\section{むすび}

従来の人手による格フレーム獲得の問題点を明らかにし，これを解決する手法
を提案した．ここで行った提案は，格フレームを決定木で表現すること(格フ
レーム木)，および，これを統計的な帰納学習アルゴリズムを用いて対訳のコー
パスから獲得することである．

この提案に添って語形，意味コードを利用した2種類の実験を行った．得られ
た結果は以下の通りである．
\begin{itemize}
\item 人間の直観に近くかつ精密な情報が学習された．
\item 意味コードを利用すると未学習データでの誤訳率が低下した．
また，動詞によって最適な意味コードの粒度は異なった．
\end{itemize}
今回，意味コードを利用して得られた誤訳率は，動詞によっては必ずしも低く
ないものがある．この原因としては，意味コードを利用した格要素では，語形
が全く利用されていないことが挙げられる．この結果，本来語形で記述すべき
成句的な表現も意味コードで抽象化されており，これが高い誤訳率につながっ
たと考えられる．これを解決するには，同一格要素内で意味コードと語形の両
者を柔軟に用いる学習アルゴリズムの開発が必要である．

もう一つ，意味コード体系そのものの問題が挙げられよう．著者らが利用した
意味コードは動詞の訳し分けを目的に開発された体系ではない．そのため，こ
れが誤訳率を高めている可能性はある．もし，著者らの目的に合うような意味
コード体系を利用できれば，さらに誤訳率を減らすことができるであろう．

\acknowledgment

類語国語辞典の研究利用を許可していただいた(株)角川書店，ならびに，配信
電文の研究利用を許可していただいたAP通信社に感謝する．この研究を進める
にあたって議論して頂いたNHK放送技術研究所，先端制作技術研究部の江原暉
将主任研究員，および機械翻訳研究グループの諸氏に感謝する．




\begin{thebibliography}{}

\bibitem[\protect\BCAY{Aizawa et~al.}{Aizawaet~al. }{1990}]{Aiz90}
Aizawa, T.\BBACOMMA\  et.~al \BBOP 1990\BBCP.
\newblock \BBOQ A Machine Translation System for Foreign News in Satellite
  Broadcasting\BBCQ\
\newblock In {\Bem Proceedings of the 13th Coling}, \lowercase{\BVOL}~3, \BPGS\
  308--310.

\bibitem[\protect\BCAY{Almuallim \& Akiba}{Almuallim \&
  Akiba}{1994}]{Alm94}
Almuallim, H.\BBACOMMA\  \& Akiba, Y. \BBOP 1994\BBCP.
\newblock \BBOQ Induction of Japanese-English Translation Rules from Ambiguous
  Examples and a Large Semantic Hierarchy\BBCQ\
\newblock {\Bem Journal of Japanese Society for Artificial Intelligence}, {\Bbf
  9}  (5), 730--740.

\bibitem[\protect\BCAY{Breiman, Friedman, Olsen, \& Stone}{Breiman
  et~al.}{1984}]{Brei84}
L., Breiman, B., Friedman, J.~H., Olsen, R.~A., \& Stone, C. \BBOP 1984\BBCP.
\newblock {\Bem Classification and Regression Trees}.
\newblock Chapman \& Hall.

\bibitem[\protect\BCAY{Quinlan}{Quinlan}{1986}]{Qui86}
Quinlan, J.~R. \BBOP 1986\BBCP.
\newblock \BBOQ Induction of Decision Trees\BBCQ\
\newblock {\Bem Machine Learning}, {\Bbf 1}, 81--106.

\bibitem[\protect\BCAY{Quinlan}{Quinlan}{1993}]{Qui93}
Quinlan, J.~R. \BBOP 1993\BBCP.
\newblock {\Bem C4.5 Programs for Machine Learning}.
\newblock Morgan Kaufmann.

\bibitem[\protect\BCAY{Tanaka}{Tanaka}{1993}]{Tan93}
Tanaka, H. \BBOP 1993\BBCP.
\newblock \BBOQ MT User Experience (STAR: Nippon Hoso Kyokai)\BBCQ\
\newblock In Nirenberg, S.\BED, {\Bem Progress in Machine Translation}, \BPGS\
  247--249. IOS \& Ohmsha.

\bibitem[\protect\BCAY{Tanaka}{Tanaka}{1994}]{Tan94a}
Tanaka, H. \BBOP 1994\BBCP.
\newblock \BBOQ Verbal Case Frame Acquisition from a Bilingual Corpus: Gradual
  Knowledge Acquisition\BBCQ\
\newblock In {\Bem Proceedings of the 15th Coling}, \lowercase{\BVOL}~2, \BPGS\
  727--731.

\bibitem[\protect\BCAY{田中}{田中}{1994}]{Tan94b}
田中英輝 \BBOP 1994\BBCP.
\newblock \JBOQ 意味コード付き対訳データからの訳し分け情報の自動学習\JBCQ\
\newblock \Jem{情報処理学会第49回全国大会}, 3\JVOL, \BPGS\ 205--206.

\bibitem[\protect\BCAY{田中, 畑田, 江原}{田中\Jetal}{1994}]{TanAndHat94} 
田中英輝, 畑田のぶ子, 江原暉将 \BBOP 1994\BBCP.
\newblock \JBOQ 日本語字幕作成用英日機械翻訳システムの研究経緯と今後\JBCQ\
\newblock {\Bem NHK R\&D}, {\Bbf 34}, 33--46.

\bibitem[\protect\BCAY{田中, 江原}{田中, 江原}{1991}]{TanAndEha91}
田中英輝\BBACOMMA, 江原暉将 \BBOP 1991\BBCP.
\newblock \JBOQ 名詞への意味マーカーの自動付与\JBCQ\
\newblock \Jem{情報処理学会第43回全国大会}, 3\JVOL, \BPGS\ 211--212.

\bibitem[\protect\BCAY{田中, 江原}{田中, 江原}{1993}]{TanAndEha93}
田中英輝\BBACOMMA, 江原暉将 \BBOP 1993\BBCP.
\newblock \JBOQ 対訳データからの「訳し分け情報」の自動抽出\JBCQ\
\newblock \Jem{情報処理学会第47回全国大会}, 3\JVOL, \BPGS\ 195--196.

\bibitem[\protect\BCAY{国立国語研究所}{国立国語研究所}{1964}]{分類64}
国立国語研究所 \BBOP 1964\BBCP.
\newblock \Jem{分類語彙表}.
\newblock 秀英出版.

\bibitem[\protect\BCAY{大野, 浜西}{大野, 浜西}{1985}]{類語85}
大野晋\BBACOMMA, 浜西正人 \BBOP 1985\BBCP.
\newblock \Jem{類語国語辞典}.
\newblock 角川書店.

\bibitem[\protect\BCAY{宇津呂, 松本, 長尾}{宇津呂\Jetal}{1993}]{UtuAndMat93}
宇津呂武仁, 松本裕治, 長尾眞 \BBOP 1993\BBCP.
\newblock \JBOQ 二言語対訳コーパスからの動詞の格フレーム獲得\JBCQ\
\newblock \Jem{情報処理学会論文誌}, {\Bbf 34}  (5), 913--924.

\end{thebibliography}

\appendix
\section{決定木作成アルゴリズム}

図~10に基本的なアルゴリズムを示す．
ステップ2の終了条件は，基本的には訳語が一種類となることである
\footnote{この条件では，枝分かれの多くなる格要素が選択されがちになり，
  各リーフに分類される事例が少なくなってしまう．これは未知データに対す
  る予測力の低い，すなわち過剰学習を起こした木である．このため，ある程
  度の誤分類を含んだ段階で停止するようになっている．この条件は経験的に
  決められているが，この問題については本論文ではこれ以上立ち入らない．
  詳細は\cite{Qui93}を参照のこと．}．ステップ3の「訳語を最も良く分類す
る格要素を選択する」という部分は以下の手順で決定される．


現在の\hspace*{0.1mm}ノード\hspace*{0.1mm}の下\hspace*{0.1mm}にある\hspace*{0.1mm}訳語の\hspace*{0.1mm}集合を\hspace*{0.5mm}$S$\hspace*{0.5mm}と\hspace*{0.1mm}し，\hspace*{0.5mm}そ\hspace*{0.1mm}の\hspace*{0.1mm}大\hspace*{0.1mm}き\hspace*{0.1mm}さ\hspace*{0.5mm}(\hspace*{0.1mm}訳\hspace*{0.1mm}語\hspace*{0.1mm}数\hspace*{0.1mm})\hspace*{0.5mm}を\hspace*{1mm} $\left|
S \right|$ \hspace*{0.5mm}と\hspace*{0.1mm}す\hspace*{0.1mm}る\hspace*{0.1mm}．\hspace*{0.5mm}$S$に\\は$k$種類\hspace*{0.1mm}の\hspace*{0.1mm}訳\hspace*{0.1mm}語\hspace*{0.1mm}が\hspace*{0.1mm}あ\hspace*{0.1mm}る\hspace*{0.1mm}と\hspace*{0.1mm}し\hspace*{0.1mm}，\hspace*{1mm}各\hspace*{0.1mm}訳\hspace*{0.1mm}語\hspace*{0.1mm}の\hspace*{0.1mm}名\hspace*{0.1mm}前\hspace*{0.1mm}を
$C_j(1\le j\le k)$で\hspace*{0.1mm}表\hspace*{0.1mm}す\hspace*{0.1mm}．$\mbox{\em freq}(C_j,S)$は$S$の中で\\の訳語
$C_j$の個数を表すとする．そうすると${{\mbox{\em freq}(C_j,S)}\over
  {\left|S\right|}}$は$S$の中での訳語$C_j$の相対頻度を表し
$-\log_2\left({{{\mbox{\em freq}(C_j,S)}\over{\left| S \right|}}}
\right)$はその情報量になる．\hspace*{-0.2mm}ここですべての訳語について情報量を計算して
その平\\均を求めると
\begin{displaymath}
  \mbox{\em info}(S)=
  -\sum\limits_{j=1}^k{{{\mbox{\em freq}(C_j,S)}\over{\left|S\right|}}}
  \times\log_2\left({{{\mbox{\em freq}(C_j,S)}\over{\left|S\right|}}}\right)
\end{displaymath}
となる．\hspace*{0.2mm}これは現在の訳語分布が持つエントロピー\hspace*{0.1mm}(乱雑さ)\hspace*{0.1mm}を示す．\hspace*{0.2mm}ここであ
る格要素$X$を選択する．そしてその制約(単語)によって$S$が$n$個の部分集
合に分割されたとする．$S_i(1\le i\le n)$を各部分集合とする．この各部分
集合に対して
\begin{displaymath}
  \mbox{\em info}_X(S)=
  \sum\limits_{i=1}^n{{{\left|{S_i}\right|}\over{\left|S\right|}}}
  \times \mbox{\em info}(S_i)
\end{displaymath}
を計算するとこれは$X$を選択した後にできた部分集合が持つエントロピーの
平均を表す．これらから
\begin{displaymath}
  \mbox{\em gain}(X)=\mbox{\em info}(S)-\mbox{\em info}_X(S)
\end{displaymath}
を計算する．これは分割前と後のエン
トロピーの減少，すなわち格要素$X$を選択して分類したことによる訳語の分
布の乱雑さの減少を示している．基本的には，これを最大にするような格要素
を最適な格要素として選択する\footnote{ただし，エントロピーの減少を利用
  した場合は，枝分かれの多い木が作成されがちになり過剰学習が発生しやす
  くなる．このためC4.5では，エントロピーの減少量を枝分かれの数の評価値
  で割った量を採用している．この詳細についても\cite{Qui93}を参照．}．


\medskip
\vbox{
  \begin{center}
    \begin{tabular}{l|p{0.7\textwidth}|}
      \cline{2-2}
      ステップ1&
      すべての訳語をルートノードに配置する．ステップ2へ\\
      \cline{2-2}
      ステップ2&
      終了条件を満足すれば終わり．そうでなければステップ3へ\\
      \cline{2-2}
      ステップ3&
      分岐ノードである．以下の手順で子を作成．\nl
      自分の下の訳語を最も良く区別する格要素を1つ選択して現在のノード
      の名前にする．\nl
      選択した格要素の制約 (単語)に従って訳語を部分集合に分割する．\nl
      それぞれの部分集合にノードを付与し，親ノードとの間に制約 (単語)
      の名前を持つリンクを張る．\nl
      作成したノードすべてに対してステップ2を実行．\\
      \cline{2-2}
    \end{tabular}\\
    \medskip
    {\small{\bf 図~10}\enskip アルゴリズムの概要}
  \end{center}}

\section{コーパスに付与したラベルの詳細}

\vbox{
  \begin{center}
    \small{\bf 表~6}\enskip 文タイプ (動詞に付与，以下の文型から抽出
      したことを示す)\\
    \bigskip
    \begin{tabular}{ll}
      AIV & 疑問副詞を用いた文\\
      GV & 動名詞句内\\
      IMV & 命令文\\
      IV & 不定詞句内\\
      PASV & 受動態\\
      PIV & 疑問代名詞を用いた文\\
      PV-ED & 過去分詞形(分詞構文／完了形など)．受動態は除く\\
      PV-ING & 現在分詞形(分詞構文／進行形など)\\
      PVQ & YES-NO疑問文\\
      RV & 関係詞節の中\\
      V & 平叙文
    \end{tabular}
  \end{center}}

\vbox{
  \begin{center}
    \small{\bf 表~7}\enskip 統語単位　(動詞以外に付与)\\
    \medskip
    \begin{tabular}{ll}
      AX & 助動詞句\\
      C-ED & 補語の過去分詞句\\
      C-ING & 補語の現在分詞句\\
      CA & 補語形容詞句\\
      D & 複合動詞の要素である副詞的小詞に付与\\
      CG & 補語の動名詞句\\
      CIN & 補語の不定詞句(詳しくはCIN：不定詞とCINc：不定詞句本体)\\
      CN & 補語名詞句\\
      DC & 副詞節(詳しくはDC：接続詞とDCc：副詞節本体)\\
      DD & 副詞句\\
      DP-ED & 副詞句としての過去分詞形\\
      DP-ING & 副詞句としての現在分詞形\\
      INF & 不定詞句：副詞的用法(詳しくはINF：不定詞とINFc：不定詞句本体)\\
      INFR & 不定詞句：結果を表す場合(詳しくはINFR：不定詞とINFRc：不
      定詞句本体)\\ 
      OC & 目的節(詳しくはOC：接続詞とOCc：目的節本体)\\
      OG & 目的語としての動名詞句\\
      OIN & 目的語としての不定詞句(詳しくはOIN：不定詞とOINc：不定詞句本体)\\
      ON & 直接目的語名詞句\\
      PN & 前置詞句(詳しくはPN：前置詞とPNc：前置詞句本体)目的・副詞的用法\\
      QN & 間接目的語名詞句\\
      SC & 主語節\\
      SG & 主語動名詞句\\
      SIN & 主語の不定詞句(詳しくはSIN：不定詞とSINc：不定詞句本体)\\
      SN & 主語名詞句
    \end{tabular}
  \end{center}}

\vfill

\section{実験で学習された格要素}

\vbox{
  \begin{center}
    \small {\bf 表~8}\enskip
    語形を利用した格フレーム木に採用された格要素 (平叙文)\\
    \medskip
    \begin{tabular}{ll}
      COME & D, DD, PN1c, PN, PNc, SN\\
      GET & C-ED, D, DD, ON, SN\\
      GIVE & AX, D, DD, ON, SN\\
      GO & AX, D, DC, DD1, PN, SN\\
      MAKE & AX, CIN, D, DD, ON, PN\\
      RUN & D, DD, PN\\
      TAKE & AX, D, ON, SIN, SN\\
      \multicolumn{2}{r}{}\\
      \multicolumn{2}{r}{格要素の数字については4.4節の脚注3を参照．}
    \end{tabular}
  \end{center}}

\vbox{
  \begin{center}
    \small{\bf 表~9}\enskip
    意味コード付き格フレームに採用された格要素 (平叙文)\\
    \medskip
    \begin{tabular}{ll}
      \begin{tabular}{ll}
        \multicolumn{2}{l}{COME}\\
        4桁 & D, DD, DP-ED, INF, PN1, PN, SN\\
        3桁 & CA, D, DC, DD, PN1, PN, PNc, SN\\
        2桁 & D, DD, DP-ED, INF, PN1, PN, SN\\
        1桁 & CA, D, DC, DD, PN1, PN, PNc, SN\\
      \end{tabular}&
      \begin{tabular}{ll}
        \multicolumn{2}{l}{GET}\\
        4桁 & AX, C-ED, D, DD, ON, PN\\
        3桁 & AX, C-ED, D, DD, ON, PN, SN\\
        2桁 & AX, C-ED, D, DD, ON, PN\\
        1桁 & C-ED, D, DD1, DD, ON, PN, SN\\
      \end{tabular}\\
      &\\
      \begin{tabular}{ll}
        \multicolumn{2}{l}{GIVE}\\
        4桁 & AX, D, DD, ON, QN, SN\\
        3桁 & D, DD, ON, QN, SN\\
        2桁 & AX, D, DD, ON, QN, SN\\
        1桁 & AX, DD, ON, PN, PNc, QN, SN\\
      \end{tabular}&
      \begin{tabular}{ll}
        \multicolumn{2}{l}{GO}\\
        4桁 & CA, D, DC, DD, PN, SN\\
        3桁 & AX, D, DC, DD, PN, PNc, SN\\
        2桁 & CA, D, DC, DD, PN, SN\\
        1桁 & AX, D, DC, DD, PN, PNc, SN\\
      \end{tabular}\\
      &\\
      \begin{tabular}{ll}
        \multicolumn{2}{l}{MAKE}\\
        4桁 & AX, CIN, D, DC, DD, ON, PNc, SN\\
        3桁 & AX, CIN, D, DD, ON, SN\\
        2桁 & AX, CIN, D, DC, DD, ON, PNc, SN\\
        1桁 & AX, CIN, D, DD, ON, PNc, SN\\
      \end{tabular}&
      \begin{tabular}{ll}
        \multicolumn{2}{l}{RUN}\\
        4桁 & D, PN, SN\\
        3桁 & D, SN\\
        2桁 & D, PN, SN\\
        1桁 & D, SN\\
      \end{tabular}\\
      &\\
      \begin{tabular}{ll}
        \multicolumn{2}{l}{TAKE}\\
        4桁 & D, ON, SIN, SN\\
        3桁 & AX, D, ON, SIN, SN\\
        2桁 & D, ON, SIN, SN\\
        1桁 & AX, D, ON, PN, PNc, SN\\
      \end{tabular}&\\
    \end{tabular}
  \end{center}}


\begin{biography}
\biotitle{略歴}
\bioauthor{田中 英輝}{
1982年九州大学工学部電子工学科卒業．
1984年同大学院修士過程修了．
同年，NHK入局．1987年より放送技術研究所にて勤務．
機械翻訳，機械学習の研究に従事．
}
\bioreceived{受付}
\biorevised{再受付}
\bioaccepted{採録}

\end{biography}
\end{document}
