<?xml version="1.0" ?>
<root>
  <title/>
  <author/>
  <jkeywords/>
  <section title="Introduction">Machinetranslationrequires(1)highaccuracyand(2)aquickresponse.First,thereisnodoubtthattranslationaccuracyisimportantinalmostanysituation,anditisdesirabletominimizehumaninterventionsuchaspre-,inter-,andpost-interactionbetweenuserandmachine,especiallyinreal-timeapplicationssuchasinterpretingtelephony.Second,apracticalthroughputisimportantformachinetranslationwitharealisticdomainnotonlyforprocessingalarge-scaletextbutalsoforreal-timeapplications.ThispaperproposesanExample-BasedApproach(EBA)usingAssociativeProcessors(APs).EBAstranslatebymimickingthebest-matchtranslationexamples(hereafter,``examples''),whicharederivedfromcorpora.EBAsareknowntoperformstructuraldisambiguation,targetexpressionselectionandwholetranslationaccurately.Therefore,theseapproachesfulfillthefirstrequirement.ThesecondrequirementisalsofulfilledbyanEBAusingAPsasfollows.ThecentralmechanismofEBAs,Example-Retrieval(ER),retrievestheexamplesmostsimilartotheinputexpressionfromanexampledatabase.ERbecomesthedominantcomponentasthesizeoftheexampledatabaseincreases.Inthiswork,ERhasbeenparallelizedbyusingAPsconsistingofanAssociativeMemoryandaTransputer.ExperimentalresultsshowthatERcanbedrasticallyacceleratedbyourmethod.Moreover,astudyofcommunicationamongAPsandanextrapolationfromthesustainedperformanceof10APsdemonstratethescalabilityofourmethodagainstthesizeoftheexampledatabase.Consequently,theEBAusingAPsmeetsthecriticalrequirementsofmachinetranslation.SectionexplainsEBAsandasentencetranslationsystemusinganEBA.SectionanalyzesthecomputationalcostofEBAsystems.SectiondescribestheaccelerationachievedbyusingAPs.Sectionexplainsthescalabilityofourmethod.Finally,Sectiontouchesonrelatedresearch.</section>
  <section title="Example-Based Approach">First,theideabehindEBAsandtheirnotablefeaturesareintroduced.Then,asentencetranslationmodelusinganEBAisdiscussed.</section>
  <subsection title="Idea and Features">NovelmodelsforNLPhavebeenstudiedinrecentyears.ThesemethodshavebeencalledExample-BasedApproaches(EBAs)becausetheyrelyonlinguisticexamples,suchastranslationpairs,derivedfromcorpora.Intheearly1980s,NagaogavethefirstproposalofanEBAformachinetranslation,whichtranslatesbymimickingthebest-matchtranslationexamples,basedonthefactthatahumantranslatesaccordingtopasttranslationexperience.Sincetheendofthe1980s,largecorporaandpowerfulcomputationaldeviceshaveallowedustoachieveNagao'smodelandtoexpandthemodeltodealwithnotonlytranslationbutalsoothertaskssuchasparsing.EBAssurpassconventionalapproachesforNLPinseveralaspects.Thefollowingisasummaryofobservationsmadeinpreviouspapers.First,EBAsareaccurateinarestricteddomain,ifsufficientexamplesareprepared.Theseapproachescandealwithsuchwellknowndifficultproblemsastargetexpressionselection(e.g.functionwords,nounphrasesandverbphrases)anddisambiguationofpp-attachment.EBAsachievehighaccuracynotonlyforthesesubproblemsinmachinetranslationbutalsoforsentencetranslation.WehavetrainedourJapanese-to-Englishsentencetranslationsystemwiththetranslationsof825sentencesinaconferenceregistrationdomain.ThesesentencescoverbasicexpressionsusedinJapaneseaptitudetestsconductedbythegovernment.ThesesentenceswerereviewedbyJapaneselinguistsaswell.Thetranslationsuccessrateforthese825sentences(traineddata)isabout98%.Inaddition,wehavetested1,056sentences(unseendata)inthesamedomain;thetranslationsuccessrateisabout71%.Second,EBAsarerobust.Thebest-matchmechanismusingthesemanticdistanceisrobustagainstincompletenessofsystemknowledge.Moreover,somedeviationsfromconventionalgrammars,whicharespecifictospokenlanguage,arehandledwell.Forexample,inspokenJapanese,particlessuchaswa,o,andniarefrequentlyomitted.TheomissionisrecoveredwellbyEBAs.Third,EBAscanoutputareliabilityscore(anumberindicatinghowmuchwecanrelyontheresult)unlikeconventionalNLP.EBAsprovideasemanticdistanceasareliabilityscore.Inourpreviousexperimentontherelationshipbetweensemanticdistanceandsuccessrate,wefoundthetendencythatthesmallerthesemanticdistance,thebetterthequality.Fourth,thecentralmechanismofEBAsislanguage-independent.Sofar,wehaveimplementedJapanese-to-Englishtranslationandviceversa.Notonlyexample-retrievalbutalsothewholetransferprocessissharedbybothsystems.</subsection>
  <subsection title="Mechanism of Sentence Translation using EBA">Here,wedescribeasentencetranslationmodelfeaturinganEBA.Asentenceistranslatedbycombiningpartialexamplesinsuchawaythattheycoverthesentencejointly.Sincetheexampleshaveaprimaryroleandthewholeprocessiscontrolledbytransfer,wecallourmodelTransfer-DrivenMachineTranslation(TDMT).Atranslationexample,whichisapieceofthetransferknowledge,describesthecorrespondencebetweenasourcelanguageexpression(SE)andtargetlanguageexpressions(TEs)asfollows:SEandTEarerepresentedbypatternsconsistingofvariablesandsurfaceexpressions.E_ijforTE_irepresentsthej-thexamplewordlistthatembodiesvariablesoftheSE.Forexample,thecorrespondencebetweentypicalJapanesenounphrasesoftheform,``XnoY''andEnglishnounphrasesisdescribedasfollows]representsaJapaneseword,jandtheliteralEnglishtranslationword,e.:WhenTDMTtranslatestheJapanesenounphrase``Oosaka[Osaka]nopaatii[party],''itretrievesthebest-matchinthetransferknowledge,i.e.X~no~Y~~=&gt;~~Y'~in~X'~~(Kyooto[Kyoto],kaigi[conference]).Accordingtothis``best-match,''TDMTgenerates``partyinOsaka''bysubstitutingEnglishnounsfortheJapanesenouns.TDMTutilizesthesemanticdistancecalculationpreviouslyproposedtoretrievethebest-matchexamples.Letussupposethataninput,I,andeachexample,E_ij,consistoftwordsasfollows:=-0.5exI&amp;=&amp;(I_1,,I_t)E_ij&amp;=&amp;(E_ij1,,E_ijt)eqnarray*Then,thesemanticdistancebetweenIandE_ijiscalculatedasfollows:=-1.5ex=-0.5exd(I,E_ij)&amp;=&amp;^t_k=1d(I_k,E_ijk)W_keqnarrayThesemanticdistancebetweenwords,d(I_k,E_ijk),isdefinedaccordingtoathesaurus(Section).TheweightW_kisthedegreetowhichthewordinfluencesthetargetexpressionselection.TheflowoftransferringanSEtothemostplausibleTEisasfollows:=0pt=0pt=0pt=0ptThesemanticdistancefromtheinputiscalculatedforallexamples.Theexamplewiththeminimum-distancefromtheinputisretrieved.ThecorrespondingTEoftheminimum-distanceexampleisselectedasthemostplausibleonefortheSE.Processes[a]and[b]combinetoformExample-Retrieval(ER),thefocusofthispaper.Thetop-levelTDMTalgorithmisasfollows:first,itproducespossiblesourcestructuresinwhichtheSEsofthetransferknowledgearecombinedtocovertheinput;second,TDMTproducesthetargetstructuresbytransferringeachSEofthesourcestructurestothemostappropriateTEusingERand[c]above.Forexample,supposetheinputJapanesesentenceisasfollows:ThesourcestructureshowninFigureisproducedbycombiningSEssuchas(XnoY),(XwaY),and(XniY).ThetargetstructureshowninFigureisproducedaccordingtothetransferknowledge.Finally,thefollowingtranslationisobtained:``Theconferenceregistrationfeeislistedintheannouncement.''</subsection>
  <section title="Analysis of Computational Cost">Example-Retrieval(ER),thekeyprocessofEBAs,retrievesthemostsimilarexamplesbycalculatingthesemanticdistancebetweentheinputandexamples.ThissectionanalyzesthecomputationalcostofERbyusingTDMT,atypicalEBAsystemintroducedintheprevioussubsection.AlthoughtheanalysisisdoneusingTDMT,thefindingsareingeneralapplicabletootherEBAsystems.First,wewillshowthatERispredominantinEBAs.Second,weinvestigatetheERcostindetail.Finally,wewillpresentarationaleforusingparallelmachinestoacceleratetheERofalarge-scaleEBAsystem.</section>
  <subsection title="Example-Retrieval Cost of Prototype System">Here,weshowthetimingfiguresoftheprototypeJapanese-to-EnglishTDMTonasequentialmachine(SPARCstation2)for746testsentencesthatarerepresentativeJapanesespokensentences.FigureshowsthattheincreaseintranslationtimeisstronglyrelatedtotheincreaseinERtime.Theminimum,averageandmaximumtranslationtime(ERtime)persentenceisabout0.26(0.001),3.53(2.49),and23.28(21.5)seconds,respectively.ERisthedominantcomponentinEBAsbecause:(1)EvenasingleERcallistime-consuming.TheERtime,T(milliseconds),risesinproportiontothesizeoftheexampledatabase,N,whichisnecessarilylarge.Intheexperiment,thefollowingapproximaterelationwasobserved.=-3ex=-3exT&amp;=&amp;0.65Neqnarray(2)ERiscalledmanytimesintranslatingasinglesentence.ThenumberofERcalls,C,risesaccordingtothesentencelength,L.Inourexperiments,thefollowingapproximaterelationbetweenthenumberofERcallsandthesentencelengthwasobserved.=-3ex=-3exC&amp;=&amp;10^L10eqnarrayTheminimum,averageandmaximumnumberofERcallspersentenceis1,about10and97,*-0.7exrespectively.</subsection>
  <subsection title="Investigation of Example-Retrieval">Tableshowsthatexample-retrievaloffrequentpatterns,whichincludefunctionwordssuchas``wa,''``no,''``o,''``ni''and``ga,''takesupthemajorityofERtimeintheJapanese-to-EnglishversionofTDMT.Aroughexplanationisasfollows.ThenumberofERcallsforapatternisproportionaltothefrequencyofthepattern.Sincepossibletranslationsofafrequentpatternare,ingeneral,diversified,thenumberofexamplesisroughlyproportionaltothefrequency.Consequently,thecomputationalcostofretrievingexamplesisproportionaltothesquareofthefrequencyofthepatterns.Inourexperiment,theERtimeforthetop10patternswasfoundtoaccountfornearly96%oftheentireERtime.ThereislittleneedtoaccelerateERforotherpatterns.Withthisrationale,wedecidedtoparallelizetheERforthetop10patternsinthesubsequentexperiment.IntheE-to-JTDMT,thefrequenciesofpatternsincludingfunctionwords,suchas``at,''``in,''``on,''``by,''``to,''and``of'',areaslargeasthoseofJ-to-ETDMT.SincetheyconsumemostoftheERtime,wecantakethesamestrategytoparallelizeonlytheERoffrequentpatterns.</subsection>
  <subsection title="Example-Retrieval Cost of Large-Scale System">Weestimatethefiguresofalarge-scaleTDMTontwosequentialmachines:(1)theSPARC-station2and(2)afuturehigh-performancemachine.Letusestimatethesizeoftheexampledatabase,N,foralarge-scalesystem.Intheprototypesystem,thevocabularysizeisabout1,500,andNis12,500.AssumingthatNisindirectproportiontothesquareofthevocabularysize,thenNgrowsrapidly.Hereafter,wetakeintoconsiderationthecaseN=1,000,000,i.e.about9*9timeslargerthanthatoftheprototypesystem.For14-wordsentencesweestimatefiguresofalarge-scaleTDMTbasedonthoseoftheprototypeTDMT.TheERtimeisproportionalto$N$,whilethetimeoftheotherprocessesareindependentof$N$.Therefore,theexpectedtranslationtimeusingaSPARCstation2intheworstcaseisabout347.2(~=~ERtime+timefortheotherprocesses~=~4.321,000,000/12,500+5.95-4.32~=~345.6+1.63)seconds.ERconsumesmostoftheCPUtime,i.e.99.5%.Thisisclearlyunacceptableforreal-timeapplications.Letusconsideramuchfasterworkstation,a4,000MIPSsequentialmachineinsteadofaSPARCstation2(28.5MIPS).Thetranslationtimeisexpectedtobeabout2.474(347.228.5/4,000)seconds.Ofthe2.474seconds,(1)theERisexpectedtotake2.462seconds(345.628.5/4,000),and(2)theotherprocessesareexpectedtoconsume0.012seconds(1.6328.5/4,000).Therefore,sinceitseemsdifficulttoavoiddisturbingnaturalcommunicationwithasequentialspeech-to-speechtranslationsystem,wehavedecidedtoutilizeparallelismtoacceleratetheER.</subsection>
  <section title="Acceleration Using Associative Processors">Example-Retrieval(ER)ispredominantinEBAsasdescribedinSection;itdoes,however,haveanaturesuitableforparallelprocessing.WeproposeacceleratingERbyusingaparalleldevice,AssociativeProcessor(AP).Thissectionexplains(1)AP;(2)ERonasingleAP;and(3)ERonmultipleAPs.</section>
  <subsection title="Associative Processors">AnAssociativeProcessor(AP)istheprocessingelementofthemassivelyparallelmachine,IXM2.IXM2hasshownthatalargeAssociativeMemory(AM)workseffectivelyasaSIMDdeviceforAIapplicationswhereassociationandsetoperationsdominatethetotalprocessingtime.TheAMnotonlyfeaturesstorageoperationsbutalsologicaloperationssuchasretrievingbycontent.ParallelsearchandparallelwriteareparticularlyimportantoperationsbecausetheyaresourcesoftheAP'sparallelismandareexploitedintheERalgorithm,aswillbeexplainedinthenextsubsection.AsspecifiedinTable,eachAPhasa4K-wordAMandanINMOST801Transputer(TP).TheAMismappedintothememoryspaceoftheTP.AsingleAPallowsonetoperform4Ksearch/writeoperationsinparallel.APsareusedinparallelTDMTasfollows:(1)examplesareloadedontheAM;(2)parallelcommunicationandsequentialprocessaredonebytheTP;(3)parallelsearch/writearetriggeredbytheTPandexecutedbytheAM.</subsection>
  <subsection title="ER on a Single Associative Processor">First,weexplainthedistancecalculationpart(process[a]inSection)ofExample-RetrievalforasinglepatternonasingleAP.Supposenisthenumberofexamplesandtistheexamplelength.Thepartcanthenbeimplementedasfollows:t-timerepetitionofn-timesummationofthedistancesbetweenthet-thwordsoftheinputandthen-thexample,multipliedbythet-thweights(seeexpression(1)inSection).Thedistancebetweenwordsisreducedtothedistancebetweencodesinathesaurus.Eachwordisassignedak-digitl-scalecodethatclearlyrepresentsthethesaurushierarchy.Inthethesaurususedinourexperiment,a3-digitdecimalcodeisassigned.ThesemanticdistancebetweencodesiscalculatedaccordingtoTable.Sequentialimplementationofthedistancecalculationrepeatsword-by-word,accumulatingthepartialsumsbycomputing,example-by-example,theconditionsinTabletoyield0,1/3,2/3or1.Letusmoveontotheparallelimplementationofthedistancecalculationpart.Thenumberofexamples,n,ismuchlargerthantheexamplelengthofapattern,t,sowehavedecidedtoparallelizethen-timesummation.Tableoutlinestheelementarysteps,i.e.paralleln-summationoft-thpartialsums.[a1]to[a4]inTablecorrespondto[c1]to[c4]inTable.Theyareexecutedone-by-oneusingsteps[i]to[iv].FigureillustratesaportionoftheAM;theinputdatais344,whichisthethesauruscodeoftheword``uchiawase[meeting].''EachcodeoftheexamplesisstoredintheAM.344,316,347representthecodesfor``kaigi[conference],''``toochaku[arrival],''``kaisetsu[explanation]''oftheexamples.Figureshowssteps[i]to[iii]of[a2]after[a1]iscompleted.Step[i]marksthethirdcell,step[ii]writes1attheflagbitofthethirdcell,andstep[iii]gets3,i.e.theaddressofthemarkedcell.Steps[i],[ii]and[iv]aredoneforallcellsbytheparallelsearch/writeoperationsoftheAMinaconstanttime.Step[iii]isdoneintheTPandrepeatedbythenumberofmarkedcells.Parallelimplementationidentifiesexamplesnotbyrepeatingbutbyexecutingthematchandconditionalbranchsimultaneouslytoperformstep[iii]withgivenvalues,i.e.0,1/3,2/3or1.TPsequentiallyperformstheprocessthatfindstheminimumofthedistancesbetweentheinputandexamplesandlocatestheexampleswhosedistanceisminimum(process[b]inSection)inExample-Retrieval.WehavealreadysuccessfullyconductedapreliminaryexperimentofERforasinglepatternonasingleAP.Intheexperiment,theAPoutperformedstate-of-the-artmachinesbyusingtheparallelsearch/writefunctionsoftheAM.</subsection>
  <subsection title="ER on Multiple Associative Processors">Accordingtotheperformanceanalysisinsubsection,wetargetedonlythetop10patternsintheexperimentformultiplepatternsonmultipleAPs.Thenumberofexamplesforeachpatternofprototypesystemwasatmostabout1,000,andinourimplementationeachAPcouldloadabout1,000examples;thus,10APsweresufficientfortheexperiment.The10APs(AP_1,AP_2,,AP_10)andtherootTP,whichwasbuiltintotheHOST(theSPARCstation2),wereconnectedinatreeconfigurationasshowninFigure.TheERalgorithmusingmultipleAPsisshowninTable.ThisalgorithmimplementsERbydistributingexamplesontomultipleAPs,retrievingexamplesoneachAPinparallelandmergingallresults.Wecomparedtwodifferentmethodsofloadingexamplesasfollows(seeFigure):FiguresandplottheaccelerationofERforTDMTusingmultipleAPsoversequentialTDMTwiththetwomethods.TheaccelerationwiththeHTmethodisgreaterthanthatwiththeHMmethodbecausethesequentialpart,i.e.step[iii]ofprocess[a]andprocess[b],isproportionaltothenumberofexamplesofthepatterninquestionontheAPsandbecausethenumbersinHTaremuchlessthanthoseinHM.Thesentence-by-sentenceaccelerationchangeslargelydependingontowhatextentthesentencecallsupERforthetop10patterns.Needlesstosay,about90sentencesarenotacceleratedbecausetheydonotincludetheERforthetop10patterns.WiththeHTmethod,theaverageaccelerationofERisabout16.4(2.4897/0.1522=theaveragetime(seconds)persentenceinthesequentialTDMT/theaveragetime(seconds)persentenceintheHTmethod).TablesummarizestheperformanceofprototypeTDMTs(ontheSPARCstation2inSectionandonAPswiththeSPARCstation2inthissection).Theseresultsdonotlooksostriking.Becausetheprototypesarerathersmall-scale,thereisnourgentneedtoachieveaccelerationevenbyparallelizingTDMT.However,sequentialimplementationcannotscaleup,asdiscussedinSection.Onotherhand,parallelTDMTonmultipleAPsexhibitsclearscalabiltyaswillbeshowninthenextsection.TablegivesatimeanalysisofasingleERcallonaprototypeparallelTDMTusingtheHTmethod(referringtothealgorithmofTable).(a)and(b)arecontrollednotbythesizeoftheexampledatabasebutbythedepthoftheAPtreebecauseanAPcansenddatatothoseAPsdirectly-connectedinparallel(e.g.AP_2AP_6,AP_2AP_7,AP_2AP_8)andviceversa.Thus,(a)and(b)aretheunittimesbetweentwoAPs~thetreedepth,2(seeFigure).TheunittimesbetweentwoAPsfor(a)and(b)arefairlysmall,i.e.severaltensofmicroseconds,becausethedatasizeisonlyseveraltensofbytesandthebandwidthofthelinkbetweentwoTPsis10Mbits/sec.Thusthesum,(c),communicationtimeamongAPs,iscontrolledbythetreedepthandasmallcoefficient.Step[m3]canberegardedasaconstantoperationbecauseevenifthetotalnumberofexamplesincreases,thenumberofexamplesforeachAPisroughlyinvariant.Step[m1]isofaconstant-timebecauseitonlydoesoperationsrelatedtoinputsending.Step[m5]canberegardedasaconstantoperationbecauseitdependsonthesizeoftheERresult,whichdoesnotsigificantlychange.Consequently,time(d)isinvariant.</subsection>
  <section title="Scalability toward Large-Scale System using Associative Processors">TheexperimentintheprevioussectionandtheestimationinthissectionshowthatERonmultipleAPsisclearlyscalableagainstthesizeoftheexampledatabase.Thatis,eventhoughthesizeoftheexampledatabaseincreasesbynearlytwofiguresfromtheprototypesystem,theERtimeincreasesslightlyandisapplicabletoreal-timeapplications.</section>
  <subsection title="Configuration">LetusestimatethetreedepthoftheAPtreenecessarytoloadexamplesofalarge-scalesystem(Section).ThetreedepthistheminimumDsatisfyingtheinequality:=-0.5ex=-0.5ex^D_x=13^xN12,50010eqnarrayTherighthandsideisthenumberofAPs.BecauseprototypeTDMT(N=12,500)requires10APs,alarge-scaleTDMT(N=1,000,000)requires800APs.Thus,thetreedepth,D,is6.Figureshows800APsinatrinarytreestructure.Weassumea4,000MIPSsequentialworkstationasthehostherebecauseitisnotconclusivelydeterminedwhetherthoseotherthanERissuitableforparallelcomputation.Thisisahybridaccelerationinthat(1)thepredominantpart,ER,isacceleratedbyAPs;(2)theotherpartisacceleratedbyahighperformancesequentialmachine(4,000MIPS).</subsection>
  <subsection title="Estimation">Weestimatethetimefortranslatingthe14-wordsentencesbasedontheextrapolatedfiguresforthe4,000MIPSmachineshowninSection.Thetranslationtimeisdividedintoparts(a)to(i),asshowninTable.(a)to(e)areexplainedaccordingtoTable.Recallthatthetreedepthof800APsis6(Figure)andthenumberofERcallsper14-wordsentenceis6.ThedistributiontimetoallAPs(a)isnearly1.8(=0.0566)millisecondsbecausetheaveragedistributiontimebetweendirectly-connectedAPsisabout0.05milliseconds,thetreedepthis6,andtheaveragenumberofERcallsisabout6.Inthesameway,thecollectiontimefromallAPs(b)isabout2.2(=0.0666)milliseconds.Thus,thetotalcommunicationtimeamongAPs(c)isabout1.8+2.2=4.0milliseconds.Thetime(d)involving(1)ERoneachAPand(2)communicationbetweentheHOSTandtherootTPisnotrelatedtothetreedepth,soitissimplymultipliedbythenumberofERcalls,6,yielding78.9=13.15*6milliseconds.Accordingly,theERtimeforonlythetop10patterns(e)is82.9milliseconds.(f)to(i)areestimatedbasedon(e)andthe4,000MIPS'sestimationinSection.TheERtimeforpatternsotherthanthetop10patterns(f)is98millisecondsbecausetheentireERtimeisestimatedtobe2462milliseconds(inSection),andERforotherthanthetop10patternsconsumes4%ofthetime(inSection).(g)isthesumof(e)and(f).(h)is12milliseconds(Section).Thegrandsum,i.e.thetranslationtimeof14-wordsentences(i),is193milliseconds.Tablesummarizestheperformanceoflarge-scalesystems.(1)TheERtimeoftheSPARCstation2intheworstcaseisunacceptablylarge(345.6seconds).TheAPsacceleratedERfrom345.6secondsto0.18seconds.(2)ThetimeforprocessesotherthanERiseffectivelyreducedfrom1.63secto0.01inverselyproportionaltotheincreaseinMIPS.ThispartisnotnecessarilysuitableforAPsandistractablewithahigh-performancesequentialmachine.</subsection>
  <section title="Related Research"/>
  <subsection title="Massively Parallel Natural Language Processing">Uptonow,somesystemsusingamassivelyparallelmachineinthefieldofnaturallanguageprocessing,suchasaparsingsystemandtranslationsystems,e.g.ASTRAL,MBT3n,havebeenproposed.Theyhavedemonstratedgoodperformance;nonetheless,theydifferfromourproposal.Thefirsttwosystems,althoughtheyuseAPs,useadifferentmechanismfortheirnaturallanguagetasks,i.e.theydonotcalculatethesemanticdistancebutpropagatemarkersthroughasemanticnetwork.Thelastsystemdealswithatranslationsubproblem,i.e.translatingnotsentencesbutnounphrases(technicalterms),usingadifferentmechanismbasedonmatchingandsimilarityonaMIMDmachine.</subsection>
  <subsection title="Speech-to-Speech Translation">Challengingresearchonspeech-to-speechtranslationbeganinthemid-1980s.Suchresearchhasbroughtaboutseveralprototypesystems.However,nolarge-scalesystemcapableofrespondinginreal-timehasemerged.Speech-to-speechtranslationconsistsofthreeprocesses,i.e.speechrecognition,spokenlanguagetranslationandspeechsynthesis.Therearetwopossiblemodelsforspeech-to-speechtranslation:(1)asimultaneousmodelwheresubsequentprocessesstartafteraslightdelayandoverlapeachother;and(2)asequentialmodelwhereprocessesstartaftertheirprecedingprocessiscompleted.Unfortunately,state-of-the-artNLPtechnologiesdonotallowustoadoptthesimultaneousmodel;thus,eachcomponentinthesequentialmodelshouldbeacceleratedasmuchaspossible.</subsection>
  <section title="Concluding Remarks">AnEBAusingAPshasbeenproposed.ThetranslationqualityofEBAsishighaccordingtoourpreviousexperiment.UsingAPs,EBAshavebeendrasticallyacceleratedwithagoodscalabilityagainstthesizeoftheexampledatabase.Consequently,anEBAusingAPsmeetsthecriticalrequirementsnecessarytobreakthroughthelimitationsofconventionalmachinetranslation,especiallyforspokenlanguagetranslation.TheaccelerationandscalabilityofourmethodusingAPsareduetothefollowing:(1)ERstronglydominatesEBAs;(2)TheAM'sparallelismisemployedforERonasingleAP;(3)thecommunicationtimeamongAPsiscontrolledbythetreedepth,whichisfairlyshallow.EBAsusingAPshavedesirablefeaturessuitableforintegrationwithspeechrecognition:(a)highaccuracy;(b)robustness;(c)outputofareliabilityscore;and(d)aquickresponse.Tightlycouplingourmodelandspeechrecognitionmightmakepossiblereal-timespeech-to-speechtranslationinthefuture.authorswishtoacknowledgethehelpreceivedfromDr.YasuhiroYAMAZAKI(thePresidentofATR-ITL),Dr.SakutaroTOMIYAMA(theDirectorofETL),Dr.HiroakiKITANO(CMU),andKadokawa-shoten(thepublisherofRuigo-Shin-Jiten).document</section>
</root>
