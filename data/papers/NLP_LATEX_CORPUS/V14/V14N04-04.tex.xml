<?xml version="1.0" ?>
<root>
  <jtitle>自動構築した大規模格フレームに基づく	構文・格解析の統合的確率モデル</jtitle>
  <jauthor>河原大輔黒橋禎夫</jauthor>
  <jabstract>本稿では，格フレームに基づき構文・格解析を統合的に行う確率モデルを提案する．格フレームは，ウェブテキスト約5億文から自動的に構築した大規模なものを用いる．確率モデルは，述語項構造を基本単位とし，それを生成する確率であり，格フレームによる語彙的な選好を利用するものである．ウェブのテキストを用いて実験を行い，特に述語項構造に関連する係り受けの精度が向上することを確認した．また，語彙的選好がどの程度用いられているかを調査したところ，60.7%という高い割合で使われていることがわかり，カバレージの高さを確認することができた．</jabstract>
  <jkeywords>格フレーム，コーパス，ウェブ，構文解析，格解析</jkeywords>
  <subsubsection title="">*係り受けの正解基準からのずれ提案モデルは，語彙的な選好を強く考慮して係り受けを決定する．しかし，解析結果が，係り受けの正解基準とずれるために，誤りとなる場合がある．行政相談委員は，~いつでも自宅でみなさんからのご相談に応じていますが，×この期間中は次のところで行政相談所を開きます．○この文において，「行政相談委員は，」の正解係り先は「開きます．」であるが，提案手法は係り先を「応じていますが，」と解析し，誤りとなる．「開きます．」，「応じていますが，」のどちらも意味的には係り先として正しいと考えられるが，基準としては文末の「開きます．」であるのでずれが生じる．このような問題を解決するには，省略関係の正解を考慮しながら評価を行う必要がある．</subsubsection>
  <section title="はじめに">近年，構文解析は高い精度で行うことができるようになった．構文解析手法は，ルールベースのもの(e.g.,)，統計ベースのもの(e.g.,)に大別することができるが，どちらの手法も基本的には，形態素の品詞・活用，読点や機能語の情報に基づいて高精度を実現している．例えば，弁当を食べて出発したExample::Simplelingexampleという文は，「弁当を食べて」のように正しく解析できる．これは，「〜を」はほとんどの場合もっとも近い用言に係るという傾向を考慮しているからである．このような品詞や機能語などの情報に基づく係り受け制約・選好を，ルールベースの手法は人手で記述し，統計ベースの手法はタグ付きコーパスから学習している．しかし，どちらの手法も語彙的な選好に関してはほとんど扱うことができない．弁当を出発する前に食べた弁当は食べて出発したlingexample(2a)では，「弁当を」がと同じように扱われ，「弁当を出発する」のように誤って解析される．(2b)においては，「〜は」が文末など遠くの文節に係りやすいという傾向に影響されて，やはり「弁当は出発した」のように誤って解析されてしまう．これらの場合，「弁当を食べる」のような語彙的選好が学習されていれば正しく解析できると思われる．統計的構文解析器においては多くの場合，語彙情報が素性として考慮されているが，それらが用いている数万文程度の学習コーパスからでは，データスパースネスの影響を顕著に受け，語彙的選好をほとんど学習することができない．さらに，2項関係の語彙的選好が十分に学習されたとしても，次のような例を解析することは難しい．太郎が食べた花子の弁当Example::1lingexample「弁当を食べる」「花子が食べる」という語彙的選好を両方とも学習しているとすると，「食べた」の係り先はこれらの情報からでは決定することができない．この例文を正しく解析するには，「食べた」は「太郎が」というガ格をもっており，ヲ格の格要素は被連体修飾詞「弁当」であると認識する必要がある．このように，語彙的選好を述語項構造としてきちんと考慮できれば構文解析のさらなる精度向上が期待できる．述語項構造を明らかにする格解析を実用的に行うためには，語と語の関係を記述した格フレームが不可欠であり，それもカバレージの大きいものが要求される．そのような格フレームとして，大規模ウェブテキストから自動的に構築したものを利用することができる．本稿では，この大規模格フレームに基づく構文・格解析の統合的確率モデルを提案する．本モデルは，格解析を生成的確率モデルで行い，格解析の確率値の高い構文構造を選択するということを行う．構文解析手法として，語彙的選好を明示的に扱うものはこれまでにいくつか提案されてきた．白井らと藤尾らは，数十〜数百万文のコーパスから語の共起確率を推定し利用している．本研究にもっとも関連している研究として，阿辺川らによる構文解析手法がある．阿辺川らは，同じ用言を係り先とする格要素間の従属関係と，格要素・用言間の共起関係を利用した構文解析手法を提案している．これら2つの関係を新聞記事30年分から収集し，PLSIを用いて確率推定を行っている．既存の構文解析器の出力するn-bestの構文木候補に対して，確率モデルに基づくリランキングを適用し，もっとも確率値の高い構文木を選択している．この手法は，PLSIを用いることによって潜在的な意味クラスを導入し，確率を中規模のコーパスから推定している．本研究は，これらの研究に対して次の点で異なる．明示的に意味，用法が分類された格フレームを用いている．解析時に格フレームを選択することにより，用言の意味的曖昧性を解消し，その意味，用法下において正確な格解析を行うことができる．非常に大規模なコーパスから構築された格フレームを用いることによって，用例の出現を汎化せずに用いている．阿辺川らの手法のようにn-best解をリランキングするのではなく，構文，格構造を生成する生成モデルを定義している．</section>
  <section title="ウェブから獲得した大規模格フレーム">格フレームは，ウェブから収集した大規模コーパスを用いて，の手法により自動構築を行う．本節では，格フレーム構築手法の概要を述べる．人間のもつ常識的知識の重要な部分である格フレームは，様々な言語現象をカバーすることが望ましい．そのような格フレームを構築するために，大規模コーパスから漸進的に確からしい情報を抽出する．まず最初に，大規模コーパスを構文解析し，その解析結果から第1段階の格フレームを構築する．格フレームを構築する際の最大の問題は，用言の用法の曖昧性である．つまり，同じ表記の用言でも複数の意味，用法をもち，とりうる格や用例が異なる．例えば，以下の2つの例は，用言は「積む」で同じであるが用法が異なっている．トラックに荷物を積む経験を積むlingexample用法が異なる格フレームを別々につくるために，我々は，格フレーム収集の単位を用言とその直前の格要素の組とした．「積む」の例では，「荷物を積む」「経験を積む」を単位として格フレームを収集する．さらに，「荷物を積む」「物資を積む」などかなり類似している格フレームをマージするためにクラスタリングを行う．上記の第1段階の構築手法では構文解析を用いているために，基本的に格助詞の付属している格要素を収集している．このため，得られる格フレームは，二重主語構文，外の関係，格変化のような複雑な言語現象には対処できないという問題がある．この問題に対処するために，上記で得られた格フレームを用いて再度テキストを解析し，新たな情報を格フレームに与える．新たに得られる情報は，1回目の格フレーム構築では扱うことができなかった係助詞句（「〜は」や「〜も」）や被連体修飾詞に関する関係である．この車はエンジンが良いガ２1lingexample例えば，上例において，構文解析の段階では「車は」は解釈できなかったが，格解析では「｛エンジン｝がよい」という格フレームを用いることによって，格フレームにガ格以外の格がないことから「車は」は2つ目のガ格であり，「｛エンジン｝がよい」は二重主語構文をとることがわかる．その問題は彼が図書館で調べているガ２2lingexampleこの例文の「問題は」は，すでに得られている格フレーム「｛問題，課題｝を｛図書館｝で調べる」のヲ格の用例群に合致するため，格解析ではヲ格と解析されるだけで，新しい情報は得られない．同様に，被連体修飾詞は構文解析では扱われないが，格解析では，格フレームのガ格，ヲ格などの用例と類似しているかどうか調べることによって解釈される．例えば，「業務を営む免許」の「免許」は，格フレーム「｛銀行，会社｝が｛業務，ビジネス｝を営む」のどの格の用例とも類似せず，外の関係と呼ばれる関係をもっていると判定され，この情報が格フレームに加えられる．上記の手法を用いて，ウェブから収集した約5億日本語文から格フレームを構築した．約350CPUの計算機グリッドを用いてこの処理を行い，約1週間で格フレームを構築することができた．この格フレームは約90,000用言からなる．その一部を表に示す．</section>
  <section title="構文・格解析の統合的確率モデル">本論文で提案する構文・格解析統合モデルは，入力文がとりうるすべての構文構造に対して確率的格解析を行い，もっとも確率値の高い格解析結果をもつ構文構造を出力する．すなわち，入力文Sが与えられたときの構文構造Tと述語項構造Lの同時確率P(T,L|S)を最大にするような構文構造T_bestと述語項構造L_bestを出力する．次のように，P(S)は一定であるので，本モデルはP(T,L,S)を最大にすることを考える．(T_best,L_best)&amp;=(T,L)P(T,L|S)&amp;=(T,L)P(T,L,S)P(S)&amp;=(T,L)P(T,L,S)align</section>
  <subsection title="構文・格解析の統合的確率モデルの概略">本論文では，依存構造に基づく確率的生成モデルを提案する．本モデルは「節」を基本単位とし，主節（文末の節）から順次生成していく．「節」とは，用言1つと，それと関係をもつ格要素群を意味する．P(T,L,S)は，文に含まれる節c_iを生成する確率の積として次のように定義する．nは文S中に存在する節の数（＝用言数）であり，ここでb_hは節c_iの係り先文節である．主節は係り先をもたないが，仮想的な係り先をとする．従来研究のほとんどは，文生成の確率を，2文節間の係り受け確率の積としていたが，本研究では式()のように，節，つまり用言と格要素群を単位として生成するモデルとしている．そのため，複数の格要素を考慮して係り受けを決定することができ，例(3)のような文も正しく解析できるようなモデルとなっている．例えば「弁当は食べて目的地に出発した．」という文を考える．「弁当は」が「食べて」に係る場合には，2つの節「弁当は食べて」「目的地に出発した．」があり，次の確率を考える．[P（|）	P（|）]「弁当は」が「出発した．」に係る場合には，2つの節「食べて」「弁当は目的地に出発した．」があり，次の確率を考える．[P（|）	P（|）]本モデルは，これらのうちもっとも確率の高い構造を採用する．節c_iは，述語項構造CS_iと用言タイプf_iに分解して考える．用言タイプとは，用言の活用や付属語列を意味する．そのため，述語項構造CS_iに含まれる用言は原型である．係り先の文節b_hも同様に，語w_hとタイプf_hに分けて考える．P(c_i|b_h)&amp;=P(CS_i,f_i|w_h,f_h)&amp;=P(CS_i|f_i,w_h,f_h)P(f_i|w_h,f_h)&amp;P(CS_i|f_i,w_h)P(f_i|f_h)	alignこの近似は，用言は係り先文節のタイプには依存しない，また用言タイプは係り先の語には依存しないと考えられるからである．例えば，P（|）は次のようになる．[P（CS（）|,）	P（|）]ただし，本モデルにおいて，副詞，連体詞，および連体修飾句は述語項構造に入れず，考慮しない．これらは用言に対して格関係を持たないので，用言格フレームにおいて扱うことができず，生成することができないためである．これらの係り先は，読点がなければ直近の係りうる文節とするなどといったルールに基づいて決定する．式()のP(CS_i|f_i,w_h)を述語項構造生成確率，P(f_i|f_h)を用言タイプ生成確率と呼び，これらについて次の2つの節で説明する．</subsection>
  <subsection title="述語項構造生成確率">述語項構造の生成モデルは，その述語項構造にマッチする格フレームの選択と，入力側の各格要素の格フレームへの対応付けを同時に行うモデルである．述語項構造CS_iは，述語v_i，格フレームCF_l，格の対応関係CA_kの3つからなると考える．格の対応関係CA_kとは，図に示すように，入力側の格要素と格フレームの格との対応付け全体を表す．対応関係は図示のもの以外にも，「弁当は」をガ格に対応付ける可能性がある．述語項構造生成確率P(CS_i|f_i,w_h)は次のようになる．P(CS_i|f_i,w_h)&amp;=P(v_i,CF_l,CA_k|f_i,w_h)&amp;=P(v_i|f_i,w_h)&amp;P(CF_l|f_i,w_h,v_i)&amp;P(CA_k|f_i,w_h,v_i,CF_l)&amp;to105ptP(v_i|w_h)&amp;to105ptP(CF_l|v_i)	&amp;to105ptP(CA_k|CF_l,f_i)alignこの近似は，述語v_iはその係り先の語w_hのみに，格フレームCF_lは述語v_iのみに，格の対応関係CA_kは格フレームCF_lと付属語列f_iに依存すると考えられることによる．用言生成確率と格フレーム生成確率は大規模コーパスの格解析結果から推定する．P(CA_k|CF_l,f_i)は，格の対応関係生成確率と呼び，以下で詳説する．</subsection>
  <subsubsection title="格の対応関係生成確率">格の対応関係CA_kを，格フレームの格スロットs_jごとに考える．格スロットs_jに入力側の格要素（体言n_j,格要素タイプf_j）が対応付けられているかどうかで場合分けすると，次のように書き換えることができる．ただし，A(s_j)は，格スロットs_jに入力側格要素が対応付けられていれば1，そうでなければ0をとる関数である．式()右辺第1項の各確率は次のように分解できる．この式の第1項と式()第2項の各確率は，f_iには依存しないと考えられるので，それぞれP(A(s_j)=1|CF_l,s_j)，P(A(s_j)=0|CF_l,s_j)となる．これらは格スロット生成確率と呼び，大規模コーパスの格解析結果から推定する．P(n_j,f_j|CF_l,f_i,A(s_j)=1,s_j)は格要素生成確率と呼ぶ．例えば，P（CS（）|,）について考える．「食べる」のある格フレームCF_食べる1がガ格とヲ格をもっているならば，この格フレームを用いたときの述語項構造生成確率としては，「弁当は」をガ格またはヲ格に対応付けるときの2つを考えることになる．以下に「弁当は」をヲ格に対応付けるときの確率を示す．P（CS（）|,）&amp;=P（食べる|出発する）&amp;P（CF_食べる1|食べる）&amp;P（A（を）=1|CF_食べる1,）&amp;P（A（が）=0|CF_食べる1,）&amp;P（弁当,は|CF_食べる1,,A（を）=1,）align*</subsubsection>
  <subsubsection title="格要素生成確率">格要素の体言n_jと格要素タイプf_jを生成する確率は独立であり，表層格の解釈は格フレームに依存しないと考え，格要素生成確率は以下のように近似する．P(n_j|CF_l,A(s_j)=1,s_j)は用例生成確率と呼び，格フレーム自体から推定する．格要素タイプf_jとしては，表層格c_j，読点の有無p_j，提題助詞「は」の有無t_jの3つを考慮する．P(f_j|s_j,f_i)&amp;=P(c_j,t_j,p_j|s_j,f_i)&amp;=P(c_j|s_j,f_i)&amp;P(p_j|s_j,f_i,c_j)&amp;P(t_j|s_j,f_i,c_j,p_j)&amp;to84ptP(c_j|s_j)&amp;to84ptP(p_j|f_i)&amp;to84ptP(t_j|f_i,p_j)alignこの近似は，c_jはs_jのみに，p_jはf_iのみに，t_jはf_iとp_jに依存すると考えられるためである．表層格生成確率は，表層格を解釈した格をタグ付けした京都テキストコーパスを用いて推定する．日本語では，読点や提題助詞はそれらの属する文節が遠くに係る場合に用いられやすいという傾向がある．このような傾向を考慮して，読点生成確率P(p_j|f_i)と提題助詞生成確率P(t_j|f_i,p_j)を以下のように定義する．P(p_j|f_i)&amp;=P(p_j|o_i,u_i)P(t_j|f_i,p_j)&amp;=P(t_j|o_i,u_i,p_j)aligno_iは，対象格要素がほかの係り先候補を越えてv_iに係る場合に1をとり，それ以外では0となる．u_iは，節の区切れとしての強さであり，強い節ほど読点や提題助詞をもつ句を受けやすい．節の強さとしては，南による節の分類を参考にして設定した5段階を考える．</subsubsection>
  <subsection title="用言タイプ生成確率">用言タイプ生成確率P(f_i|f_h)は，文節b_hのタイプを条件にしたときに，それに係っている節c_iの用言タイプを生成する確率である．この確率は，節c_iが連用節であるか連体節であるかで次のように異なる．節c_iが連用節の場合は，節間の係り受けに大きな影響を及ぼすと考えられる読点の有無と連用節のタイプ（強さ）を考慮する．これに加えて，c_iがほかの係り先候補を越えてb_hに係るかどうかを考慮する．節c_iが連体節である場合は，受側すなわち体言のタイプには依存しないと考え，次のように定義する．</subsection>
  <section title="実験">提案手法によって解析した構文・述語項構造の評価実験を行った．各パラメータは表のリソースから最尤推定によって計算した．これらのリソースは一度の処理で得られたものではなく，構文解析，格フレーム構築，格解析という順番で処理を行い，得られたものである．ここにおける格解析は，シソーラスに基づく類似度を用いた格解析である．格フレームはウェブテキスト約5億文から自動構築したものを用い，格解析済みデータはウェブテキスト約600万文を格解析することによって得たものを用いた．構文構造の候補としては，ルールベースの構文解析器KNPが出力するすべての候補を用いた．</section>
  <subsection title="構文解析実験">構文解析実験は，ウェブテキスト675文を形態素解析器JUMANに通した結果を提案システムに入力することによって行う．その675文には，京都テキストコーパスと同じ基準で係り受けのタグ付けを行い，これを用いて係り受けの評価を行った．文末から2つ目までの文節以外の係り受けを評価し，その評価結果を表に示す．表において，「CaboCha」とは，SVMに基づく統計的構文解析器CaboChataku/software/cabocha/（形態素解析器JUMANの結果を入力できる最後のバージョンであるCaboCha0.36を用いた．）を表し，「KNP」とは，構文解析器KNPを表しており，いずれのシステムにも同じ形態素解析結果を入力している．係り受けの精度比較のため，「CaboCha」には「KNP」による文節区切りの結果を入力し，文節区切りも一致させている．表より，提案手法は「CaboCha」や「KNP」より精度がよいことがわかる．マクネマー検定を行った結果，提案手法の精度は「CaboCha」と「KNP」より有意(p&lt;0.05)に上回っていることがわかった．また，表には，係り受けのタイプごとの精度も併せて示してある．述語項構造と密接に関係しているのは，「体言用言」の係り受けであり，その中で中心的なのは「係助詞句以外」である．その精度は「KNP」と比べて1.6%向上しており，エラー率は10.9%減少している．これより提案手法が，述語項構造に関係する係り受けの解析に有効であることがわかる．表に，「KNP」では誤りになるが，提案手法によって正解になった例を挙げる．四角形で囲まれた文節の係り先が×下線部から○下線部に変化したことを示している．また，以下に提案手法の主な誤り原因を挙げる．</subsection>
  <subsection title="格解析実験">述語項構造が正しく認識されているかを評価するために，係助詞句と被連体修飾詞の格が正しく認識できているかどうかを調べた．ウェブテキスト215文に対して京都コーパスと同様の基準で関係タグを付与し，それと自動解析結果を比較した．精度を表に示す．ベースラインとは，類似度に基づく格解析手法である．この表より，ベースラインから大幅に改善しており，提案手法が有効であることがわかる．</subsection>
  <subsection title="格フレームのカバレージ">解析における格フレームのカバレージを調べるために，格要素がその係り先用言の格フレームの用例になっているかどうかを調べた．正しい係り受けのみを評価したところ，60.7%の格要素が格フレームの用例となっていた．比較のため，新聞記事26年分の2,600万文から構築した格フレームで同様の実験を行ったところ，35.1%であった．これより，ウェブテキスト5億文から構築した格フレームは高いカバレージをもっていることが確認された．また，英語の統計的構文解析器において，テスト文中の2項間の依存関係が学習コーパス中に存在する割合が約1.5%であるという報告がある．言語・リソースの違いがあるので直接の比較はできないが，格フレームのカバレージは非常に高いと思われる．</subsection>
  <section title="関連研究">これまでに，語彙的選好を明示的に扱う構文解析手法がいくつか提案されてきた．白井らは，PGLRの枠組みに基づく統計的構文解析手法を提案している．語彙的選好として，例えばP（パイ|を，食べる）のような確率を新聞記事5年分から学習している．しかし，本研究で用いたような格フレームは導入しておらず，用言の意味的曖昧性を区別せずに確率推定を行っている．京都テキストコーパス中の比較的短い500文を用いて評価を行い，84.34%の解析精度であったと報告している．藤尾らは，語の共起確率に基づく構文解析手法を提案している．2つの語が係り受けをもつ確率と距離確率の積で定義した確率モデルを用いており，それらの確率はEDRコーパスから学習している．EDRコーパス1万文を用いて評価を行い，86.89%であったと報告している．阿辺川らは，同じ用言を係り先とする格要素間の従属関係と，格要素・用言間の共起関係を利用した構文解析手法を提案している．これら2つの関係を新聞記事30年分から収集し確率モデルを学習している．既存の構文解析器の出力するn-bestの構文木候補に対して，確率モデルに基づくリランキングを適用し，もっとも確率値の高い構文木を選択している．京都テキストコーパス中の約9,000文を用いて評価を行い，既存の構文解析器よりも0.26%高い91.21%の精度を実現している0pt^3．さらに，阿辺川らの被連体修飾詞の解析を統合することによって，0.04%高い91.25%の精度を得ている．一方，語彙情報を素性として用いている様々な機械学習手法が提案されている．その中でもっとも良い精度を実現しているのは，工藤らが提案している統計的構文解析手法である．この手法は，SVMに基いてチャンキングを段階的に適応していくモデルであり，京都テキストコーパスから学習している．同コーパス（約40,000文）を用いて2分割交差検定により評価を行い，90.46%の精度を実現している0pt^3．しかし，数万文程度のタグ付きコーパスからでは，係り先候補間の語彙的選好を十分学習するのはほとんど困難であると思われる．なお，本論文の実験で比較対象とした「CaboCha」は，本手法を実装した解析器である．</section>
  <section title="おわりに">本論文では，ウェブから獲得した格フレームに基づく構文・格解析の統合的確率モデルを提案した．このモデルによって，構文解析の精度が向上することを確認した．今後は，省略・照応解析を統合することによって，格フレームに基づく構文・格・省略・照応解析の統合的確率モデルを構築する予定である．document</section>
</root>
