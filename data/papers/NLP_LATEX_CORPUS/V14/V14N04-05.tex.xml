<?xml version="1.0" ?>
<root>
  <section title="Introduction">Emotionrecognitionofspeechhasgainedincreasingattentioninrecentyears.Emotionrecognitionisaprocedurethatconvertsahuman'svoiceintoanemotionalsymbol,suchasanger,sadness,orhappiness.Computerscanflexiblyreacttotheuser,usingthissymbolizedinformation.Applicationsofemotionrecognitionincludeaspeechdialogsystem,acallcentersystem,andasecuritysystem.Ifthesystem'sreactionschangedependingontheuser'sfeelings,theman-machineinterfacewouldbecomemorefriendlyandeasiertouse.Sinceitisdifficultevenforhumantodiscriminateemotionsfromspeech,therearesomeapplicationsinwhichemotionrecognitionhelps,ifacertainlevelofaccuracycanbeachieved.Therehavebeenmanystudiesonemotionrecognition.Mostofthemuseprosodicinformationastheirfeatureparameters.However,theaccuracyofemotionrecognitionusingprosodicinformationislow.Inparticular,theaccuracyfallsbelow50%inmostspeaker-independentemotionrecognitionsystemsforfour(ormore)emotions.Featureextractionandclassificationaretwoimportantmodulesofemotionrecognition.Manyclassificationtechniqueshavebeenproposed,suchasneuralnetwork(NN),decisiontree,k-nearestneighbor(K-NN),supportvectormachine(SVM),discriminantanalysis,andhiddenmarkovmodel(HMM).However,thedifferencebetweentherecognitionratesofthesetechniquesisrathersmall,andthereforeweconcludethatitismoreimportanttofindthebestfeatureparametersforemotionrecognition.Prosodicfeaturesarecurrentlyusedinmostemotionrecognitionsystems.Itiscommonlythoughtthattheprosodicfeaturesofspeechhaveusefulinformationfordiscriminatingemotions.Prosodicfeaturesaremadeoffundamentalfrequencyandenergy,whichmeansthatthereareonlytwoindependentcomponentsineachframe,eventhoughanumberofutterance-levelvariables(minimum,maximum,average,andregressioncoefficients,etc)canbederived.Wethinkthatthisisapartofthereasonwhyemotionrecognitionalgorithmsthatuseprosodicfeaturesarenotsufficientlyaccurate.However,itissaidthatphoneticfeatureshavelessinformationfordiscriminatingemotions.Actually,therearemoreindependentcomponentsinthephoneticfeaturesofspeechthanintheprosodicfeaturesofspeech.Forexample,12--16dimensionalMel-FrequencyCepstralCoefficients(MFCCs)havebeenusedastheeffectivephoneticfeaturesforspeechrecognition.Ifevenasmallamountofusefulinformationiskeptinthephoneticfeatures,theaccuracyofemotionrecognitioncanbeimprovedbyincreasingthenumberofindependentphoneticfeatures.WedescribetheeffectivenessofMFCCasthefeatureforemotionrecognitions.VariousalgorithmsofemotionrecognitionusingMFCChavebeenproposed.ThealgorithmsproposedbyKwonandbySchullerarebothbasedontheutterance-levelfeaturessuchasthemaximum,average,variance,etc.EventhoughthesefeaturesaremadefromMFCCs,thedetailedstructureoftheframe-levelfeaturesisomitted.Contrastingly,Nweetal.usedtheframe-levelMFCCswithoutsummarizingthemovertheutterance.Theiralgorithmyieldsanaverageaccuracyof78%inspeaker-dependentemotionrecognitionforsixspecificemotions.TheyusediscreteHMMstodealwiththedynamicsoftheinputfeature,butthepreciseinformationoftheMFCCswaslostintheclusteringprocessofeachframe.Therefore,weproposeanemotionrecognitionalgorithmthatfocusesmoreonpreciseclassificationoftheMFCCs.Torealizesuchapreciseclassification,wegivetheemotionlabeltoeachframeusingmulti-templateMFCCclustering.Theproposedalgorithmissimpleenoughtorealizeimmediateresponseeveninalow-endcomputer,aswellasthehigheraccuracythantheconventionalmethod.Section2describestheemotionrecognitionalgorithmthatwepropose.InSection3,theperformanceevaluationandacomparisonbetweentheproposedandconventionalprosody-basedandMFCC-basedalgorithmsaredescribed.TheconcludingremarksarepresentedinSection4.</section>
  <section title="Emotion Recognition Algorithm">Theproposedalgorithmiscomposedofthreemodules(Featureextraction,Frame-levelclassification,andUtterance-levelvoting)andonedatabase.TheprocessingflowisshowninFigure.First,thevoicedataisdividedintoanalysisframes.Next,eachanalysisframeisconvertedtoafeaturevector.Then,anappropriateemotionlabelisattachedtoeachfeaturevector.Finally,theemotionlabelsoftheentireutterancearecollectedandtheemotionoftheutteranceisdetermined.</section>
  <subsection title="Feature Extraction">WeuseMel-FrequencyCepstralCoefficient(MFCC)asthemainfeatureforemotionrecognition.FigureshowsafeatureextractionflowchartusingMFCC.First,thevoicedataisdividedintoframes.EachframeiswindowedusingaHammingwindow.Second,theanalysisframeisconvertedtothefrequencydomainusingashorttimeFouriertransform.Third,acertainnumberofsub-bandenergiesarecalculatedusingamelfilterbank,whichisanonlinear-scalefilterbankthatimitatesahuman'sauralsystem.Fourth,thelogarithmofthesub-bandenergiesiscalculated.Finally,theMFCCiscomputedbyaninverseFouriertransform.Intheproposedalgorithm,weuse28melfiltersand16MFCCs.</subsection>
  <subsection title="Frame-level Classification">Inthissectionwewillattempttodescribethemethodforclassifyinganalysisframes.Eachemotionisexpressedbyacodebook,andeachcodewordisrepresentedasavectorinthefeaturespace.Whenwehaveaninputfeaturevector,wecalculatethedistancebetweentheinputandallthecodewords.Finally,theemotionlabelofthenearestcodewordbecomestheclassificationresultoftheanalysisframe.Toavoidtheeffectofadifferentscalingofdimensions,weuseaMahalanobis-generalizeddistanceinsteadofaEuclideandistance.Figureillustrateshowwecalculatedthedistancesbetweentheinputandallthecodewordsinthefeaturedspace.Next,wewilldescribethecodebooktrainingprocedure.Thisdatabasestoresthecodebooksforthetargetemotions.ThetrainingprocedureisillustratedinFigure.First,thetrainingdataisclassifiedbyemotion.Next,thetrainingdataisdividedintoanalysisframes,andthenconvertedtofeaturevectors.Then,allthefeaturevectorsarecollected.Finally,thecodebookisgeneratedbyclusteringthesefeaturevectorstogetherusingtheLBGalgorithm.</subsection>
  <subsection title="Utterance-level Voting">Inthisprocess,emotionofoneutteranceisdecidedbyvotingofframe-levelemotionlabels.Anemotionwiththelargestnumberofvotesbecomestheemotionlabeloftheutterance.Figure5illustratesthevotingfortheemotionsofhotangerandneutralwithinoneutterance.Inthisexample,``hotanger''has39votesand``neutral''has11votes(Figure(1)).Amodificationwasaddedtoreducethecountofincorrectvotes.WeobservedthatthelabelingresultismadeofsomelongqueuesofaspecificlabelandadditionalshortspotsofotherlabelsasshowninFigure(1).Weassumedthatthereliabilityofthelabelingishighinthelongqueuesandlowintheshortspots.Therefore,weusedonlythelabelsoftheframesinthelongqueuestodecidethelabeloftheutterance.Intheproposedalgorithm,athresholdLwasintroduced,andthelabele_i(oftheframei)wasincludedinthevoteonlyife_k|k=i-L+1,,i-1,iareallthesame.Thisprocessingiscalled``utterance-levelsmoothing''.Figure(2)showsanexampleofL=4.</subsection>
  <section title="Evaluation Experiments">Twosetsofexperimentswereconductedtoevaluatetheperformanceoftheproposedalgorithm.Thefirstexperimentwasfortherecognitionofthetwoemotions(hotanger,andneutral),andthesecondonewasfortherecognitionoffouremotions(hotanger,neutral,sadness,andhappiness).TheconditionsfortheexperimentalanalysisareshowninTable.Inourexperiments,weusedtheemotionalspeechdatabasefromtheLinguisticDataConsortium.Eachutterancecorrespondstooneemotion.Oneutteranceiscomposedof3--4words,eithernumbers(``twothousandone'';``fourhundredten'')ordates(``Septemberninth'';``Decembertenth'').Theutteranceswerespokenbysevenactors(threemalesandfourfemales),mostlyintheirmid-20's.Theexperimentswereconductedinaspeaker-independentmannertoavoidtheeffectofindividuality.Theutterancesofthesixactorswereusedfortraining,andtheutterancesofoneactorwereusedfortheevaluation.Thesessionswererepeatedseventimesbyswitchingtheirroles.Theaverageofthesevensessionswascalculatedtoobtainthefinalrecognitionaccuracy.</section>
  <subsection title="Performance Evaluation">Theresultsofthetwo-andfour-emotionexperimentsareshowninFigure.Theverticalaxisrepresentstheaccuracy,andthehorizontalaxisrepresentsthecodebooksizeforeachemotion.Inthetwo-emotionexperiments,ahighaccuracywasobtainedevenwithverysmallcodebooks.Inthefour-emotionexperiments,theaccuracywaslowwiththesmallcodebooks,butitimprovedwiththeincreaseinthenumberofcodewordsupto64,wheretheperformancebecamesaturated.Tableshowstheconfusionmatricesfortheexperimentswith64codewords.Thecolumnistheemotionincludedintheutterance.Therowistherecognitionresultbytheproposedalgorithm.Two-emotionutterancesareaccuratelyrecognizedasshowninTable(A).Theaccuracyofthetwoemotionsis98.8%.Thereislittledifferenceintheaccuracybetweenhotangerandneutral.However,theaccuraciesforthefour-emotionsarenoteven,asshownTable(B).Inparticular,theaccuraciesofsadnessandhappinessarequitelow.Theaccuracyofthefouremotionsis66.4%.TableandTableshowsconfusionmatricesofaccuracyofeachspeaker.Weconfirmedthedifferenceoftherecognitionratewassmallinthetwo-emotionexperiment(Table(A)).However,inthefour-emotionexperiment,thedifferenceoftherecognitionratewasratherlarge(Table(B)).</subsection>
  <subsection title="Comparison between Proposed and Conventional Prosody-based Algorithm">Theproposedalgorithmandtheconventionalalgorithmwerecomparedtoevaluatetheperformanceoftheproposedalgorithm.Theproposedalgorithmwasimplementedasdescribedintheprevioussection.Thatis,theLBGalgorithmwasusedforthetrainingoftheemotionclusters,andk-nearestneighboralgorithmwasusedfortheemotionrecognition.WereferredtoTatoetal.toimplementthemostwell-knownemotionrecognitionalgorithm.Weextracted43prosodicfeaturesfromvoiced/unvoicedspeech,fundamentalfrequency,jitter,andenergy(Table).Thenormalizedprosodicfeatureswereusedforthisexamination.TheevaluationresultsareshowninTable.Itwasconfirmedthattheaccuracyoftheproposedalgorithmwashigherthantheprosody-basedalgorithm.Inparticular,theaccuracyofthefourdiscreteemotionswasimprovedby16.5points.Fromtheseresults,itwasconfirmedthatthefeatureofthisproposedalgorithmwasmoreeffectivethantheprosodicfeature.</subsection>
  <subsection title="Comparison between Proposed and Conventional MFCC-based Algorithms">WecomparetherecognitionratesofourproposedalgorithmandtheconventionalMFCC-basedalgorithm.WereferredtoNweetal.toimplementtheMFCC-basedemotionrecognitionalgorithm.InNwe,etal.,theyuseMFCCandlogfrequencypowercoefficients(LFPCs)torepresentthespeechsignalsandafour-statediscreteergodicHMMastheclassifier.WeuseHiddenMarkovModelToolkit(HTK)toimplementtheNwe'salgorithm.However,HTKdoesnotsupportdiscreteergodicHMM.Instead,weevaluatevarioustopologiesofleft-to-rightHMM.Ifthevariationoftheaccuracyamongvarioustopologiesissmallandtheaccuracyoftheproposedalgorithmisahigherthanthatofthebesttopologyoftheleft-to-rightHMM,theadvantageoftheproposedalgorithmwouldbeconfirmed.Toevaluatetheconventionalalgorithm,wehadmadetwotypesofHMMasshowninFigure7.ThefirstoneistheHMMwithskipsbetweenstates(Figure(a)).InthistypeofHMM,anyforwardtransitionispermitted.ThesecondoneistheHMMwithoutskips(Figure(b)).Moreover,thenumberofstatesofHMMwasvaried(1,2,4,and8).TheexperimentalresultsareshowninFigure.Theverticalaxisrepresentstherecognitionaccuracy,andthehorizontalaxisrepresentsnumberofHMMstates.HMM(Skip)correspondstotheresultsobtainedwithHMMwithskips.HMM(NoSkip)correspondstotheresultobtainedwithHMMwithoutskips.Therecognitionaccuracyoftheproposedalgorithm(Proposed)isalsoshownbythehorizontallineforreference.WeconfirmedthattheaccuracyoftheproposedalgorithmwashigherthantheconventionalMFCC-basedalgorithm.Inparticular,theaccuracyofthefour-emotionswasimprovedby4points.TheaccuracyoftheconventionalMFCC-basedalgorithmhasnotchangedmuchthroughtheexperimentswithvarioustopologies.</subsection>
  <subsection title="Effective Features of MFCC in Emotion Recognition">WeinvestigatedtheeffectofeachfeatureoftheMFCCfeaturevector.Recognitionthatusedonlyonefeaturewasevaluated.Inthisexperiment,thecodebookwasfixedat64words.TherecognitionrateofeachfeatureelementisshowninFigure.Theverticalaxisrepresentstheaccuracy,andthehorizontalaxisrepresentsthefeatures.TheaccuracyofMFCCsishigherthanthatofMFCCs.Inparticular,theaccuracyofalow-dimensionalMFCCishigh.The1stMFCCexceeds80%accuracy.</subsection>
  <section title="Conclusion">Wereportedanewapproachtoemotionrecognition.WeproposedanemotionrecognitionalgorithmusingMFCC.Evaluationexperimentsshowedthattheproposedalgorithmproduces66.4%recognitionaccuracyinspeaker-independentemotionrecognitionexperimentsforfouremotions(hotanger,neutral,sadness,andhappiness).Thisrecognitionaccuracywashigherthantheaccuracyoftheconventionalprosody-basedandMFCC-basedemotionrecognitionalgorithms,whichconfirmedthepotentialoftheproposedalgorithm.Theaccuracyof66.4%isnothighenoughforgeneraluse,buttheimprovementwouldmakesomeexistingapplicationsmoreeffective.However,wearefarfromsatisfiedwiththeaccuracyoftheproposedalgorithm.Furtherstudyisneededtoexploreadditionalfeaturesforclassifyingmoreemotions,anddevelopinganimprovedemotionrecognitionalgorithmusingthesefeatures.wouldliketothankMr.Moriwaki,N.,Mr.Horry,Y.,Dr.Yano,K.Dr.Osakabe,N.,AdvancedResearchLaboratory,Hitachi,Ltd.,andMr.Masui,S.,Hitachi(China)Research&amp;DevelopmentCorporationfortheirsupportduringthisresearch.document</section>
</root>
