\documentstyle[epsf,nlpbbl]{jnlp_e_b5}

\setcounter{page}{3}
\setcounter{巻数}{10}
\setcounter{号数}{4}
\setcounter{年}{2003}
\setcounter{月}{7}

\受付{May}{15}{2002}
\再受付{September}{3}{2002}
\再々受付{December}{6}{2002}
\採録{April}{11}{2003}

\setcounter{secnumdepth}{2}

\title{}
\author{}
\jkeywords{}




\etitle{Morpho-syntactic Rules for Detecting Japanese \\
Term Variation: Establishment and Evaluation}


\eauthor{Fuyuki Yoshikane\affiref{NIAD}  \and
         Keita Tsuji\affiref{NII}        \and
         Kyo Kageura\affiref{NII}        \and
         Christian Jacquemin\affiref{Labo}}


\headauthor{Yoshikane et~al.}
\headtitle{Morpho-syntactic Rules for Detecting Japanese Term Variation}

\affilabel{NIAD}
          {Faculty of University Evaluation and Research, National Institution for Academic Degrees (NIAD)}
          {Faculty of University Evaluation and Research, National Institution for Academic Degrees (NIAD)}
\affilabel{NII}
          {National Institute of Informatics (NII)}
          {National Institute of Informatics (NII)}
\affilabel{Labo}
          {Laboratoire d'Informatique pour la M\'ecanique et les Sciences de l'Ing\'enieur (LIMSI-CNRS), France}
          {Laboratoire d'Informatique pour la M\'ecanique et les Sciences de l'Ing\'enieur (LIMSI-CNRS), France}





\eabstract{
In this paper, we describe a rule-based mechanism that detects Japanese 
term variations from textual corpora. The system operates on the basis 
of meta-rules that map syntactic and morpho-syntactic variants of terms 
to the original forms of terms. The framework used here has been 
successfully applied to such languages as English and French, and we 
show here that it also works well in detecting Japanese term variants, 
once we properly take into account specific characteristics of the 
Japanese language. We also discuss the potential of this work for 
IR-related applications.
}

\ekeywords{Variation, Paraphrase, Term Extraction, 
Automatic Indexing, Information Retrieval}


\begin{document}

\maketitle
\thispagestyle{empty}
\section{Introduction}


In technical terminology, complex or multi-word terms consisting of two or more elements constitute the majority of terms \cite{Ishii} \footnote{Here we use multi-word terms and complex terms synonymously, although they are not exactly the same.}, so multi-word terms are expected to play an important role in information retrieval (IR). However, multi-word terms do not always appear in text in their canonical forms. Rather, they tend to be syntactically and/or morphologically transformed into their variants, while keeping the identity of the concepts they represent. By detecting those variants properly, therefore, we can expect an improvement in recall in retrieval, without affecting precision.

The purpose of this study is to establish rules that map variants of Japanese terms to their original forms. We evaluate the established rules by using Fastr \cite{Jacquemin94}, a system which was developed for the recognition of term variants in textual corpora and which has been successfully applied to such languages as French and English \cite{Jacquemin96,Jacquemin97}.

We briefly summarize the general characteristics of the Fastr system in Section~2. In Section~3, we describe the Japanese rules for detecting term variants. In Section~4, the results of an experiment in detecting term variants using Fastr is reported.

\section{General Framework for Term Variant Detection Using Fastr}

In this section, we describe the framework we used for the Japanese term variant detection reported in this paper using Fastr, a unification-based partial parser aimed at detecting term variants in texts \cite{Jacquemin94}. In the Fastr framework, one has to describe the linguistic knowledge and rules necessary for detecting term variants using: (1) meta-rules, which describe the skeletons of variation patterns; (2) term rules, which list and describe the structures of the original terms whose variants are to be detected; and (3) single-word rules (or entries), which define the single-words or morphemes used in the original terms.

The outline of the Fastr mechanism is as follows. In a first step, for each parsed sentence in a target corpus, words are morphologically analyzed and only term rules linked with at least one word in the current sentence are activated. Each active rule is analyzed through a depth-first recursive descent. If the parse of the rule skeleton is achieved successfully, the feature structures linked with the text occurrence in the current sentence and with the nodes of the term rule are unified. If unification succeeds, the text occurrence is reported as a non-variant (original) occurrence. In a second step, variant rules are dynamically generated by transforming each active term rule through meta-rules. Each variant rule is then parsed and the corresponding text occurrence is reported as a variant occurrence if the parse succeeds.

For instance, a term rule of ``language processing" and single-word rules of its constituent words, i.e. ``language" and ``processing", are described as follows.

\vspace*{5mm}
\noindent
Rule ($N_1$ $\rightarrow$ $N_2$ $N_3$)\\
$<N_2$ lem$>$ = `language'\\
$<N_3$ lem$>$ = `processing'\\

\noindent
Word `language' :\\
$<$cat$>$ = N.\\
Word `processing' :\\
$<$cat$>$ = N.\\

\noindent
By applying term rules such as this to meta-rules, Fastr generates variant rules which detect variants of the term.

Meta-rules consist of context-free skeletons and constraint equations. The syntactic structures of original terms and their potential variants are described in the context-free skeletons, and various conditions relating to lemmas (lem), part-of-speech categories (cat), etc. are described in the constraint equations. The following is an example of a meta-rule:

\vspace*{5mm}
\noindent
Meta-rule Coordination ($X_1$ $\rightarrow$ $X_2$ $N_3$) = ($X_1$ $\rightarrow$ $X_2$ $N_5$ $C_4$ $N_3$)\\
$<X_2$ cat$>$ = $N$ $|$ $A$
\vspace*{5mm}

\noindent
The skeleton on the left represents the structure of the original term, and the one on the right represents its associated variants. The constraint equation limits the part-of-speech category of $X_2$. Here $N$, $A$, $C$ and $X$ represent a noun, an adjective, a coordinating conjunction and any syntactic category, respectively. Identical symbols with the same subscript in the skeletons on the left and right are the same.

Thus, this meta-rule indicates that a complex term which consists of an element which is either a noun or an adjective followed by a noun may have a variant whose structure is the first element in the original term followed by a coordinating conjunction, a new noun and the noun element in the original term.

Assume that we have defined a term rule of ``language processing" as described above. Then, by applying the term rule to the meta-rule ``coordination", Fastr generates a new rule:

\vspace*{5mm}
\noindent
Rule ($X_1$ $\rightarrow$ language processing) = ($X_1$ $\rightarrow$ language $N_5$ $C_4$ processing)
\vspace*{5mm}

\noindent
By this rule, Fastr can detect terms such as ``language understanding or processing" as a variant of ``language processing".

Fastr can recognize not only syntactic transformations but also morphological transformations. This is done by adding constraint equations which link morphological derivatives through their roots. For instance, the following meta-rule generates a variant including Adjective ($A_4$) which is a derivative of Noun ($N_2$).

\vspace*{5mm}
\noindent
Meta-rule Noun-Adjective($X_1$ $\rightarrow$ $N_2$ $N_3$) = ($X_1$ $\rightarrow$ $A_4$ $N_3$)\\
$<A_4$ root$>$ = $<N_2$ root$>$
\vspace*{5mm}

\noindent
By this meta-rule, Fastr detects, for instance, ``categorial grammar" as a variant of ``category grammar" because ``categorial" and ``category" share the same root\footnote{In addition to the two types of variations mentioned here, i.e. syntactic variations and morpho-syntactic ones, Fastr has the facility to recognize semantic variations where synonyms are found.}.

\section{Japanese Meta-rules}

\subsection{Linguistic Units of Japanese}

In European languages such as English or French, words are divided by spaces (though morphemes are not necessarily divided by spaces, e.g. ``roadblock" [road + block], ``kindly" [kind + ly], etc.) \cite{Lieber}. Therefore, the boundary surrounding a word is reasonably clear. In Japanese, on the other hand, there is no delimiting symbol, making it difficult to define the word boundary consistently.
For instance, the sentence ``概念を曖昧にあらわした ([it] ambiguously expressed concepts)" can be divided into words in the following three ways. Here $N$, $A$, $V$, $S$ and $MD$ represent a noun, an adjective, a verb, a postpositional particle and an auxiliary verb.

\vspace*{5mm}
\noindent
a: 概念/を/曖昧/に/あらわし/た ({\it gainen[N] wo[S] aimai[N] ni[S] arawashi[V] ta[MD]})\\
b: 概念/を/曖昧/に/あらわした ({\it gainen[N] wo[S] aimai[N] ni[S] arawashita[V]})\\
c: 概念を/曖昧に/あらわした ({\it gainenwo[N] aimaini[A] arawashita[V]})\\

\noindent
The grammar of (a) follows the traditional Japanese grammar taught in schools. The grammar of (b) regards some auxiliary verbs as conjugations of verbs \cite{Mikami,Kindaichi,Teramura}. The grammar of (c), moreover, regards postpositional particles as declensions of nouns \cite{Matsushita,Suzuki}. That is, conjugation and declension are considered to be syntagmatic in (a), while they are considered to be paradigmatic in (c) \cite{Muraki}.

Actually, the definition of a word (or a morpheme) is not necessarily consistent among Japanese morphological analysis systems. For example, the morphological analyzers Chasen \cite{Matsumoto99} and JUMAN \cite{Matsumoto98}, which are the most widely used morphological analyzers for Japanese, divide the above sentence as follows.

\vspace*{5mm}
Chasen: 概念{\it [N]}/を{\it [S]}/曖昧{\it [N]}/に{\it [S]}/あらわし{\it [V]}/た{\it [MD]}

JUMAN: 概念{\it [N]}/を{\it [S]}/曖昧に{\it [A]}/あらわした{\it [V]}

\vspace*{5mm}
\noindent
In this case, the division by Chasen is in accord with (a) and that by JUMAN is nearer to (b) or (c). Chasen decomposes sentences into more elementary units than JUMAN does, e.g. Chasen treats not only auxiliary verbs but also the inflection of {\it na}-adjectives\footnote{Adjectives which consist of a root noun and the inflection な({\it na}) are called {\it na}-adjectives. Their inflectional paradigm is だ({\it da}), に({\it ni}), な({\it na}).} as independent units.

As our aim is to develop a practically-applicable system, we do not go into the theoretical issue concerning the definition of a word (or a morpheme) but simply follow the definition in the morphological analysis system we use. In this paper, we used Chasen (version 2.0) as a morphological analyzer for Japanese. The reason why we chose Chasen and not JUMAN is that, as we will show later, we can describe all meta-rules for Japanese term variants through syntactic operations if we follow the scheme defined in Chasen. Thus, we use morphemes, based on the scheme of Chasen, as linguistic units for describing meta-rules. And then we define the part-of-speech scheme for describing meta-rules on the basis of the scheme of Chasen. The part-of-speech scheme will be shown shortly.

Even though our work is made in correlation with Chasen, it can be easily extended to other morphological analyzers through a slight modification of meta-rules.

\subsection{Part-of-speech Scheme}

The syntactic categories used in this paper are given in Table 1. These categories basically depend on the broad-level categories used by Chasen. However, we have also used some detailed-level categories used by Chasen and our original categories for the purpose of describing accurate meta-rules. The number of defined categories is 21, and we use them in morpheme rules, term rules and meta-rules.

Some categories in Table 1 need explanation. Firstly, $VS$ represents the s-inflectional or {\it sahen}-verb\footnote{The {\it sahen}-verb or s-inflectional verb is referred to as such because the inflectional paradigm is bound to such patterns as ``{\it sa}", ``{\it shi}", ``{\it su}", ``{\it se}", etc.} する({\it suru}) (``do"), which functions as both a common and special auxiliary verb. By tacking on the {\it sahen}-verb, {\it sahen}-nouns can be turned into verbs. For such syntactic characteristics, the {\it sahen}-verb ($VS$) is distinguished from common verbs ($V$), and {\it sahen}-nouns ($NS$) are distinguished from the other nouns ($N$ and $NA$). 

$NA$ represents special nouns which are also roots of {\it na}-adjectives. They are turned into adjectives by adding the auxiliary verb だ({\it da}) to the end. $SC$, meanwhile, represents postpositional particles connecting coordinate nouns; these correspond to English coordinating conjunctions such as ``and" and ``or". It was necessary to distinguish them from the other postpositional particles ($S$) in order to describe an important variation pattern of Japanese terms.

In Japanese, the syntactic functions of derivatives are determined by suffixes (tail-positional elements, $TP$) in principle. According to their functions, we can classify suffixes into three categories: suffixes deriving verbs ($TPV$); suffixes deriving adjectives ($TPA$); and suffixes deriving nouns\footnote{The majority of suffixes deriving nouns are one-Chinese-character suffixes \cite{Nomura,Kageura}.}. Moreover, suffixes deriving nouns are classified into three subcategories, i.e. suffixes deriving {\it sahen}-nouns ($TPNS$), suffixes deriving nouns which can be roots of {\it na}-adjectives ($TPNA$) and the other suffixes which derive common nouns ($TPN$). We have to distinguish not only between $NS$, $NA$ and $N$ but also between $TPNS$, $TPNA$ and $TPN$ in order to describe accurate meta-rules because their syntactic behavior is different.

In some linguistic theories, suffixes or inflections of {\it na}-adjectives are regarded not as independent units but as parts of a word. In this paper, however, we treat them as independent units according to the scheme of Chasen.

\begin{table*}
\begin{center}
{\small
\begin{tabular}{|l|l|l|}
\hline
Symbol & Category & Example\\
\hline
\hline
\underline{$L$} & Delimiters & ・\\
$LSD$ & Final punctuations & 。\\
\hline
\underline{$V$} & Verbs (except $VS$) & 用いる({\it mochiiru})\\
\underline{$VS$} & {\it Sahen}-verb {\it suru} & する({\it suru})\\
\hline
\underline{$A$} & Adjectives & 深い({\it fukai})\\
\hline
\underline{$N$} & Nouns (except $NS$ and $NA$) & 階層({\it kaisou})\\
\underline{$NS$} & {\it Sahen}-nouns & 推論({\it suiron})\\
\underline{$NA$} & Nouns which can be roots of {\it na}-adjectives & 曖昧({\it aimai})\\
\hline
\underline{$RB$} & Adverbs & なんだか({\it nandaka})\\
\hline
$CC$ & Conjunctions & けれども({\it keredomo})\\
\hline
$RT$ & {\it Rentaishi} & いろんな({\it ironna})\\
\hline
$EX$ & Exclamations & おはよう({\it ohayou})\\
\hline
$PP$ & Prefixes & お({\it o})\\
\hline
\underline{$S$} & Postpositional particles (except $SC$) & の({\it no})\\
\underline{$SC$} & Postpositional particles connecting coordinate nouns & および({\it oyobi})\\
\hline
\underline{$TPV$} & Suffixes deriving verbs & られる({\it rareru})\\
\underline{$TPA$} & Suffixes deriving adjectives & らしい({\it rashii})\\
\underline{$TPN$} & Suffixes deriving nouns (except $TPNS$ and $TPNA$) & 性({\it sei})\\
\underline{$TPNS$} & Suffixes deriving {\it sahen}-nouns & 化({\it ka})\\
\underline{$TPNA$} & Suffixes deriving roots of {\it na}-adjectives & 的({\it teki})\\
\hline
\underline{$MD$} & Auxiliary verbs & だ({\it da})\\
\hline
\end{tabular}
}
\begin{center}
\scriptsize{{\it Rentaishi} is a Japanese category which functions only as a modifier of nouns.\\}
\end{center}

\end{center}
\caption{Categories in the part-of-speech scheme}
\end{table*}

\subsection{Patterns of Japanese Term Variations}

What types of variants should be detected? It depends on the purpose of a study. In this study, assuming IR-related applications, we will describe the meta-rules (or the linguistic rules) that can detect `variants in a broader sense'. In indexing documents, it is not necessary to distinguish between terms and other grammatical units because the important factor is the concepts represented by them rather than their surface forms. Thus, even if an original term is transformed into a phase or a passage, we regard it as a variant if the concept of the original term is kept in it. That is, we define a variant as a term, a phrase, or a passage in which the content-bearing elements of an original term appear with their relation in modification kept.

In order to define the meta-rules detecting Japanese term variations, we examine the patterns of variations both empirically and rationally. After listing the patterns of variations on the basis of actual variants of terms observed to occur in texts, we check them linguistically.

\subsubsection{Data Observation}

We prepared terminological and textual data in the field of information processing or its subdomain as empirical data.

The terminological data was extracted from the Japanese technical terms dictionary in the EDR (Electronic Dictionary Research Institute) database, which lists terms in information processing \cite{EDR}. The technical terms dictionary has approximately 120,000 records which contain linguistic information identifying each term's headword, part-of-speech and so on. We sampled 40,000 records, and used the headwords among them as terminological data.

For textual data, we used the NACSIS Academic Conference Database \cite{NACSIS}. Each record in the database has 35 bibliographic fields, such as title, author, abstract, host society and so on. We extracted the records of conferences hosted by Japanese Society for Artificial Intelligence from the database. The number of extracted records is 2,031. Among them, we used titles and abstracts as textual data.

Both the terminological and textual data were processed by Chasen 2.0. For each term in the terminological data, we extracted corresponding sentences from the textual data, i.e. sentences which include all the constituent elements of the term. For instance, the sentence ``(...)\underline{学習}される\underline{概念}の階層構造が人間の直観に沿わないものとなる" was extracted for the term ``概念学習". At this stage, only verbs, adjectives, nouns and adverbs were checked.

\subsubsection{Seven Elementary Patterns}

After examining the variants observed in the extracted sentences\footnote{The part-of-speech categories found in the observed variants are underlined in Table 1. Those categories are used in describing meta-rules.}, we identified the following seven patterns. In the first three patterns (A, B, C), one or more elements of the term are transformed by means of their combination with functional elements such as suffixes or formal verbs. In some cases, by adding/eliminating/changing suffixes or formal verbs, the elements are transformed into their derivatives of another part-of-speech. Generally, transformations of the following three patterns are bidirectional.

\begin{description}

\item[A. Noun-Noun variations]:\\
Some nouns can be transformed into their noun derivatives by adding special suffixes, e.g. 的({\it teki}), 化({\it ka}), etc. These suffixes derive nouns from nouns. Likewise, by eliminating these suffixes or replacing them with other suffixes, we can derive noun derivatives in some cases. Let us conveniently call this type of variation the noun-noun variation.\\
For example:\\
概念の階層({\it gainen[N] no[S] kaisou[N]}) $\leftrightarrow$ 概念の階層化({\it gainen[N] no[S] kaisou[N] ka[TPNS]})\\
(hierarchy of concepts $\leftrightarrow$ hierarchization of concepts)\\
階層的構造({\it kaisou[N] teki[TPNA] kouzou[N]}) $\leftrightarrow$ 階層化構造({\it kaisou[N] ka[TPNS] kouzou[N]})\\
(hierarchical structure $\leftrightarrow$ hierarchized structure)\\

\item[B. Noun-Adjective variations]:\\
Some nouns function as roots of {\it na}-adjectives. These nouns can be transformed into adjectives by adding the inflection of {\it na}-adjectives. In the part-of-speech scheme of Chasen, the inflection な({\it na})\footnote{Its basic form is だ({\it da}).} is regarded as an auxiliary verb. Contrarily, by eliminating the inflection な({\it na}), we can derive nouns from {\it na}-adjectives. We call the variations relating to such types of derivation noun-adjective variations.\\
For example:\\
曖昧情報({\it aimai[NA] jouhou[N]}) $\leftrightarrow$ 曖昧な情報({\it aimai[NA] na[MD] jouhou[N]})\\
(ambiguous information)\\

\item[C. Noun-Verb variations]:\\
The {\it sahen}-verb する({\it suru}) can be connected to {\it sahen}-nouns and constitute various verbs derived from the {\it sahen}-nouns. We call variations relating to such types of derivation, i.e. verbalization and nominalization of the {\it sahen}-verb and {\it sahen}-nouns, noun-verb variations.\\
For example:\\
概念の学習({\it gainen[N] no[S] gakushuu[NS]}) $\leftrightarrow$ 概念を学習する({\it gainen[N] wo[S] gakushuu[NS] suru[VS]})\\
(concept learning $\leftrightarrow$ learn concepts)\\

\end{description}

In actual texts, the variations described above are often combined with the following syntactic variations.

\begin{description}

\item[D. Composition/Decomposition]:\\
A compound noun can be transformed into a decompounded noun phrase by inserting the postpositional particle の({\it no}) between constituent nouns if the preceding noun modifies the following noun. Such a decompounded term is a variant of the original compound noun. In compounding variations, on the other hand, the postpositional particle between nouns is omitted.\\
In some decompounding variations, the appropriate verb which expresses the relation between the constituent nouns is inserted. In that case, the verb must be inserted together with a case-marking postpositional particle such as を({\it wo})\footnote{を({\it wo}) makes objective cases.}.\\
For example:\\
情報検索({\it jouhou[N] kensaku[NS]}) $\leftrightarrow$ 情報の検索({\it jouhou[N] no[S] kensaku[NS]})\\
(information retrieval $\leftrightarrow$ retrieval of information)\\
組合せ問題({\it kumiawase[N] mondai[N]}) $\rightarrow$ 組合せを求める問題({\it kumiawase[N] wo[S] motomeru[V] mondai[N]})\\
(combination problem $\rightarrow$ combination seeking problem)\\

\item[E. Modification]:\\
By adding a modifier to the constituent element of the original term, the term is turned into its variants with narrower or more informative senses. The modifier is placed before the modified element. Here, we only consider internal modification where a modifier is inserted inside the string of the original term. As external modification adds a new element without changing the structure of the original term, we can detect the existence of external variants without defining meta-rules.\\
Nouns, adjectives and verbs are used for the modification of nouns. In the internal modification of compound nouns, however, only a noun can be inserted between nouns as a modifier without a postpositional particle.\\
For example:\\
全文検索({\it zenbun[N] kensaku[NS]}) $\rightarrow$ 全文キーワード検索({\it zenbun[N] kiiwaado[N] kensaku[NS]})\\
(full text search $\rightarrow$ full text keyword search)\\

\item[F. Coordination]:\\
Terms which consist of two coordinated elements with a common head element or a common argument are considered to be variants of the original terms which consist of one of the coordinated elements with the head or argument in question. The variation combining two terms with a common {\it sahen}-verb する({\it suru}) is also classified into this pattern.\\
In this type of variation, a postpositional particle or a delimiter is often inserted between the two coordinated elements.\\
For example:\\
分解原理({\it bunkai[NS] genri[N]}) $\rightarrow$ 分解・結合原理({\it bunkai[NS] -[L] ketsugou[NS] genri[N]})\\
(resolution theory $\rightarrow$ resolution and combination theory)\\
知識の獲得({\it chishiki[N] no[S] kakutoku[NS]}) $\rightarrow$ 知識の生成や獲得({\it chishiki[N] no[S] seisei[NS] ya[SC] kakutoku[NS]})\\
(knowledge acquisition $\rightarrow$ knowledge generation and acquisition)\\
比較する({\it hikaku[NS] suru[VS]}) $\rightarrow$ 比較・検討する({\it hikaku[NS] -[L] kentou[NS] suru[VS]})\\
(compare $\rightarrow$ compare and examine)\\

\item[G. Permutation]:\\
Some compound nouns can be transformed into their variants through the permutation of their constituent elements but still maintain the identity of the concepts they represent in a broad sense. Especially when one of the permuted elements is a {\it sahen}-noun and the other is its object, the semantic relationship between the two is kept identical in many cases, although the syntactic roles of the elements change, i.e. the head becomes the modifier and vice versa.\\
For example,\\
メモリ共有({\it memori[N] kyouyuu[NS]}) $\leftrightarrow$ 共有メモリ({\it kyouyuu[NS] memori[NS]})\\
(memory sharing $\leftrightarrow$ shared memory)\\

In 共有メモリ as well as in メモリ共有, the relationship ``memory is shared" is kept. If the semantic relationship between constituent elements does not change like in this example, we can expect that documents containing an original term and those containing its variants share a connected topic. Therefore, we call this type of variation ``permutation", and introduce it as a variation pattern to be extracted in indexing documents.
\end{description}

\vspace*{5mm}
Among the above seven patterns, the five patterns A-D and G are considered to be synonymous variations. On the other hand, E and F are considered to be non-synonymous variations which add another substantial meaning to the original meaning\footnote{Contrary to E (modification), `eliminating some of the constituent elements' makes broader terms whose meanings are more generalized than that of the original term. Applying the detection of broader terms to IR (document indexing), the precision is expected to considerably decrease though the recall increases. In this paper, therefore, we do not deal with this type of transformation, where transformed terms lose part of the meaning of the original term. Also we do not deal with the transformation into acronyms or abbreviations, because it cannot be handled in the framework based on morpho-syntactic rules.}.

Table 2 shows sample meta-rules for each variation pattern (A-G). These meta-rules consist of context-free skeletons and constraint equations, where we use the symbols shown in Table 1 to represent part-of-speech categories. The symbols ``*", ``+" and ``?" in context-free skeletons are regular expressions, carrying their well-established meanings, e.g., ``*" means ``zero or more times".

The categories of variations described in Table 2 are analogous to those in English \cite{Jacquemin97} \footnote{In the English language, permutations are most often combined with a composition/decomposition variation \cite{Daille}.}. Because Chasen treats suffixes and the inflection of {\it na}-adjectives as independent units, we can define Japanese noun-noun, noun-adjective and noun-verb variations (A-C) as well as the other types of variations (D-F) through syntactic operations.

In actual texts, some types of variations are frequently observed while others seldom appear. By reexamining the aforesaid data\footnote{We sampled 40,000 terms from the EDR database and 200 documents from the NACSIS database. For each term, we manually extracted its variants from the sampled documents. 204 terms, among the 40,000 terms, transform into their variants and appear at least one time in the 200 documents.}, we counted the frequencies of variants. Table 3 shows the number of variants including each variation pattern. The table also shows the total number of observed variants. Since some variants include two or more patterns, the sum of the number of variants including each variation pattern is not consistent with the total number of variants.

\begin{table}
\begin{center}
{\footnotesize
\begin{tabular}{ll} 
\hline
Variation & Sample meta-rule and example of transformation\\
\hline
\hline
A. Noun-noun & ($X_1$ $\rightarrow$ $X_2$ $X_3$) $=$ ($X_1$ $\rightarrow$ $X_2$ $X_4$ $X_3$)\\
　 variations & $<X_2$ cat$>$ = $N$ $|$ $NS$ $|$ $NA$, \quad $<X_3$ cat$>$ $=$ $N$ $|$ $NS$ $|$ $NA$,\\
& $<X_4$ cat$>$ = $TPN$ $|$ $TPNS$ $|$ $TPNA$\\
& 　　共有[NS]メモリ[N]$\rightarrow$共有[NS]化[TPNS]メモリ[N]\\
\hline

B. Noun-adjective & ($X_1$ $\rightarrow$ $NA_2$ $X_3$) $=$ ($X_1$ $\rightarrow$ $NA_2$ $MD_4$ $X_3$)\\
　 variations & $<X_3$ cat$>$ = $N$ $|$ $NS$ $|$ $NA$\\
& 　　曖昧[NA]情報[N]$\rightarrow$曖昧[NA]な[MD]情報[N]\\
\hline

C. Noun-verb & ($X_1$ $\rightarrow$ $X_2$ $S_3$ $NS_4$) $=$ ($X_1$ $\rightarrow$ $X_2$ $S_5$ $NS_4$ $VS_6$)\\
　 variations & $<X_2$ cat$>$ = $N$ $|$ $NS$ $|$ $NA$, \quad $<S_5$ lem$>$ $=$ `を'\\
& 　　概念[N]の[S]学習[NS]$\rightarrow$概念[N]を[S]学習[NS]する[VS]\\
\hline

D. Composition/ & ($X_1$ $\rightarrow$ $X_2$ $X_3$) $=$ ($X_1$ $\rightarrow$ $X_2$ $S_4$ $X_3$)\\
　 Decomposition & $<X_2$ cat$>$ = $N$ $|$ $NS$ $|$ $NA$, \quad $<X_3$ cat$>$ $=$ $N$ $|$ $NS$ $|$ $NA$,\\
& $<S_4$ lem$>$ $=$ `の'\\
& 　　情報[N]検索[NS]$\rightarrow$情報[N]の[S]検索[NS]\\
& ($X_1$ $\rightarrow$ $X_2$ $X_3$) $=$ ($X_1$ $\rightarrow$ $X_2$ $S_4$ ($V$ $|$ $NS$ $VS$ $|$ $X$ $TPNS$ $VS$) $TPV^?$ $MD^?$ $X_3$)\\
& $<X_2$ cat$>$ = $N$ $|$ $NS$ $|$ $NA$, \quad $<X_3$ cat$>$ $=$ $N$ $|$ $NS$ $|$ $NA$\\
& 　　組合せ[N]問題[N]$\rightarrow$組合せ[N]を[S]求める[V]問題[N]\\
\hline

E. Modification & ($X_1$ $\rightarrow$ $X_2$ $X_3$) $=$ ($X_1$ $\rightarrow$ $X_2$ ($N$ $|$ $X$ $TPN)^+$ $X_3$)\\
& $<X_2$ cat$>$ = $N$ $|$ $NS$ $|$ $NA$, \quad $<X_3$ cat$>$ $=$ $N$ $|$ $NS$ $|$ $NA$\\
& 　　全文[N]検索[NS]$\rightarrow$全文[N]テキスト[N]検索[NS]\\
& ($X_1$ $\rightarrow$ $X_2$ $S_3$ $X_4$) $=$ ($X_1$ $\rightarrow$ $X_2$ $S_3$ ($A$ $|$ $NA$ $MD$ $|$ $X$ $TPNA$ $MD)^+$ $X_4$)\\
& $<X_2$ cat$>$ = $N$ $|$ $NS$ $|$ $NA$, \quad $<X_3$ cat$>$ $=$ $N$ $|$ $NS$ $|$ $NA$\\
& 　　ネットワーク[N]の[S]解析[NS]$\rightarrow$\\
& 　　ネットワーク[N]の[S]動的[NA]な[MD]解析[NS]\\
\hline

F. Coordination & ($X_1$ $\rightarrow$ $X_2$ $X_3$) $=$ ($X_1$ $\rightarrow$ $X_2$ $L_4$ $X_5$ $X_3$)\\
& $<X_2$ cat$>$ = $N$ $|$ $NS$ $|$ $NA$, \quad $<X_3$ cat$>$ $=$ $N$ $|$ $NS$ $|$ $NA$,\\
& $<X_5$ cat$>$ = $N$ $|$ $NS$ $|$ $NA$\\
& 　　分解[NS]原理[N]$\rightarrow$分解[NS]・[L]結合[NS]原理[N]\\
& ($X_1$ $\rightarrow$ $X_2$ $S_3$ $X_4$) $=$ ($X_1$ $\rightarrow$ $X_2$ $S_3$ $X_5$ ($SC$ $|$ $L$) $X_4$)\\
& $<X_2$ cat$>$ = $N$ $|$ $NS$ $|$ $NA$, \quad $<X_4$ cat$>$ $=$ $N$ $|$ $NS$ $|$ $NA$,\\
& $<X_5$ cat$>$ = $N$ $|$ $NS$ $|$ $NA$\\
& 　　知識[N]の[S]獲得[NS]$\rightarrow$知識[N]の[S]生成[NS]や[SC]獲得[NS]\\
\hline

G. Permutation & ($X_1$ $\rightarrow$ $NS_2$ $N_3$) $=$ ($X_1$ $\rightarrow$ $N_3$ $NS_2$)\\
& 　　共有[NS]メモリ[N]$\rightarrow$メモリ[N]共有[NS]\\
\hline
\end{tabular}
}
\end{center}
\caption{Japanese meta-rules}
\end{table}

\begin{table*}[htb]
\begin{center}
\begin{tabular}{c|rr|rr}
\hline
& \multicolumn{2}{|c}{token} & \multicolumn{2}{|c}{type}\\
\hline
\hline
Total & 529 & 100\,\% & 316 & 100\,\%\\
\hline
A & 97 & 18.3\,\% & 64 & 20.3\,\%\\
B & 35 & 6.62\,\% & 28 & 8.86\,\%\\
C & 121 & 22.9\,\% & 69 & 21.8\,\%\\
D & 315 & 59.6\,\% & 198 & 62.7\,\%\\
E & 164 & 31.0\,\% & 139 & 44.0\,\%\\
F & 15 & 2.84\,\% & 15 & 4.75\,\%\\
G & 144 & 27.2\,\% & 96 & 30.4\,\%\\
\hline
\end{tabular}
\end{center}
\caption{Number of variants including each variation pattern}
\end{table*}

\subsection{Composition of Meta-rules}

We defined 64 meta-rules on the basis of the seven elementary patterns mentioned in Section~3.3. All of the meta-rules are shown in Appendix. They are composed of one or more elementary patterns of variations. Because in some cases variants are produced through the combination of elementary patterns, we have to define meta-rules including two or more patterns as well as simple meta-rules including only one pattern. The number of times each pattern is included in a meta-rule is as follows: A (noun-noun variations)-12; B (noun-adjective variations)-9; C (noun-verb variations)-21; D (composition/decomposition)-21; E (modification)-51; F (coordination)-13; and G (permutation)-13. As for meta-rules including E (modification), we allowed inserting from zero to five modifiers.

Among the 64 meta-rules, 52 meta-rules include two or more elementary patterns. Table 4 shows the number of meta-rules and an example of transformation for each combination of the patterns. For instance, the variation ``知識獲得 $\rightarrow$ 獲得された文法知識 (knowledge acquisition $\rightarrow$ acquired grammatical knowledge)" includes the following three operations.

\vspace*{5mm}

\noindent
G (permutation): ``知識獲得({\it chishiki[N] kakutoku[NS]}) $\rightarrow$ 獲得知識({\it kakutoku[NS] chishiki[N]})"\\
C (noun-verb variations): ``獲得知識({\it kakutoku[NS] chishiki[N]}) $\rightarrow$ 獲得された知識({\it kakutoku[NS] sa[VS] re[TPV] ta[MD] chishiki[N]})"\\
E (modification): ``獲得された知識({\it kakutoku[NS] sa[VS] re[TPV] ta[MD] chishiki[N]}) $\rightarrow$ 獲得された文法知識({\it kakutoku[NS] sa[VS] re[TPV] ta[MD] bunpou[N] chishiki[N]})"\\

\noindent
This type of variation can be detected by the meta-rule which is composed of C, E and G.

{
\begin{table}
\begin{center}
\footnotesize
\begin{tabular}{|rl|r|l|}
\hline
& Combination & NM & Example of transformation\\
\hline
\hline
1. & A+B+C & 1 & 自動検索({\it jidou[N] kensaku[NS]}) $\rightarrow$ 自動的に検索する\\
& & & ({\it jidou[N] teki[TPNA] ni[S] kensaku[NS] suru[VS]})\\
2. & A+B+E & 4 & 幾何モデル({\it kika[N] moderu[N]}) $\rightarrow$ 幾何的な論理モデル\\
& & & ({\it kika[N] teki[TPNA] na[MD] ronri[N] moderu[N]})\\
3. & A+C+D+E & 2 & 概念階層({\it gainen[N] kaisou[N]}) $\rightarrow$ 概
 念を自動的に階層化する\\ 
& & & ({\it gainen[N] wo[S] jidou[N] teki[TPNA] ni[S] kaisou[N] ka[TPNS] suru[VS]})\\
4. & A+D+E+G & 1 & 共有化メモリ ({\it kyouyuu[NS] ka[TPNS] memori[N]}) $\rightarrow$ メモリの仮想的共有\\
& & & ({\it memori[N] no[S] kasou[N] teki[TPNA] kyouyuu[NS]})\\
5. & A+E & 3 & 幾何モデル({\it kika[N] moderu[N]}) $\rightarrow$ 幾何的論理モデル\\
& & & ({\it kika[N] teki[TPNA] ronri[N] moderu[N]})\\
6. & A+E+G & 1 & 共有化メモリ({\it kyouyuu[NS] ka[TPNS] memori[N]}) $\rightarrow$ メモリ仮想共有\\
& & & ({\it memori[N] kasou[N] kyouyuu[NS]})\\
7. & B+C & 2 & 曖昧表現({\it aimai[NA] hyougen[NS]}) $\rightarrow$ 曖昧に表現する\\
& & & ({\it aimai[NA] ni[S] hyougen[NS] suru[VS]})\\
8. & B+E & 2 & 幾何的モデル({\it kika[N] teki[TPNA] moderu[N]}) $\rightarrow$ 幾何的な論理モデル\\
& & & ({\it kika[N] teki[TPNA] na[MD] ronri[N] moderu[N]})\\
9. & C+D+E & 3 & 概念学習({\it gainen[N] gakushuu[NS]}) $\rightarrow$ 概念を帰納的に学習する\\
& & & ({\it gainen[N] wo[S] kinou[NS] teki[TPNA] ni[S] gakushuu[NS] suru[VS]})\\
10. & C+D+E+G & 1 & 獲得知識({\it kakutoku[NS] chishiki[N]}) $\rightarrow$ 知識を自動的に獲得する\\
& & & ({\it chishiki[N] wo[S] jidou[N] teki[TPNA] ni[S] kakutoku[NS] suru[VS]})\\
11. & C+E & 6 & 概念の学習({\it gainen[N] no[S] gakushuu[NS]}) $\rightarrow$ 概念を帰納的に学習する\\
& & & ({\it gainen[N] wo[S] kinou[NS] teki[TPNA] ni[S] gakushuu[NS] suru[VS]})\\
12. & C+E+G & 4 & 知識獲得({\it chishiki[N] kakutoku[NS]}) $\rightarrow$ 獲得された文法知識\\
& & & ({\it kakutoku[NS] sa[VS] re[TPV] ta[MD] bunpou[N] chishiki[N]})\\
13. & D+E & 10 & 共通知識({\it kyoutsuu[NA] chishiki[N]}) $\rightarrow$ 共通の抽象的知識\\
& & & ({\it kyoutsuu[NA] no[S] chuushou[N] teki[TPNA] chishiki[N]})\\
14. & D+E+F & 1 & 学習制御({\it gakushuu[NS] seigyo[NS]}) $\rightarrow$ 学習および対話の自動制御\\
& & & ({\it gakushuu[NS] oyobi[SC] taiwa[NS] no[S] jidou[N] seigyo[NS]})\\
15. & D+E+G & 2 & 共有メモリ({\it kyouyuu[NS] memori[N]}) $\rightarrow$ メモリの仮想的共有\\
& & & ({\it memori[N] no[S] kasou[N] teki[TPNA] kyouyuu[NS]})\\
16. & D+F & 1 & 知識獲得({\it chishiki[N] kakutoku[NS]}) $\rightarrow$ 知識の生成や獲得\\
& & & ({\it chishiki[N] no[S] seisei[NS] ya[SC] kakutoku[NS]})\\
17. & E+F & 4 & 分解原理({\it bunkai[NS] genri[N]}) $\rightarrow$ 分解・結合基本原理\\
& & & ({\it bunkai[NS] -[L] ketsugou[NS] kihon[N] genri[N]})\\
18. & E+G & 4 & 共有メモリ({\it kyouyuu[NS] memori[N]}) $\rightarrow$ メモリ仮想共有\\
& & & ({\it memori[N] kasou[N] kyouyuu[NS]})\\
\hline
\end{tabular}
\begin{center}
NM: number of meta-rules.
\end{center}

\end{center}
\caption{Combination of elementary patterns}
\end{table}
}

\section{Experiments and Evaluations}

In order to check the validity of the established meta-rules, we carry out the following experiments and calculate the precision and the recall. In this paper, we evaluate meta-rules themselves and not the performance of Fastr, which crucially depends on dictionaries as well as rules. So, the recall in this experiment implies the ratio of extracted variants to all the variants of dictionary entries appearing in a corpus.

Moreover, we apply term variant detection to automatic indexing and evaluate the performance.

\subsection{Data}

For two academic domains, i.e. artificial intelligence and architecture, we carry out experiments in which we detect variants of the entries in a technical term dictionary by Fastr. Artificial intelligence is the domain where we have observed data to define the patterns of meta-rules. So we naturally expect that the defined meta-rules can be appropriately applied to this domain. By comparing the results of the experiment in architecture with those of that in artificial intelligence, we examine the domain dependency of the defined meta-rules.

As an information source for each domain, we used a technical term dictionary of several thousand terms. For artificial intelligence, we compiled 3,245 term rules\footnote{We excluded terms that have a morpheme of over 30 bytes, which cannot be processed by Fastr, and terms that consist of one morpheme.} and 2,392 single-word (morpheme) rules from {\it Encyclopedia of Artificial Intelligence} (Shapiro and Eckroth 1991) \footnote{The reason why we did not use EDR database, which was used in defining meta-rules, for artificial intelligence is that in the comparison of the two domains it is better to use dictionaries of similar sizes. In architecture, no dictionaries in electronic form whose sizes are close to EDR's size were available for us.}. For architecture, on the other hand, we compiled 6,519 term rules and 5,197 single-word (morpheme) rules from {\it Japanese Scientific Terms: Architecture} \cite{Japanese}.

For each domain, we equipped those rules with Fastr and, in order to apply them, prepared a corpus consisting of the titles and abstracts of documents extracted from the NACSIS Academic Conference Database. For artificial intelligence, we used 2,031 documents (13,855 sentences, 364,470 morphemes), all the data belonging to the field of artificial intelligence, which are the same data that we had observed for the purpose of defining the patterns of meta-rules in Section~3.3. For architecture, we used 2,000 documents\footnote{We sampled the 2,000 documents which were published earliest. The number 2,000 is almost equal to the number of all the documents of artificial intelligence.} (11,452 sentences, 337,175 morphemes) extracted from the data belonging to the field of architecture. We applied our meta-rules to those textual data\footnote{In our experiments, terms and sentences are decomposed into morphemes by Chasen 2.0.}, and evaluated precision and recall in term variant detection.

\subsection{Evaluation of Meta-rules}

\subsubsection{Overview of Results}

By using Fastr, in which term rules, single-word (morpheme) rules and meta-rules are implemented, we tried to extract variants of technical terms from the aforementioned textual data. We then manually judged whether the extracted variants are correct or not.

While Table 5 shows the number of extracted terms which appear in their original forms (entry forms in the dictionary), Table 6 shows the number of extracted variants. For original terms and for variants, we counted the number of tokens and the number of types. For variants, moreover, we calculated the precision, i.e. the ratio of correct variants to extracted variants, and the recall, i.e. the ratio of extracted variants to all the variants of dictionary entries appearing in a corpus\footnote{As for the recall, the values shown in Table 6 are approximate ones which we calculated from 200 sampled documents for each domain.}. Both the precision and the recall are approximately 80\,\% in architecture as well as in artificial intelligence. Thus, we can expect that our meta-rules do not depend on domains so seriously.

\begin{table*}[htb]
\begin{center}
\begin{tabular}{l|c|c}
\hline
& token & type\\
\hline
\hline
Artificial Intelligence & 5,923 & 548\\
Architecture & 13,653 & 1,368\\
\hline
\end{tabular}

\end{center}
\caption{Number of original terms extracted from the textual data}
\end{table*}

\begin{table*}[htb]
\begin{center}
\begin{tabular}{l|ccc|c}
\hline
& \multicolumn{4}{|c}{token}\\
\hline
& correct & incorrect & precision & recall\\
\hline
\hline
Artificial Intelligence & 1,139 & 287 & 79.9\,\% & 80.6\,\%\\
Architecture & 832 & 152 & 84.6\,\% & 77.5\,\%\\
\hline
\hline
& \multicolumn{4}{|c}{type}\\
\hline
& correct & incorrect & precision & recall\\
\hline
\hline
Artificial Intelligence & 482 & 146 & 76.8\,\% & 78.6\,\%\\
Architecture & 382 & 83 & 82.2\,\% & 76.9\,\%\\
\hline
\end{tabular}

\end{center}
\caption{Number of extracted variants}
\end{table*}

Comparing Table 6 with Table 5, one will find that the ratio of variants to original terms in the corpus of architecture is far lower than that of artificial intelligence, both in the number of tokens and in the number of types. From the fact that variants do not appear so frequently in architecture, one might intuitionally infer that architecture is a domain whose technical terms have little flexibility in comparison to those of artificial intelligence. However, we should not simply compare the two domains, as they have differences in condition, such as the number of term rules.

For each elementary pattern of variations defined in the previous section, we calculated the precision of meta-rules which include the pattern. The result of artificial intelligence is shown in Table 7, and that of architecture is shown in Table 8.

\begin{table*}[htb]
\begin{center}
\begin{tabular}{l|ccc|ccc}
\hline
& \multicolumn{3}{|c}{token} & \multicolumn{3}{|c}{type}\\
\hline
& correct & incorrect & precision & correct & incorrect & precision\\
\hline
\hline
A & 152 & 24 & 86.4\,\% & 68 & 13 & 84.0\,\%\\
B & 44 & 3 & 93.6\,\% & 31 & 2 & 93.9\,\%\\
C & 196 & 90 & 68.5\,\% & 97 & 38 & 71.9\,\%\\
D & 557 & 143 & 79.6\,\% & 254 & 77 & 76.7\,\%\\
E & 431 & 74 & 85.4\,\% & 216 & 61 & 78.0\,\%\\
F & 32 & 2 & 94.1\,\% & 29 & 2 & 93.6\,\%\\
G & 108 & 161 & 40.2\,\% & 39 & 80 & 32.8\,\%\\
\hline
\end{tabular}

\end{center}
\caption{Number of variants extracted by \\ the elementary patterns (Artificial Intelligence)}
\end{table*}

\begin{table*}[htb]
\begin{center}
\begin{tabular}{l|ccc|ccc}
\hline
& \multicolumn{3}{|c}{token} & \multicolumn{3}{|c}{type}\\
\hline
& correct & incorrect & precision & correct & incorrect & precision\\
\hline
\hline
A & 104 & 24 & 81.3\,\% & 27 & 15 & 64.3\,\%\\
B & 9 & 1 & 90.0\,\% & 9 & 1 & 90.0\,\%\\
C & 30 & 4 & 88.2\,\% & 18 & 3 & 85.7\,\%\\
D & 184 & 50 & 78.6\,\% & 123 & 32 & 79.4\,\%\\
E & 482 & 67 & 87.8\,\% & 270 & 48 & 84.9\,\%\\
F & 9 & 2 & 81.8\,\% & 5 & 2 & 71.4\,\%\\
G & 194 & 52 & 78.9\,\% & 49 & 17 & 74.2\,\%\\
\hline
\end{tabular}

\end{center}
\caption{Number of variants extracted by the elementary patterns
(Architecture)}
\end{table*}

Most of the patterns achieved over 70\,\% precision. But the precision of pattern G (permutation) is noticeably low in artificial intelligence. It is about 40\,\% for the token count and about 30\,\% for the type count. In architecture, the precision of pattern G is not so low.

It was found that different meta-rules perform differently. So we must consider the selection of meta-rules which satisfy the demands of applications. The performance of pattern G is especially low. The 13 meta-rules listed as Nos. 4 (A+D+E+G), 6 (A+E+G), 10 (C+D+E+G), 12 (C+E+G), 15 (D+E+G) and 18 (E+G) in Table 4 include pattern G. For them, we must select appropriate meta-rules with care, especially in artificial intelligence. Tables 9 and 10 show the number of variants extracted by combinations of pattern G and others (A-F), and the precision of those combinations. From the tables, it is observed that pattern G achieves relatively high performance when it is combined with pattern C (noun-verb variations). If high performance is required concerning pattern G, we must select meta-rules which include not only pattern G but also pattern C, i.e. Nos. 10 (C+D+E+G) and 12 (C+E+G) in Table 4.

\begin{table*}[htb]
\begin{center}
\begin{tabular}{l|ccc|ccc}
\hline
& \multicolumn{3}{|c}{token} & \multicolumn{3}{|c}{type}\\
\hline
& correct & incorrect & precision & correct & incorrect & precision\\
\hline
\hline
A & 1 & 3 & 25.0\,\% & 1 & 3 & 25.0\,\%\\
B & 0 & 0 & - & 0 & 0 & -\\
C & 38 & 47 & 44.7\,\% & 26 & 30 & 46.4\,\%\\
D & 15 & 57 & 20.8\,\% & 9 & 36 & 20.0\,\%\\
E & 6 & 34 & 15.0\,\% & 5 & 27 & 15.6\,\%\\
F & 0 & 0 & - & 0 & 0 & -\\
\hline
\end{tabular}

\end{center}
\caption{Number of variants extracted by \\ combinations of G and other patterns (Artificial Intelligence)}
\end{table*}

\begin{table*}[htb]
\begin{center}
\begin{tabular}{l|ccc|ccc}
\hline
& \multicolumn{3}{|c}{token} & \multicolumn{3}{|c}{type}\\
\hline
& correct & incorrect & precision & correct & incorrect & precision\\
\hline
\hline
A & 6 & 0 & 100\,\% & 1 & 0 & 100\,\%\\
B & 0 & 0 & - & 0 & 0 & -\\
C & 19 & 2 & 90.5\,\% & 7 & 2 & 77.8\,\%\\
D & 51 & 11 & 82.3\,\% & 26 & 9 & 74.3\,\%\\
E & 15 & 7 & 68.2\,\% & 13 & 5 & 72.2\,\%\\
F & 0 & 0 & - & 0 & 0 & -\\
\hline
\end{tabular}

\end{center}
\caption{Number of variants extracted by \\ combinations of G and other patterns (Architecture)}
\end{table*}

\subsubsection{Diagnosis}

As mentioned above, the recall of our meta-rules is about 80\,\%. Because of errors in the morphological analysis and the deficiency in the definition of meta-rules which deal with terms consisting of three or more elements, some variants in the corpora could not be extracted. The total precision of our meta-rules is also about 80\,\%. That is, 20\,\% of variants extracted by the meta-rules are incorrect. Those errors can be classified into four types.

\vspace{5mm}
\noindent
(i) The first type results from errors in the morphological analysis. For example, the morphological analyzer Chasen judges some nouns as suffixes ($TPN$, $TPNS$, $TPNA$), and rule A (noun-noun variation) is incorrectly applied.

\vspace{5mm}
\noindent
合成構造({\it gousei[NS] kouzou[N]}) $\rightarrow$? 合成ばり構造({\it gousei[NS] bari[TPN] kouzou[N]})

\vspace{5mm}
\noindent
In this example, Fastr extracts 合成構造 as a variant of 合成ばり構造 in accordance with Chasen, which judges ばり as a suffix. But ばり is an ambiguous word, and in this case it is not a suffix (meaning ``tension") but a noun (meaning ``beam"). Therefore, this example is incorrect as a variation of the kind rule A assumes. This type of error lowered the precision of rule A, especially in architecture.

This type of error is not an essential problem of meta-rules. It is caused by errors in the morphological analysis (dividing terms into morphemes, judging their parts-of-speech). In this experiment, we used Chasen as a morphological analyzer. However, we have also verified that JUMAN allows for similar errors. If we can grasp the characteristic errors of a morphological analyzer and describe meta-rules considering them, then precision will be improved in practical use.

\vspace{5mm}
\noindent
(ii) The second type of error results from incorrect extraction of the modification structure. Most of the incorrect variants extracted by rule F (coordination) were caused by this type of error.

\vspace{5mm}
\noindent
構成制御({\it kousei[NS] seigyo[NS]}) $\rightarrow$? [記憶の]\footnote{Actually, the string inside [ ] is not extracted.}構成および対話の制御([{\it kioku[NS] no[S]}] {\it kousei[NS] oyobi[SC] taiwa[NS] no[S] seigyo[NS]})

\vspace{5mm}
\noindent
For example, [記憶の]構成および対話の制御 (organization [of memory] and control of dialog) may not be a correct variant of 構成制御 (organization control) because 構成 in the variant does not modify 制御. 構成 in the variant modifies an external element, 記憶, which is not described in meta-rules. But Fastr cannot recognize such modification because it only scans the patterns described in meta-rules. Thus, Fastr sometimes extracts a modification structure incorrectly. This is related to the limitation of the formal mechanism of Fastr, i.e. it cannot take into account the full structure of phrases or sentences.

This type of error might be avoided by extracting candidates for variants, e.g. correct noun phrases, through the syntactic analysis of textual data beforehand. By processing only candidates by Fastr, we will be able to improve precision in extraction.

\vspace{5mm}
\noindent
(iii) The third type of error involves extracting an incorrect variant whose relationship between constituent elements is different from that of the original term. This type of error typically occurs when rule E (modification) or G (permutation) is applied. As for rule G, most of the errors are of this type.

\vspace{5mm}
\noindent
時間推論({\it jikan[N] suiron[NS]}) $\rightarrow$? 推論時間({\it suiron[NS] jikan[N]})

\vspace{5mm}
\noindent
The original term in this example implies ``inference concerning time", while the variant implies ``time required for inferring". Since the identity of the concept is not kept in this example, this variant is incorrect. In artificial intelligence, the same errors were very often observed, which lowered the precision of rule G.

This type of error may be avoided by refining the meta-rules. As for rule G (permutation), as mentioned previously, it produces relatively high performance when it is combined with rule C (noun-verb variation). In this experiment, moreover, we found that the performance of the G-C combination is further improved when a passive auxiliary verb follows the verb in the variant or the original term. By putting such constraints on meta-rules, we are able to refine them. However, it is difficult to completely distinguish whether the `semantic' relationship between constituent elements changes or not by such `syntactic' constraints.

\vspace{5mm}
\noindent
(iv) The fourth type of error involves an extracted variant that does not keep the technical sense represented by the original term although the relationship between the constituent elements of the original term is kept in the variant.

\vspace{5mm}
\noindent
因子分析({\it inshi[N] bunseki[NS]}) $\rightarrow$? [汚染]因子の分析([{\it osen[NS]}] {\it inshi[N] no[S] bunseki[NS]})

\vspace{5mm}
\noindent
For example, [汚染]因子の分析 (analysis of the [pollution] factor) may not be a correct variant of 因子分析 (the factor analysis). Because the sense of a kind of multivariate analysis, which the original term implies, is lost through decompounding, although the relationship between ``factor" and ``analysis", i.e. ``the factor is analyzed", is maintained. When it comes to terms which have a special sense as an idiom, the original sense is often lost through decompounding into a noun phrase, as in this example.

This type of error is hardly avoided in the Fastr framework. This is the limitation of a framework for term variant detection which is only based on syntactic and morphological rules.

Table 11 shows the number of variants containing each type of error. The ratio of variants containing each type of error to all the incorrect variants is also shown in the table\footnote{Because some variants contain more than one type of error, the sum of the ratios exceeds 100\,\%.}.

\begin{table*}[htb]
\begin{center}
\begin{tabular}{l|cccc}
\hline
& \multicolumn{4}{|c}{token}\\
& (i) & (ii) & (iii) & (iv)\\
\hline
\hline
Artificial Intelligence & 3 & 51 & 168 & 81\\
& 1.05\,\% & 17.8\,\% & 58.5\,\% & 28.2\,\%\\
\hline
Architecture & 25 & 70 & 41 & 21\\
& 16.5\,\% & 46.1\% & 27.0\,\% & 13.8\,\%\\
\hline
\hline
& \multicolumn{4}{|c}{type}\\
& (i) & (ii) & (iii) & (iv)\\
\hline
\hline
Artificial Intelligence & 3 & 46 & 87 & 20\\
& 2.05\,\% & 31.5\,\% & 56.0\,\% & 13.7\,\%\\
\hline
Architecture & 16 & 26 & 26 & 19\\
& 19.3\,\% & 31.3\,\% & 31.3\,\% & 22.9\,\%\\
\hline
\end{tabular}

\end{center}
\caption{Number of incorrect variants containing each type of error}
\end{table*}

\subsection{Usefulness in Indexing}

We also evaluate the results of our experiments from the viewpoint of the application to the automatic indexing of documents.

Regarding the experiments of term extraction as automatic indexing over a controlled vocabulary, we have examined to what extent the exploitation of variant conflation increases the coverage of indexing. Here we use the variants which were successfully extracted by Fastr with our meta-rules.

As mentioned above, the number of artificial intelligence terms is 3,245 and that of architecture is 6,519, and the number of artificial intelligence documents is 2,031 and that of architecture is 2,000. For each term defined in the term rules, we counted the documents where its variants appear but the original term does not. The averages are shown in the column AP/DT of Table 12. Put simply, if we use variants of terms in indexing documents, we can find, on average, about four (artificial intelligence) or three (architecture) additional documents per term in the corpus\footnote{Here we only consider the terms whose variants appear in the corpus at least once.}.

As for the terms which appear in the corpus in both their original and variant forms, moreover, we calculated the ratio of the documents indexed by variants but not by the original terms to the documents indexed by the original terms. The averages are shown in the column RAP of Table 12. The results indicate that the number of indexed documents increases by 77\,\% (artificial intelligence) or 54\,\% (architecture) if we take variant forms into consideration.

Applying the framework of term variant detection to IR, we can expect an improvement in recall.

\begin{table}[htb]
\begin{center}
\begin{tabular}{rl|c|cc|c}
\hline
& & DT & AP & AP/DT & RAP\\
\hline
\hline
Artificial Intelligence & Type 1 terms & 135 & 653 & 4.84 & 0.77\\
& Type 2 terms & 89 & 184 & 2.07 & --\\
\hline
& Total & 224 & 837 & 3.74 & --\\
\hline
\hline
Architecture & Type 1 terms & 160 & 534 & 3.34 & 0.54\\
& Type 2 terms & 68 & 99 & 1.46 & --\\
\hline
& Total & 228 & 633 & 2.78 & --\\
\hline
\end{tabular}
\end{center}
{\footnotesize
Type 1 terms: both the original form and its variants appear in the corpus.\\
Type 2 terms: only the variants appear in the corpus.\\
DT: number of different terms.\\
AP: total number of additional documents retrieved by variants, where only variants appear.\\
RAP: average of the ratio of the number of additional documents retrieved by variants to the number of documents retrieved by original terms.
}
\caption{Additional documents retrieved by variants}
\end{table}

\section{Discussion}

In this paper, we have shown that the framework of Fastr can be successfully applied to Japanese term variant detection in the two domains, artificial intelligence and architecture. The meta-rules defined in this paper were able to detect variations of technical terms in both the domains with about 80\,\% precision. And, taking the detected variants into consideration, we could increase the coverage in indexing documents. Thus, we can expect an improvement in recall without the sacrifice of precision in IR, if the framework is properly applied.

As for the precision of individual meta-rules, we found some differences between the two domains. The most remarkable difference was observed in the meta-rules incorporating ``permutation". As for those meta-rules, their performance in artificial intelligence was very poor when compared to their performance in architecture. The two domains also differed in the ratio of variants to their original forms extracted from the corpus, and this is the reason for the difference in performance when we used the extracted variants in indexing documents. Using other data, we should examine whether these differences are due to data (dictionaries and corpora) or domain characteristics.

Lastly, we summarize the peculiarity of Japanese. In the Japanese language, as we previously mentioned, there is no delimiting symbol which explicitly divides words. Thus, we cannot clearly distinguish morpho-syntactic variations, which are transformations inside a word, from syntactic variations. However, so far as we define meta-rules practically, we can describe morpho-syntactic variations---as well as syntactic ones---as transformations of the phrase structure by means of syntactic operations, regarding morphemes instead of words as syntactic units. In this paper, we have defined all meta-rules through syntactic operations because the morphological analyzer Chasen, which we depended on, treats not only suffixes but also inflections of {\it na}-adjectives as independent morphemes.

However, it does not necessarily arrive at a consensus to regard a suffix or an inflection as an independent unit. For example, as mentioned above, JUMAN, another morphological analyzer, does not divide a {\it na}-adjective into its stem and inflection but treats a {\it na}-adjective whole as one morpheme. Therefore, morphological operations are necessary for describing meta-rules of noun-adjective variations if we depend on the scheme of JUMAN \cite{Yoshikane}.

There is reasonable separation between syntactic and morpho-syntactic variations in English and French, unlike in the Japanese language. However, in a framework where words would first go through a morphological analyzer that decomposes them into elementary morphemes, the analysis of variations in English and French could be made in the same way as in Japanese. It would provide a more unified framework and perhaps a better integration of syntax and morphology.


\bibliographystyle{nlpbbl}
\nocite{Shapiro}
\bibliography{citevariation}




\appendix

{\scriptsize
\begin{verbatim}
"0001 (E) 開発環境 → 開発支援環境"
Metarule ModiInsN( X1 → X2 X3 ) = X1 → X2 ＜｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ X3:
     ＜X3 cat＞ = N｜NS｜NA

"0002 (E) 関数の拡張 → 関数の自然な拡張"
Metarule ModiInsA( X1 → X2 S3 X4 ) = X1 → X2 S5 ＜｛A｜NA MD｜X TPNA MD｜X TPA｝? ｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ X4:
     ＜S3 lem＞ = 'の'
     ＜S5 lem＞ = 'の'
     ＜X2 cat＞ = N｜NS｜NA
     ＜X4 cat＞ = N｜NS｜NA

"0003 (E) 状態の変数 → 状態を表す変数"
Metarule ModiInsV( X1 → X2 S3 X4 ) = X1 → X2 S5 ＜｛A｜NA S｜X TPNA S｜X TPA｜RB｝? ｛V｜NS VS｜X TPNS VS｝TPV? MD?｛A｜NA MD｜X TPNA MD｜X TPA｝? ｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ X4:
     ＜S3 lem＞ = 'の'
     ＜X2 cat＞ = N｜NS｜NA
     ＜X4 cat＞ = N｜NS｜NA

"0004 (DE) 内部情報 → 内部に持つ情報"
Metarule DecoInsV( X1 → X2 X3 ) = X1 → X2 S4 ＜｛A｜NA S｜X TPNA S｜X TPA｜RB｝? ｛V｜NS VS｜X TPNS VS｝TPV? MD?｛A｜NA MD｜X TPNA MD｜X TPA｝? ｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ X3:
     ＜X2 cat＞ = N｜NS｜NA
     ＜X3 cat＞ = N｜NS｜NA

"0005 (DE) フレームワーク → フレーム・ワーク"
Metarule DecoInsL( X1 → X2 X3 ) = X1 → X2 L4 ＜＜｛N｜NS｜NA｝L＞＊＞ X3:
     ＜L4 lem＞ = '・'
     ＜X2 cat＞ = N｜NS｜NA
     ＜X3 cat＞ = N｜NS｜NA

"0006 (DE) 関数計算 → 関数の計算"
Metarule DecoInsS( X1 → X2 X3 ) = X1 → X2 S4 ＜｛A｜NA MD｜X TPNA MD｜X TPA｝? ｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ X3:
     ＜S4 lem＞ = 'の'
     ＜X2 cat＞ = N｜NS｜NA
     ＜X3 cat＞ = N｜NS｜NA

"0007 (DEG) 追加資料 → 資料の追加"
Metarule DecoInsS( X1 → NS2 X3 ) = X1 → X3 S4 ＜｛A｜NA MD｜X TPNA MD｜X TPA｝? ｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ NS2:
     ＜S4 lem＞ = 'の'
     ＜X3 cat＞ = N｜NS｜NA

"0008 (DE) 通信多重化 → 通信の多重化"
Metarule DecoInsS( X1 → X2 X3 X4 ) = X1 → X2 S5 ＜｛A｜NA MD｜X TPNA MD｜X TPA｝? ｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ X3 X4:
     ＜S5 lem＞ = 'の'
     ＜X2 cat＞ = N｜NS｜NA
     ＜X3 cat＞ = N｜NS｜NA
     ＜X4 cat＞ = TPN｜TPNS

"0009 (ADEG) 共有化メモリ → メモリの共有"
Metarule DecoInsS( X1 → X2 TPNS3 X4 ) = X1 → X4 S5 ＜｛A｜NA MD｜X TPNA MD｜X TPA｝? ｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ X2:
     ＜TPNS3 lem＞ = '化'
     ＜S5 lem＞ = 'の'
     ＜X4 cat＞ = N｜NS｜NA

"0010 (DE) 意味解析ルーチン → 意味解析のルーチン"
Metarule DecoInsS( X1 → X2 X3 X4 ) = X1 → X2 ＜｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ X3 S5 ＜｛A｜NA MD｜X TPNA MD｜X TPA｝? ｛N｜NS｜NA｜X ｛TPN｜TPNS｜TPNA｝｝＊＞ X4:
     ＜S5 lem＞ = 'の'
     ＜X3 cat＞ = N｜NS｜NA
     ＜X4 cat＞ = N｜NS｜NA

"0011 (DE) 計算機制御プログラム → 計算機の制御プログラム"
Metarule DecoInsS( X1 → X2 X3 X4 ) = X1 → X2 S5 ＜｛A｜NA MD｜X TPNA MD｜X TPA｝? ｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ X3 ＜｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ X4:
     ＜S5 lem＞ = 'の'
     ＜X2 cat＞ = N｜NS｜NA
     ＜X3 cat＞ = N｜NS｜NA
     ＜X4 cat＞ = N｜NS｜NA

"0012 (DE) フレーム・ワーク → フレームワーク"
Metarule CompOmiL( X1 → X2 L3 X4 ) = X1 → X2 ＜｛N｜NS｜NA｝＊＞ X4:
     ＜L3 lem＞ = '・'
     ＜X2 cat＞ = N｜NS｜NA
     ＜X4 cat＞ = N｜NS｜NA

"0013 (DE) 対象のモデル → 対象モデル"
Metarule CompOmiS( X1 → X2 S3 X4 ) = X1 → X2 ＜｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ X4:
     ＜S3 lem＞ = 'の'
     ＜X2 cat＞ = N｜NS｜NA
     ＜X4 cat＞ = N｜NS｜NA

"0014 (DEG) 資料の追加 → 追加資料"
Metarule CompOmiS( X1 → X2 S3 NS4 ) = X1 → NS4 ＜｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ X2:
     ＜S3 lem＞ = 'の'
     ＜X2 cat＞ = N｜NS｜NA

"0015 (DE) 計算機の制御プログラム → 計算機制御プログラム"
Metarule CompOmiS( X1 → X2 S3 X4 X5 ) = X1 → X2 ＜｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ X4 ＜｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ X5:
     ＜S3 lem＞ = 'の'
     ＜X2 cat＞ = N｜NS｜NA
     ＜X4 cat＞ = N｜NS｜NA
     ＜X5 cat＞ = N｜NS｜NA

"0016 (DE) 意味解析のルーチン → 意味解析ルーチン"
Metarule CompOmiS( X1 → X2 X3 S4 X5 ) = X1 → X2 ＜｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ X3 ＜｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ X5:
     ＜S4 lem＞ = 'の'
     ＜X2 cat＞ = N｜NS｜NA
     ＜X3 cat＞ = N｜NS｜NA
     ＜X5 cat＞ = N｜NS｜NA

"0017 (EF) 分解原理 → 分解・結合原理"
Metarule CoorArgL( X1 → X2 X3 ) = X1 → X2 L4 ＜｛N｜NS｜NA｝＜S｛A｜NA MD｜X TPNA MD｜X TPA｝? ｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞?＞ X3:
     ＜L4 lem＞ = '・'
     ＜X2 cat＞ = N｜NS｜NA
     ＜X3 cat＞ = N｜NS｜NA

"0018 (DEF) 学習制御 → 学習および対話の制御"
Metarule CoorArgSC( X1 → X2 X3 ) = X1 → X2 SC4 ＜｛N｜NS｝＞ S5 ＜｛A｜NA MD｜X TPNA MD｜X TPA｝? ｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ X3:
     ＜S5 lem＞ = 'の'
     ＜X2 cat＞ = N｜NS
     ＜X3 cat＞ = N｜NS｜NA

"0019 (EF) 曖昧な情報 → 曖昧あるいは複雑な情報"
Metarule CoorArgSC( X1 → NA2 MD3 X4 ) = X1 → NA2 SC5 ＜｛NA｜X TPNA｝＞ MD3 ＜｛A｜NA MD｜X TPNA MD｜X TPA｝? ｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ X4:
     ＜X4 cat＞ = N｜NS｜NA

"0020 (EF) 質的な分析 → 質的あるいは量的な分析"
Metarule CoorArgSC( X1 → X2 TPNA3 MD4 X5 ) = X1 → X2 TPNA3 SC6 ＜｛NA｜X TPNA｝＞ MD4 ＜｛A｜NA MD｜X TPNA MD｜X TPA｝? ｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ X5:
     ＜X5 cat＞ = N｜NS｜NA

"0021 (EF) 質的分析 → 質的あるいは量的分析"
Metarule CoorArgSC( X1 → X2 TPNA3 X4 ) = X1 → X2 TPNA5 SC6 ＜X TPNA MD?｛A｜NA MD｜X TPNA MD｜X TPA｝? ｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ X4:
     ＜TPNA3 lem＞ = '的'
     ＜TPNA5 lem＞ = '的'
     ＜X4 cat＞ = N｜NS｜NA

"0022 (DF) 知識獲得 → 知識の生成や獲得"
Metarule CoorHead( X1 → X2 X3 ) = X1 → X2 S4 ＜｛N｜NS｜NA｝｛SC｜L｝＞ X3:
     ＜S4 lem＞ = 'の'
     ＜X2 cat＞ = N｜NS｜NA
     ＜X3 cat＞ = N｜NS｜NA

"0023 (F) 知識の獲得 → 知識の生成や獲得"
Metarule CoorHead( X1 → X2 S3 X4 ) = X1 → X2 S5 ＜｛N｜NS｜NA｝｛SC｜L｝＞ X4:
     ＜S3 lem＞ = 'の'
     ＜S5 lem＞ = 'の'
     ＜X2 cat＞ = N｜NS｜NA
     ＜X4 cat＞ = N｜NS｜NA

"0024 (F) 狭い概念 → 狭い意味と概念"
Metarule CoorHead( X1 → A2 X3 ) = X1 → A2 ＜｛N｜NS｜NA｝｛SC｜L｝＞ X3:
     ＜X3 cat＞ = N｜NS｜NA

"0025 (F) 曖昧な情報 → 曖昧な環境と情報"
Metarule CoorHead( X1 → NA2 MD3 X4 ) = X1 → NA2 MD3 ＜｛N｜NS｜NA｝｛SC｜L｝＞ X4:
     ＜X4 cat＞ = N｜NS｜NA

"0026 (F) 帰納的な推論 → 帰納的な学習と推論"
Metarule CoorHead( X1 → X2 TPNA3 MD4 X5 ) = X1 → X2 TPNA3 MD4 ＜｛N｜NS｜NA｝｛SC｜L｝＞ X5:
     ＜X5 cat＞ = N｜NS｜NA

"0027 (F) 生じやすい障害 → 生じやすい故障・障害"
Metarule CoorHead( X1 → X2 TPA3 X4 ) = X1 → X2 TPA3 ＜｛N｜NS｜NA｝｛SC｜L｝＞ X4:
     ＜X4 cat＞ = N｜NS｜NA

"0028 (F) 比較する → 比較・検討する"
Metarule CoorNS( X1 → NS2 VS3 ) = X1 → NS2 L4 NS5 VS3:

"0029 (F) 高度化する → 高度化・複雑化する"
Metarule CoorNS( X1 → X2 TPNS3 VS4 ) = X1 → X2 TPNS3 L5 X6 TPNS7 VS4:
     ＜TPNS3 lem＞ = ＜TPNS7 lem＞

"0030 (CDE) 概念学習 → 概念を帰納的に学習する"
Metarule NSVSV( X1 → X2 NS3 ) = X1 → X2 S4 ＜｛A｜NA S｜X TPNA S｜X TPA｜RB｝?＞ NS3 VS5:
     ＜S4 lem＞ = 'を'
     ＜X2 cat＞ = N｜NS｜NA

"0031 (CE) 概念の学習 → 概念を帰納的に学習する"
Metarule NSVSV( X1 → X2 S3 NS4 ) = X1 → X2 S5 ＜｛A｜NA S｜X TPNA S｜X TPA｜RB｝?＞ NS4 VS6:
     ＜S3 lem＞ = 'の'
     ＜S5 lem＞ = 'を'
     ＜X2 cat＞ = N｜NS｜NA

"0032 (CE) 概念の階層化 → 概念を帰納的に階層化する"
Metarule NSVSV( X1 → X2 S3 X4 TPNS5 ) = X1 → X2 S6 ＜｛A｜NA S｜X TPNA S｜X TPA｜RB｝?＞ X4 TPNS5 VS7:
     ＜S3 lem＞ = 'の'
     ＜S6 lem＞ = 'を'
     ＜X2 cat＞ = N｜NS｜NA

"0033 (C) 深い探索 → 深く探索する"
Metarule NSVSV( X1 → A2 NS3 ) = X1 → A2 NS3 VS4:

"0034 (C) 分かりやすい表現 → 分かりやすく表現する"
Metarule NSVSV( X1 → X2 TPA3 NS4 ) = X1 → X2 TPA3 NS4 VS5:

"0035 (ABC) 帰納推論 → 帰納的に推論する"
Metarule NSVSV( X1 → X2 NS3 ) = X1 → X2 TPNA4 S5 NS3 VS6:

"0036 (BC) 帰納的推論 → 帰納的に推論する"
Metarule NSVSV( X1 → X2 TPNA3 NS4 ) = X1 → X2 TPNA3 S5 NS4 VS6:

"0037 (BC) 曖昧検索 → 曖昧に検索する"
Metarule NSVSV( X1 → NA2 NS3 ) = X1 → NA2 S4 NS3 VS5:

"0038 (CEG) システム実装 → 実装されたシステム"
Metarule NSVSV( X1 → X2 NS3 ) = X1 → NS3 VS4 ＜TPV? MD?｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ X2:
     ＜X2 cat＞ = N｜NS｜NA

"0039 (CE) 実装システム → 実装されたシステム"
Metarule NSVSV( X1 → NS2 X3 ) = X1 → NS2 VS4 ＜TPV? MD?｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ X3:
     ＜X3 cat＞ = N｜NS｜NA

"0040 (CEG) 実装するシステム → システム実装"
Metarule NSVSN( X1 → NS2 VS3 X4 ) = X1 → X4 ＜｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ NS2:
     ＜X4 cat＞ = N｜NS｜NA

"0041 (CEG) 実装されるシステム → システム実装"
Metarule NSVSN( X1 → NS2 VS3 TPV4 X5 ) = X1 → X5 ＜｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ NS2:
     ＜X5 cat＞ = N｜NS｜NA

"0042 (CEG) 実装したシステム → システム実装"
Metarule NSVSN( X1 → NS2 VS3 MD4 X5 ) = X1 → X5 ＜｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ NS2:
     ＜X5 cat＞ = N｜NS｜NA

"0043 (CE) 実装するシステム → 実装システム"
Metarule NSVSN( X1 → NS2 VS3 X4 ) = X1 → NS2 ＜｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ X4:
     ＜X4 cat＞ = N｜NS｜NA

"0044 (CE) 実装されるシステム → 実装システム"
Metarule NSVSN( X1 → NS2 VS3 TPV4 X5 ) = X1 → NS2 ＜｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ X5:
     ＜X5 cat＞ = N｜NS｜NA

"0045 (CE) 実装したシステム → 実装システム"
Metarule NSVSN( X1 → NS2 VS3 MD4 X5 ) = X1 → NS2 ＜｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ X5:
     ＜X5 cat＞ = N｜NS｜NA

"0046 (CDEG) 実装システム → システムを実装する"
Metarule NSVSV( X1 → NS2 X3 ) = X1 → X3 S4 ＜｛A｜NA S｜X TPNA S｜X TPA｜RB｝?＞ NS2 VS5:
     ＜S4 lem＞ = 'を'
     ＜X3 cat＞ = N｜NS｜NA

"0047 (CDE) 階層化概念 → 概念を階層化する"
Metarule NSVSV( X1 → X2 TPNS3 X4 ) = X1 → X4 S5 ＜｛A｜NA S｜X TPNA S｜X TPA｜RB｝?＞ X2 TPNS3 VS6:
     ＜S5 lem＞ = 'を'
     ＜X4 cat＞ = N｜NS｜NA

"0048 (CDE) 情報検索 → 情報が検索される"
Metarule NSVSV( X1 → X2 NS3 ) = X1 → X2 S4 ＜｛A｜NA S｜X TPNA S｜X TPA｜RB｝?＞ NS3 VS5 TPV6:
     ＜S4 lem＞ = 'が'
     ＜X2 cat＞ = N｜NS｜NA

"0049 (AE) 幾何モデル → 幾何的モデル"
Metarule NtoN( X1 → X2 X3 ) = X1 → X2 X4 ＜｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ X3:
     ＜X3 cat＞ = N｜NS｜NA
     ＜X4 cat＞ = TPN｜TPNS｜TPNA

"0050 (AE) 幾何的モデル → 幾何モデル"
Metarule NtoN( X1 → X2 TPNA3 X4 ) = X1 → X2 ＜｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ X4:
     ＜TPNA3 lem＞ = '的'
     ＜X4 cat＞ = N｜NS｜NA

"0051 (ACDE) 概念階層 → 概念を階層化する"
Metarule NtoN( X1 → X2 X3 ) = X1 → X2 S4 ＜｛A｜NA S｜X TPNA S｜X TPA｜RB｝?＞ X3 TPNS5 VS6:
     ＜S4 lem＞ = 'を'
     ＜X2 cat＞ = N｜NS｜NA
     ＜X3 cat＞ = N｜NS｜NA

"0052 (ACDE) 概念階層 → 階層化された概念"
Metarule NtoN( X1 → X2 X3 ) = X1 → X3 TPNS4 VS5 ＜TPV? MD?｛A｜NA MD｜X TPNA MD｜X TPA｝? ｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ X2:
     ＜X2 cat＞ = N｜NS｜NA
     ＜X3 cat＞ = N｜NS｜NA

"0053 (AE) 共有化メモリ → 共有メモリ"
Metarule NtoN( X1 → X2 TPNS3 X4 ) = X1 → X2 ＜｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ X4:
     ＜TPNS3 lem＞ = '化'
     ＜X4 cat＞ = N｜NS｜NA

"0054 (AEG) 共有化メモリ → メモリ共有"
Metarule NtoN( X1 → X2 TPNS3 X4 ) = X1 → X4 ＜｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ X2:
     ＜TPNS3 lem＞ = '化'
     ＜X4 cat＞ = N｜NS｜NA

"0055 (BE) 曖昧情報 → 曖昧な情報"
Metarule NtoAMD( X1 → NA2 X3 ) = X1 → NA2 MD4 ＜｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ X3:
     ＜X3 cat＞ = N｜NS｜NA

"0056 (BE) 幾何的表現 → 幾何的な表現"
Metarule NtoAMD( X1 → X2 TPNA3 X4 ) = X1 → X2 TPNA3 MD5 ＜｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ X4:
     ＜X4 cat＞ = N｜NS｜NA

"0057 (ABE) 幾何表現 → 幾何的な表現"
Metarule NtoAMD( X1 → X2 X3 ) = X1 → X2 TPNA4 MD5 ＜｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ X3:
     ＜X2 cat＞ = N｜NS｜NA
     ＜X3 cat＞ = N｜NS｜NA

"0058 (ABE) 幾何的な表現 → 幾何表現"
Metarule AtoNMD( X1 → X2 TPNA3 MD4 X5 ) = X1 → X2 ＜｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ X5:
     ＜TPNA3 lem＞ = '的'
     ＜X5 cat＞ = N｜NS｜NA

"0059 (ABE) 幾何的な表現 → 幾何的表現"
Metarule AtoNMD( X1 → X2 TPNA3 MD4 X5 ) = X1 → X2 TPNA6 ＜｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ X5:
     ＜TPNA3 lem＞ = '的'
     ＜TPNA6 lem＞ = '的'
     ＜X5 cat＞ = N｜NS｜NA

"0060 (ABE) 複雑な情報 → 複雑化した情報"
Metarule AtoNMD( X1 → NA2 MD3 X4 ) = X1 → NA2 TPNS5 ＜TPV? MD?｛A｜NA MD｜X TPNA MD｜X TPA｝? ｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ X4:
     ＜X4 cat＞ = N｜NS｜NA

"0061 (EG) 共有メモリ → メモリ共有"
Metarule Perm( X1 → NS2 X3 ) = X1 → X3 ＜｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ NS2:
     ＜X3 cat＞ = N｜NS｜NA

"0062 (EG) メモリ共有 → 共有メモリ"
Metarule Perm( X1 → X2 NS3 ) = X1 → NS3 ＜｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ X2:
     ＜X2 cat＞ = N｜NS｜NA

"0063 (EG) 共有化メモリ → メモリ共有化"
Metarule Perm( X1 → X2 TPNS3 X4 ) = X1 → X4 ＜｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ X2 TPNS3:
     ＜X4 cat＞ = N｜NS｜NA

"0064 (EG) メモリ共有化 → 共有化メモリ"
Metarule Perm( X1 → X2 X3 TPNS4 ) = X1 → X3 TPNS4 ＜｛N｜NS｜NA｜X｛TPN｜TPNS｜TPNA｝｝＊＞ X2:
     ＜X2 cat＞ = N｜NS｜NA
\end{verbatim}
}


\begin{biography}

\biotitle{}

\bioauthor{Fuyuki Yoshikane}
{
Fuyuki Yoshikane received the B.E. and M.E. degree in library and information science from University of Tokyo, Japan, in 1994 and 2000 respectively. After two years in Ph.D course of Graduate School of Education, University of Tokyo, since 2002, he has been a research fellow in National Institution for Academic Degrees, Tokyo, Japan.
}

\bioauthor{Keita Tsuji}
{
Keita Tsuji received the B.E. and M.E. degree in library and information science from University of Tokyo, Japan, in 1994 and 1997 respectively. After four years in Ph.D course of Graduate School of Education, University of Tokyo, since 2001, he has been a research associate in National Institute of Informatics, Tokyo, Japan.
}

\bioauthor{Kyo Kageura}
{
Kyo Kageura was born in 1964. He received his Ph.D. degree from the University of Manchester in 1993. He has been working at the Department of Research and Development, the National Center for Science Information Systems (NACSIS), Japan, since 1988. Since 2000, he works as an associate professor of the Human and Social Informatics Division, the National Institute of Informatics, Japan. His main research interest is in media studies. He is also carrying out research in qualitative and quantitative modeling of term formation and terminological growth and the logical foundation of the theory of terminology. He is a member of the Japan Society of Library and Information Science and the International Quantitative Linguistic Society.
}

\bioauthor{Christian Jacquemin}
{
Christian Jacquemin received his Ph.D degree in Computer Science at the University of Paris 7 in 1991 and his Habilitation Thesis in Computer Science at the University of Nantes in 1997. From 1991 to 1997, he was assistant professor at the University of Nantes and researcher at IRIN, Research Institute in Computer Sciences, Nantes. Since 1998 he is researcher at CNRS LIMSI, and since 2000, he is Professor of Computer Science at the University of Paris 11. He is co-editor of the Journal ``Natural Language Engineering", Oxford University Press, he published a book at MIT Press in 2001 on term extraction and co-edited a book at John Benjamins of computational term processing. His current interests include information visualization, virtual reality, and immersive environments.
}

\bioreceived{Received}
\biorevised{Revised}
\biorerevised{Rerevised}
\bioaccepted{Accepted}
\end{biography}


\end{document}
