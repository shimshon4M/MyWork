\documentstyle[epsf,jnlpbbl]{jnlp_j_b5}

\setcounter{page}{109}
\setcounter{巻数}{10}
\setcounter{号数}{1}
\setcounter{年}{2003}
\setcounter{月}{1}
\受付{2002}{3}{8}
\再受付{2002}{6}{10}
\再々受付{2002}{7}{24}
\採録{2002}{10}{4}

\setcounter{secnumdepth}{2}

\title{サポートベクトルマシンを用いた中国語解析実験}
\author{吉田 辰巳\affiref{TUT} \and 大竹 清敬\affiref{ATR} 
\and 山本 和英\affiref{ATR}\affiref{NUT}}

\headauthor{吉田，大竹，山本}
\headtitle{サポートベクトルマシンを用いた中国語解析実験}

\affilabel{TUT}{豊橋技術科学大学知識情報工学系}
{Department of Knowledge-based Information Engineering, Toyohashi
University of Technology}
\affilabel{ATR}{ATR音声言語コミュニケーション研究所}
{ATR Spoken Language Translation Research Laboratories}
\affilabel{NUT}{長岡技術科学大学電気系}
{Department of Electrical Engineering, Nagaoka University of
Technology}

\jabstract{
現在入手可能な解析器と言語資源を用いて中国語解析を行った場合にどの程
度の精度が得られるかを報告する．解析器としては，サポートベクトルマシン
(Support Vector Machine) を用いた YamCha を使用し，中国語構
文木コーパスとしては，最も一般的な Penn Chinese Treebank を使用した．
この両者を組み合わせて，形態素解析と基本句同定解析 (base phrase
chunking) の2種類の解析実験を行った．形態素解析実験の際には，
一般公開されている統計的モデルに基づく形態素解析器 MOZ との比較実験も行っ
た．この結果，YamChaによる形態素解析精度は約88\%でMOZよりも4\%以上高いが，
実用的には計算時間に問題があることが分かった．また基本句同定解析精度は約
93\%であった．
}

\jkeywords{中国語解析，サポートベクトルマシン，同定解析，YamCha，MOZ}

\etitle{Performance Evaluation of Chinese Analyzers\\
 with Support Vector Machines}

\eauthor{Tatsumi Yoshida\affiref{TUT} \and
Kiyonori Ohtake\affiref{ATR} \and
Kazuhide Yamamoto\affiref{ATR}\affiref{NUT}}

\eabstract{
We will report performances of currently and publicly available Chinese
analyzers and resources.  We use YamCha, a tool based on Support Vector
Machines, and the Penn Chinese Treebank as a language resource.
Combining these two, we measure the performances of Chinese analysis,
i.e., word segmentation, part-of-speech tagging, and base phrase
chunking.  In the experiment of word segmentation and part-of-speech
tagging, we also report the performance of MOZ, a statistical
morphological analyzer, which is also available to the public.  We found
that the accuracy of morphological analysis using YamCha attains around
88\%, which is over 4\% higher than that of MOZ, although it is
computationally very expensive.  We also found that the accuracy for
base phrase chunking is approximately 93\%.
}

\ekeywords{Chinese analysis, Support Vector Machine, Chunking, YamCha,
MOZ}

\begin{document}
\maketitle
\thispagestyle{empty}


\section{はじめに}
自然言語処理を進める上で，形態素解析器をはじめとする言語解析器は，コー
パスなどの言語資源と同様に最も重要な道具である．近年では，この重要性は
研究者間でほぼ認識されており，英語や日本語に対する形態素解析器と構文解
析器はいずれも複数のものが作成，そして公開または市販され，我々研究者は
その恩恵に預かっている．

ところが，中国語に関しては以上の状況は同じではない．我々の知る限り，日
本国内はもちろん，中国においても誰もが手軽に使える中国語解析器が研究者
の間で広範に知られている，という状況にはなく，まだ十分に解析器が整備さ
れているとは言えない．

この背景の一つは，中国語解析の困難性であると考える．中国語は英語のよう
に概ね単語ごとに分かち書きされてはおらず，単語分割が必要である．また，
文字種が単語分割のための大きな情報を持つ日本語とは異なり，ほぼ単一文字
種(漢字)である．さらに，複数品詞を持つ語が多いため品詞付与も容易ではな
い．たとえば，中国語の介詞(前置詞)のほとんどは動詞からの転成であるため日
本語や英語にはほとんど存在しない内容語と機能語との間で品詞付与の曖昧性
が生じる．
たとえば``\lower.25ex\hbox{\underline{\epsfxsize=1.1zw\epsfbox[0 10 99 90]{Chinese_Chars/dao.eps}}\epsfxsize=1.1zw\epsfbox[0 10 99
90]{Chinese_Chars/bei.eps}\epsfxsize=1.1zw\epsfbox[0 10 99
90]{Chinese_Chars/jing.eps}\epsfxsize=1.1zw\epsfbox[0 10 99
90]{Chinese_Chars/le.eps}}''（北京に着いた）の
``\lower.25ex\hbox{\epsfxsize=1.1zw\epsfbox[0 10 99 90]{Chinese_Chars/dao.eps}}''は動詞（到着する）であるが
``\underline{\lower.25ex\hbox{\epsfxsize=1.1zw\epsfbox[0 10 99 90]{Chinese_Chars/dao.eps}}}\lower.25ex\hbox{\epsfxsize=1.1zw\epsfbox[0
10 99 90]{Chinese_Chars/bei.eps}\epsfxsize=1.1zw\epsfbox[0 10 99 90]{Chinese_Chars/jing.eps}\epsfxsize=1.1zw\epsfbox[0 10 99
90]{Chinese_Chars/qu.eps}}''（北京に行く）の``\lower.25ex\hbox{\epsfxsize=1.1zw\epsfbox[0 10 99
90]{Chinese_Chars/dao.eps}}''は介詞（$\cdots$に）であり，すなわち
``\lower.25ex\hbox{\epsfxsize=1.1zw\epsfbox[0 10 99 90]{Chinese_Chars/dao.eps}\epsfxsize=1.1zw\epsfbox[0 10 99
90]{Chinese_Chars/bei.eps}\epsfxsize=1.1zw\epsfbox[0 10 99 90]{Chinese_Chars/jing.eps}}''だけでは
``\lower.25ex\hbox{\epsfxsize=1.1zw\epsfbox[0 10 99 90]{Chinese_Chars/dao.eps}}''の品詞は決定できない．また日本語における
「−する」(動詞)「−い」(形容詞)などの明確な文法標識を持たないため，内容語間の曖昧性も比較的多い．たとえば中国語の
``\lower.25ex\hbox{\epsfxsize=1.1zw\epsfbox[0 10 99 90]{Chinese_Chars/dan.eps}\epsfxsize=1.1zw\epsfbox[0 10 99 90]{Chinese_Chars/xin.eps}}''は日本語の「心配(名詞)/心配する(動詞)/心配だ(形容詞)」のすべてに相当する．

我々は現在，中日翻訳，並びに中国語換言処理の研究を行っている\cite{張
2002}．これらの処理は中国語が入力であるため，表層処理を行わない限り
中国語解析器が必要である．このため我々は，現在入手可能な解析器や言語資
源を組み合わせて中国語解析を行うことを試みた．ここで，中国語構文木コー
パスとしては，現在一般的な Penn Chinese Treebank (以下，CTBとする)
を使用した．
一方，解析器としてはサポートベクトルマシン (Support Vector Machine，以下，
SVM) に基づく YamCha を使用した．SVM ならびに
YamCha については\ref{節:YamCha}節でその概要を述べる．

本報告では，形態素解析と基本句同定解析 (base phrase chunking) の2種類を
行った．\ref{節:形態素解析}節で形態素解析について，\ref{節:基本句同
定解析}節で基本句同定解析について述べる．それぞれの解析で，学習文テス
トと未知文テストの2種類の解析精度を測定し，考察を行った．形態素解析
実験では，連接コスト最小法に基づく形態素解析器MOZを使用して，
解析精度の比較を行った．さらに，日本語と比較してどの程度中国語の
形態素解析が難しいのかを調べるために京都大学テキストコーパスを用いて実験
した．また，品詞タグ付けに限定すれば，CTBよりも大きなコーパスが入手可能
であることから，CTBの約11倍の大きさを持つ人民日報タグ付きコーパスを用い
ての形態素解析実験も行った．

本報告の主な目的は，上記の解析器と言語資源を用いて中国語解析器を構築し
た場合，どの程度の解析精度が得られるのかを報告することにある．すなわち，
この解析器にどのような問題がありどのような改善が可能かを提案するという
提供者の視点ではなく，使用者の視点，すなわち中国語処理に携わる研究者に
とってこの解析器がどの程度有用であり，使用の際にはどのような点に注意が
必要か，などを報告することに主眼がある．

いずれも容易に得られるツールと言語資源を組み合わせた場合にどのような精
度が得られるかを測定，報告することは誰にでもできる作業である．しかし，
研究者が研究の必要性のためできるだけ高精度の解析器を求める状況にある場
合，本報告のような報告によって解析の期待精度を予め知った上で同一の解析
器を構築できる．あるいは，研究上より高精度の解析器が必要な場合は最初か
ら別の選択肢を考えることもできる．このように，我々は中国語処理を行う
研究者への有益性を考え，我々で測定した解析精度を技術資料として報告する
ことにした．


\section{中国語解析のための言語資源と解析環境}

本節では，我々の実験で使用した言語資源と解析環境の概略を述べる．

\subsection{中国語構文木コーパス}
\label{節:CTB}

我々は，入手可能な中国語言語資源として，現在最も一般に知られていると考
えられる Penn Chinese Treebank (CTB)を使用した．CTBは，米
国ペンシルバニア大学 (University of Pennsylvania) の Chinese Treebank
Project により作成された構文木コーパスである．
このコーパスの概要ならびに入手方法を付録\ref{CTB_CORPUS}に示す．
以下に述べる実験では，このプロジェクトの最終版であるLDC2000T48を用いた．
また，CTB で定義されている中国語品詞数は33，句情報の数は17である．この一
覧を巻末の付録\ref{tagset}に示す．


\subsection{SVMによる同定解析}
\label{節:YamCha}

SVMは，$d$次元の特徴ベクトル(パターン) $\bf{x}$を定められた二つのク
ラス (A, B) のいずれかに識別する2値クラスの線形識別器である．また，SVMで
は，カーネルトリックと呼ばれる計算技術によって非線形識別器を実現できる．
従来の手法と比べて多くの面で優位性を示し，文字認識や画像認識など，様々な
分野で応用されている．

識別器は，識別関数$f(\bf{x})$の形によって与えられ，$f$が正ならク
ラスA, $f$が負ならクラスBに識別される．$f(\bf{x})=0$を満たす
$\bf{x}$の集合を識別面と呼ぶ．SVMの大きな特徴の一つは，マー
ジン最大化である．マージンとは，識別面と特徴ベクトル間
の最小距離であり，マージンが大きいほうが，汎化能力が高く，テストパターン
を精度良く識別できる．一般に，学習パターンを識別する超平面は複数存在する．
SVMでは，上に述べた理由から超平面と学習パターンとの最小距離を最大にする
超平面を求め，これを識別面とする．

決定した超平面からの最小距離に対応する特徴ベクトルをサポートベクトルと呼ぶ．
またサポートベクトル以外の特徴ベクトルは最終的に得られる識別関数に一切
影響を及ぼさない．したがって，出現頻度などの統計量を用いる識別器(たとえば
決定木など)とは性質が異なる．

SVMを用いることの短所は，(1) 2値クラス識別器であるため多クラスを考慮に
入れた識別関数の最適化ができない (2) 計算量が大きい (3) 問題に適したカーネ
ルトリック(カーネル関数)の明確な選択方法は知られていない，などである
\cite{Maeda2001}．

自然言語処理における同定解析(chunking)とは，与えられた言語的な要素列
(文字列，単語列など)をより上位概念の言語的要素(単語，句，文など)にまと
めあげるために，各要素に情報を付与する一連の処理を指す．たとえば，単語の
分かち書きや形態素解析，文節まとめあげ，テキストセグメンテーション，文
書分類などはすべて同定解析とみなすことができる．

工藤は，SVM に基づく汎用的な同定解析器としてYamCha (Yet Another
Multipurpose CHunk Annotator)
\footnote{\tt http://cl.aist-nara.ac.jp/\~{ }taku-ku/software/yamcha/}
を公開している．YamCha は同定解析を各要素に
対する情報付与と見なすため，一般的な解析器として用いることが可能である．

SVM は2値識別器であるため，情報付与 (tagging) のような多値クラスの識別問
題を扱うためには，何らかの拡張を行う必要がある．これに対して YamCha で
は，{\it pairwise classification}\cite{Kressel99} （一対比較分類）と
呼ばれる手法を採用している．これは，$K$クラスの識別問題を解くために，各
クラス2つの
組み合わせを識別する$K\times(K-1)/2$種類の識別器を作成し，最終的にそれ
らの多数決でクラスを決定する手法である．

SVMを用いた自然言語解析の例として英文の基本句同定実験\cite{Kudo2000b}
や，日本語の係り受け解析実験\cite{Kudo2002b}があり，
従来手法と比較して高い解析結果を示している．
また，平と春野は，SVMを用いた文書分類について，高い分類精度を得るためには
品詞によるフィルタリングをした後，全単語を入力として用いればよいことを
示している\cite{Taira2000}．

\section{YamCha による形態素解析}
\label{節:形態素解析}

SVMを用いた中国語の解析器として，我々はYamChaを用いた．この
節ではYamChaによる中国語の形態素解析について述べる．

形態素解析を文字のならびを形態素へまとめあげる同定解析と見なす．したがっ
て，各文字がチャンク(chunk)を構成する1要素に相当する．チャンクとは，
同定解析における同定単位を指し，ここでは形態素に相当する．SVMの学習のた
めにCTB を正解データとして用いる．


\subsection{YamChaの準備}

YamCha で扱うデータ形式は，複数のトークンと複数のカラムから構成される．
各行は入力のトークンに対応する．形態素解析を行う場合は，1トークンが1文
字に対応する．各カラムにはトークンに付与された属性が記述される．また，
各カラムはタブまたはスペースによって区切られている必要がある．YamChaに
よって推定(学習)すべき属性は最後のカラムに与える．ここでは，形態素解析
を行うので，第1カラムには，形態素の要素である1文字を記述し，第2カラム
には，YamCha で推定する情報を記述する．この情報には，形態素の区切り位
置を示す情報と，形態素に付与する品詞情報の両方が含まれる．また，文と文
の境界は，EOSと記述した行，もしくは空行を付与することで同定する．

トークンがチャンクに含まれるか否かの状態を示すためにIOB2モデルを用いた
\cite{Sang99}．これは，あるトークンがチャンクの先頭ならば B タグを付与
し，チャンクに含まれる先頭以外のトークンならば I タグを付与し，チャン
クに含まれない場合には O タグを付与するモデルである．一方，本実験ではす
べてのトークンが何らかのチャンクに含まれるため O タグは用いられ
ない．

付与する品詞タグセットは CTB のタグセットと同一である．また，CTBにおい
て品詞が ``-NONE-'' の形態素は構文構造上形式的に配置され，実体を持たな
いため対象外とする．最終的にトークンに付与されるタグはB/Iタグと品詞タ
グを``--''で結んだものとなる．CTBからYamChaで中国語形態素解析を行う
ための書式へ変換する概要を図\ref{YamChaFormat}に示す．

\begin{figure}[htb]
\begin{center}
\epsfxsize=30zw
\epsfbox{Figures/yamcha_format.eps}
\caption{YamCha用中国語形態素解析データ書式}
\label{YamChaFormat}
\end{center}
\end{figure}

以上の処理で得られた学習データを YamCha に与え，SVM のモデルを作成する．
その際に，素性として使用したデータはYamChaの標準設定に従った．すなわち，
推定するトークンと，その前方，および後方2トークンの計5トークンにおける
文字データと前方2トークンの推定タグを素性として学習した．解析方向は前
方からである．これは，使用する素性を変化させた場合の精度を検討した
予備実験の結果において，YamCha の標準設定が最も高い精度であったためである．
これらの関係を図\ref{features}に示す．
また，YamCha で学習を行うために用いたSVMの実装は同じく工藤が公開している
TinySVM 0.08
\footnote{\tt http://cl.aist-nara.ac.jp/\~{ }taku-ku/software/TinySVM/}
である．

\begin{figure}[htb]
\begin{center}
\epsfxsize=15zw
\epsfbox{Figures/features.eps}
\end{center}
\caption{学習素性}
\label{features}
\end{figure}


\subsection{形態素解析器 MOZ}

本報告では，YamChaと同程度の時間的コストで実現できる中国語形態素解析器
としてMOZ\footnote{\tt http://cl.aist-nara.ac.jp/student/tatuo-y/ma/}
\cite{Yamasita2000}をとりあげ，両者の比較を行う．本節では，MOZ に関
する概略を述べる．

MOZで形態素解析を行うためには，形態素辞書と接続表が必要となる．MOZはコ
スト最小法に基づく解析器であるので，形態素辞書と接続表にはそれぞれコス
トを与えなければならない．ここでは，CTBから得られる情報(品詞2つ組の頻
度や形態素の頻度など)から形態素辞書と接続表，ならびにそれらのコストを
求める．すなわち，形態素辞書は形態素とその出現確率から，品詞接続表は品
詞bi-gramによって与える．MOZでは，品詞接続表にtri-gram以上のデータを用
いることができるが，データ過疎性 (data sparseness) による精度低下を避け
るために本実験では品詞bi-gramのみを用いた．

形態素を $w_i$，品詞を$POS_i$，$x$の頻度を$C(x)$と表記すると品詞
が$POS_i$である形態素$w_i$の出現確率を式(\ref{mor_p}) で与える．ここで，
$C(w_i,\ POS_i)$は形態素$w_i$，かつその品詞が$POS_i$である頻度を示して
いる．

\begin{equation}
 p(w_i\ |\ POS_i) = \frac{C(w_i,\ POS_i)}{C(POS_i)} 
\label{mor_p}
\end{equation}
また，品詞接続表の確率は式(\ref{con_p})で与える．ここで$C(POS_i,\ POS_j)$
は品詞$POS_i$のあとに品詞$POS_j$が出現した頻度である．
\begin{equation}
p(POS_j\ |\ POS_i) = \frac{C(POS_i,\ POS_j)}{C(POS_i)}
 \label{con_p}
\end{equation}

システムで扱う最高コストを128として，コスト化係数を求める．コスト化係
数は式(\ref{cost_co})により与えられる．ここで最小確率は，すべての
$p(w_i\ |\ POS_i)$および$p(POS_j\ |\ POS_i)$における最小値である．

\begin{equation}
 コスト化係数 = |\ 最高コスト / \log(最小確率)|
\label{cost_co}
\end{equation}
形態素辞書ならびに接続表のコストはそれぞれの確率から
式(\ref{costing})により与えられる．
\begin{equation}
コスト= \big\lceil | \log(確率) \times コスト化係数|\big\rceil
 \label{costing}
\end{equation}

以上述べた方法により形態素辞書ならびに接続表のコストを計算する．


\subsection{学習文テスト}
まず，CTB 全体を学習データ (4181文\footnote{CTBの説明には4185文とあるが，
我々が発見した明らかな誤り，たとえば句点のみを1文とするなど，を除くと
4181文となった．})
とし，この中から無作為に抽出した1割
の文（418文） を解析する学習文テスト(closed test)を行った．
具体的には，YamCha と MOZ をそれぞれ用いて，418文からなるテストデータを
解析し，その結果を CTB の正解と比較し，評価した．
その結果から，再現率(recall)と適合率(precision)を算出した．再現
率と適合率はそれぞれ式(\ref{recall})および式(\ref{precision})とした．

\begin{equation}
 再現率 = 解析結果中の正解形態素数 / 正解形態素数
\label{recall}
\end{equation}

\begin{equation}
 適合率 = 解析結果中の正解形態素数 / 解析結果の形態素数
\label{precision} 
\end{equation}

再現率と適合率からF値 (F-measure)も求めた．F値は再現率$R$と適合率$P$の調
和平均であり，
式(\ref{F-measure})によって与えられる．
\begin{equation}
F値 = \frac{2 \times R \times P}{R + P}
\label{F-measure} 
\end{equation}

ただし，本実験では正解形態素数を求める場合に形態素分割のみ正解の場合と，
分割ならびに品詞の両方の2段階の条件を設けて評価した．
この結果を表
\ref{MorclosedTest}に示す．また，品詞誤りの上位10件を表
\ref{MorclosedResult2}に示す．ここで，出現率とは誤りの総数に対する各誤
りの割合を示す．

\begin{table}[htb]
  \begin{center}
   \caption{学習文テストの正解率}
   \label{MorclosedTest}
   \begin{tabular}{ll|rrr|rrr} 
\hline
&   &\multicolumn{3}{c}{分割のみ} & \multicolumn{3}{|c}{分割と品詞付与} \\
\hline
対象 &解析器 & 再現率 & 適合率 &F値    & 再現率 & 適合率 &F値\\ 
\hline
CTB  &YamCha & 99.91\%& 99.93\%&99.92\%& 99.58\%& 99.60\%&99.59\%\\ 
     &MOZ    & 97.78\%& 98.82\%&98.82\%& 93.74\%& 94.73\%&94.23\%\\ 
\hline
PKU  &YamCha & 99.95\%& 99.94\%&99.94\%& 99.76\%& 99.75\%&99.75\%\\
     &MOZ    & 98.72\%& 99.17\%&98.94\%& 94.71\%& 95.14\%&94.92\%\\
\hline
   \end{tabular}

  \end{center}
\end{table}

\begin{table}[htb]
  \begin{center}
   \caption{学習文テスト(CTB)における品詞誤りの上位10件}
   \begin{tabular}{r@{ - }l|r|r@{ - }l|r}  \hline
\multicolumn{3}{c|}{YamCha}    & \multicolumn{3}{c}{MOZ} \\ 
\hline
    正解 & 解析 & 出現率 & 正解 & 解析& 出現率\\ 
\hline
    NN   & NR   & 14/33  & VV & NN &84/401 \\ 
    NR   & NN   & 6/33   & DEC& DEG&78/401 \\ 
    VV   & NN   & 5/33   & NN & VV &38/401 \\ 
    AD   & JJ   & 2/33   & NN & NR &19/401 \\ 
    AD   & NN   & 1/33   & VV & P  &14/401 \\ 
    NN   & VV   & 1/33   & CC & AD &13/401 \\ 
    JJ   & AD   & 1/33   & NN & JJ &12/401 \\ 
    CD   & OD   & 1/33   & JJ & NN &10/401 \\ 
    SP   & DEC  & 1/33   & AD & JJ &9/401 \\ 
    DEC  & DEG  & 1/33   & JJ & AD &7/401 \\ 
    ---& ---&  ---       & P  & AD &7/401 \\ 
\hline
   \end{tabular}
   \label{MorclosedResult2}
  \end{center}
\end{table}



\subsection{未知文テスト}
次に，CTB 全体を母集団とする 10分割交差検定(cross validation)による未
知文テスト(open test)を行った．

まず，CTB全体 (4181文) を無作為に 10 等分し，1割をテストデータ，残り
の9割を学習データとする．この方法で10組の学習データとテストデータを作
成した．YamCha と MOZ それぞれに対して，10組の学習データとテストデータを
用いて学習ならびにテストを行い，平均値を求めた．

実験結果を表\ref{MOR_Opentest}に示す．
また，品詞誤りの上位10件の内訳を表\ref{MoropenResult2}に示す．表中の
null は未知語のために付与されたタグを示している．

\begin{table}[htb]
 \begin{center}
  \caption{未知文テスト結果}\label{MOR_Opentest}
\begin{tabular}{ll|rrr|rrr} 
\hline
& &\multicolumn{3}{c}{分割のみ} & \multicolumn{3}{|c}{分割と品詞付与}\\
\hline
対象 &解析器       & 再現率 & 適合率 & F値 & 再現率 & 適合率    &F値\\ 
\hline
CTB      &YamCha & 93.04\%& 93.71\%&93.37\%& 87.58\% & 88.20\%&87.89\%\\ 
         &MOZ    & 92.19\%& 85.89\%&88.93\%& 86.32\% & 80.42\%&83.26\%\\ 
\hline
京大     &YamCha & 92.02\%& 93.23\%&92.62\%& 88.17\%& 89.33\%&88.74\%\\
(10万語) &JUMAN  & 98.97\%& 98.65\%&98.80\%& 93.49\%& 93.19\%&93.34\%\\
\hline
PKU&YamCha & 86.66\%& 87.52\%&87.09\%& 80.19\%& 80.99\%&80.59\%\\
(10万語)    &MOZ    & 90.05\%& 80.58\%&85.05\%& 84.57\%& 75.67\%&79.87\%\\
\hline
PKU      &YamCha & 95.19\%& 95.19\%&95.19\%& 91.72\%& 91.72\%&91.72\%\\
         &MOZ    & 95.68\%& 93.42\%&94.58\%& 89.87\%& 87.75\%&88.72\%\\ \hline
  \end{tabular}
 \end{center}
\end{table}
\begin{table}[htb]
  \begin{center}
   \caption{未知文テストにおける誤り品詞の上位10件}
   \label{MoropenResult2}
   \begin{tabular}{r@{ - }l|r|r@{ - }l|r}  \hline
\multicolumn{3}{c|}{YamCha}    & \multicolumn{3}{c}{MOZ} \\ 
\hline
    正解 & 解析 & 出現率     & 正解 & 解析& 出現率\\ 
\hline
     VV  &  NN  &  1037/5447 &  VV  & NN  &1016/5859 \\
     NN  &  VV  &  579/5447  &  DEC & DEG &933/5859 \\
     DEC &  DEG &  578/5447  &  NN  & VV  &701/5859 \\  
     JJ  &  NN  &  402/5447  &  NN  & NR  &263/5859 \\ 
     DEG &  DEC &  334/5447  &  DEG & DEC &186/5859 \\ 
     NR &   NN  & 323/5447   &  VV  & P   &184/5859 \\ 
     NN  &  NR  & 266/5447   &  NN  & JJ  & 126/5859 \\ 
     VA  &  NN  & 174/5447   &  JJ  & AD  &118/5859 \\ 
     AD  &  NN  & 126/5447   &  NN  & null&116/5859 \\ 
     AD  &  VV  & 115/5447   &  CC  & AD  &100/5859 \\ 
\hline
   \end{tabular}
  \end{center}
\end{table}

未知文テストにおいて YamCha が1学習データを学習するために要した処理時
間と1テストデータを解析するために要した処理時間などを表\ref{MorTime} 
に示す．測定時は，CPU: Pentium III 600 MHz，メモリ: 256 MB，OS: Linux 
の計算機を用いた．ただし，YamCha で解析を行うためには，アーキテクチャ非
依存のテキスト形式のモデルファイル (学習結果を格納するファイル) をアーキ
テクチャ依存のバイナリ形式にコンパイルする必要がある．その際にテキスト形
式のモデルをメモリ上に展開するため大量のメモリを必要とする．この実験では，
約 650 MB のメモリを必要としたため，コンパイル作業だけは，CPU: Pentium
III 733 MHz，メモリ: 960 MB の計算機を使用した．このコンパイルに要した時
間は約5分であった．

本実験のあと，コンパイルに必要なメモリ量を抑える目的からプログラムを修正
した．その結果，速度を少々犠牲にするが，80MB程度のメモリで，上記のモデル
ファイルをコンパイル可能となった．処理時間は，CPU: Pentium III 600MHz，
メモリ: 256 MBの計算機で約 7分であった．この YamCha 0.1 に対するプログラ
ムの差分はWWWページ
\footnote{\tt http://www.slt.atr.co.jp/\~{ }kohtake/} にて公開している
\footnote{2002年11月に公開された YamCha 0.2 では，この修正が反映されてお
りプログラムの差分を適用する必要はない．}．

\begin{table}[htb]
 \begin{center}
  \caption{YamChaでの処理時間}
  \begin{tabular}{l|ll} 
\hline
                & 学習     & 解析 \\
\hline
    タグの種類  & 53       & ---   \\ 
    文数       & 約3700    & 約400\\
    トークン数 & 約15万8千 &約1万7千\\ 
    処理時間   & 約 6時間  &約35分\\ 
\hline
  \end{tabular}
  \label{MorTime}
 \end{center}
\end{table}

一方，MOZ が学習データ(約3700文)から形態素辞書と接続表のコストを求めるた
めに必要とした時間は約3秒であり，1テストデータ(約400文)を解析するために
必要とした時間は約1秒であった．


\subsection{未知語の性質}
次に，テストデータに含まれる形態素のうち，学習データに含まれていないも
のを未知語と定義し，その性質を調べた．
未知文テストにおける平均未知語率などを求めた．結果を表\ref{OpenUnknown}
に示す.
未知語率とはテストデータの単語数に占める未知語数の割合を指し，平均未知語
率とはテストセット全体での未知語率の平均を示している．
\begin{table}[htb]
 \begin{center}
\caption{未知文テストにおける平均未知語率など}
\label{OpenUnknown}
  \begin{tabular}{l|ccccc}
\hline
                  & 平均未知語率    &        & &\multicolumn{2}{c}{形態素数}\\ 
対象      &[異なり/のべ](\%)& 平均文長& 平均形態素長 &異なり&のべ\\
\hline
CTB (10分割)      &21.74 / 7.05     & 41.10 字& 1.72 字      &12079&103901\\
京大10万語 (10分割)&24.18/ 8.38      & 43.91 字& 1.77 字
                  &14613&102310\\
PKU10万語 (10分割) &26.85 / 10.87     & 40.71 字& 1.64 字
                  &16810&102741\\
PKU (11分割) & 15.79 / 2.84        & 41.85 字& 1.64 字       &61846&1118794\\ 
\hline
  \end{tabular}
 \end{center}
\end{table}

未知語に対する解析器の性質をより詳しく調べるため，以下の実験を行った．ま
ず，CTB から 40 記事を無作為に選択する．そのうち 30 記事を学習データ，
残り 10 記事をテストデータとする．次にテストデータから順に1記事ずつ学
習データに加えていき，計11個の学習データを作成する．それぞれの学習デー
タに基づく解析器で同一のテストデータ 10 記事を解析した．以上の実験概要
を図\ref{UnknownWordExp} に示す．

\begin{figure}[htb]
\begin{center}
\epsfxsize=50mm
\epsfbox{Figures/UkW_Exp.eps}
\caption{未知語と解析精度に関する実験の概要}
\label{UnknownWordExp}
\end{center}
\end{figure}

テストデータに含まれる単語数は1111，のべ語数は2954である．
11のテストセットにおける学習データの単語，
未知語数ならびに未知語率を表\ref{UkTEST}に示す．

\begin{table}[htb]
\begin{center}
\caption{未知語と解析精度に関する実験におけるテストセットの単語数}
\label{UkTEST}
\begin{tabular}{c|rrrrrr}
\hline
 & \multicolumn{2}{c}{単語数}&\multicolumn{2}{c}{未知語数} &
 \multicolumn{2}{c}{未知語率(\%)}\\
テストセット             & 異なり &のべ & 異なり &のべ & 異なり &のべ \\
\hline
0            & 2809 & 10819 & 572    & 770 & 51.49  &26.07\\ 
1            & 2892 & 11402 & 489    & 639 &44.01   &21.63\\ 
2            & 2921 & 11512 & 460    & 594 &41.40   &20.11\\ 
3            & 3044 & 11929 & 337    & 442 &30.33   &14.96\\ 
4            & 3075 & 12134 & 306    & 406 &27.54   &13.74\\ 
5            & 3127 & 12402 & 254    & 337 &22.86   &11.41\\ 
6            & 3190 & 12764 & 191    & 252 &17.19   &8.53\\ 
7            & 3265 & 12993 & 116    & 150 &10.44   &5.08\\ 
8            & 3293 & 13274 & 88     & 115 &7.92    &3.89\\ 
9            & 3319 & 13393 & 62     & 75  &5.58    &2.54\\ 
10           & 3381 & 13773 & 0      & 0   &0.00    &0.00\\ 
\hline
\end{tabular}
\end{center} 
\end{table}

各テストセットにおける未知語率 (異なり)と解析精度の関係を図
\ref{MorCrbyUk}に示す．図では，解析精度をF値で示す．
なお，未知語率 (のべ)と精度の関係を図示していないが，
図\ref{MorCrbyUk}とほぼ同一の図となるため省略する．


\begin{figure}[htb]
  \begin{center}
   \epsfxsize=20zw
   \epsfbox{Figures/UK_plots.eps}
   \caption{未知語率[異なり]と解析精度}
   \label{MorCrbyUk}
  \end{center}
\end{figure}


次に，未知語がある場合の解析結果を調査した．テストセット$t_i$
における未知語の集合を$UK(t_i)$，$w \in UK(t_i)$ のうち
形態素分割に成功した形態素の集合を$USeg(t_i)$とする．
さらに，$w \in USeg(t_i)$ のうち品詞も正しく解析された形態素の
集合を$USP(t_i)$とする．これらの集計結果を
表\ref{UK_Analyze_table}に示す．
また，MOZでは入力に未知語が含まれる場合，
解析不能で停止することはないが，最終的に未知語と判断された文字列を1文字
ずつ，nullという品詞を与えて出力する．
したがって，MOZでの解析で $|USP(t_i)|$を
示していないのは，MOZでは未知語がnullと解析されるため，$USP(t_i)$は
空集合となるためである．
一方，MOZでの解析において$USeg(t_i)$が得られるのは，
正解が1文字の形態素である場合に形態素分割が成功したと見なすからである．

\begin{table}[htb]
 \begin{center}
  \caption{未知語とその解析結果}
\label{UK_Analyze_table}
\begin{tabular}{cc|rr|r}
\hline
      &     & \multicolumn{2}{c|}{YamCha}          & MOZ\\
\cline{3-5}
 $t_i$& $|UK(t_i)|$ & $|USeg(t_i)|$ & $|USP(t_i)|$ & $|USeg(t_i)|$\\
\hline
 0    & 572         & 376           & 249          & 87\\
 1    & 489         & 319           & 212          & 78\\
 2    & 460         & 299           & 199          & 77\\
 3    & 337        & 225           & 157         & 56\\
 4    & 306        & 202           & 147          & 47\\
 5    & 254        & 174           & 127          & 35\\
 6    & 191        & 135           & 95          & 27\\
 7    & 116         & 82           & 59           & 18\\
 8    & 88         & 60            & 43           & 15\\
 9    & 62          & 47            & 35           & 10\\
\hline
\end{tabular}
 \end{center}
\end{table}

\subsection{言語依存性とコーパスの大きさ}
これまで，CTBをコーパスとして，中国語の形態素解析について2つの解析器，
YamCha と MOZ を比較してきた．しかしその未知文テストの結果は，表
\ref{MOR_Opentest}に示す通り，これまでに報告されている日本語の形態素解析
器の精度より低い．ここでは，その原因が，中国語の言語としての解析の難しさ
にあるのか，コーパスの量にあるのかを検討する．

\subsubsection{日本語形態素解析におけるYamCha}
CTBで用いられているのは，新華社通信の新聞記事である．
そこで，SVMに基づく形態素解析器の日本語に対する精度を検証するために
我々は，京都大学テキストコーパス 第 3.0 版（以下，京大コーパスと呼ぶ）を
用いて実験を行った．このコーパスの詳細については付録\ref{KYODAI_CORPUS}
を参照されたい．

CTBの大きさが約10万語であるところから，
我々は，京大コーパスのうち1月1, 3, 4, 5日の記事，4117文，102310単語を用いる
ことにした．CTB全体では，4181文，99720単語である．

我々が選択した京大コーパスの一部についてCTBに対する実験と同様に，
10分割交差検定を実施した．
この検定における平均未知語率などを表\ref{OpenUnknown}に示す．

京大コーパスを用いた実験における品詞は，JUMAN\footnote{\tt{http://www-nagao.kuee.kyoto-u.ac.jp/nl-resource/juman.html}}が定義する品詞のうち
品詞細分類までを含めたものとした．この結果，タグセットの大きさは，
41となり，CTBの33より大きい．
YamChaの学習に用いたデータの例を以下に示す．
\vspace*{5mm}

\begin{quote}
\begin{verbatim}
今 B-名詞-時相名詞        話 B-名詞-普通名詞
年 I-名詞-時相名詞        題 I-名詞-普通名詞
の B-助詞-接続助詞        の B-助詞-接続助詞
大 B-名詞-普通名詞        力 B-名詞-普通名詞
相 I-名詞-普通名詞        士 I-名詞-普通名詞
撲 I-名詞-普通名詞        た B-接尾辞-名詞性名詞接尾辞
を B-助詞-格助詞　        ち I-接尾辞-名詞性名詞接尾辞
支 B-動詞
え I-動詞
る I-動詞
\end{verbatim}
\end{quote}

\vspace*{5mm}

実験結果を表\ref{MOR_Opentest}に示す．参考までに我々が選択した京大コーパ
スの一部を JUMANで形態素解析した結果もあわせて示す．

日本語を対象とした実験でも，同程度の大きさのコーパスでは，同程度の精度となった．
\subsubsection{コーパスの大きさと解析精度}
CTBは既に述べた通り4181文，99720単語からなるコーパスだが，大きいとは
言えない．そのため，10分割交差検定を行っても，テストセットにおける未知語
率が非常に大きくなり，精度が低くなる．

品詞タグ付けされたコーパスがあれば，それを形態素解析器のために
利用することが可能である．CTBのように構文木を備えている必要はない．
品詞タグ付けされた中国語のコーパスはいくつかあるが，
我々は，CTBよりも大きく，そして同じ新聞記事という点から人民日報タグ付き
コーパスを使用した．
人民日報タグ付きコーパスに関しては，付録\ref{PKU_CORPUS}にその概要等を示
す．ただし，我々は人民日報半年分であるコーパス全てではなく，無償公開して
いる1ヶ月分のデータを用いた．
人民日報タグ付きコーパス1ヶ月分(以下，
PKUと呼ぶ)は，44011文，1121447単語からなるコーパスである．
定義されているタグセット
\footnote{\tt http://www.icl.pku.edu.cn/research/corpus/addition.htm}の大き
さは39である．しかしながら，実際のPKUには
ここに定義されていないタグが7種類(Bg: 8, Mg: 7, Rg: 10, Yg: 1, na: 1,
nx: 459, vvn: 1，コロンの後の数値は頻度を示す)出現する．ここで，nxは，定
義されているタグxに該当することがわかったので，nx以外の6種類のタグを含む
文を除いた．その結果，PKUは，43913文，1118794単語からなるコーパスとなった．

PKUはCTBの約11倍の大きさを持つ．まず，同程度のコーパスの大きさでの精度を
検証した．PKUをランダムに文単位で11等分し，そのうちの1つ (約10万語)を
用いて10分割交差検定を行った．結果を表\ref{MOR_Opentest}に示す．以下，こ
のPKUの10万語のコーパスをPKU10万語と表記する．

PKU10万語を用いての未知文テストとCTBでの未知文テストでの
精度の違いは，両者のコーパスの違いに起因する．
両者はともに約10万語のコーパスであるが，
表\ref{OpenUnknown}から，PKUの方が未知語が多く，なおかつ1文あたりの平均
形態素数がCTBより約1単語多いことがわかる（CTBでの1文平均形態素数
$=41.10/1.72=23.90$，PKU10万語での1文平均形態素数$=40.71/1.64=24.82$）．
さらに，PKUは，CTBと比較してタグセットが大きく，
タグ一種類あたりの学習データが少なくなることからタグの推定がより難しくなっ
ている．したがって，PKU10万語での結果は，同じ10万語のCTBと比較して精度が
大きく低下したと考える．

MOZが再現率の面で YamCha を上回るのは，辞書を用いる利点が活かされてい
ると考える．YamCha は1文字単位でタグの推定を行う．タグの推定に用いるのは
推定対象文字とその前後2文字，さらに直前に推定した2つのタグの計
7つの素性である．したがって，学習量が不十分な状態では，ある形態素が
学習，テストコーパスの両方に含まれている場合でも，テストデータ
における当該形態素とその周辺文字列の組合せを素性として学習している可能性
は低いためSVMが誤る可能性が大きくなる．
一方，MOZでは，一度辞書に登録された形態素は，形態素分割お
よび品詞の曖昧性が生じない限り正しく再現される．さらに学習コーパスが大き
くない場合では，これらの曖昧性が発生する頻度は低いと予想する．したがって
MOZが再現率の面で，YamCha を上回ったと考える．

次に，PKU全てを学習コーパスとし，学習文テストを行った．
テストデータとしてPKUから無作為に抽出した3993文，
101218単語を解析した．なお，学習に用いる
コーパスが大きくなることから，より大きな分解能が必要になると
考え，MOZのコスト化係数を128から1024へと変更した．
実験結果を表\ref{MorclosedTest}に示す．
この結果から，学習コーパスが100万語を越えても，
YamCha は変わらず高い性能を示していることがわかる．

学習に用いるコーパスの大きさが非常に大きくなった場合の
2つの解析器のふるまいを検討するために，
11等分したデータを用いて11分割交差検定を行った．
結果を表\ref{MOR_Opentest}に示す．

\subsection{形態素解析結果に関する考察}
以上得られた形態素解析に関する実験結果について考察する．

まず，YamChaとCTBを使用した場合の未知文に対する形態素解析(分割と品詞付
与)精度(F値)は87.9\%であった．同条件でMOZが83.3\%であることを考えると，
言語資源としてCTBしか得られない条件下ではYamChaを使用したほうが
高精度な解析器を実現できる．

次に，解析時間についてはYamChaが極端に遅い．学習時間も同様である．よっ
て解析時に実時間性を問われる状況においてはMOZを使用すべきである．
YamChaでは，既に述べたように，一対比較分類に基づき品詞付与を行うため，
品詞数が大きくなると，その2乗に比例するSVMが必要となる．
そのため，品詞数の増加とともに，学習，解析時間が増大する．

品詞付与誤りの傾向では，1節で述べたように中国語において本質的に解析の
難しいと予想される箇所で両解析器共に誤っており，解析器としての誤りのく
せはあまり見受けられない．

未知語に対する頑健性については，YamChaのほうが優れる．実
験では，YamChaは未知語の約4割を正しく解析しており，頑健性を確かめられ
た．この割合は，未知語率が変化しても，大きく変化することはなく，
実験した範囲の未知語率(51.5\%から6.4\%)で，40\%から45\%程度であった．
このことから，未知語率が大きくなったからといってそれに影響されて
極端に精度が低下することはないと予想する．
一方，MOZは，未知語に対して1文字ずつにnullという品詞を付与して出力する
のみであるため，何らかの拡張を行わない限り品詞の推定を行えない．したがっ
て，再現率に対して適合率が低くなる傾向がある．また，YamChaにはこのような
傾向はなく，適合率が再現率を若干上回る傾向を示す．これらのことから，
入力文中に多くの未知語の存在が予想される場合，あるいは学習データの語彙傾
向と異なる入力文を解析する場合はYamChaを用いたほうがよい．

ただし，一般的な状況としてコーパスとは別個に単語集合を入手できる場合が
ある．この場合にはMOZを使うべきだろう．YamChaでは単語集合があってもこ
の情報を学習に反映させることができず，コーパス中の出現単語のみが学習対
象であるためである．

言語資源をより活用しているのはYamChaであるが，
辞書を用いないことから語彙的整備ができない．
また人間の内省による知見を反映させにくい．
したがって，既に大量のタグ付きコーパスが
存在する状況では，MOZのような，接続コストを統計的言語モデルに基づいて
推定する手法が頑健で，整備しやすい解析器となる．逆に，タグ付きコーパス
が充分に整備されていない言語の解析器を必要とする場合，あるいは，新たに
定義した品詞に対する解析器が，その品詞で解析されたコーパスが充分に存在
しない状況で必要となる場合には YamCha が有効である．

また，中国語に固有の解析の難しさが考えられるが，日本語を対象とした実験の
結果から，同程度のコーパスならびにそのタグセットの大きさの場合では顕著な
違いは見られなかった（表\ref{MOR_Opentest}）．
しかしながら，京大コーパスの平均未知語率が，CTBのそれと比較して大きく
（表\ref{OpenUnknown}），さらに，京大コーパスのタグセット (41) が CTB 
のそれ (33) より大きい．これは，日本語解析の実験条件が中国語解析の条件
に比べ，厳しいことを示す．それにもかかわらず，実験結果は同程度の精度を
示した．これらのことから中国語解析が，日本語解析に比べて難しいと判断す
る．さらに，表\ref{MOR_Opentest}は，京大コーパスの解析結果とCTBの解
析結果において単語分割のみと分割と品詞付与との間の逆転現象があることを示
している．これは，中国語解析の困難な点は，品詞付与にあるという我々の予見
を裏付ける結果と考える．

一方で，より大きなコーパスを用いることにより，高精度な解析器が
実現可能であることが，表\ref{MOR_Opentest}からわかる．
また，表\ref{MorclosedTest}に示した学習文テストの結果から，学習コーパ
スをさらに大きくするとYamChaはさらに精度を向上させる可能性がある．
それに対し，MOZは，PKU（100万語以上の大きさを持つコーパス）を用いての学
習文テスト結果において分割と品詞付与のF値が約95\%（表\ref{MorclosedTest}）
だったことから，現状の枠組のままでは，F値で95\%程度がその性能の限界だと
考える．これをさらに向上させるためには，接続表への
tri-gram規則の適用ならびにその補完などが可能である．しかし，
浅原らは，中国語の場合には，tri-gramの規則自体があまり有効ではなく，
品詞体系の詳細化が精度の向上に寄与することを実験結果から予測している
\cite{Asahara2002a}．


\section{YamChaによる基本句同定解析}
\label{節:基本句同定解析}

本節では，YamChaを用いた基本句同定解析 (base phrase chunking) 実験につい
て述べる．基本句同定解析とは，形態素解析結果すなわち品詞付与された単語
列を入力として，最も下位の構造を同定し，その構造に対して構文的情報を付
与する処理である．ここで，最も下位の構造を基本句，基本句に対する構文的
情報を句情報と本報告では呼ぶことにする．

このように，基本句同定解析は一段階の構文解析と考えることができる．した
がって，構文解析は同定解析を繰り返すことで実現できる\cite{Abney91}．
工藤らは，SVMに基づく同定解析の段階適用が，日本語の係り受け解析に有効で
あることを示している\cite{Kudo2002b}．

\subsection{学習データ}
同定解析の学習データは，形態素解析と同様にCTBを用いた．CTBが表現する構
文木では，葉が形態素に相当する．基本句同定解析では，葉に最も近い位置に
付与されている構造が基本句であり，形態素が基本句を構成する要素に該当す
る．すなわち，形態素情報を入力として基本句の区切り位置と句情報を推定す
る．

\subsection{学習文テスト}

CTB全体 (3572文\footnote{形態素解析実験と異なる理由は，見出しなどを対
象から除いているためである．}) を学習データとして無作為に抽出した1割の
文(357文)を解析する学習文テストを行った．この結果を表\ref{BP_close}に
示す．学習文テストにおけるテストデータは7746の基本句からなる．そのうち
YamChaは7745の基本句を同定した．このうち7741の基本句の同定に成功し，
そのうち句情報も正解だったものは7740であった．すなわち，学習文テストで
はYamChaはほとんどすべての解析に成功した．


\begin{table}[htb]
 \begin{center}
\caption{基本句同定解析の学習文テストの結果}
\label{BP_close}
  \begin{tabular}{l|ccc}
\hline
                        & 再現率(\%)& 適合率(\%) & F値(\%)\\
\hline
 基本句同定のみ         & 99.94 & 99.95 & 99.94\\
 基本句同定と句情報付与 & 99.92 & 99.94 & 99.93\\
\hline
  \end{tabular}
 \end{center}
\end{table}


\subsection{未知文テスト}

次に，形態素解析実験と同様に，10分割交差検定による未知文テストを行った．
学習の素性は，形態素の文字列情報，品詞情報，基本句の区切りを示すIOB 
タグとその基本句の句情報である．すべての学習条件は YamCha の標
準設定に従っている．

形態素解析実験と同様に再現率と適合率を求めた．未知文テストの結果を表
\ref{BphopenResult1}に示す．なお，正解基本句数の平均は7734.4であり，
YamChaが出力した基本句数の平均は7691.9である．


\begin{table}[htb]
  \begin{center}
   \caption{基本句同定解析の未知文テストの結果}
   \begin{tabular}{l|ccc}
\hline
                         & 再現率(\%) & 適合率 (\%) & F値(\%)\\ 
\hline
  基本句同定のみ         &  94.61 & 95.12 & 94.86\\
  基本句同定と句情報付与 &  93.44 & 93.94 & 93.69\\ 
\hline
   \end{tabular}
   \label{BphopenResult1}
  \end{center}
\end{table}

基本句同定，句情報付与共に比較的高い精度を得られることがわかった．また，
句情報付与における誤りのうち出現率の大きい上位5種を表
\ref{BphopenResult2}に示す．

\begin{table}[htb]
  \begin{center}
   \caption{誤りパターンの上位5種}
   \begin{tabular}{r@{ - }l|r}  \hline
    正解 & 解析 & 出現率 (\%)\\ \hline
     IP  &  VP  & 47.58 (432/908)    \\
     VP  &  IP  & 35.35 (321/908)  \\
     IP  &  NP  & 2.31  (21/908)  \\
     UCP  & IP  & 2.20 (20/908)   \\
     PRN  & IP  & 1.98 (18/908)   \\ \hline
   \end{tabular}
   \label{BphopenResult2}
  \end{center}
\end{table}

1テストセットあたりの学習・解析時間は，表\ref{BphTime}の通りである．な
お，形態素解析実験と同一の計算機を使用した場合，モデルファイルのコンパ
イルに要した時間は約1分だった．

\begin{table}[htb]
 \begin{center}
  \caption{基本句同定解析の処理時間}
  \begin{tabular}{l|ll} \hline
                  & 学習    & 解析 \\
\hline
    タグの種類    & 29 種類 &---  \\ 
    文数          & 約3200  & 約350\\
    トークン数 & 約 8万6千 & 約9500  \\ 
    処理時間      & 約 2 時間 & 約 3 分\\ 
\hline
  \end{tabular}
  \label{BphTime}
 \end{center}
\end{table}

基本句同定解析は，形態素解析と比べて高い解析精度を得た．また，学習・解
析に要する時間がいずれも短い．これは付与する情報の種類が少ない，および
トークン数が少ないためである．

\section{まとめ}
本報告では，SVMに基づく言語解析器を用いて，Penn Chinese Treebank を言語
資源とした時にどの程度の中国語解析精度が得られるかを報告した．以下にその
結果をまとめる．

\begin{itemize}
\item 
      SVMに基づく解析器（YamCha）による形態素解析精度は単語単位で約88\%
      であり，コスト最小法に基づく解析器（MOZ）よりも4\%以上高い．未知
      語の約4割を正しく解析でき，未知語に対する頑健性は高い．ただし計算
      量に問題があり，解析時間，学習時間共に非常に長い．大量のタグ付き
      コーパスが入手できる場合は，YamCha，MOZのいずれを用いても，さら
      に高精度の解析器（110万語のコーパスの場合，F値でそれぞれ約92\%，
      89\%）を実現できる．そのような場合には，解析，学習にかかる時間を
      考慮すると，MOZを用いるべきである．なお，中国語形態素解析は日本
      語のそれと比較して顕著な困難さは見られなかった．

\item
      基本句同定解析 (base phrase chunking) の精度は約93\%．約3200文の学習
      に約2時間，357文の解析に約3分を要する．
\end{itemize}
\vspace*{8mm}

\acknowledgment

本研究は，通信・放送機構の研究委託「大規模コーパスベース音声対話翻訳技術の
研究開発」により実施したものです．

\newpage
\bibliographystyle{jnlpbbl}
\bibliography{ca_svm}

\newpage
\appendix
\vspace*{-1em}
\section{CTB のタグセット}
\vspace*{-1em}
\label{tagset}
CTBにおける全品詞の説明と，全ての句情報の説明をCTBのタグ付与指針
\cite{CTB_guide2000}から転記する．

\begin{center}
\begin{tabular}{llll}
\multicolumn{2}{l}{\bf 品詞情報：} & & \\
 AD&adverbs &    M& measure word (including classifiers)\\
   AS&aspect marker &    MSP&some particles\\
   BA&\lower.25ex\hbox{\epsfxsize=1.1zw\epsfbox[0 10 99
 90]{Chinese_Chars/ba.eps}} in ba-const &    NN&common nouns\\
   CC&coordinating conj &    NR&proper nouns\\
   CD&cardinal numbers &    NT&temporal nouns\\
   CS&subordinating conj &    OD&ordinal numbers\\
   DEC&\lower.25ex\hbox{\epsfxsize=1.1zw\epsfbox[0 10 99 90]{Chinese_Chars/de.eps}} for relative-clause etc. &
 ON&onomatopoeia\\ 
   DEG&associative \lower.25ex\hbox{\epsfxsize=1.1zw\epsfbox[0 10 99 90]{Chinese_Chars/de.eps}} &    P& prepositions (excluding
 \lower.25ex\hbox{\epsfxsize=1.1zw\epsfbox[0 10 99 90]{Chinese_Chars/ba.eps}} and
 \lower.25ex\hbox{\epsfxsize=1.1zw\epsfbox[0 10 99 90]{Chinese_Chars/bei4.eps}})\\ 
   DER&\lower.25ex\hbox{\epsfxsize=1.1zw\epsfbox[0 10 99 90]{Chinese_Chars/de_toku.eps}} in V-de const. and V-de-R &
 PN&pronouns\\ 
   DEV&\lower.25ex\hbox{\epsfxsize=1.1zw\epsfbox[0 10 99 90]{Chinese_Chars/de_ti.eps}} as the head of DVP &    PU&punctuation\\
DT&determiner &    SB&\lower.25ex\hbox{\epsfxsize=1.1zw\epsfbox[0 10 99 90]{Chinese_Chars/bei4.eps}} in short bei-construction\\
   ETC&tags for \lower.25ex\hbox{\epsfxsize=1.1zw\epsfbox[0 10 99 90]{Chinese_Chars/deng.eps}} and
 \lower.25ex\hbox{\epsfxsize=1.1zw\epsfbox[0 10 99 90]{Chinese_Chars/deng.eps}\epsfxsize=1.1zw\epsfbox[0 10 99 90]{Chinese_Chars/deng.eps}}  &
 SP&sentence-final particle\\
 & in coordination phrases & VA&predicative adjective\\
   FW&foreign words &   VC&copula \lower.25ex\hbox{\epsfxsize=1.1zw\epsfbox[0 10 99 90]{Chinese_Chars/shi.eps}}\\ 
   IJ&interjection &     VE&\lower.25ex\hbox{\epsfxsize=1.1zw\epsfbox[0 10 99 90]{Chinese_Chars/you.eps}} as the main verb\\
   JJ&noun-modifier other than nouns &     VV&other verbs\\
   LB&\lower.25ex\hbox{\epsfxsize=1.1zw\epsfbox[0 10 99 90]{Chinese_Chars/bei4.eps}} in long bei-construction &  & \\
   LC&localizer & & \\
\end{tabular}
\end{center}

\vspace*{-1em}
\begin{center}
\begin{tabular}{llll}
\multicolumn{2}{l}{\bf 句情報：} & &\\
ADJP&adjective phrase & LCP&phrase formed by ``XP + LP''\\
ADVP&adverbial phrase headed  & LST&list marker\\
 & by AD (adverb) &NP& noun phrase\\
CLP&classifier phrase & PP& preposition phrase\\
CP& clause headed by C (complementizer) & PRN&parenthetical\\
DNP&phrase formed by ``XP + DEG'' & QP& quantifier phrase\\
DP& determiner phrase & UCP&unidentical coordination phrase\\
DVP&phrase formed by ``XP + DEV'' & VP& verb phrase \\
FRAG & fragment &  & \\
IP& simple clause headed by I (INFL) & &\\
\end{tabular} 
\end{center}


\section{本報告で用いた言語情報資源}
以下に，本報告で用いた言語情報資源についてまとめる．
\subsection{Penn Chinese Treebank}
\label{CTB_CORPUS}
米国ペンシルバニア大学 (University of Pennsylvania) の Chinese Treebank
Project により作成された構文木コーパスである．
このプロジェクトは1998 年夏に始まり，最終版(LDC2000T48)を2000年12月に，
また同一内容で誤りを修正した Penn Chinese Treebank Version 2.0
(LDC2001T11)を2001年に公開した．新華社通信 
(\lower.25ex\hbox{\epsfxsize=5.5zw\epsfbox[0 10 482
90]{Chinese_Chars/xinhua_news.eps}}, Xinhua News Agency) の1994年から
1998年の325記事から構成される，約10万語の大きさのコーパスである．


CTBは\lower.25ex\hbox{\epsfxsize=5.5zw\epsfbox[0 10 482
90]{Chinese_Chars/xinhua_news.eps}}の325記事に対して，単語分割，品詞付
与，構文情報付与されたコーパスである．コーパス作成作業は，(1)作業者1名
が全情報を付与する (2) 別の作業者1名が点検する，という方法で行った．文
字コードにはGBコードを使用し，データの書式は English Penn Treebank と
ほぼ同一である．

\subsubsection*{入手方法}
Linguistic Data Consortium (LDC)より出版されている．
そのため，入手方法は通常のLDCのコーパスを入手する方法と同じである．
本報告で使用した LDC2000T48 は2000年のメンバーに配布可能である．
メンバー以外であっても，US\$100にて入手することができる．
関連URIを以下に示す．

\begin{tabular}{ll}
{Chinese Treebank Project}&{\tt\small http://www.ldc.upenn.edu/ctb/}\\
LDC &{\tt\small http://www.ldc.upenn.edu/}\\
CTB 最終版 &{\tt\small http://www.ldc.upenn.edu/Catalog/LDC2000T48.html}\\
CTB Version 2.0 & {\tt\small http://www.ldc.upenn.edu/Catalog/LDC2001T11.html}\\
\end{tabular}
\subsection{人民日報タグ付きコーパス}
\label{PKU_CORPUS}
富士通研究開発中心有限公司 (\lower.25ex\hbox{\epsfxsize=14.3zw\epsfbox[0 10 1247 90]{Chinese_Chars/fujitu.eps}}) と，北京大学 (\lower.25ex\hbox{\epsfxsize=4.4zw\epsfbox[0 10 386 90]{Chinese_Chars/beijingdaxue.eps}}) および人民日報社 (\lower.25ex\hbox{\epsfxsize=5.5zw\epsfbox[0 10 482 90]{Chinese_Chars/renminribao.eps}}) が協力し作成した，中国で最も権威を持ち影響力のある中国全国紙のタグ付きコーパスである．\lower.25ex\hbox{\epsfxsize=4.4zw\epsfbox[0 10 383 90]{Chinese_Chars/renminribao_4char.eps}}の1998年の新聞記事半年分，約1,300万文字＝約730万単語からなる．\lower.25ex\hbox{\epsfxsize=5.5zw\epsfbox[0 10 482 90]{Chinese_Chars/renminribao.eps}\epsfxsize=6.6zw\epsfbox[0 10 581 90]{Chinese_Chars/xinwenxinxi_center.eps}}
から，大学や研究所などでの研究利用に限定して，人民元2,000元（約3万円，実費）にて有償公開している．また，この内1ヶ月分を無償公開している．

\subsubsection*{入手方法}
正規版の入手に関しては，WWWページ
\footnote{\tt{http://www.fujitsu.com.cn/support/}}に記載されている連絡
先へ問いあわせる．
また，本報告で用いた無償公開版は，
\lower.25ex\hbox{\epsfxsize=13.4zw\epsfbox[0 10 1154
90]{Chinese_Chars/beijingdaxue_keisan.eps}}の公開ページ
\footnote{\tt{http://www.icl.pku.edu.cn/research/corpus/dwldform1.asp}}
にある必要項目を満たすことにより入手できる．

その他，関連URIを以下に示す．

\noindent
\begin{tabular}{ll}
\multicolumn{2}{l}{日本語による説明： {\tt\small http://pr.fujitsu.com/jp/news/2001/08/28.html}}\\
\lower.25ex\hbox{\epsfxsize=13.4zw\epsfbox[0 10 1154 90]{Chinese_Chars/beijingdaxue_keisan.eps}}： &\\
\multicolumn{2}{r}{{\tt\small http://www.icl.pku.edu.cn/Introduction/corpustagging.htm}} \\
\multicolumn{2}{l}{タグセット一覧：  {\tt\small http://www.icl.pku.edu.cn/research/corpus/addition.htm}}\\
\end{tabular}
\subsection{京都大学テキストコーパス}
\label{KYODAI_CORPUS}
京都大学テキストコーパスは，毎日新聞1995年1月1日から17日までの全記事，約
2万文，1月から12月までの社説記事，約2万文，計約4万文に対して，京都大学
の形態素解析システム(JUMAN)，構文解析システム(KNP)で自動解析を行い，そ
の結果を人手で修正したコーパスである．
ただし，コーパスとして含んでいるのは形態素・構文の付加情報のみであり，
毎日新聞の記事そのものは含まれていない．そのためコーパス本来の
形式とするためには別途毎日新聞CD-ROMが必要である．
毎日新聞CD-ROMを用意し，京都大学テキストコーパスの配布パッケージに
含まれるプログラムを使用して完全なコーパスの形式へ変換する．

\subsubsection*{入手方法}
以下のページより入手できる．

\noindent
{\tt http://www-nagao.kuee.kyoto-u.ac.jp/nl-resource/corpus.html}

\newpage

\begin{biography}
\biotitle{略歴}
\bioauthor{吉田 辰巳}{1979年生．
2002年 豊橋技術科学大学工学部知識情報工学課程卒業．
現在，豊橋技術科学大学大学院工学研究科修士課程知識情報工学専攻在学中．
自然言語処理，特にテキスト自動要約の研究に従事．\\
{\tt e-mail: gaizi@smlab.tutkie.tut.ac.jp}}
\bioauthor{大竹 清敬}{1973年生．2001年豊橋技術科学大学大学院工学研究科博士後期課程電子・情報工学専攻修了．博士（工学）．同年より
国際電気通信基礎技術研究所(ATR)に所属し，現在，音声言語コミュニケーション研究所研究員．自然言語処理，特に換言処理，要約処理，機械翻訳の研究に従事．
言語処理学会，人工知能学会，情報処理学会各会員．\\
{\tt e-mail: otake@fw.ipsj.or.jp}
}
\bioauthor{山本 和英}{
1969年生．
1996年豊橋技術科学大学大学院工学研究科博士後期課程システム情報工学専攻修了．
博士（工学）．
同年より国際電気通信基礎技術研究所 (ATR) に所属し，
現在音声言語コミュニケーション研究所客員研究員（非常勤）．
1998年中国科学院自動化研究所国外訪問学者．
2002年より長岡技術科学大学電気系講師．
換言処理, 要約処理, 機械翻訳, 中国語及び韓国語処理の研究に従事．
言語処理学会，情報処理学会，ACL 各会員．\\
{\tt e-mail: yamamoto@fw.ipsj.or.jp}
}

\bioreceived{受付}
\biorevised{再受付}
\biorerevised{再々受付}
\bioaccepted{採録}

\end{biography}
\end{document}
