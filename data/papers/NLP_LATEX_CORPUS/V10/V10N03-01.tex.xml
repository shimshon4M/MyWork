<?xml version="1.0" ?>
<root>
  <title>SENSEVAL-2日本語辞書タスク</title>
  <author>白井清昭</author>
  <jabstract>SENSEVALは語義曖昧性解消を対象としたコンテストである．本論文では，第2回SENSEVAL(SENSEVAL-2)における日本語辞書タスクの概要について報告する．日本語辞書タスクでは，語の意味の区別(曖昧性)を岩波国語辞典によって定義した．参加者には，岩波国語辞典，訓練データ，評価データの3つが配布された．訓練データは，3,000個の新聞記事中の単語に正しい語義を付与したコーパスである．一方評価データは，参加者のシステムが語義を選択するべき単語を含んだ新聞記事である．評価単語の種類は，名詞50，動詞50，合わせて100個である．また各評価単語毎に100ずつ語義を選択するとしたため，評価単語の総数は10,000である．正解データは，評価対象となる10,000個の単語について，二名の作業者が独立に正しい語義を付与して作成した．この際，二者の語義が一致した割合は0.863であり，Cohenのは0.657であった．また，二者の語義が一致しなかった場合には，第三者が正しい語義を選んだ．日本語辞書タスクには，3団体7システムが参加した．ベースラインシステムのスコア(正解率)が0.726であるのに対し，一番成績の良かった参加者のシステムのスコアは0.786であった．</jabstract>
  <jkeywords>語義曖昧性解消，国語辞典，コーパスの注釈付け</jkeywords>
  <addtolength>-7mm</addtolength>
  <addtolength>-7mm</addtolength>
  <addtolength>-9mm</addtolength>
  <addtolength>-9mm</addtolength>
  <addtolength>-9mm</addtolength>
  <addtolength>-9mm</addtolength>
  <addtolength>-7mm</addtolength>
  <addtolength>-7mm</addtolength>
  <addtolength>-7mm</addtolength>
  <addtolength>-9mm</addtolength>
  <addtolength>-9mm</addtolength>
  <addtolength>-9mm</addtolength>
  <addtolength>-9mm</addtolength>
  <addtolength>-7mm</addtolength>
  <section title="はじめに">語義曖昧性解消(WordSenseDisambiguation，以下WSD)は機械翻訳，情報検索など，自然言語処理の多くの場面で必要となる基礎技術である．SENSEVALはWSDのコンテストであり，WSDの共通の評価データを作成し，その上で様々なシステム・手法を比較することによってWSDの研究・技術を向上させることを目的としている．SENSEVALは過去2回行われている．第1回のSENSEVAL~は1998年夏に，第2回のSENSEVAL-2~は2001年春に行われた．SENSEVAL-2では，9言語を対象に37研究グループが参加した．日本語を対象としたタスクとしては，辞書タスクと翻訳タスクの2つが行われた．辞書タスクでは語の意味の区別(曖昧性)を国語辞典によって定義し，翻訳タスクではこれを訳語選択によって定義した．本論文は，SENSEVAL-2の日本語辞書タスクについて，タスクの概要，データ，コンテストの結果について報告する．まず，日本語辞書タスクの概要について述べる．SENSEVAL-2では，タスクをlexicalsampletaskとallwordstaskに大別している．lexicalsampletaskは特定(数十〜数百)の単語だけをWSDの対象とし，allwordstaskでは評価テキスト中のすべての単語を対象とする．日本語辞書タスクはlexicalsampletaskである．以下，本論文では，評価の対象として選ばれた単語を評価単語と呼び，評価単語の評価データ中での実際の出現を評価インスタンス，または単にインスタンスと呼ぶ．辞書タスクでは，単語の語義を岩波国語辞典~の語義立てによって定義した．参加者は，テキスト中の評価インスタンスに対して，該当する語義を岩波国語辞典の語釈の中から選択し，その語釈に対応したID(以下，語義ID)を提出する．評価テキストは毎日新聞の1994年の新聞記事を用いた．語義を決定する評価単語の数は100と設定した．また，評価単語のそれぞれについて100インスタンスずつ語義を決めるとした．すなわち，評価インスタンスの総数は10,000である．本タスクには3団体，7システムが参加した．本論文の構成は以下の通りである．節では，辞書タスクで用いたデータの概要を述べる．節では，正解データの作成手順について述べる．また，正解データを作成する際，1つの評価インスタンスに対して二人の作業者が独立に正しい語義を選択したが，そのときの語義の一致率などについても報告する．節では，参加者のシステムの概要やスコアなどについて述べ，コンテストの結果に関する簡単な考察を行う．最後に節では，本論文のまとめを行う．</section>
  <section title="データ">本節では，辞書タスクで用いられた3つのデータ，岩波国語辞典，訓練データ，評価データについて述べる．</section>
  <subsection title="岩波国語辞典">節で述べたように，辞書タスクでは，岩波国語辞典によって語義を定義する．岩波国語辞典の見出しの数は60,321，語義の総数は85,870であり，一見出し当たりの平均語義数は1.42である．岩波国語辞典の語釈文の例を図~に示す．また，岩波国語辞典では，語義は階層構造を持つ．例えば，図~では，「理を欠くこと」という語義がア,イの語義の上位にある．階層構造の最大の深さは3である．辞書タスクでは，語義の定義として，形態素解析された岩波国語辞典の語釈文と，それに対応する語義IDが参加者に配布された．なお，語釈文の形態素解析結果は人手修正されている．</subsection>
  <subsection title="訓練データ">訓練データは，毎日新聞の1994年の3,000記事を解析したコーパスである．このコーパスに付与されている情報を以下にまとめる．形態素情報(分かち書き，品詞，読み，基本形)コーパスに含まれる形態素数は880,000である．これらは人手修正されている．UDCコード各記事には，テキストの分類カテゴリを表わす指標として，国際十進分類法(UniversalDecimalClassification,UDC)によるコード番号~が人手によって付与されている．語義情報各単語には，その単語の意味に該当する語義IDが付与されている．但し，語義IDはコーパスの全ての単語ではなく，以下の条件を満たす単語のみに付与されている．名詞，動詞，形容詞のいずれかである岩波国語辞典に見出しがある多義である語義が付与されている形態素の総数は148,558である．語義IDは全て人手によって付与された．また，1つの単語に語義IDを付与した人は1人である．複数の人が同じ単語に語義IDを付与し，それらを照合するといった作業は行われていない．</subsection>
  <subsection title="評価データ">評価データは，評価インスタンスとその正解となる語義IDを含むテキストである．評価テキストとして毎日新聞の1994年の記事を用いた．これらは訓練データの記事とは異なる．評価データに付与されている情報は以下の通りである．形態素情報(分かち書き，品詞)これらは自動解析されたものである．訓練データとは異なり，人手による修正はされていない．したがって，訓練データで学習したWSDシステムを評価データに適用した際，訓練データと評価データにおける分かち書きや品詞付けの違いによって誤りが生じる可能性がある．本来は評価データの形態素情報も人手修正するべきであったが，今回は準備期間が短かったために断念した．UDCコード訓練データにおけるUDCコードと同じ．語義情報(正解データ)評価インスタンスには正解となる語義IDが付与されている．また，訓練データとは異なり，1つのインスタンスに対して最低2人の人が語義IDを付与している(詳細は節を参照)．もちろん，この情報はコンテストの際には参加者に配布されない．</subsection>
  <subsection title="付加情報">本節で述べた岩波国語辞典，訓練データ，評価データの付加情報のほとんどは，RWCPによって作成され，1997年から既に公開されているデータである．訓練データの語義情報については，それ以外の情報についてはを参照していただきたい．これに対し，評価データの語義情報，すなわち正解となる語義IDのデータは，今回のコンテストのために新たに作成した．節では，正解データの作成過程ならびにその概要について述べる．</subsection>
  <section title="正解データの作成">正解データの作成は以下のように行った．まず評価単語を100語選定した．次に，各評価単語毎に100，合計10,000の評価インスタンスを選定した．さらに，各評価インスタンスに対し，のべ二人の作業者が語義IDを付与した．本節では，正解データ作成の過程，ならびに二者の語義IDの一致度などについて報告する．</section>
  <subsection title="評価単語，評価インスタンスの選定">評価単語を選定する際には，以下の点を考慮した．評価単語の品詞は名詞または動詞とした．訓練データにおける出現頻度が50以上の単語を評価単語とした．訓練データにおける語義の頻度分布のエントロピーE(w)を考慮した．E(w)の定義を式~()に示す．式()において，P(s_i|w)は単語wの語義がs_iとなる確率を表わす．E(w)の値が大きい単語は，語義の頻度分布が一様であり，語義を決定することが比較的難しい単語であると考えられる．一方，E(w)の値が小さい単語は，1つの語義が集中して現われる傾向が強く，語義の決定も比較的易しいと考えられる．評価単語の選定の際には，E(w)をWSDの難易度の目安とした．具体的には，以下の3つの難易度クラスを設定し，それぞれのクラスから評価単語をまんべんなく選ぶようにした．高難易度の単語クラス(E(w)1)中難易度の単語クラス(0.5E(w)&lt;1)低難易度の単語クラス(E(w)&lt;0.5)品詞別，難易度クラス別の評価単語数の内訳を表~に示す．また，評価単語の一覧を付録に示す．表~において，「語義数」は評価単語の岩波国語辞典における語義の数の平均を，「E(w)」は評価単語毎に求めた訓練データにおけるエントロピーの平均を表わす．次に，評価テキストである1994年の毎日新聞の記事中から評価インスタンスを選択した．これらの記事には，RWCPによって，形態素情報とUDCコードが付加情報として与えられている．各評価単語毎に，日付の古い記事から順に100語を選択し，それらを評価インタンスとした．ただし，訓練データの記事やUDCコードが付与されていない記事は対象外とした．評価単語は100語であるので，評価インタンスの総数は10,000である．また，評価インスタンスが選ばれた記事の総数は2,130となった．</subsection>
  <subsection title="語義IDの付与">10,000語の評価インスタンスに対して，その単語の意味に該当する語義IDを人手で付与した．語義IDを付与した作業者は6名で，言語学や辞書編纂の知識をある程度持っている人達である．また，本タスクの訓練データはRWCPが作成したコーパスを利用しているが，今回の作業者の中には訓練データへ語義IDを付与した人も含まれる．その手順を以下にまとめる．二人の作業者が独立に語義IDを付与する．その際の大まかな指針は以下の通りである．1つの語義IDを選択する．複数の語義IDは選択しない．どの階層の語義IDを選んでもよい．岩波国語辞典の語釈の中に該当するものがなければ，UNASSIGNABLE(該当無し)とする．ただし，なるべくUNASSIGNABLEとすることは避け，岩波国語辞典の語釈の中から語義IDを選択する．二者が選んだ語義IDが一致していれば，それを正解の語義IDとする．二者が選んだ語義IDが一致していなければ，第三者がその中から正しいと思われるものを選択する．ただし，第三者が，二者が選んだ語義ID以外の語義IDが正しいと判断した場合には，三者が選んだ3つの語義IDの全てを正解とする．語義IDを選択する際，どの階層の語義IDを選んでもよいとしたが，階層構造の末端以外の語義IDが選択されたインスタンスの数は94であり，階層の上の語義IDはあまり選ばれなかった．また，二者の語義IDが一致せず，第三者も違う語義IDを選んだインスタンスの数は28であり，その全体に対する割合は0.3,%と非常に少なかった．表~は，作業者二人が最初に選んだ語義IDの一致率を示したものである．評価インスタンス全体における一致率は86.3,%であった．名詞と動詞とで一致率を比較すると，それほど差が見られないことがわかる．また，名詞，動詞ともに，難易度の高いクラスの単語ほど一致率が低くなるが，その傾向は名詞よりも動詞の方が強いことがわかる．一方，表~は評価単語毎に計算したCohenの~の平均を示したものである．とは二系列のデータがどの程度一致しているかを測るためによく用いられる統計的尺度であり，式()で与えられる．&amp;=&amp;P_o-P_e1-P_e[2mm]P_o&amp;=&amp;_ix_iin[2mm]P_e&amp;=&amp;_i=1^kx_+ix_i+n^2eqnarray式()と()において，nはインスタンスの総数を，x_ijは作業者Aが語義i，作業者Bが語義jを与えたインスタンスの数を，x_i+,x_+iはそれぞれ作業者A,Bが語義iを与えたインスタンスの数を表わす．P_oは二人の作業者が同じ語義を付与した実際の確率であり，P_eは二人の作業者の語義付与が独立であるときに同じ語義を付与する期待値である．は両者の比から計算され，その値が大きいほど，二者の語義付与が一致していることを示す．その最大値は1である．評価単語100語のの平均は0.657であり，決して大きいとは言えない．このことは，岩波国語辞典の語釈の中から正しい語義を選択する作業は人間でも難しく，付与される語義が人によって揺れやすいことを示唆している．また，表~を見ると，表~の一致率とは異なり，名詞で難易度クラスがのときのの値が不自然に低いことがわかる．これは，一致率とが作業者間の語義の一致度に関して必ずしも同じ傾向を示すわけではないためと考えられる．例えば，100個の評価インスタンスに対して，作業者Aが語義~!_1を100回付与し，作業者Bが語義~!_1を99，語義~!_2を1回付与したとする．このとき，一致率は0.99と高いのに対し，は0となる．直観的には，の値はもっと大きいと考えられるが，これは統計的に信頼できるを求めるのに十分な量のサンプルがなかったためかもしれない．今回の作業では，1つの評価単語のインスタンスの数は100なので，を求める際のサンプル数nも100である．</subsection>
  <section title="コンテスト"/>
  <subsection title="参加団体">辞書タスクには3団体7システムが参加した．参加団体とそのシステムの特徴は以下の通りである．いずれのシステムも訓練データを利用した教師あり学習を行っている．通信総研(CRL)以下の4つのシステムによって回答を提出した．システム名とその概要は以下の通りである~．CRL1分類器としてサポートベクトルマシンを使用したシステム．学習に用いる素性としては，対象語及びその周辺にある単語の表記，品詞，構文情報，意味クラスやUDCコードなどを用いている．CRL2分類器としてシンプルベイズを使用したシステム．学習に用いる素性はCRL1と同じ．CRL3シンプルベイズとサポートベクトルマシンの混合モデル．個々の対象単語毎に，それぞれの分類器の精度を学習データを用いたクロスバリデーションによって評価し，精度の高い分類器を選択している．CRL4CRL3と同じような混合モデル．CRL1と同じ素性を用いたシンプルベイズとサポートベクトルマシン，CRL1の素性のうち構文素性を使わないシンプルベイズとサポートベクトルマシンの4つの分類器を使用している．奈良先端科学技術大学院大学(NAIST)以下の1つのシステムによって回答を提出した．その概要は以下の通りである~．NAIST分類器としてサポートベクトルマシンを用いている．学習に用いる素性は，対象語及びその周辺にある単語の表記や品詞の情報などである．さらに，独立成分分析(IndependentComponentAnalysis,ICA)や主成分分析(PrincipleComponentAnalysis,PCA)といった手法を用いて，素性空間の再構築を行っている．また，複数の素性空間によって学習された分類器を混合している．東京工業大学(TITECH)以下の2つのシステムによって回答を提出した．システム名とその概要は以下の通りである~．TITECH1分類器として決定リストを用いている．学習に用いる素性は，対象語及びその周辺にある単語の表記，品詞やUDCコードである．また，訓練データの他に，岩波国語辞典の語釈文中の例文からも決定リストの規則を学習している．TITECH2TITECH1とほぼ同じであるが，評価データに付与された形態素情報の誤りを自動修正することを試みている．</subsection>
  <subsection title="評価基準">SENSEVAL-2では，全ての言語のタスクにおける共通の評価基準として，以下に述べる3つの評価基準がある．辞書タスクでも，この評価基準に従ってシステムの評価を行った．fine-grainedscoring正解の語義IDとシステムの語義IDが完全に一致していれば正解とする．coarse-grainedscoring正解の語義IDとシステムの語義IDが，語義の階層構造の一番上の層で一致していれば正解とする．mixed-grainedscoring正解の語義IDとシステムの語義IDが完全に一致していなくても，語義の階層構造に従って部分的にスコアを与える方式で，fine-grainedとcoarse-grainedの中間にあたる．語義の階層構造において，正解の語義IDがシステムが出力する語義IDの親であるなら正解とみなす(図~(a))．逆に，システムの語義IDが正解の語義IDの親であるなら，といった部分的なスコアを与える(図~(b))．参加者は，1つの評価インスタンスに対して複数の語義IDを返してもよい．また，インスタンスの意味がその語義IDである確率をつけて返してもよい．確率をつけずに複数の語義IDを回答した場合には，全ての語義IDの確率が等しいとして取り扱われる．複数の語義IDが提出されたときには，各語義IDの確率に従ってスコアの重み付き平均をとる．また，正解の語義IDが複数ある場合は，正解の語義ID毎にスコアを計算し，その和を全体のスコアとする．</subsection>
  <subsection title="評価結果と考察">本項では，コンテストの結果とそれに関する考察について述べる．まず，システムの評価結果を図~に示す．図~において，``Baseline''は訓練データにおける最頻出語義を選択したときのスコアを，``Agree''は2人の作業者の語義IDが一致した割合を示している．参加システムの中で一番スコアが良かったのはCRL4である．しかし，どのシステムもベースラインを上回り，お互いのスコアの差も3,%程度で，それほど大きな差は見られなかった．3つの評価基準によるスコアのうち，coarse-grainedscoreはBaselineも含めてほとんど差はない．また，mixed-grainedとfine-grainedでは，システム間の差に見られる傾向はほとんど同じである．そのため，以後の考察はfine-grainedscoreについてのみ行う．</subsection>
  <subsubsection title="品詞別に見た評価結果">図は，品詞別に見た各システムのスコア(fine-grained)を示したグラフである．ベースラインを比べると，動詞の方が名詞よりも平均エントロピーが大きい(表~)にも関わらず，約3,%ほどスコアが高い．これは，特にエントロピーの高い評価単語が動詞にいくつかあり，それらが動詞の平均エントロピーを大きくしているためと考えられる．参加者のシステムを比べると，名詞のスコアは比較的差が小さいが，動詞のスコアは差が大きい．特に通信総研のシステムは動詞に対するスコアが高く，このことが全体の評価においても他のシステムよりもスコアが高い要因となっている．この原因を明らかにするために，CRL1が正解しNAISTとTITECH2が不正解であった動詞のインスタンス(139事例)を抜き出し，どのような動詞に対してCRLのシステムが正しく語義を決めることができるのかについて調査した．通信総研の4つのシステムの中からCRL1を選択したのは，CRL1が学習アルゴリズムとしてサポートベクトルマシンを採用したシステムであり，同じくサポートベクトルマシンを用いたNAISTと比較するためである．また，東工大の2つのシステムの中からTITECH1を選択したのは，TITECH1の方がTITECH2に比べて若干スコアが高いためである．調査の結果，「描く」「問う」などの動詞について，CRL1は他のシステムよりも正解率が高いことがわかった．これらの動詞の岩波国語辞典の語釈文と，各システムが出力した語義の頻度を図~に示す．しかし，これらの例を見ただけでは，CRL1がNAISTやTITECH1に比べて動詞のスコアが高い原因はわからない．原因のひとつとして考えられるのは，CRL1がNAISTやTITECH1と比べて，より多くの素性を用いていることである(項参照)．但し，この推察を裏付けるためには，各システムが個々のインスタンスに対して語義を決める際に手がかりとした素性を明らかにする必要がある．例えば，図~に示したインスタンスに対して，CRL1がNAISTやTITECH2が考慮していない構文素性などの素性を特に手がかりとしていることが明らかになれば，それらの素性が動詞の語義曖昧性解消に有効であると結論できる．但し，著者は，各システムが語義を決定する際に一番有力な手がかりとした素性に関する情報を持っていないため，上記の考察を具体的に検証することはできなかった．しかし，このように複数のWSDシステムの出力を詳細に比較することは，WSDに有効な素性を明らかにし，今後のWSDシステムの精度向上につながる可能性がある．</subsubsection>
  <subsubsection title="難易度別に見た評価結果">図~は，難易度別に見た各システムのスコア(fine-grained)を示したグラフである．クラスの単語については，ベースラインや作業者の一致率も含めて，各システムのスコアにほとんど差がない．これは，クラスの単語の語義を決定するタスクが比較的容易であったためと考えられる．これに対し，難易度の高いやの単語では，システム間の相違は全体での評価(図~)とほぼ同じである．</subsubsection>
  <subsubsection title="参加システムの比較">表~は，10,000語の対象インスタンスを(a)3つの参加者の全てのシステムが正解，(b)1システムだけが正解，(c)1システムだけが不正解，(d)全てのシステムが不正解，の4つに分類し，その内訳を調べたものである．通信総研と東工大のシステムとしては一番スコアの良いCRL4とTITECH1を選択し，比較の対象とした．また，NAISTは複数の語義を確率付きで回答するシステムであったが，出力された複数の語義の中に正解が含まれていればそのインスタンスに対して正解したとみなすと，NAISTのシステムのパフォーマンスが過大に評価され，システムの公平な比較ができない．そこで，確率の一番大きい語義のみを出力したとみなして他のシステムと比較することにした．ちなみに，確率の一番大きい語義のみを出力したときのNAISTのfine-grainedスコアは0.753である．表~(b),(c)から，参加者のシステムの回答が完全に一致していない事例の数は2,100であることがわかる．これらの事例から，それぞれのWSDシステムの特徴を考察することができる．例えば，表~(c)の事例は，他のシステムは正しい語義を出力したのに対し，あるシステムだけが正しい語義を出力できなかったことを表わす．付録に具体的な事例をいくつか紹介する．これらの事例を調べれば，現在のWSDシステムがうまく語義を決定することができない要因を探ることができる．但し，の考察で述べたように，各システムが個々のインスタンスに対して語義を決定する際に一番有力な手がかりとした素性に関する情報が必要である．また，自然言語処理における様々なタスクにおいて，votingと呼ばれる技術に関する研究が近年盛んに行われている．votingとは，複数のシステムの結果を混合することによりパフォーマンスを向上させる技術で，WSDに応用した研究もいくつか報告されている~．表~から，3つのシステムのいずれかに正解が含まれるインスタンスの割合は0.865であることがわかる．これは，3つのシステムの出力を組み合わせたときに得られるスコアの上限であり，単独のシステムよりも8,%程度精度が向上することを意味する．したがって，日本語のWSDにおいても，votingは精度を向上させる技術として有望であろう．</subsubsection>
  <subsubsection title="未知の語義">未知の語義とは，ここでは訓練データに1回も現われない語義を指す．今回のコンテストでは，未知の語義を正解とするインスタンスの数は108であった．未知の語義に対する各システムのスコアを表~に示す．各システムは訓練データを用いた機械学習を行っているため，未知の語義に対するスコアは全体のスコアに比べて著しく劣る．また，参加者のシステムを比較すると，東工大のシステムのスコアが特に高いことがわかる．東工大システムのみが正解した例を図~に示す．図~に示したように，[二]1が正解となるインスタンスに対して，TITECH1は正解と同じ語義を出力するのに対し，CRL4,NAISTは訓練データの頻出語義である[一]1アや[一]1ウを出力することがわかった．東工大のシステムが訓練データにない語義を正確に返すのは，語釈文中の例文からも決定リストの規則を学習しているためである．東工大システムの開発者に，「目」の語義を[二]1に決めた決定リストの規則を問い合わせたところ，式()の規則であることがわかった．式()の規則は，訓練データの例文ではなく，語義[二]1の語釈文中の例文「三番—の問題」から学習されたものである．このように，WSDシステムを構築する際に複数の知識源を利用すること---東工大システムの場合は訓練データ(語義タグ付きコーパス)と辞書の語釈文---は，WSDの精度向上に有効な手段であると考えられる．なお，訓練データ中に[二]1の語義が現われなかった理由は以下の通りである．訓練データにおいては，図~のような「目」の品詞は``名詞接尾''になっている．訓練データに語義を付与する際に，接尾語は対象外としたため，これらの単語には語義が付与されていない．ところが，評価データにおいては，RWCの品詞体系の大分類が``名詞''または``動詞''の単語を対象インスタンスとしたため，品詞が``名詞接尾''の単語も語義を決める対象となっている．このため，学習データに含まれない，接尾語としての意味[二]1を正解とするインスタンスが評価データに頻出した．このような状況は明らかにタスクの設定として不適切である．これは主催者側の過失であり，反省点としたい．</subsubsection>
  <subsubsection title="作業者の一致率とシステムのスコア">図~は，作業者が付与した語義の一致率を横軸，参加者の7システムの平均スコアを縦軸とし，100個の評価単語の結果をプロットしたグラフである．この図から，作業者の一致率とシステムの平均スコアには正の相関関係があることが読みとれる．しかし，評価単語の中には，作業者の一致率が高いのにも関わらず，システムの平均スコアが低い単語がある．具体的には「開発」「核」「精神」「乗る」「生まれる」「かかる」などである．これらの一部の単語の語義と，参加システムが出力した語義の頻度を付録~に示す．このような単語は，人間にとっては正しい語義を選択するのは易しいが，現状のWSDシステムではうまく語義を決めることができない単語である．したがって，特にこれらの単語について，システムが語義の選択を誤る原因を考察すれば，システムの性能を向上させることができると期待される．</subsubsection>
  <section title="おわりに">本論文では，SENSEVAL-2の日本語辞書タスクの概要について報告した．辞書タスクは，タスク設定自体はオーソドックスなものであるが，日本語を対象とした語義曖昧性解消に関するコンテストとしては始めての試みである．本タスクで用いられた正解データや参加者のシステムの結果は，SENSEVAL-2のウェブサイトで公開されている．これらのデータが今後の語義曖昧性解消の研究に貢献することを願う．</section>
  <section title="評価単語">辞書タスクの評価単語の一覧を以下に示す．また，日本語タスクには辞書タスクと翻訳タスクの2つがあるが，両タスクの評価単語は同じものを使用した．ただし，翻訳タスクの評価単語の数は40である．下表で*のついた単語は，翻訳タスクの評価単語でもある．</section>
  <section title="1つのシステムだけが不正解となる事例">CRL4,NAIST,TITECH1のうち，1つのシステムだけが不正解となった事例を紹介する．CRL4のみが不正解となる事例2mm100mmNAISTのみが不正解となる事例2mm100mmTITECH1のみが不正解となる事例2mm100mm*1cm</section>
  <section title="一致率が高くシステムのスコアが低い単語の例">2人の作業者の一致率が高いのにも関わらず，7つの参加システムの平均スコア(fine-grained)が低い単語の例を以下に示す．以下の表は，作業者の語義付けが一致したインスタンスに対する参加システムの出力語義の頻度分布である．CはCRL4，NはNAIST，TはTITECH1を表わす．「開発」(一致率0.93，参加システムの平均スコア0.493)2人の作業者がともに正解を1アとした場合．2mm100mm「核」(一致率0.97，参加システムのスコアの平均0.721)2人の作業者がともに正解を1とした場合．2mm100mm「乗る」(一致率0.91，参加システムのスコアの平均0.567)2人の作業者がともに正解を3アとした場合．2mm100mm「生まれる」(一致率1，参加システムのスコアの平均0.719)2人の作業者がともに正解を2とした場合．2mm100mmdocument</section>
</root>
