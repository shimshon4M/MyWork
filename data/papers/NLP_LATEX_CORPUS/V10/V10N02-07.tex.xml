<?xml version="1.0" ?>
<root>
  <title>SVDPACKCとその語義判別問題への利用</title>
  <author>新納浩幸佐々木稔</author>
  <jabstract>本論文ではフリーの特異値分解ツールSVDPACKCを紹介する．その利用方法を解説し，利用事例として語義判別問題を扱う．近年，情報検索では潜在的意味インデキシング（LatentSemanticIndexing，LSI）が活発に研究されている．LSIでは高次元の索引語ベクトルを低次元の潜在的な概念のベクトルに射影することで，ベクトル空間モデルの問題点である同義語や多義語の問題に対処する．そして概念のベクトルを構築するために，索引語文書行列に対して特異値分解を行う．SVDPACKCは索引語文書行列のような高次元かつスパースな行列に対して特異値分解を行うツールである．またLSIは，高次元の特徴ベクトルを重要度の高い低次元のベクトルに圧縮する技術であり，情報検索以外にも様々な応用が期待される．ここではSVDPACKCの利用事例として語義判別問題を取り上げる．SENSEVAL2の辞書タスクの動詞50単語を対象に実験を行った．LSIに交差検定を合わせて用いることで，最近傍法の精度を向上させることができた．また最近傍法をベースとした手法は，一部の単語に対して決定リストやNaiveBayes以上の正解率が得られることも確認できた．</jabstract>
  <jkeywords>特異値分解，SVDPACKC，LSI，語義の曖昧性解消</jkeywords>
  <subsubsection title=" matrix">ファイルの記述方法特異値分解の対象となる行列(A)は，ハーウェル・ボーイング形式（Harwell-Boeingformat）と呼ばれる列方向の圧縮形式を用いてファイルmatrixに記述する．これによりスパース行列を少ない記述量で簡単に表現することができる．最初に注意として，行列(A)の大きさを(mn)とした場合，SVDPACKCでは(mn)を仮定している．そのために，実際に特異値分解したい行列(A)の列数の方が行数よりも大きい場合は，行列(A)の転置行列(A^T)に対して特異値分解を行う必要がある．この場合，の(U)は(V)と置き換えなければならないこともある．ここでは，matrixの記述形式の説明として，以下のような行列(A)を考える．[A=[]]この行列に対して，１列目から順に非ゼロ要素を取り出し，以下のような表を作る．次にこの表から位置の行の部分だけを取り出す．次に各列の最初の非ゼロ要素のデータ番号を列ポインタに記述する．例えば，1列目であれば，最初の非ゼロ要素は|(1,1)|の|1.0|であり，これに対するデータ番号は1である．次に2列目であれば，最初の要素は|(2,2)|の|2.1|であり，これに対するデータ番号は3である．これを各列，順に記述したものが列ポインタである．つまり列ポインタの要素数は配列(A)の列数となる．ハーウェル・ボーイング形式とは，行列に対して，このような表を作り，列ポインタ，行位置，値を記述した形式である．これはスパース行列を圧縮した表現となる．matrixでは4行目以降に，列ポインタ，行位置，値が記述されている．列ポインタについては最後の非ゼロ要素のデータ番号に１を足したものが付け加えられることに注意する．先の例では以下のようになる．matrixの最初の4行は行列に関するその他の情報が記述されている．3行目以外意味はない．1行目はデータの名前であり，サンプルファイルのbelladit.Zを参考に適当につければよい．2行目，4行目も意味はなく，belladit.Zの通りに記述すれば良い．3行目は以下のように5つのデータを空白で区切って記述すればよい．この行の1列目（|rra|），5列目（0）はこの通り記述すれば良い．2列目（7）は行列(A)の行数，3列目（4）は列数，4列目（10）は非ゼロ要素の総数を記述する．結果，先の例において，matrixは以下のようになる．</subsubsection>
  <section title="はじめに">本論文はフリーの特異値分解ツールSVDPACKCを紹介する．その利用方法を解説し，利用事例として多義語の曖昧性解消問題（以下，語義判別問題と呼ぶ）を扱う．情報検索ではベクトル空間モデルが主流である．そこでは文書とクエリを索引語ベクトルで表し，それらベクトル間の距離をコサイン尺度などで測ることで，クエリと最も近い文書を検索する．ベクトル空間モデルの問題点として，同義語（synonymy）と多義語（polysemy）の問題が指摘されている．同義語の問題とは，例えば，``car''というクエリから``automobile''を含む文書が検索できないこと．多義語の問題とは，例えば，ネットサーフィンについてのクエリ``surfing''に対して，波乗りに関する文書が検索されることである．これらの問題は文書のベクトルに索引語を当てることから生じている．そこでこれら問題の解決のために文書のベクトルを潜在的（latent）な概念に設定することが提案されており，そのような技術を潜在的意味インデキシング（LatentSemanticIndexing，以下LSIと略す）と呼んでいる．LSIの中心課題はどのようにして潜在的な概念に対応するベクトルを抽出するかである．その抽出手法にLSIでは特異値分解を利用する．具体的には索引語文書行列(A)に対して特異値分解を行い，その左特異ベクトル（(AA^T)の固有ベクトル）を固有値の大きい順に適当な数(k)だけ取りだし)は(A)の転置行列を表す．，それらを潜在的な概念に対応するベクトルとする．LSIは魅力的な手法であるが，実際に試してみるには，特異値分解のプログラムが必要になる．低次元の特異値分解のプログラムは比較的簡単に作成できるが，現実の問題においては，高次元かつスパースな行列を扱わなくてはならない．このような場合，特異値分解のプログラムを作成するのはそれほど容易ではない．そこで本論文では，この特異値分解を行うためのツールSVDPACKCを紹介する．このツールによって高次元かつスパースな行列に対する特異値分解が行え，簡単にLSIを試すことができる．またLSIの情報検索以外の応用として，語義判別問題を取り上げSVDPACKCの利用例として紹介する．実験ではSENSEVAL2の日本語辞書タスクで出題された単語の中の動詞50単語を対象とした．LSIに交差検定を合わせて用いることで，最近傍法の精度を向上させることができた．また最近傍法をベースとした手法は，一部の単語に対して決定リストやNaiveBayes以上の正解率が得られることも確認できた．</section>
  <section title="LSI と 特異値分解">情報検索において，(m)個の索引語を予め決めておけば，文書は(m)次元の索引語ベクトルとして表現できる（ここでは列ベクトルとして考える）．検索対象の文書群が(n)個ある場合，各文書(d_i)に対する(m)次元の索引語ベクトルが(n)個並ぶので，(mn)の索引語文書行列(A)ができる．(mn)の行列(A)の特異値分解とは，行列(A)を以下のような行列(U)，()，(V)の積に分解することである．ここで，(U)は(mm)の直交行列，(V)は(nn)の直交行列である．また()は(mn)の行列であり，(rank(A)=r)とすると，対角線上に(r)個の要素(_1,_2,,_r)（ただし(_1_2,_r)）が並んだ行列である．それ以外の()の要素は0である．行列(A)の特異値分解を行ったとき，(U)は(A)の列ベクトルが張る空間の正規直交基底となっている．そして(U)内の列ベクトルは左側にあるものほど基底としての重要度が高い．そこで，(U)の最初の(k)個の列ベクトルを使って，索引語ベクトルを表すことにする．具体的には索引語ベクトルを(U)の最初の(k)個の列ベクトルに射影させればよい．つまり，(U)の最初の(k)個の列ベクトルで作成される(mk)の行列を(U_k)とおくと，索引語ベクトル(d)は(U_k^Td)によって(k)次元のベクトルで表現できることになる．実際の検索では，(m)次元の索引語ベクトルで表現されていたクエリ(q)も(U_k^Tq)によって(k)次元のベクトルで表現し，(U_k^Td)と(U_k^Tq)の距離によって，(d)と(q)の距離を測ればよい．例えば，(d)と(q)の距離(dist(d,q))は以下のように測ることができる．以上より，情報検索にLSIを利用するためには，索引語文書行列(A)の特異値分解から得られる行列(U)が求まれば良いことがわかる．</section>
  <section title="特異値分解ツール SVDPACKC"/>
  <subsection title="入手とコンパイル">行列(A)の特異値分解を行うツールがSVDPACKCである．SVDPACKCはフリーで配布されており，以下のURLから入手できる．SVDPACKCには特異値分解を行うC言語のプログラムが8つ入っている．この中で最も計算速度が優れているのはlas2と名付けられているランチョス法を使ったプログラムである．単に特異値分解の結果だけを得たいのであればlas2を利用すれば良く，他のプログラムをコンパイルする必要はない．ここではlas2だけをコンパイルする．las2.cをコンパイルする前に，las2.cの中でコメントアウトされているマクロ定数|UNIX_CREAT|を有効にしておく．/*#defineUNIX_CREAT*/→#defineUNIX_CREATverbatimこれによってlas2による特異値分解の結果がファイルに保存される．次に，las2.hのマクロ定数|LMTNW|，|NMAX|，|NZMAX|の値を適当に調整する．これらは取り得るメモリの最大サイズ，行列の最大サイズ，行列中の非ゼロ要素の最大個数を定義したものであり，どの程度の大きさの行列を扱えるかを示している．扱う問題や利用する計算機にもよるだろうが，本論文での実験では予め与えられている値の10倍の数に変更した．実行時に行列のサイズに関するエラーが出た場合は，これらの値を設定し直してコンパイルする．またlas2.cの中ではrandom関数を自前で用意しているために，stdlib.hで定義されているrandom関数と競合する場合がある．ここではlas2.c中のstdlib.hをincludeしないことにした．#include&lt;stdlib.h&gt;→/*#include&lt;stdlib.h&gt;*/verbatimコンパイルはmakefileのコンパイラの指定（CC）を利用するコンパイラに合わせて，以下を実行する．ここではLinuxのgccで問題なくコンパイルできた．またマニュアルには，CRAYY-MP，IBMRS/6000-550，DEC5000-100，HP9000-750，SPARCstation2及びMachintoshII/fxでSVDPACKCが動作することが記載されている．またWindows2000上のBorlandC++Compiler5.5を利用してもlas2.cをコンパイルできた．更にWindows2000上のcygwin+gcc環境でもコンパイルできた．特別なライブラリは使われていないので，多くの環境でコンパイル可能と思われる．</subsection>
  <subsection title="利用方法">las2は内部で２つのファイルを読む込む．１つは特異値分解を行いたい対象の行列が記述されたファイルmatrixであり，もう１つはパラメータを記述したファイルlap2である．この２つのファイルを適切に用意することで，las2を実行することができる．配布キットでは，サンプルの行列がbelladit.Zという名前の圧縮されたファイルとして提供されている．このファイルから，例えば以下のコマンドにより，matrixファイルを作り，las2を試してみる．lap2はこのサンプル用にキット内に用意されている．zcatbelladit.Z&gt;matrixverbatim実行は以下のように単にコマンド名だけを入力する．./las2verbatim結果はlav2とlao2というファイルに保存される．lav2には特異値分解したときのにおける(U)や(V)の配列が保存される．ただし，バイナリファイルなので直接見ることはできない．lao2には特異値分解したときのの()，つまり特異値の列とその他の情報（行列の大きさや実行時間等）が保存される．これはテキストファイルなので中身を確認できる．</subsection>
  <subsection title="出力結果の利用">las2による特異値分解の結果はlav2とlao2というファイルに保存される．lao2はテキストファイルであり，内容の確認は容易である．重要部分はファイルの下方に記載されている固有値の列である．この部分を適当に切り取って利用すればよい．また，lao2では，固有値は値の小さい順に出力されていることに注意すべきである．固有値は大きい方が重要な意味を持つため，ファイルの下方に書かれた特異値ほど重要である．lav2はバイナリファイルであり，las2のソースをみて出力形式を確認すれば，特異値分解結果の(U)や(V)を得ることが可能である．結局，lav2をテキストファイルの形式に変換する何らかのプログラムを自作する必要がある．ただし，そのようなプログラムを作成するのであれば，las2のソースを直接変更して，テキストファイルの形式で出力させた方が簡単である．例えば，las2.cの334行目で(U)が出力されているので，以下のように変更する．これで|lav2|に(U)の中身がテキスト形式で出力される．ファイル名も変更したいときは，148行目の以下の部分を書き換える．out2=&quot;lav2&quot;;verbatimまたlas2.cの756行目で(V)が出力されているので，以下のように変更する．以上のようにして，テキストファイルの形式で(U)や(V)を得ることができる．これらのファイルは，(U)や(V)の列ベクトルが，順に出力されている形になるが，その順序はlao2の固有値の順序に対応している．つまり，固有値の大きな順に(k)個の列ベクトルを取り出すときには，下方にあるベクトルから順に(k)個取り出さなければならないことに注意する．</subsection>
  <section title="語義判別問題への利用">ここでは情報検索以外へのLSIの応用として語義判別問題を取り上げる．SENSEVAL2の日本語辞書タスクで課題として出された動詞50単語を実験の対象とする．</section>
  <subsection title="最近傍法の利用">単語(w)は(k)個の語義を持つとし，各語義を(c_i)((i=1〜k))で表す．単語(w)の語義判別問題とは，テキストに単語(w)が現れたときに，その文脈上での単語(w)の語義(c_j)を判定する問題である．文脈を(m)個の素性のベクトル((f_1,f_2,,f_m))で表現した場合，この語義判別問題は分類問題となり，帰納学習の手法により解決できる．ここでは最近傍法（NearestNeighbor法，以下NN法と略す）を用いる．NN法は与えられた素性ベクトルと最も距離が近い訓練事例中の素性ベクトルを選び，そのクラスを出力とする手法である．今，単語(w)の訓練データの事例数を(n)とし，各事例を(m1)の素性ベクトル(d_i)（(i=1〜n)）で表す．すると訓練データ全体の集合は(mn)の行列(A)として表せる．実際の語義判別は，単語(w)の現れた文脈を素性ベクトル(q)で表し，以下の式で求められる訓練事例(d)のクラスを返すことで行える．[d=arg_d_idist(q,d_i)]ここでのNN法は，(dist(q,d_i))を単純なコサイン尺度で計算することにする．また行列(A)を特異値分解し，を利用して(dist(q,d_i))を定義したものをLSI法と呼ぶことにする．</subsection>
  <subsection title="素性の設定">ここでは語義判別の手がかりとなる属性として以下のものを設定した．e1直前の単語e2直後の単語e3前方の内容語２つまでe4後方の内容語２つまでe5e3の分類語彙表の番号e6e5の分類語彙表の番号verbatim例えば，語義判別対象の単語を「出す」として，以下の文を考える（形態素解析され各単語は原型に戻されているとする）．短い/コメント/を/出す/に/とどまる/た/。verbatimこの場合，「出す」の直前，直後の単語は「を」と「に」なので，|`e1=を'|，|`e2=に'|となる．次に，「出す」の前方の内容語は「短い」と「コメント」なので，|`e3=短い'|，|`e3=コメント'|の２つが作られる．またここでは句読点も内容語に設定しているので，「出す」の後方の内容語は「とどまる」「。」となり，|`e4=とどまる'|，|`e4=。'|が作られる．次に「短い」の分類語彙表の番号を調べると，|3.1920_1|である．ここでは分類語彙表の4桁目と5桁目までの数値をとることにした．つまり|`e3=短い'|に対しては，|`e5=3192'|と|`e5=31920'|が作られる．「コメント」は分類語彙表には記載されていないので，|`e3=コメント'|に対しては|e5|に関する素性は作られない．次は「とどまる」の分類語彙表を調べるはずだが，ここでは平仮名だけで構成される単語の場合，分類語彙表の番号を調べないことにしている．これは平仮名だけで構成される単語は多義性が高く，無意味な素性が増えるので，その問題を避けたためである．もしも分類語彙表上で多義になっていた場合には，それぞれの番号に対して並列にすべての素性を作成する．結果として，上記の例文に対しては以下の8つの素性が得られる．e1=を，e2=に，e3=短い，e3=コメント，e4=とどまる，e4=。，e5=3192，e5=31920，verbatim上記の例のようにして，「出す」に対するすべての訓練事例の素性を集め，各素性に1番から順に番号をつける．例えば，本論文の実験では「出す」に対しては978種類の素性があり，上記例の素性にはのように番号が振られた．以上より，上記例文に対する素性ベクトルは第21次元目，第60次元目，第134次元目，第302次元目，第379次元目，第406次元目，第789次元目，第790次元目の各要素が1であり，その他の要素がすべて0の978次元のベクトルとなる．</subsection>
  <subsection title="交差検定の利用">LSI法を利用した場合，NN法と比較して，必ずしも精度が向上するわけではなく，逆に精度が悪化する場合もある．そのため単純にすべての単語に対して，LSI法を用いることはできない．そこで交差検定を行い，次元圧縮の効果が確認できる単語のみLSI法を用いることにする．このようにNN法とLSI法を融合した手法をLSI+NN法と呼ぶことにする．ここでの交差検定では訓練データを4分割し，3つを訓練データ，1つをテストデータとする．組合わせを変えて，合計4通りの実験を行う．各実験では，NN法とLSI法のテストデータに対する正解率を測る．また特異値分解を使って圧縮する次元数は75とした．ただし行列(A)のランク数が75以下の場合は，行列(A)のランク数にした．付録のにlas2の結果をまとめている．そこではSENSEVAL2の日本語辞書タスクの動詞50単語の各単語に対する行列(A)の大きさ，非ゼロ要素の密度，圧縮した次元数，次元圧縮に要したメモリと時間が記されている．ただし，これらは4通りの実験での平均である．また次元圧縮に要したメモリと時間はlas2の出力ファイル|lao2|から得ている．各単語に対して，4通りの実験の平均をとった結果がである．特異値分解を利用することで正解率が向上したものは，で〇印のつけた以下の14単語である．これらに対してLSI法を用いることにする．iu,kawaru,kangaeru,kimaru,kuru,kuwaeru,susumu,chigau,nerau,fukumu,matomeru,mamoru,mitomeru,wakaruverbatim</subsection>
  <subsection title="特異値分解を用いた語義判別実験">実際は選出した14単語のみに対してLSI法を行えば良いが，交差検定の効果も示すために，すべての単語に対してLSI法を試みた．圧縮する次元数は100に設定した．ただし行列(A)のランク数が100以下の場合は，行列(A)のランク数にした．付録のにlas2の結果をまとめている．そこではSENSEVAL2の日本語辞書タスクの動詞50単語の各単語に対する行列(A)の大きさ，非ゼロ要素の密度，圧縮した次元数，次元圧縮に要したメモリと時間が記されている．また次元圧縮に要したメモリと時間はlas2の出力ファイル|lao2|から得ている．次にSENSEVAL2で配布されたテスト文を用いて正解率を測った結果がである．スコアの算出は解答結果に部分点を与えるmixed-gainedscoringという方式を用いている．わずかではあるが，LSI+NN法の方がNN法よりも精度が高かった．また選択した14単語のうちLSI法を利用することで精度が上がった単語（選択が正しかった単語）は8単語，下がった単語（選択が誤った単語）は6単語である．逆に選択しなかった36単語のうちNN法の方が精度が良かった単語（選択が正しかった単語）は31単語，LSI法の方が精度が良かった単語（選択が誤った単語）は5単語であった．つまり，全体の50単語のうち選択が正しかった単語は39単語（78,%），選択が誤った単語は11単語（22,%）である．単純にすべてNN法を選択した場合，選択が正しくなる単語は37単語（74,%），選択が誤る単語は13単語（26,%）であるため，交差検定の効果が確認できる．また同様の素性を用いて，決定リスト（DLと略す），NaiveBayes（NBと略す）を用いた判別も行った．結果をに示す．ほとんどの単語で，決定リストやNaiveBayesはNN法やLSI法よりも良い結果を出しているが，一部ではNN法やLSI法の方が良い値を出している．単語によってはNN法をベースとした方が良い場合もあることを示している．</subsection>
  <section title="考察">SVDPACKCが扱える行列の大きさについて述べておく．|las2.c|からメモリ割り当ての関数|malloc|の部分を抜き出してみると，|las2|は(mn)の行列の特異値分解を行うのに，大ざっぱに見積もって，(8mn)バイト強のメモリを必要としていることがわかるの(mn)倍である．sizeof(double)=8として(8mn)を得ている．．この点から考えると，必要メモリが約200Mバイトとなる(250001000)位の大きさが現実的な最大サイズだと思われる．確認のために，非ゼロ要素の密度が1,%であり，平均2のポアソン分布に従って，非ゼロ要素の整数値（1〜6）が配置されるような(250001000)の行列(A)を人工的に作成し，その行列に対してlas2で特異値分解を行ってみた．Pentium-41.5GHzメモリ512MバイトのLinux環境での実行時間は227秒，要したメモリは228Mバイトであった．この程度の大きさの行列であれば，実行時間は大きな問題にはならないと思われる．ただし，行列の大きさを変更して同じ条件で試してみるとの結果が得られた．メモリは行列のサイズにほぼ比例するが実行時間は指数関数的に増加しているので，実行時間の面からも，この程度の大きさの行列がSVDPACKCで扱える限度だと思われる．ちなみに(250002000)の行列ではメモリ不足で実行できなかった．ただし実験で用いたマシンにスワップは設定されていないことを注記しておく．スワップを利用すれば，更に大きな行列も扱えるが，その場合は実行時間の方で問題が生じるであろう．実際に情報検索で用いられる索引語ベクトルの次元数は少なくとも数十万単位になり，検索対象の文書も100万文書以上となるであろう．その場合の索引語文書行列(A)の大きさは巨大なスパース行列である．このような巨大な行列になると，SVDPACKCによって一気に特異値分解を行うのは不可能である．この問題に対しては最初に小さな行列で特異値分解を行い，その後に文書や索引語の追加に従って特異値分解の更新を行うfolding-inとよばれる手法や大規模な文書集合から文書をランダムサンプルし，そこから特異値分解を行う手法などが提案されている．あるいは概念ベクトルの選択に特異値分解以外の手法を使うアプローチもある（など）．最近では言語横断検索にもLSIが利用されているが，そこでも大規模な行列の特異値分解をどう行うかが問題点として上がっている．結局，現実の情報検索で現れるような大規模な行列に対しては，SVDPACKCを直接利用することはできない．しかしアイデアを試すための中規模の実験であれば，十分にその役割を果たせる．実験では圧縮する次元の数を交差検定では75に，実際の評価では100に固定している．この値は適当である．最適な次元数については様々な議論があるが，ここではSVDPACKCの利用例として紹介した実験であるため，最適な次元数を推定する処理は行わなかった．ちなみに実際の評価における次元数を100から増減させた場合の，実験結果をに示す．次元数を100に圧縮するといっても行列のランク数がそれ以下であれば，100よりも小さい数になるので，100のときに圧縮された次元数を基準に-20,-10,+10,+20と次元数を変更させて実験を行った．また表中にMAXとあるのは，行列のランク数で圧縮した場合を示す．これが圧縮できる次元の最大値である．大まかな傾向としては次元数が多い方が精度は高いようである．ただし最高精度を記録する次元数は個々の単語によって異っており，最適な次元数は問題に依存すると言える．LSIのアイデア自体は情報検索以外にも適用できる．ここでは語義判別問題への利用を試みた．他にも文書分類への応用が報告されている．このような教師付き学習のタイプでは，訓練事例数が大規模なものになることはないため，SVDPACKCが利用できる．また素性ベクトルの次元圧縮という手法は，統計学では主成分分析，パターン認識ではKarhunen-Lo`eve展開や線形判別法として知られている手法である．またデータマイニングの分野ではデータ数が非常に大きいために，現実的には機械学習手法を直接適用できないという問題がある．そのために類似する事例集合を抽象表現として表される事例に変換し，変換後の事例に対して機械学習手法を適用するDataSquashingという手法が使われる．これは索引語ベクトルではなく文書ベクトルに対する次元圧縮の手法に対応する．このように次元圧縮の手法は様々な分野で重要であり，新しい手法が次々と提案されている（例えばなど）．次元圧縮の手法として，特異値分解は古典的と言えるが，ベースとなる手法として容易に試すことのできる意味でもSVDPACKCは有用であろう．最後にLSIを分類問題に利用する場合の注意を述べておく．次元圧縮を行う手法は種々あるが，それらは2つに大別できる．１つは「表現のための次元圧縮」であり，もう１つは「判別のための次元圧縮」である．「表現のための次元圧縮」は素性ベクトルの分布全体のもつ情報をできるだけ反映できるように次元を圧縮する．一方，「判別のための次元圧縮」はクラスをできるだけ明確に分離できるように次元を圧縮する．主成分分析やKarhunen-Lo`eve展開は前者であり，線形判別法は後者である．そして特異値分解も「表現のための次元圧縮」に属する手法である．このため，特異値分解を行ったからといって必ずしも判別精度が高まることは保証されない．「表現のための次元圧縮」が判別精度向上に寄与できる問題は，非常に高次元のベクトルを扱う問題（例えば情報検索や音声・画像認識）だと思われる．このような場合，「表現のための次元圧縮」は``次元の呪い''に対抗できる可能性がある．あるいは次元数が多くなったときに素性間に共起性（依存関係）が生じる傾向があり，それが精度向上に悪影響を及ぼすが，そのような依存関係を解消できる可能性ももつ．特異値分解による次元圧縮が判別精度の向上に寄与できるかどうかは未知である．本研究では交差検定を行うことで，精度向上に寄与できそうな問題を選別しておくというアプローチをとった．しかし，LSI法は平均的には他の学習手法よりも精度が低かった．LSI法を語義判別問題に利用するためには，また別の工夫が必要になるだろう．１つの利用可能性としてはbagging手法の１つの学習器として使うことが考えられる．実際に，LSI法は数個の単語に関しては他の学習手法よりも精度が高かった．またNN法まで含めるとその数は更に増える．SENSEVAL2の辞書タスクでは様々な学習手法を融合して用いる手法が最も良い成績を納めた．そこでは，決定リスト，NaiveBayes，SVMの学習手法を用意し，交差検定の結果から単語毎に利用する学習手法を設定している．ここで用いたNN法やLSI法も１つの学習手法としてエントリーさせておけばよい．今回の実験では，多くの単語に対してLSI法はNN法よりも正解率が低かった．特に，語義判別の場合，素性ベクトルの次元数(m)に比べ，訓練事例数(n)が小さい．特異値分解で圧縮する次元数の最大値は(n)なので，この点でかなり制約があった．交差検定を用いることでNN法の精度を高めることができたが，他の学習手法と比べると精度の面ではまだ十分ではない．今後はNN法やLSI法が他の学習手法よりも正解率が高かった単語について，その原因を調査する．これによってLSIを語義判別問題のような分類問題に利用する方法を探ってゆく．またLSIが利用可能な他の問題を調べゆく．</section>
  <section title="おわりに">本論文ではフリーの特異値分解ツールSVDPACKCを紹介した．その利用方法を解説し，利用事例として語義判別問題を扱った．SENSEVAL2の辞書タスクの動詞50単語を対象に実験を行ったところ，交差検定を合わせて用いることで，NN法を改良できた．またNN法やLSI法は，一部の単語に対して決定リストやNaiveBayes以上の正解率が得られることも確認できた．特異値分解は，情報検索のLSIだけではなく，高次元の特徴ベクトルを重要な低次元のベクトルに射影する手法で必要とされる．このために様々な応用が期待される．今後はここでの実験の結果を詳しく調査し，LSIが利用可能な問題を調べてゆきたい．document</section>
</root>
