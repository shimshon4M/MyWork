



\makeatletter
\newcommand{\LEQQ}{}
\newcommand{\GEQQ}{}
\newcommand{\gl@align}[2]{}
\makeatother

\documentstyle[epsf,jnlpbbl]{jnlp_j_b5}

\setcounter{page}{111}
\setcounter{巻数}{10}
\setcounter{号数}{2}
\setcounter{年}{2003}
\setcounter{月}{4}
\受付{2002}{4}{17}
\再受付{2002}{10}{11}
\再々受付{2002}{11}{28}
\採録{2003}{1}{10}



\setcounter{secnumdepth}{2}

\title{単語意味属性を使用したベクトル空間法}
\author{池原  悟\affiref{KUEE} \and 村上 仁一\affiref{KUEE}  \and 木本 泰博\affiref{KUEE}}

\headauthor{池原，村上，木本}

\headtitle{単語意味属性を使用したベクトル空間法}

\affilabel{KUEE}{鳥取大学工学部知能情報工学科，鳥取市}
{Faculty of Engineering, Tottori University, Tottori-shi, 680-8552,\\ Japan}

\jabstract{
従来，ベクトル空間法において，ベクトルの基底数を削減するため，ベクトル
の基軸を変換する方法が提案されている．この方法の問題点として，計算量が
多く，大規模なデータベースへの適用が困難であることが挙げられる．
これに対して，本論文では，特性ベクトルの基底として，単語の代わりに単語の
意味属性（「日本語語彙大系」で規定された約2,710種類）を使用する方法を提
案する．この方法は，意味属性間の包含関係に基づいた汎化が可能で計算コスト
もきわめて少なく，容易にベクトルの次元数を圧縮できることが期待される．ま
た，単語の表記上の揺らぎに影響されず，同義語，類義語も考慮されるため，従
来の単語を基底とする文書ベクトル空間法に比べて，検索漏れを減少させること
が期待される．
BMIR-J2の新聞記事検索（文書数約5,000件）に適用した実験結果によれば，提案
した方法は，次元数の削減に強い方法であり，検索精度をあまり落とすことなく，
文書ベクトルの基底数を300〜600程度まで削減できることが分かった．また，単語を
基底とした文書ベクトルの方法と比べて高い再現率が得られることから，キーワー
ド検索におけるＫＷ拡張と同等の効果のあることが分かった．
}

\jkeywords{情報検索，ベクトル空間法，意味解析，意味属性，汎化}

\etitle{Vector Space Model bsased on \\Semantic Attributes of Words}
\eauthor{Satoru Ikehara\affiref{KUEE} \and Jin'ichi Murakami\affiref{KUEE} \and Yasuhiro Kimoto\affiref{KUEE}}


\eabstract{
In order to reduce the dimension of VSM (Vector Space Model) for
information retrieval and clustering, this paper proposes a new
method, Semantic-VSM, which uses the Semantic Attribute System defined
by ``A-Japanese-Lexicon'' instead of literal words used in conventional
VSM.
The attribute system consists of a tree structure with 2,710
attributes, which includes 400 thousand literal words. Using this
attribute system, the generalization of vector elements can be
performed easily based on upper-lower relationships of semantic
attributes, so that the dimension can easily be reduced at very low
cost. Synonyms are automatically assessed through semantic attributes
to improve the recall performance of retrieval systems.
Experimental results applying it to BMIR-J2 database of 5,079
newspaper articles showed that the dimension can be reduced from 2,710
to 300 or 600 with only a small degradation in performance. High
recall performance was also shown compared with conventional VSM.
}

\ekeywords{Information Retrieval, Vector Space Model, Semantic Analysis, Semantic Attribute, Generalization }

\begin{document}
\maketitle
\thispagestyle{empty}



\section{はじめに}

近年，情報化社会の進展と共に大量の電子化された文書情報の中から，自分が
必要とする文書情報を効率良く検索することの必要性が高まり，従来のKW検
索に加えて，全文検索，ベクトル空間法による検索，内容検索，意味的類似性
検索など，さまざまな文書検索技術の研究が盛んである．その中で，文書中の
単語を基底とする特性ベクトルによって文書の意味的類似性を表現するベクト
ル空間法は，利用者が検索要求を例文で与える方法であり，KW検索方式に比
べて検索条件が具体的に表現されるため，検索精度が良い方法として注目され
ている．

しかし，従来のベクトル空間法は，多数の単語を基底に用いるため，類似度計
算にコストがかかることや，検索要求文に含まれる単語数が少ないとベクトル
がスパースになり，検索漏れが多発する恐れのあることなどが問題とされてい
る．

これらの問題を解決するため，さまざまな研究が行われてきた．例えば，簡単
な方法としては， $tf \cdot idf$ 法 \cite{Salton}などによって，文書デー
タベース中での各単語の重要度を判定し，重要と判定された語のみをベクトル
の基底に使用する方法が提案されている．また，ベクトル空間法では，ベクト
ルの基底に使用される単語は，互いに意味的に独立であることが仮定されてい
るのに対して，現実の言語では，この仮定は成り立たない．そこで，基底の一
次結合によって，新たに独立性の高い基底を作成すると同時に，基底数を減少
させる方法として，KL法\cite{Borko}やLSI法
\cite{Golub}，\cite{Faloutsos}，\cite{Deerwester}が提案されている．

KL法は，単語間の意味的類似性を評価する方法で，クラスタリングの結果得ら
れた各クラスターの代表ベクトルを基底に使用する試みなどが行われている．
これに対して，LSI法は，複数の単語の背後に潜在的に存在する意味を発見し
ようとする方法で，具体的には，データベース内の記事の特性ベクトル全体か
らなるマトリックスに対して，特異値分解（SVD）の方法\cite{Golub}を応用し
て，互いに独立性の高い基底を求めるものである．この方法は，検索精度をあ
まり低下させることなく基底数の削減が可能な方法として着目され，数値デー
タベースへの適用\cite{Jiang}も試みられている．しかし，ベクトルの基底軸
を変換するための計算コストが大きいことが問題で，規模の大きいデータベー
スでは，あらかじめ，サンプリングによって得られた一定数の記事のみからベ
クトルの基底を作成する方法\cite{Deerwester}などが提案されている．この
ほか，単語の共起情報のスパース性の問題を避ける方法としては，擬似的なフィー
ドバック法（２段階検索法とも呼ばれる）
\cite{Burkley}，\cite{Kwok}なども試みられている．
また，ベクトルの基底とする単語の意味的関係を学習する方法としては，従来
から，Mining Term Association と呼ばれる方法があり，最近，インターネッ
ト文書から体系的な知識を抽出するのに応用されている\cite{Lin}．しかし，
現実には，単語間の意味的関係を自動的に精度良く決定することは容易でない．

これに対して，本論文では，ベクトル空間法において，検索精度をあまり低下さ
せることなく，基底数を容易に削減できることを期待して，単語の意味属性をベ
クトルの基底として使用する方法を提案する．この方法は，従来の特性ベクトル
において基底に使用されている単語を，その意味属性に置き換えるものである．
単語意味属性としては，日本語語彙大系\cite{池原}に定義された意味属性体系
を使用する．この意味属性体系は，日本語の名詞の意味的用法を約2,710種類に
分類したもので，属性間の意味的関係（is-a関係とhas-a関係）が12段の木構造
によって表現されている．また，日本語の単語30万語に対して，どの意味属性
（１つ以上）に属す単語であるかが指定されている．従って，本方式では，意味
属性相互の意味的上下関係を利用すれば，検索精度をあまり落とさずにベクトル
の基底数を削減できる．同時に基底として使用すべき必要最低限の意味属性の組
を容易に決定できることが期待される．また，本方式では，検索要求文に使用さ
れた単語とデータベース内の記事中の単語の意味的な類似性が，単語意味属性を
介して評価されるため，再現率の向上が期待できる．すなわち，従来の単語を基
底とした文書ベクトル空間法では，ベクトルの基底として使用された単語間のみ
での一致性が評価されるのに対して，本方式では，すべての単語（30万語）が検
索に寄与するため，検索漏れの防止に役立つと期待される．

本論文では，TRECに登録された情報検索テストコレクションBMIR-J2\cite{木谷}
を検索対象とした検索実験によって，従来の単語を用いた文書ベクトル空間法と
比較し，本方式の有効性を評価する．

\section{意味属性体系を基底とした文書ベクトル空間法}
\label{vector}

\subsection{単語を基底とした文書ベクトル空間法（W-VSM）}

従来の単語を基底とした文書ベクトル空間法では，文もしくは文書の意味的類似
性はその中に出現した単語の組で表現されるものと仮定している．すなわち，文
書の意味的類似性を表現するために使用される単語の番号を$ i \ \ (1 \leq ｉ 
\leq n) $ とし，文書中での単語$i$の重みを$w_i$とするとき，文書は，以下の
ような特性ベクトルで表わされる．

\begin{equation}
\label{equ1}
 V = (w_1,w_2, \cdots ,w_i, \cdots ,w_n)  
\end{equation}

ベクトルの基底とすべき単語としては，キーワード検索の場合と同様，データベー
ス全体に使用された単語の出現統計から， $tf \cdot idf$ 値などによって重要
と判断された単語を通常使用している．また，重み$w_i$ の値としては，文中に
単語$i$が使用されているときは1，使用されていないときは0とする方法と，文
中に使用された単語の出現頻度とする方法がある．また，各文書全体の相対的重
みはいずれも等しいとする立場から，ベクトルの絶対値が1となるよう正規化す
る方法も採られている．本論文では以後，式\ref{equ1}で与えられる特性ベクト
ルを「単語を基底とした文書ベクトル」と呼び，このベクトルを使用したベクト
ル空間法を「単語を基底とした文書ベクトル空間法 W-VSM（Word-Vector Space Model）」
と呼ぶ．

\subsection{単語を基底とした文書ベクトル空間法における意味的類似度}

単語を基底とした文書ベクトル空間法において，文書の意味類似度を特性ベクトルで表
現したとき，異なる文書$D_i$，$D_j$間の意味的類似性$ sim (D_i，D_j)$は，
それぞれの文書に対して求めた特性ベクトルの内積として，式\ref{equ2}のよう
に表現される．

\begin{equation}
\label{equ2}
   sim (D_i,D_j)=V_i \cdot V_j 
\end{equation}

但し，$V_i \cdot V_j $ は，それぞれ，文書$ D_i$，$D_j$ の特性ベクトルを表す．

従って，単語を基底とした文書ベクトル空間法を用いた情報検索では，利用者の与えた検索要求
文について特性ベクトルを求めて，データベースに収録された各文書の特性ベ
クトルとの間で類似度を計算し，類似度がある一定値以上の文書を抽出してい
る．また，単語を基底とした文書ベクトル空間法では，任意の文書をつなぎ合わせた文書につい
ての特性ベクトルも容易に合成できるから，類似度の高い文書相互間で順にベ
クトル合成を行えば，文書全体を容易にクラスタリングすることができる．

\subsection{単語意味属性を基底とした文書ベクトル空間法（S-VSM）}
\label{word_meaning}

本論文では，単語の代わりに，その単語の意味属性を使用する方法を提案する．
本方式では，すべての単語を $k$ 個の意味属性に分類したのち，分類された
意味属性を要素とする特性ベクトルによって文書の意味的類似性を表現する．すなわち，
対象とする文書 $ D_j$において$i$番目の意味属性を持つ単語全体の重み
$S_i$とするとき，文書$D_j$の特性ベクトル$V_j$は，次式で表現される．

\begin{equation}
\label{equ3}
  V_j=(S_1,S_2, \cdots ,S_i, \cdots ,S_k) 
\end{equation}

重み$S_i$の与え方としては，種々の方法が考えられるが，本論文では，単語を
基底とした文書ベクトル空間法の場合と同様，$tf \cdot idf$法の考えを適用し，
以下の方法で得られた値とする．

\begin{enumerate}

\item データベースに収録された文書全体に対して，意味属性 $S_i$に属す
単語が出現した頻度の合計を求め，それぞれの$idf$値を計算する．

\item 文書$ D_j $を対象に，意味属性$ S_i $に属す単語が出現した頻度の
合計を求め，その値を文書$ D_j $ の$tf$値とする．

\item 上記で得られた$tf$値と$idf$値から，意味属性$ S_i $ の$tf \cdot
idf $値を求める．

\item 上記で得られた$tf \cdot idf $ 値を$ | V_j | = 1 $となるように正規
化する．

\end{enumerate}

なお，式\ref{equ1}で与えられる特性ベクトルを「単語を基底とした文書ベクト
ル」と呼んだのに対して，以下では，式\ref{equ3}で与えられる特性ベクトルを
「単語意味属性を基底とした文書ベクトル」と呼び，このベクトルを使用したベ
クトル空間法を「単語意味属性を基底とした文書ベクトル空間法
S-VSM（Semantic-Vector Space Model）」と呼ぶ．

\subsection{日本語単語の意味属性体系}
\label{goitaikei}

単語の代わりに意味属性を基底とする文書ベクトル空間法では，単語の意味属性
についての分類体系が必要である．本論文では，意味分類体系として，最近，
「日本語語彙大系」\cite{池原}で提案された日本語名詞の意味属性体系を使用
する．図\ref{imi}に意味属性体系の一部を示す．

\begin{figure}[h]
 \begin{center}
 
\epsfile{file=figure/zu1.eps,scale=0.77}
 \end{center}
 \caption{一般名詞意味属性体系の一部 }
 
 \label{imi}
\end{figure}


この意味属性体系は，日本語名詞の意味的な用法を2,710種類の意味属性に分
類したもので，意味属性間の意味的関係（is-a関係，has-a関係）が，12段の
木構造で表現されている．また，単語意味辞書では，日本語名詞30万語のそれ
ぞれが，どのような意味属性を持つか（一つ以上）が規定されている．従って，
文書中に使用された名詞の出現頻度が分かれば，\ref{equ3}式のベクトルの要
素$S_i$は，$i$番目の意味属性を持つ名詞の出現頻度から\ref{word_meaning}
章で述べた方法で容易に求めることができる．

\subsection{単語意味属性を基底とした文書ベクトルの効果}

情報検索において，従来の単語を基底とした文書ベクトル空間法W-VSMに比べて，
単語意味属性を基底とする文書ベクトル空間法S-VSMが，どのような効果を持つ
かについて考察する．


\begin{enumerate}

\item ベクトルの基底数削減の可能性

従来の単語を基底とした文書ベクトル空間法では，ベクトルの基底として使用さ
れる名詞の意味は，互いに独立であることが仮定されているが，現実にはこの仮
定は成り立たない．そのため，ベクトルの基底数を減少させるため，従来，基底をクラ
スタリングで得られたクラスターのベクトルとしたり，特異値分解（SVD:
Singular Value Decomposition）によって得られたベクトルに変換する方法の研
究\cite{Deerwester}が行われてきた．しかし，これらの方法は，ベクトルの変
換に多くのコストを要する点が問題であった．

これに対して，本論文で基底として使用する単語意味属性は，木構造によって
意味的上下関係（is-a関係とhas-a関係）が規定されている（\ref{goitaikei}
節参照）．この関係を利用して基底数を削減するため，計算コストはきわめて
小さい．また，あまり効果のない意味属性を上位の意味属性で代用できるので，
削減された意味属性も検索精度に寄与できるため，従来の方法と同様，検索精
度をあまり落とすことなく，基底数が削減できると期待される．

\item 検索漏れの減少の可能性

従来の単語を基底とした文書ベクトル空間法では，文書中に出現した単語のうち，ベク
トルの基底として選択された単語のみがその文書の意味に反映する．そのため，
意味が同じであっても，表記が異なる語は別の語として判定される．また，同義
語や類義語を含む文書であっても，それが基底として採用されない限り検索の対
象とならない．

これに対して，単語意味属性を基底とした文書ベクトル空間法では，
\ref{word_meaning}，\ref{goitaikei}節で述べたように，30万語の名詞
が2,710の意味属性にマッピングされ，検索要求文に使用された単語とデータベース内の記事中の単語の意味的
な類似性が，単語意味属性を介して評価される．すなわち，文書中に使用される
語は，それが異表記語，同意語，同義語のいずれでであっても，その意味が特性
ベクトルに反映するため，情報検索において，検索漏れの削減の効果が期待でき
る．

\item 適合率の低下

単語意味属性を基底とした文書ベクトル空間法では，1つの単語に対して
意味属性による検索をおこなうため，複数の単語を検索する
のと等価になる．そのため適合率の低下が予想される．

\end{enumerate}

\section{必要最小限の意味属性の決定}

本論文では，\ref{vector}章で述べた単語意味属性を基底とした文書ベクトルの
効果を評価するため，日本語語彙大系で定義された意味属性2,710種類のすべて
を使用する場合と，その中から必要最小限と見られる意味属性を選択して使用す
る場合について検索精度を調べる．本章では，意味属性の上下関係に着目した汎
化により，ベクトルの基底として使用すべき必要最小限の意味属性の組を発見す
る方法について述べる．

\subsection{汎化の方法}
\label{generation}

汎化とは，モデル学習において，事例から規則を発見するための帰納的推論の一
種である．ここでは，特性ベクトルの基底数を減少させるため，情報検索に効果
が少ないと推定される意味属性を直属上位の意味属性に縮退させることを汎化と
呼ぶ．本論文では，汎化によって基底から削除された意味属性の$tf \cdot idf$
値 は，その上位の意味属性の$tf \cdot idf $値に加えることとする．汎化の対
象とする意味属性の選び方については，様々な方法が考えられるが，ここでは，
意味属性の粒度と意味属性の$tf \cdot idf $値に着目する方法を考える．

\subsubsection{粒度による汎化 \ \ S-VSM(g)}

ベクトルの基底に使用される意味属性は，12段の木構造からなり，下位になるほ
ど意味の粒度が相対的に小さくなる．そこで，各意味属性の位置する段数を粒度
と考え，ある一定の粒度より小さい意味属性を汎化する．図\ref{generation-fig}に，
8段以下の意味属性を7段目の意味属性に汎化する場合の例を示す．

\subsubsection{$tf \cdot idf$ 値による汎化 \ \ S-VSM(w)}

検索対象となるデータベースの文書全体での $tf \cdot idf$ 値の小さい意味属
性は，検索に寄与する程度が小さいと考えられるため， $tf \cdot idf$ 値の小
さい意味属性を汎化の対象とする．汎化によって削除された意味属性の $tf
\cdot idf$ 値は，上位直属の意味属性の $tf \cdot idf$ 値に加算する．直属
の意味属性が削除されているときは，さらに上位の意味属性の $tf \cdot idf$ 
値に加算する．図\ref{generation-fig}に， $tf \cdot idf$ 値が5以下の意味属性
を汎化する場合の例を示す．

\begin{figure}[ht]
 \begin{center}
  
\epsfile{file=figure/zu2.eps,scale=0.92}
 \end{center}
 \caption{汎化の方法}
 
 \label{generation-fig}
\end{figure}

\subsection{必要最小限の意味属性の決定}
\label{decision}

ベクトル空間法では，計算量を削減する観点から，ベクトルの基底数を減少させ
ることが望まれる．しかし，多くの場合，検索精度は低下させずに基底数を削減
することは困難である．そこで，前節で述べた汎化の方法を使用し，検索精度を
ある一定値以上低下させない範囲で，必要最小限の意味属性の組を求める方法を
考える．


\subsubsection{ 粒度による汎化 \ \  S-VSM(g)}

元来，特性ベクトルで表現される文書の意味の粒度は，ベクトルの基底に単語そ
のものを使用する場合が最も細かい．意味属性を使用する方法では，すでに意味
的な汎化が行われており，意味の粒度は荒くなっている．粒度に着目した汎化が
さらに進めば検索精度は次第に低下すると考えられるため，必要最小限の意味属
性の組を発見するには，順次，汎化を進めながら，検索精度の変化を追跡する必
要がある．その結果，検索精度が低下する直前に使用した意味属性の組を必要最
小限の組とする．

\subsubsection{  $tf \cdot idf$ 値による汎化 \ \ S-VSM(w) }

\begin{enumerate}

\item {基本的な考え方}

データベース中で $tf \cdot idf$ 値の小さい意味属性が汎化の対象となる．
しかし，必ずしも， $tf \cdot idf$ 値の小さい意味属性のすべてを汎化すれ
ばよいとは限らない．いま，データベース内に収録された文書が検索対象とな
る確率はすべて均等だとし，すべての文書を対象に求めた特性ベクトルの和を
$V_t$とする．$V_t$要素$n_i$の値の小さい意味属性$\# i$は，検索精度に与
える影響が少ないから，情報検索において少ないベクトルの基底数で高い検索
精度を得るには，各$n_i$の値がバランスしていることが必要である．すなわ
ち， $tf \cdot idf$ 値の低い意味属性でも，基底間でアンバランスが増大す
るような汎化は，検索精度低下の原因となるから，高い検索精度を得るために
は，データベース内の文書全体で出現する$tf \cdot idf$ 値がバランスする
ような意味属性を特性ベクトルの基底に選定する必要がある．

\item {汎化すべき意味属性の選択基準}

汎化すべき意味属性の選択基準について考える．データベース内に収録された
文書全体の特性ベクトルを式\ref{equ vec}とする．

\begin{equation}
\label{equ vec}
V_t=(n_1,n_2, \cdots ,n_i, \cdots ,n_m) 
\end{equation}

ただし，$n_i$は，意味属性$ \# i $に属す単語のデータベース全体での $tf
\cdot idf$ 値の和を，また，$m$は，基底に使用される意味属性の数を示す．
ここで，各$n_i$の値の均等さを変動によって評価するとし，評価関数$H$を以
下のように定義する．

\begin{equation}
  H = (n_1-n)^2  +(n_2-n)^2 + \cdots +(n_i-n)^2  + \cdots + (n_m-n)^2   
\end{equation}
  但し $n$は $n_i$の平均値とする．

\begin{equation}
   n  = \sum _{i=1} ^ {m} {n_i \over m}
\end{equation}

基底のバランスを向上させるには，$H$の値が，減少するような基底（意味属
性$ \# i$ ）を選んで汎化を行う．そこで，意味属性$ \# i $を汎化すること
を考える．$ \# i$ の直属上位の意味属性の番号を$ \# j $とすると，汎化で
は，$n_i$の値が$n_j$に加算され，基底数$m$が１だけ減少する．従って，こ
のようにして得られた$H$の値を$H_1$とすると，$H$と$H_1$の差は，近似的に
\footnote {$H_1$の平均$n'$は $n'= \frac{m}{m-1} n$ となる．ここで$m >> 1 $
とすると $\frac {m}{m-1} \simeq 1$から$n' \simeq n$となる．}式\ref{equ uvi1}が得られる．

\begin{equation}
\label{equ uvi1}
  H - H_1  \simeq (n_i-n)^2 +(n_j-n)^2 -(n_i+n_j-n)^2
\end{equation}

ここで，条件から，$ H - H_1 > 0 $ とおくと，
式\ref{equ uvi2}が得られる．

\begin{equation}
\label{equ uvi2}
     n_i \cdot n_j < n ^2 / 2
\end{equation}

以上から，汎化すべき基底は，その重 $tf \cdot idf$ 値と直属上位の基底の 
$tf \cdot idf$ 値との積が，基底の平均値の二乗値の半分以下になるものを
選択する．

\item 汎化の手順

具体的には，以下の手順で汎化を行う．

\begin{enumerate}

\item 汎化 
\label{itemitem1}

上下関係にある意味属性$ n_i$，$n_j$のすべての組のうち，積が最も小さい
組を汎化する．

\item  検索

情報検索実験を行い，検索精度を求める．

\item 停止

検索精度の低下がある閾値以下の値のときは(a)に戻り，それ以
上の時は，汎化を停止する．

\end{enumerate}


\end{enumerate}

\subsection{ベクトル変換のための計算コスト}

\ref{generation}節で述べた汎化は，基本となるベクトルの軸を変換する点で
は，従来のKL法やLSI法と同様である．そこで，そのために必要な計算コスト
を比較する．まず，ベクトルの基底数を削減するのに要するコストについて考
える．

データベースに収録された文書の総数と削減前のベクトルの基底数の和を$N$，
削減後のベクトル基底数を$k$とすると，単語を基底とした文書ベクトル空間法の場合，通常，
計算量は$N^4$もしくは$N^5$に比例すると言われている．LSI方式でも，特異
値分解に必要な計算量は，$N^2 \cdot k^3$に比例する．このため，データベー
スの規模が増大すると急激に計算量が増大することが大きな問題であった．
\footnote{
なお，大規模疎行列の固有値計算アルゴリズムはKrylov 部分空間法の一種で
ある Lanczos法を用いて高速に解くことができる．この方法は，一定次元の部
分空間における近似固有ベクトルをもとに新たに初期ベクトルを計算し，反復
法として用いることによって記憶容量を低減させる．反復 Lanczos法は，特
に疎行列を扱う場合に実際的な解法であるといえるが，固有値が近接している
場合，正確な計算が難しいことが知られている．}

これに対して，使用される意味属性の総数を$M$，段数を$d$（日本語語彙大系
の場合 $M=2,710$,$d=10$ ）とすると，単語意味属性を基底とした文書ベクトルにおいて粒度
による汎化を行うときは，必要最小限の意味属性の数を求めるための計算コス
トは，ほぼ，$M \cdot d$に比例する．また$tf \cdot idf$ 値による汎化の場
合は，ほぼ，$M^2-k^2$に比例する．また，必要最小限の意味属性の組が決定
した後，文書毎の特性ベクトルを変換することは容易で，その計算コストは，
文書量に比例する．

\section{実験}
\label{experiment}

本章では，情報検索の精度と必要最小限の意味属性の組に関する実験を行い，
提案した方式の特徴を評価する．

\subsection{使用する文書}

実験には，TRECに登録された「情報検索評価用テストコレクションBMIR-J2」
\cite{木谷}（以下BMIR-J2）を利用する．BMIR-J2は，1994年の毎日新聞より国
際十進分類（UDC）で経済，工学，工業技術一般に分類される記事5,080件を対
象とするもので，文書集合，検索要求，正解判定結果から構成される．検索要求
は「$ \sim $ に関する記事が欲しい」という形式で統一され，「$ \sim $」
の部分にあたる名詞句が列挙されている．また，検索要求に対する正解として，
下記の通り，2種類の記事が示されている．

\begin{enumerate}

\item ランクＡ

検索要求を主題としている記事

\item ランクＢ

検索要求の内容を少しでも含む記事

\end{enumerate}

\subsection{評価のパラメータ}

実験結果は，以下の4つのパラメータを用いて評価する．

\begin{enumerate}

\item $ sim $ :文書類似度 

 \begin{equation} sim (D_i，D_j) = V_i \cdot V_j  \end{equation}

 （但し，$V_i \cdot V_j$ は，それぞれ，文書$ D_i，D_j$の特性ベクトル）

\item  $R$ : 再現率（recall factor）

\begin{equation}
    R = {抽出された正解文書数 \over データベース中の正解文書数}
\end{equation}

\item $P$ : 適合率（precision factor）

\begin{equation}
    P = {抽出された正解文書数 \over 抽出された文書数 }
\end{equation}

\item  $F$ : 検索精度（f-parameter） 

\begin{equation}
\label{equ-12}
 F = { { (b ^ 2 + 1) \cdot P \cdot R} \over { b^2 \cdot P + R}}
\end{equation}

\end{enumerate}

但し，式\ref{equ-12}のパラメータ$b$は，$P$に対する$R$の相対的な重みを
示す．実験では，両者を対等と考え，$b=1$とする．

\subsection{実験の方法}

検索要求として新聞記事が与えられたとき，類似した新聞記事を検索すること
を考え，「主題が一致している新聞記事」を正解とする．具体的には，主題が
一致している記事（ランクA）のうちの1つを検索要求用の記事に使用し，デー
タベース内に収録された5,079件の記事の中から残りのランクAの記事を検索す
る．検索要求用の記事を替えながら，この手順を90回繰り返し，平均の検索精
度で評価する．従来の単語を基底とした文書ベクトル空間法による実験では，データベース記事
全体を対象に使用されている名詞の$tf \cdot idf$ 値を求め，その値の大き
い順に基底とする名詞を決定する．また，基底毎の重要度を考慮し，各単語ベ
クトルの要素の値には，単語の文書中での出現頻度に$idf$値を掛けた値を使
用する．なお，情報検索では，ある一定値以上の類似度を持つ文書を抽出の対
象とするが，その値の選び方によって，再現率，適合率の値は変化する．そこ
で，検索の精度評価では，いずれの場合も，$F$値が最大となるよう類似度を
設定する．

\section {実験結果}

\subsection{単語意味属性を基底とした文書ベクトル（S-VSM）と
単語を基底とした文書ベクトル空間法（W-VSM）の比較}

2,710種類の意味属性のすべてを使用する場合について情報検索実験を行い，
従来の単語を基底とした文書ベクトル空間法（W-VSM）と検索精度を比較する．

本論文の方法による検索精度を従来の単語を基底とした文書ベクトル空間法と比
べた結果を図\ref{result1}に示す．図\ref{result1}では，情報検索において類
似度$ \alpha $以上の文書を抽出した場合について，$ \alpha $と再現率$R$，
適合率$P$の関係を示している．なお，類似度0.7以上とする場合は，検索される
文書が1件程度となってしまい，信頼できないので，グラフから削除した．


\begin{figure}[ht]
 \begin{center}
 
\epsfile{file=figure/gra22.eps,scale=0.82}
 \end{center}
 \caption{記事の類似度と検索の精度の関係 }
 
 \label{result1}
\end{figure}


また，この結果から得られた類似度$ sim $と検索精度$F$値の関係を
図\ref{result2}に示す．


\begin{figure}[ht]
 \begin{center}
\epsfile{file=figure/gra1.eps,scale=0.92}
 \end{center}
 \caption{記事の類似度と検索の精度の関係}
 
 \label{result2}
\end{figure}

\newpage
これらの図から，以下のことが分かる．

\begin{enumerate}
  
\item 単語意味属性を基底とした
文書ベクトルは，単語を基底とした文書ベクトル空間法に比べて，すべての類似度
領域で，再現率が高く，適合率が低い．


\item 検索精度（$F$値の最大値）は，両者は殆ど変わらない．

\end{enumerate}

\subsection{粒度による汎化（S-VSM(g)）と$tf \cdot idf$値による汎化（S-VSM(w)）の比較}

\ref{decision}節で述べたような，意味属性の粒度に着目する汎化
（S-VSM(g)）と意味属性の$tf \cdot idf$ 値に着目する汎化（S-VSM(w)）の２
つの汎化の方法を用いて，ベクトルの基底として使用する意味属性の数と検索
精度の関係を求めた．その結果を図\ref{result3}に示す．また，このうち，
意味属性の$tf \cdot idf$ 値による汎化の場合について，汎化に伴う評価関
数$H$の値の変化を同図に示す．なお，ここでは，$b=1$ とした．


\begin{figure}[ht]
 \begin{center}
 
\epsfile{file=figure/gra3.eps,scale=0.86}
 \end{center}
 \caption{必要最小限の基底数の決定 }
 
 \label{result3}
\end{figure}



図\ref{result3}の結果から，検索精度をあまり低下させない範囲（ピーク値
の$10 \sim 20\% $以内の低下）で必要最小限のベクトルの基底数を求めると表
\ref{result4}の結果を得る．


\begin{table}[ht]
 \begin{center}
  \caption{必要最小限の基底数  }
 
  \label{result4}
  \begin{tabular}{|c|c|c|c|}  
\hline
\raisebox{-1.8ex}[0pt][0pt]{ 方式種別 }& \raisebox{-1.8ex}[0pt][0pt]{基底数削減の方法} & 
 \multicolumn{2} {|c|}{ 検索精度($F$値)低下の許容度 }  \\
\cline{3-4}
	   &                  & ピーク値の10\% & ピーク値の20\%  \\
\hline
 本論文の方法  & 粒度による汎化 ( W-VSM(g) ) & 900属性  & 700属性  \\
\cline{2-4}
(単語意味属性を基底)     & $tf \cdot idf$ 値による汎化 ( W-VSM(w) ) & 600属性  & 300属性  \\
\hline
従来の方法  & $tf \cdot idf $ による方法 & 2,200属性 & 1,500属性  \\
(単語を基底)	& & & \\
\hline
  \end{tabular}
(注)意味属性を上位8段まで使用 \\
 \end{center}
\end{table}

これらの図表から，以下のことが示される．

\begin{enumerate}

\item 今回の実験では，単語意味属性を基底とする文書ベクトル空間法は，従来の単語
を基底とする文書ベクトル空間法に比べて，基底数が小さくても検索精度が高い
ことが示された．

\item 汎化の方法としては，粒度による汎化（S-VSM(g)）より$tf \cdot idf$ 値による汎化（S-VSM(w)）の方が基底数
削減に強い．

\end{enumerate}

必要最小限の基底数について見ると，十分な基底数を持つ場合に比べて，検索精
度を10 $ \sim$ 20\% 以上低下させないためには，単語を基底とする文書ベクト
ル法では，最低2,000程度の基底数が必要とされるのに対して，単語意味属性ベ
クトルを用いて，$tf \cdot idf$ 値による汎化では，基底数を約300 $ \sim $
600程度まで削減できる．

\section{考察}

\subsection{単語意味属性を基底とする文書ベクトル空間法と単語を基底とする文書ベクトル空間法の比較}

実験によれば，単語意味属性を基底とする文書ベクトルは，単語を基底とする文
書ベクトル空間法に比べて，再現率が高いことが分かった．本研究では，簡単の
ため，文書中に使用された単語の頻度から直接，意味属性の$tf \cdot idf$ 値
を求めることとし，複数の意味を持つ単語は，その $tf \cdot idf $ 値 を，該
当する複数の意味属性に均等に加える方法を採った．これは，単語を基底とする
文書ベクトルの場合と同じ扱いであるが，適合率を減少させる原因の一つと考え
られる．これに対して，文書中で使用された単語の多義解消を行うことができれ
ば，適合率の向上は可能であると期待される．

ただし，今回の実験は，BMIR-J2における新聞記事検索のタスクであり，文書
数も約5,000件と少ない．今後検索する分野が変化したときや，文章数が増加
した場合，これらの結論が変わってくる可能性がある．今後，これらの課題を
追求する必要がある．

\subsection{意味属性体系}

本研究に使用した意味属性体系は，元来，単語多義の解消を狙って開発された
ものであり，複数の語義を持つ単語は，通常，複数の意味属性を持つ構造となっ
ている．日本語語彙大系には，さらに，動詞と名詞の共起関係から，両者の文
中での意味を特定するための仕組みが定義されている．そこで，これらの情報
を使用した意味解析によって文書中で使用された単語の意味的用法を決定し，
その後，該当する意味の重みを求めることにすれば，質問文と同じ単語が使用
された文書でも意味の異なる用法の文書は検索対象外とすることができるため，
適合率は向上すると期待される．

\subsection{基底数の削減のためのテストデータ}

実験では，提案した単語意味属性を基底とした文書ベクトル空間法と従来の単語
を基底とした文書ベクトル空間法が基底数削減にどれだけ強いかを比較評価する
ため，情報検索方式の評価実験用として広く提供されているBMIRのデータセット
（検索条件と正解付き）を使用した．実験はいずれもオープンテストである．こ
れは，以下に述べるように，この種の研究では大量のデータを対象としたオープ
ンテストは困難なためである．

すなわち，本手法では，検索対象とするデータベースに対して必要最小限の意味
属性の組を発見することが必要であるが，そのためには，汎化を進める過程で検
索精度が低下するかどうかの評価が必要で，検索結果についてあらかじめ正解を
知っておく必要がある．しかし，大規模なデータベースの場合，様々な検索条件
について，あらかじめ正しい検索結果を知ることは通常難しい（この事情は他の
検索方式の場合も同様である）．　

ところで，本方式を現実のシステムに応用するには，部分的な標本（例えば，数
千件程度の記事）に対して今回と同様の実験により必要最小限の意味属性の組決
める必要があるが，必要な意味属性の数（これを$n$個とする）が分かれば，$n$ 
個を構成する意味属性の種類は，データベースの規模に応じてさらに最適化する
ことができる．すなわち，大規模なデータベースでも単語の出現頻度統計を取る
のは比較的容易であるから，単語統計から作成された意味属性を初期値
とし，意味属性数が$n$となるまで汎化すれば，残った$n$個の意味属性は，デー
タベース全体から見て最適な組み合わせとなり，運用段階においてもクローズド
テストに近い検索精度が得られるものと期待できる．

\subsection{必要最小限の意味属性}

粒度による汎化（S-VSM(g)）において文書ベクトル数を700に汎化したときに残っ
た単語意味属性を調査した．この結果．汎化で残った単語意味属性の多くは，汎
化をする前に$tf \cdot idf$ 値が大きく，かつ頻度も多い単語意味属性であっ
た．例として「抽象」，「名詞」，「事」など意味意味属性であった．

\subsection{ $tf \cdot idf$値による汎化 と 頻度による汎化}

\ref{generation}節において，必要最小限の意味属性の決定するために，粒度に
よる汎化（S-VSM(g)）と $tf \cdot idf$ 値による汎化（S-VSM(w)）を示した．本
論文でしめした両方法は，どちらも$tf \cdot idf$ 値を利用しているが，単語
の出現頻度を利用する方法も考えられる．そこで\ref{experiment}節の実験の前
に，予備実験として，出現頻度を利用する場合と$tf \cdot idf$ 値を利用する
場合で$F$値がどちらが高くなるか調査した．この結果，$tf \cdot idf$ 値を利
用する場合のほうが良い値を示した．そのため，以後の実験においては$tf
\cdot idf$ 値を利用した．

なお，頻度が大きいが $tf \cdot idf$値が小さくなる単語意味属性は，，「自
尊，卑下」，「敬称（女）」，「自信，自棄」，「生産行程」，「自信」などであっ
た．また，比較的頻度が小さいが $tf \cdot idf$値が大きくなる単語意味属性
は，「乗り物」，「親、祖父母，先祖」，「親」，「報償」，「庭園」，「休
養」，「余暇」などであった．

\section{結論}

従来，ベクトル空間法では，文書の意味を表す特性ベクトルの基底に，文中に
現れる単語を使用していた．本論文では，単語の代わりに単語の意味属性
（「日本語語彙大系」で規定された約2,710件）を使用する方法を提案した．
また，意味属性間の意味的上下関係に着目したベクトルの基底の汎化の方法を
提案し，情報検索の精度を低下させない範囲で，基底数を削減する方法を示し
た．この方法は，基底数を削減するための計算量が，データベース内の文書数
に依存しないため，大規模なデータベースへの適用が容易である．

BMIR-J2の新聞記事（5,080記事）の検索に適用した実験結果によれば，提案し
た方法は，単語の表記上の揺らぎに影響されず，同義語や類義語の存在も検索
の対象となることから，従来の方法と比べて高い再現率が得られた．その反面，
単語を基底とする文書ベクトルの場合に比べて，不適切な記事を拾いやすく，適合率が低下する
ことが分かった．この効果は，キーワード検索においてシソーラスを使用した
ＫＷ拡張の効果に相当する．また，本方式は，次元数の削減に強い方法であり，
従来の方法に比べて，検索精度を落とすことなく，ベクトルの基底数を大幅に
削減できることが分かった．

今回は，単語の多義性の問題は考慮しなかったが，単語意味属性を基底とする文
書ベクトルでは，意味属性体系の持つ能力を用いて単語の多義を解消した後，基
底とする意味属性の重みを計算する方法が可能と考えられるので，今後は，この
方法についても検討していきたい．また，基底数をさらに削減する方法として，
意味属性体系の上位のノードから順に，不適切な記事を拾いやすいノードを選択
してベクトルの基底から削除する方法についても検討していく予定である．






\bibliographystyle{jnlpbbl}
\bibliography{jpaper}


\begin{biography}
\biotitle{略歴}
\bioauthor{池原悟}{

1967阪大・基礎工・電気卒．1969同大大学院修士課程修了．同年日本電信電
話公社入社，電気通信研究所配属．1996鳥取大学工学部教授に着任，現在に至
る．工博．この間，数式処理，トラフィック理論，自然言語処理の研究に従事．
1996スタンフォード大学客員教授．1982情報処理学会論文賞，1993同研究賞，
1995日本科学技術情報センタ賞（学術賞），同年人工知能学会論文賞受賞．
2002電気通信普及財団テレコムシステム技術賞，電
子情報通信学会，人工知能学会，言語処理学会，各会員．

}

\bioauthor{村上仁一}{

1984 筑波大・第３学群基礎工学類卒．1986同大大学院修了．同年NTT情報処理研
究所に入社．1991より1995 ATR自動翻訳電話研究所に出向．1998 鳥取大学助教授就
任，現在に至る．この間，自然言語処理，音声認識の研究に従事．日本音響学会，
電子情報通信学会，各会員．

}

\bioauthor{木本泰博}{

1998 鳥取大・工学部・知能情報工学科卒．2000 同大大学院修士課程修了．同年
積水ハウス入社．現在に至る．

}
\bioreceived{受付}
\biorevised{再受付}
\biorerevised{再々受付}
\bioaccepted{採録}

\end{biography}

\end{document}






