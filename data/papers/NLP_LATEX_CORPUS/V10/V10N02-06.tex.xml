<?xml version="1.0" ?>
<root>
  <title>単語意味属性を使用したベクトル空間法</title>
  <author>池原悟村上仁一木本泰博</author>
  <jabstract>従来，ベクトル空間法において，ベクトルの基底数を削減するため，ベクトルの基軸を変換する方法が提案されている．この方法の問題点として，計算量が多く，大規模なデータベースへの適用が困難であることが挙げられる．これに対して，本論文では，特性ベクトルの基底として，単語の代わりに単語の意味属性（「日本語語彙大系」で規定された約2,710種類）を使用する方法を提案する．この方法は，意味属性間の包含関係に基づいた汎化が可能で計算コストもきわめて少なく，容易にベクトルの次元数を圧縮できることが期待される．また，単語の表記上の揺らぎに影響されず，同義語，類義語も考慮されるため，従来の単語を基底とする文書ベクトル空間法に比べて，検索漏れを減少させることが期待される．BMIR-J2の新聞記事検索（文書数約5,000件）に適用した実験結果によれば，提案した方法は，次元数の削減に強い方法であり，検索精度をあまり落とすことなく，文書ベクトルの基底数を300〜600程度まで削減できることが分かった．また，単語を基底とした文書ベクトルの方法と比べて高い再現率が得られることから，キーワード検索におけるＫＷ拡張と同等の効果のあることが分かった．</jabstract>
  <jkeywords>情報検索，ベクトル空間法，意味解析，意味属性，汎化</jkeywords>
  <subsubsection title="粒度による汎化   S-VSM(g)">ベクトルの基底に使用される意味属性は，12段の木構造からなり，下位になるほど意味の粒度が相対的に小さくなる．そこで，各意味属性の位置する段数を粒度と考え，ある一定の粒度より小さい意味属性を汎化する．図に，8段以下の意味属性を7段目の意味属性に汎化する場合の例を示す．</subsubsection>
  <section title="はじめに">近年，情報化社会の進展と共に大量の電子化された文書情報の中から，自分が必要とする文書情報を効率良く検索することの必要性が高まり，従来のKW検索に加えて，全文検索，ベクトル空間法による検索，内容検索，意味的類似性検索など，さまざまな文書検索技術の研究が盛んである．その中で，文書中の単語を基底とする特性ベクトルによって文書の意味的類似性を表現するベクトル空間法は，利用者が検索要求を例文で与える方法であり，KW検索方式に比べて検索条件が具体的に表現されるため，検索精度が良い方法として注目されている．しかし，従来のベクトル空間法は，多数の単語を基底に用いるため，類似度計算にコストがかかることや，検索要求文に含まれる単語数が少ないとベクトルがスパースになり，検索漏れが多発する恐れのあることなどが問題とされている．これらの問題を解決するため，さまざまな研究が行われてきた．例えば，簡単な方法としては，tfidf法などによって，文書データベース中での各単語の重要度を判定し，重要と判定された語のみをベクトルの基底に使用する方法が提案されている．また，ベクトル空間法では，ベクトルの基底に使用される単語は，互いに意味的に独立であることが仮定されているのに対して，現実の言語では，この仮定は成り立たない．そこで，基底の一次結合によって，新たに独立性の高い基底を作成すると同時に，基底数を減少させる方法として，KL法やLSI法，，が提案されている．KL法は，単語間の意味的類似性を評価する方法で，クラスタリングの結果得られた各クラスターの代表ベクトルを基底に使用する試みなどが行われている．これに対して，LSI法は，複数の単語の背後に潜在的に存在する意味を発見しようとする方法で，具体的には，データベース内の記事の特性ベクトル全体からなるマトリックスに対して，特異値分解（SVD）の方法を応用して，互いに独立性の高い基底を求めるものである．この方法は，検索精度をあまり低下させることなく基底数の削減が可能な方法として着目され，数値データベースへの適用も試みられている．しかし，ベクトルの基底軸を変換するための計算コストが大きいことが問題で，規模の大きいデータベースでは，あらかじめ，サンプリングによって得られた一定数の記事のみからベクトルの基底を作成する方法などが提案されている．このほか，単語の共起情報のスパース性の問題を避ける方法としては，擬似的なフィードバック法（２段階検索法とも呼ばれる），なども試みられている．また，ベクトルの基底とする単語の意味的関係を学習する方法としては，従来から，MiningTermAssociationと呼ばれる方法があり，最近，インターネット文書から体系的な知識を抽出するのに応用されている．しかし，現実には，単語間の意味的関係を自動的に精度良く決定することは容易でない．これに対して，本論文では，ベクトル空間法において，検索精度をあまり低下させることなく，基底数を容易に削減できることを期待して，単語の意味属性をベクトルの基底として使用する方法を提案する．この方法は，従来の特性ベクトルにおいて基底に使用されている単語を，その意味属性に置き換えるものである．単語意味属性としては，日本語語彙大系に定義された意味属性体系を使用する．この意味属性体系は，日本語の名詞の意味的用法を約2,710種類に分類したもので，属性間の意味的関係（is-a関係とhas-a関係）が12段の木構造によって表現されている．また，日本語の単語30万語に対して，どの意味属性（１つ以上）に属す単語であるかが指定されている．従って，本方式では，意味属性相互の意味的上下関係を利用すれば，検索精度をあまり落とさずにベクトルの基底数を削減できる．同時に基底として使用すべき必要最低限の意味属性の組を容易に決定できることが期待される．また，本方式では，検索要求文に使用された単語とデータベース内の記事中の単語の意味的な類似性が，単語意味属性を介して評価されるため，再現率の向上が期待できる．すなわち，従来の単語を基底とした文書ベクトル空間法では，ベクトルの基底として使用された単語間のみでの一致性が評価されるのに対して，本方式では，すべての単語（30万語）が検索に寄与するため，検索漏れの防止に役立つと期待される．本論文では，TRECに登録された情報検索テストコレクションBMIR-J2を検索対象とした検索実験によって，従来の単語を用いた文書ベクトル空間法と比較し，本方式の有効性を評価する．</section>
  <section title="意味属性体系を基底とした文書ベクトル空間法"/>
  <subsection title="単語を基底とした文書ベクトル空間法（W-VSM）">従来の単語を基底とした文書ベクトル空間法では，文もしくは文書の意味的類似性はその中に出現した単語の組で表現されるものと仮定している．すなわち，文書の意味的類似性を表現するために使用される単語の番号をi(1ｉn)とし，文書中での単語iの重みをw_iとするとき，文書は，以下のような特性ベクトルで表わされる．ベクトルの基底とすべき単語としては，キーワード検索の場合と同様，データベース全体に使用された単語の出現統計から，tfidf値などによって重要と判断された単語を通常使用している．また，重みw_iの値としては，文中に単語iが使用されているときは1，使用されていないときは0とする方法と，文中に使用された単語の出現頻度とする方法がある．また，各文書全体の相対的重みはいずれも等しいとする立場から，ベクトルの絶対値が1となるよう正規化する方法も採られている．本論文では以後，式で与えられる特性ベクトルを「単語を基底とした文書ベクトル」と呼び，このベクトルを使用したベクトル空間法を「単語を基底とした文書ベクトル空間法W-VSM（Word-VectorSpaceModel）」と呼ぶ．</subsection>
  <subsection title="単語を基底とした文書ベクトル空間法における意味的類似度">単語を基底とした文書ベクトル空間法において，文書の意味類似度を特性ベクトルで表現したとき，異なる文書D_i，D_j間の意味的類似性sim(D_i，D_j)は，それぞれの文書に対して求めた特性ベクトルの内積として，式のように表現される．但し，V_iV_jは，それぞれ，文書D_i，D_jの特性ベクトルを表す．従って，単語を基底とした文書ベクトル空間法を用いた情報検索では，利用者の与えた検索要求文について特性ベクトルを求めて，データベースに収録された各文書の特性ベクトルとの間で類似度を計算し，類似度がある一定値以上の文書を抽出している．また，単語を基底とした文書ベクトル空間法では，任意の文書をつなぎ合わせた文書についての特性ベクトルも容易に合成できるから，類似度の高い文書相互間で順にベクトル合成を行えば，文書全体を容易にクラスタリングすることができる．</subsection>
  <subsection title="単語意味属性を基底とした文書ベクトル空間法（S-VSM）">本論文では，単語の代わりに，その単語の意味属性を使用する方法を提案する．本方式では，すべての単語をk個の意味属性に分類したのち，分類された意味属性を要素とする特性ベクトルによって文書の意味的類似性を表現する．すなわち，対象とする文書D_jにおいてi番目の意味属性を持つ単語全体の重みS_iとするとき，文書D_jの特性ベクトルV_jは，次式で表現される．重みS_iの与え方としては，種々の方法が考えられるが，本論文では，単語を基底とした文書ベクトル空間法の場合と同様，tfidf法の考えを適用し，以下の方法で得られた値とする．データベースに収録された文書全体に対して，意味属性S_iに属す単語が出現した頻度の合計を求め，それぞれのidf値を計算する．文書D_jを対象に，意味属性S_iに属す単語が出現した頻度の合計を求め，その値を文書D_jのtf値とする．上記で得られたtf値とidf値から，意味属性S_iのtf値を求める．上記で得られたtfidf値を|V_j|=1となるように正規化する．なお，式で与えられる特性ベクトルを「単語を基底とした文書ベクトル」と呼んだのに対して，以下では，式で与えられる特性ベクトルを「単語意味属性を基底とした文書ベクトル」と呼び，このベクトルを使用したベクトル空間法を「単語意味属性を基底とした文書ベクトル空間法S-VSM（Semantic-VectorSpaceModel）」と呼ぶ．</subsection>
  <subsection title="日本語単語の意味属性体系">単語の代わりに意味属性を基底とする文書ベクトル空間法では，単語の意味属性についての分類体系が必要である．本論文では，意味分類体系として，最近，「日本語語彙大系」で提案された日本語名詞の意味属性体系を使用する．図に意味属性体系の一部を示す．この意味属性体系は，日本語名詞の意味的な用法を2,710種類の意味属性に分類したもので，意味属性間の意味的関係（is-a関係，has-a関係）が，12段の木構造で表現されている．また，単語意味辞書では，日本語名詞30万語のそれぞれが，どのような意味属性を持つか（一つ以上）が規定されている．従って，文書中に使用された名詞の出現頻度が分かれば，式のベクトルの要素S_iは，i番目の意味属性を持つ名詞の出現頻度から章で述べた方法で容易に求めることができる．</subsection>
  <subsection title="単語意味属性を基底とした文書ベクトルの効果">情報検索において，従来の単語を基底とした文書ベクトル空間法W-VSMに比べて，単語意味属性を基底とする文書ベクトル空間法S-VSMが，どのような効果を持つかについて考察する．ベクトルの基底数削減の可能性従来の単語を基底とした文書ベクトル空間法では，ベクトルの基底として使用される名詞の意味は，互いに独立であることが仮定されているが，現実にはこの仮定は成り立たない．そのため，ベクトルの基底数を減少させるため，従来，基底をクラスタリングで得られたクラスターのベクトルとしたり，特異値分解（SVD:SingularValueDecomposition）によって得られたベクトルに変換する方法の研究が行われてきた．しかし，これらの方法は，ベクトルの変換に多くのコストを要する点が問題であった．これに対して，本論文で基底として使用する単語意味属性は，木構造によって意味的上下関係（is-a関係とhas-a関係）が規定されている（節参照）．この関係を利用して基底数を削減するため，計算コストはきわめて小さい．また，あまり効果のない意味属性を上位の意味属性で代用できるので，削減された意味属性も検索精度に寄与できるため，従来の方法と同様，検索精度をあまり落とすことなく，基底数が削減できると期待される．検索漏れの減少の可能性従来の単語を基底とした文書ベクトル空間法では，文書中に出現した単語のうち，ベクトルの基底として選択された単語のみがその文書の意味に反映する．そのため，意味が同じであっても，表記が異なる語は別の語として判定される．また，同義語や類義語を含む文書であっても，それが基底として採用されない限り検索の対象とならない．これに対して，単語意味属性を基底とした文書ベクトル空間法では，，節で述べたように，30万語の名詞が2,710の意味属性にマッピングされ，検索要求文に使用された単語とデータベース内の記事中の単語の意味的な類似性が，単語意味属性を介して評価される．すなわち，文書中に使用される語は，それが異表記語，同意語，同義語のいずれでであっても，その意味が特性ベクトルに反映するため，情報検索において，検索漏れの削減の効果が期待できる．適合率の低下単語意味属性を基底とした文書ベクトル空間法では，1つの単語に対して意味属性による検索をおこなうため，複数の単語を検索するのと等価になる．そのため適合率の低下が予想される．</subsection>
  <section title="必要最小限の意味属性の決定">本論文では，章で述べた単語意味属性を基底とした文書ベクトルの効果を評価するため，日本語語彙大系で定義された意味属性2,710種類のすべてを使用する場合と，その中から必要最小限と見られる意味属性を選択して使用する場合について検索精度を調べる．本章では，意味属性の上下関係に着目した汎化により，ベクトルの基底として使用すべき必要最小限の意味属性の組を発見する方法について述べる．</section>
  <subsection title="汎化の方法">汎化とは，モデル学習において，事例から規則を発見するための帰納的推論の一種である．ここでは，特性ベクトルの基底数を減少させるため，情報検索に効果が少ないと推定される意味属性を直属上位の意味属性に縮退させることを汎化と呼ぶ．本論文では，汎化によって基底から削除された意味属性のtfidf値は，その上位の意味属性のtfidf値に加えることとする．汎化の対象とする意味属性の選び方については，様々な方法が考えられるが，ここでは，意味属性の粒度と意味属性のtfidf値に着目する方法を考える．</subsection>
  <subsection title="ベクトル変換のための計算コスト">節で述べた汎化は，基本となるベクトルの軸を変換する点では，従来のKL法やLSI法と同様である．そこで，そのために必要な計算コストを比較する．まず，ベクトルの基底数を削減するのに要するコストについて考える．データベースに収録された文書の総数と削減前のベクトルの基底数の和をN，削減後のベクトル基底数をkとすると，単語を基底とした文書ベクトル空間法の場合，通常，計算量はN^4もしくはN^5に比例すると言われている．LSI方式でも，特異値分解に必要な計算量は，N^2k^3に比例する．このため，データベースの規模が増大すると急激に計算量が増大することが大きな問題であった．これに対して，使用される意味属性の総数をM，段数をd（日本語語彙大系の場合M=2,710,d=10）とすると，単語意味属性を基底とした文書ベクトルにおいて粒度による汎化を行うときは，必要最小限の意味属性の数を求めるための計算コストは，ほぼ，Mdに比例する．またtfidf値による汎化の場合は，ほぼ，M^2-k^2に比例する．また，必要最小限の意味属性の組が決定した後，文書毎の特性ベクトルを変換することは容易で，その計算コストは，文書量に比例する．</subsection>
  <section title="実験">本章では，情報検索の精度と必要最小限の意味属性の組に関する実験を行い，提案した方式の特徴を評価する．</section>
  <subsection title="使用する文書">実験には，TRECに登録された「情報検索評価用テストコレクションBMIR-J2」（以下BMIR-J2）を利用する．BMIR-J2は，1994年の毎日新聞より国際十進分類（UDC）で経済，工学，工業技術一般に分類される記事5,080件を対象とするもので，文書集合，検索要求，正解判定結果から構成される．検索要求は「に関する記事が欲しい」という形式で統一され，「」の部分にあたる名詞句が列挙されている．また，検索要求に対する正解として，下記の通り，2種類の記事が示されている．ランクＡ検索要求を主題としている記事ランクＢ検索要求の内容を少しでも含む記事</subsection>
  <subsection title="評価のパラメータ">実験結果は，以下の4つのパラメータを用いて評価する．sim:文書類似度（但し，V_iV_jは，それぞれ，文書D_i，D_jの特性ベクトル）R:再現率（recallfactor）P:適合率（precisionfactor）F:検索精度（f-parameter）但し，式のパラメータbは，Pに対するRの相対的な重みを示す．実験では，両者を対等と考え，b=1とする．</subsection>
  <subsection title="実験の方法">検索要求として新聞記事が与えられたとき，類似した新聞記事を検索することを考え，「主題が一致している新聞記事」を正解とする．具体的には，主題が一致している記事（ランクA）のうちの1つを検索要求用の記事に使用し，データベース内に収録された5,079件の記事の中から残りのランクAの記事を検索する．検索要求用の記事を替えながら，この手順を90回繰り返し，平均の検索精度で評価する．従来の単語を基底とした文書ベクトル空間法による実験では，データベース記事全体を対象に使用されている名詞のtfidf値を求め，その値の大きい順に基底とする名詞を決定する．また，基底毎の重要度を考慮し，各単語ベクトルの要素の値には，単語の文書中での出現頻度にidf値を掛けた値を使用する．なお，情報検索では，ある一定値以上の類似度を持つ文書を抽出の対象とするが，その値の選び方によって，再現率，適合率の値は変化する．そこで，検索の精度評価では，いずれの場合も，F値が最大となるよう類似度を設定する．</subsection>
  <subsection title="単語意味属性を基底とした文書ベクトル（S-VSM）と単語を基底とした文書ベクトル空間法（W-VSM）の比較">2,710種類の意味属性のすべてを使用する場合について情報検索実験を行い，従来の単語を基底とした文書ベクトル空間法（W-VSM）と検索精度を比較する．本論文の方法による検索精度を従来の単語を基底とした文書ベクトル空間法と比べた結果を図に示す．図では，情報検索において類似度以上の文書を抽出した場合について，と再現率R，適合率Pの関係を示している．なお，類似度0.7以上とする場合は，検索される文書が1件程度となってしまい，信頼できないので，グラフから削除した．また，この結果から得られた類似度simと検索精度F値の関係を図に示す．これらの図から，以下のことが分かる．単語意味属性を基底とした文書ベクトルは，単語を基底とした文書ベクトル空間法に比べて，すべての類似度領域で，再現率が高く，適合率が低い．検索精度（F値の最大値）は，両者は殆ど変わらない．</subsection>
  <section title="考察"/>
  <subsection title="単語意味属性を基底とする文書ベクトル空間法と単語を基底とする文書ベクトル空間法の比較">実験によれば，単語意味属性を基底とする文書ベクトルは，単語を基底とする文書ベクトル空間法に比べて，再現率が高いことが分かった．本研究では，簡単のため，文書中に使用された単語の頻度から直接，意味属性のtfidf値を求めることとし，複数の意味を持つ単語は，そのtfidf値を，該当する複数の意味属性に均等に加える方法を採った．これは，単語を基底とする文書ベクトルの場合と同じ扱いであるが，適合率を減少させる原因の一つと考えられる．これに対して，文書中で使用された単語の多義解消を行うことができれば，適合率の向上は可能であると期待される．ただし，今回の実験は，BMIR-J2における新聞記事検索のタスクであり，文書数も約5,000件と少ない．今後検索する分野が変化したときや，文章数が増加した場合，これらの結論が変わってくる可能性がある．今後，これらの課題を追求する必要がある．</subsection>
  <subsection title="意味属性体系">本研究に使用した意味属性体系は，元来，単語多義の解消を狙って開発されたものであり，複数の語義を持つ単語は，通常，複数の意味属性を持つ構造となっている．日本語語彙大系には，さらに，動詞と名詞の共起関係から，両者の文中での意味を特定するための仕組みが定義されている．そこで，これらの情報を使用した意味解析によって文書中で使用された単語の意味的用法を決定し，その後，該当する意味の重みを求めることにすれば，質問文と同じ単語が使用された文書でも意味の異なる用法の文書は検索対象外とすることができるため，適合率は向上すると期待される．</subsection>
  <subsection title="基底数の削減のためのテストデータ">実験では，提案した単語意味属性を基底とした文書ベクトル空間法と従来の単語を基底とした文書ベクトル空間法が基底数削減にどれだけ強いかを比較評価するため，情報検索方式の評価実験用として広く提供されているBMIRのデータセット（検索条件と正解付き）を使用した．実験はいずれもオープンテストである．これは，以下に述べるように，この種の研究では大量のデータを対象としたオープンテストは困難なためである．すなわち，本手法では，検索対象とするデータベースに対して必要最小限の意味属性の組を発見することが必要であるが，そのためには，汎化を進める過程で検索精度が低下するかどうかの評価が必要で，検索結果についてあらかじめ正解を知っておく必要がある．しかし，大規模なデータベースの場合，様々な検索条件について，あらかじめ正しい検索結果を知ることは通常難しい（この事情は他の検索方式の場合も同様である）．ところで，本方式を現実のシステムに応用するには，部分的な標本（例えば，数千件程度の記事）に対して今回と同様の実験により必要最小限の意味属性の組決める必要があるが，必要な意味属性の数（これをn個とする）が分かれば，n個を構成する意味属性の種類は，データベースの規模に応じてさらに最適化することができる．すなわち，大規模なデータベースでも単語の出現頻度統計を取るのは比較的容易であるから，単語統計から作成された意味属性を初期値とし，意味属性数がnとなるまで汎化すれば，残ったn個の意味属性は，データベース全体から見て最適な組み合わせとなり，運用段階においてもクローズドテストに近い検索精度が得られるものと期待できる．</subsection>
  <subsection title="必要最小限の意味属性">粒度による汎化（S-VSM(g)）において文書ベクトル数を700に汎化したときに残った単語意味属性を調査した．この結果．汎化で残った単語意味属性の多くは，汎化をする前にtfidf値が大きく，かつ頻度も多い単語意味属性であった．例として「抽象」，「名詞」，「事」など意味意味属性であった．</subsection>
  <section title="結論">従来，ベクトル空間法では，文書の意味を表す特性ベクトルの基底に，文中に現れる単語を使用していた．本論文では，単語の代わりに単語の意味属性（「日本語語彙大系」で規定された約2,710件）を使用する方法を提案した．また，意味属性間の意味的上下関係に着目したベクトルの基底の汎化の方法を提案し，情報検索の精度を低下させない範囲で，基底数を削減する方法を示した．この方法は，基底数を削減するための計算量が，データベース内の文書数に依存しないため，大規模なデータベースへの適用が容易である．BMIR-J2の新聞記事（5,080記事）の検索に適用した実験結果によれば，提案した方法は，単語の表記上の揺らぎに影響されず，同義語や類義語の存在も検索の対象となることから，従来の方法と比べて高い再現率が得られた．その反面，単語を基底とする文書ベクトルの場合に比べて，不適切な記事を拾いやすく，適合率が低下することが分かった．この効果は，キーワード検索においてシソーラスを使用したＫＷ拡張の効果に相当する．また，本方式は，次元数の削減に強い方法であり，従来の方法に比べて，検索精度を落とすことなく，ベクトルの基底数を大幅に削減できることが分かった．今回は，単語の多義性の問題は考慮しなかったが，単語意味属性を基底とする文書ベクトルでは，意味属性体系の持つ能力を用いて単語の多義を解消した後，基底とする意味属性の重みを計算する方法が可能と考えられるので，今後は，この方法についても検討していきたい．また，基底数をさらに削減する方法として，意味属性体系の上位のノードから順に，不適切な記事を拾いやすいノードを選択してベクトルの基底から削除する方法についても検討していく予定である．document</section>
</root>
