\documentstyle[epsf,tascmac,jnlpbbl]{jnlp_j_b5}

\def\sign{}
\def\vec#1{}
\def\argmax{}


\setcounter{巻数}{10}
\setcounter{号数}{4}
\setcounter{年}{2003}
\setcounter{月}{7}
\setcounter{page}{109}


\受付{2002}{10}{25}
\再受付{2003}{1}{7}
\採録{2003}{4}{10}


\title{サポートベクタマシンを用いた対訳表現の抽出}
\author{佐藤 健吾\affiref{KEIOCS} \and 斎藤 博昭\affiref{KEIOCS}}

\headauthor{佐藤，斎藤}
\headtitle{サポートベクタマシンを用いた対訳表現の抽出}



\affilabel{KEIOCS}{慶應義塾大学大学院理工学研究科開放環境科学専攻}
{Department of Science for Open and Environmental Systems，
  Keio University}


\jabstract{
  本論文では，機械学習の一手法であるサポートベクタマシンを用いて文対応
  付き対訳コーパスから対訳表現を抽出する手法を提案する．
  サポートベクタマシンは従来からある学習モデルに比べて汎化能力が高く過
  学習しにくいためにデータスパースネスに対して頑健であり，カーネル関数
  を用いることによって素性の依存関係を自動的に学習することができるとい
  う特徴を持つ．
  本手法では対訳モデルの素性として，対訳辞書による素性，語数による素性，
  品詞による素性，構成語による素性，近傍に出現する語による素性を使用し，
  サポートベクタマシンに基づく対訳表現の対応度を用いて対訳表現を抽出す
  る．
  既存の手法は対訳表現の対応度の計算に単語の共起関係を利用しているため
  にデータスパースネスに陥りやすく，低頻度の対訳表現の抽出は困難である
  のに対して，
  本手法は，訓練コーパスによって対訳モデルをあらかじめ学習する必要があ
  るが，一旦モデルを学習してしまえば低頻度の対訳表現でも抽出が可能であ
  るという特徴を持つ．
}

\jkeywords{対訳辞書，機械翻訳，サポートベクタマシン}

\etitle{Extracting Word Sequence Correspondences\\
  Based on Support Vector Machines}
\eauthor{Kengo SATO\affiref{KEIOCS} \and Hiroaki
  SAITO\affiref{KEIOCS}}

\eabstract{
  This paper proposes a learning and extracting method of
  bilingual word sequence correspondences from aligned parallel
  corpora based on Support Vector Machines (SVMs),
  which are robust against data sparseness because of high
  ability of generalization
  and can learn dependencies of features by using a kernel function.
  Our method learns a translation model using features such as
  translation dictionaries, the number of words, part-of-speech,
  constituent words and neighbor words,
  and extracts bilingual word sequence correspondences by using the
  correspondence level based on SVMs.
  Conventional methods cannot extract bilingual word sequence
  correspondences which appear infrequently because of data sparseness
  which is caused by correspondence levels based on word co-occurrences.
  Our method, however, can extract them
  by the model which has been already learned by training corpora.
}

\ekeywords{translation dictionary, machine translation, support vector machines}

\begin{document}



\maketitle
\section{はじめに}
\label{sec:intro}
\thispagestyle{empty}
機械翻訳，言語横断的な検索や要約など複数の言語を同時に扱うシステムにお
いて対訳辞書は必要不可欠であり，その品質がシステム全体の性能を左右する．
これらに用いられる対訳辞書は現在，人手によって作成されることが多い．
しかし，人手による作成には限界があり，品質を向上するためには膨大な労力
が必要であること，辞書の記述の一貫性を保つことが困難であることが問題と
なる．
このことからコーパスから自動的に対訳辞書を作成しようとする研究が近年盛
んに行われている~\cite{tanaka_96,kitamura_97,melamed_97,yamamoto_01,kaji_01}．
しかし，これらの研究の多くは対訳表現の対応度の計算に単語の共起関係を利
用しているためにデータスパースネスに陥りやすく，そのため小規模なコーパ
スから対訳表現を抽出することは難しい．
対訳コーパス自体があまり多くない現状では，小規模な対訳コーパスからでも
対訳表現を抽出できることが望ましい．

本論文では，サポートベクタマシン~\cite{vapnik_book_99}を用いて文対応付
き対訳コーパスから対訳表現を抽出する手法を提案する．
サポートベクタマシンは訓練事例と分割境界の距離 (マージン) を最大化する
戦略に基づく手法であり，従来からある学習モデルに比べて汎化能力が高く過
学習しにくいために，データスパースネスに対して頑健であるという特徴を持
つ．
さらにカーネル関数を用いることによって非線形な分割境界を学習したり，素
性同士の依存関係を自動的に学習することが可能である．
このため，自然言語処理の分野でも
テキスト分類~\cite{joachims_98,taira_99}，
Chunk 同定~\cite{kudo_00b}，
構文解析~\cite{kudo_00a}
などに応用されている．

我々の手法は，訓練コーパスによって対訳モデルをあらかじめ学習する必要が
あるが，一旦モデルを学習してしまえば，訓練コーパスにおいて出現回数が少
ない対訳表現あるいは訓練コーパスにおいて出現しなかった対訳表現でさえも
抽出することができる．
したがってある程度大規模な対訳コーパスから優れた対訳モデルを学習してお
けば，サポートベクタマシンの高い汎化能力によって低頻度の対訳表現でも抽
出が可能であるという特徴を持つ．

本論文の構成は以下の通りである．
\ref{sec:svm}~節ではサポートベクタマシンについて説明し，
\ref{sec:SVMdict}~節ではサポートベクタマシンを用いて対訳表現を抽出する
手法を述べる．
\ref{sec:experiment_discussion}~節では我々が提案した手法の有効性を示す
ために行った実験の結果とそれに対する考察を述べる．
\ref{sec:related_works}~節において関連研究との比較を行う．
最後に \ref{sec:conclusion}~節で本論文のまとめを述べる．

\section{サポートベクタマシン}
\label{sec:svm}

サポートベクタマシン
(Support Vector Machine, 以下 SVM) \cite{vapnik_book_99} は，$d$~個
の素性を持つ事例を $d$~次元ベクトルによって表し，${\bf R}^d$ において
2~つのクラスに線形分離する二値分類器である．
与えられた事例 $\vec{x}=(x_1,x_2,\ldots,x_d)^t$ がクラス
$X_{-1}, X_{+1}$ のどちらに属するかを
式~(\ref{eq:classify})によって判別する．
\begin{equation}
  \label{eq:classify}
  f(\vec{x}) = \sign(g(\vec{x})) = \left\{
    \begin{array}{rl}
      +1 & \left( \vec{x} \in X_{+1} \right)\\
      -1 & \left( \vec{x} \in X_{-1} \right)\\
    \end{array}
  \right.
\end{equation}
ここで $g$ は 2~つのクラスを分離する超平面であり，$\vec{w}$
と $b$ は学習によって決定する．
\begin{equation}
  \label{eq:hyperplain}
  g(\vec{x}) = \vec{w}^t \vec{x} + b
\end{equation}
訓練事例 $\vec{x}_1,\ldots,\vec{x}_n$ に対する教師信号
$y_1,\ldots,y_n$ を以下のように与える．
\[ y_i = \left\{
  \begin{array}{rl}
    +1 & \left( \vec{x}_i \in X_{+1} \right) \\
    -1 & \left( \vec{x}_i \in X_{-1} \right) \\
  \end{array}
  \right.
\]
訓練事例が線形分離可能である場合には，式~(\ref{eq:classify})を満たすよ
うな $\vec{w}$, $b$ は複数存在することから以下のような制約を与える．
\begin{equation}
  \label{eq:constraint}
  \forall i, \quad y_i (\vec{w}^t \vec{x}_i + b) - 1 \geq 0
\end{equation}
SVM では，訓練事例と分割境界の間の距離 (マージン) を最大化する戦略に基
づきパラメータ $\vec{w}$, $b$ を決める．
詳しい導出は文献~\cite{vapnik_book_99} に譲るが，
最終的にマージンの最大化の問題は式~(\ref{eq:constraint}) の条件の下に
$||\vec{w}||^2/2$ を最小化する問題に帰着する~\footnote{実際の実験では
  ある程度の誤りを許すソフトマージン項を追加したモデルを用いた．}．
これを 2~次計画法によって解くことで最適な分離平面 $g$ が得られる．

事例 $\vec{x}$ に対して $g(\vec{x})$ の符号はクラスを表し，絶対値はク
ラス分けの確信度を表す．
また，以下のようにシグモイド関数によって $\vec{x}$ がクラス $X_{+1}$
に分類される確率を近似することができる~\cite{platt_99}．
\begin{equation}
  \label{eq:sigmoid}
  P(\vec{x} \in X_{+1}|\vec{x}) = \frac{1}{1+\exp(-g(\vec{x}))}
\end{equation}

\paragraph{非線形分離への拡張 :}

線形分離が困難な事例に対しても，前処理として非線形な写像
$\phi : {\bf R}^d \mapsto {\bf R}^{d'}$ を用いてそれらをより高次元に写
像することによって線形分離できる場合がある．
写像先の空間 ${\bf R}^{d'}$ において線形分離を行えば
元の空間 ${\bf R}^d$ において非線形分離を行っているのと同じことになる．
詳しい導出は省略するが，SVM では学習，識別アルゴリズムにおいて事例間の
内積しか使用していない点を生かし，各事例間の
内積 $\vec{x}_i^t \vec{x}_j$ を式~(\ref{eq:kernel}) に置き換えることに
よって高次元への写像を実現する．
\begin{equation}
  \label{eq:kernel}
  K(\vec{x}_i,\vec{x}_j) = \phi(\vec{x}_i)^t \phi(\vec{x}_j)
\end{equation}
$K$ はカーネル関数と呼ばれる．
実際には $\phi$ 自体の計算をする必要がないので，計算量の面でも非常に効
率的である．
よく使われるカーネル関数の例としては
多項式型カーネル関数~(\ref{eq:poly_kernel})
などが知られている．
\begin{equation}
  \label{eq:poly_kernel}
  K(\vec{x}_i, \vec{x}_j) = (\vec{x}_i^t \vec{x}_j + 1)^p
\end{equation}
$p$~次の多項式型カーネル関数による非線形分離は，元の空間 ${\bf R}^d$
においては $p$~個の素性の依存関係を考慮していることに相当する．

\section{SVM を用いた対訳表現の抽出}
\label{sec:SVMdict}

本論文で提案する手法は，対訳文となっている日本語文と英語文から，その中
に含まれる句の対訳関係を抽出する．その手法は以下の 2~つの手順から構成さ
れる．
\begin{quote}
  \begin{enumerate}
  \item
    訓練コーパスにおいて対訳関係となっている表現 (対訳対) とそうでない
    表現を人手によって分類，前者を正事例，後者を負事例とし，これらから
    SVM によって対訳モデルを学習する (\ref{sec:learn}~節)．
  \item
    対訳文となっている日英両言語の文を構文解析し，得られた句構造から対
    訳対と成り得る候補 (対訳対候補) の集合を作成する．それらを対訳モデ
    ルに入力することによって対訳関係であるかどうかを判別する
    (\ref{sec:extraction}~節)．
\end{enumerate}
\end{quote}
本手法の概略図を図~\ref{fig:struct} に示す．
\begin{figure}[tbp]

\begin{center}
\epsfile{file=struct.eps,width=.8\columnwidth}
  \caption{本手法の概略}
  \label{fig:struct}

\end{center}
\end{figure}

\subsection{使用する素性}
\label{sec:feature}

SVM を対訳関係の抽出に用いるためには，対訳対候補から素性ベクトルを作成
する必要がある．
本論文で提案する手法では表~\ref{tab:features} のような素性を用いて素性
ベクトルを構成した．

\begin{table}[t]
  \centering
  \caption{対訳表現の抽出に使用した素性}
  \label{tab:features}
  \begin{small}
    \begin{tabular}{|ll|r|}
      \hline
      & \multicolumn{1}{c|}{素性} & \multicolumn{1}{c|}{個数} \\
      \hline\hline

      & 既存の辞書を使用する素性 &  \\ \hline
      (1a) & 対訳対候補内の対訳単語対 & 1,558 \\
      (1b) & 対訳対候補が現れる文脈で共起する語同士の対訳単語対 & 2,408 \\
      \hline

      & 単語数を使用する素性 & \\ \hline
      (2a) & 日本語句の単語数 & 1\\
      (2b) & 英語句の単語数 & 1\\
      \hline

      & 構成する品詞に関する素性 & \\ \hline
      (3a) & 日本語句における名詞，動詞，形容詞，形容動詞，副詞の出現割合 & 5\\
      (3b) & 英語句における名詞，動詞，形容詞，副詞の出現割合 & 4\\
      \hline

      & 構成する語に関する素性 & \\ \hline
      (4a) & 日本語句に出現する語 & 3,006 \\
      (4b) & 英語句に出現する語 & 2,654 \\
      \hline

      & 句の近傍に出現する語に関する素性 & \\ \hline
      (5a) & 日本語句の近傍に現れる語 & 3,266 \\
      (5b) & 英語句の近傍に現れる語 & 2,859 \\
      \hline\hline

      
      & \multicolumn{1}{c|}{合計} & 15,762\\
      \hline
    \end{tabular}
  \end{small}
\end{table}

既存の辞書を使用する素性を 2~種類用いる．
素性~(1a) は，対訳対候補に含まれる語について辞書引きを行い，対訳となっ
ている単語の組 (対訳単語対) が対訳対候補に含まれていればそれを素性とす
る．
対訳関係となっている表現には対訳単語対が多く含まれることに基づく素性で
ある．
辞書に含まれる対訳単語対を素性ベクトルの次元に割り当て，対訳対候補内に
対訳単語対が現れた場合は対応する次元の値を 1 とし，そうでなければ 0 と
する．
素性~(1b) は，対訳対候補の近傍に出現した語について辞書引きを行い，辞書
に含まれる対訳単語対を素性とする．
「対訳関係にある表現は近傍に出現している語の出現文脈も (言語の違いこそ
あれ) 似ている」という考え~\cite{kaji_01} に基づく素性である．
本論文における実験では，同一文に現れる語を近傍とした．
辞書に含まれる対訳単語対を素性ベクトルの次元に割り当て，対訳対候補の近
傍に対訳単語対が現れた場合は対応する次元の値を 1 とし，そうでなければ
0 とする．
対訳辞書という既存の知識を素性という形で有効に利用することによって精度
の向上を期待することができる．

素性~(2a)(2b) は，対訳関係となっている表現は両言語の句の構成語数に相関
関係があるという考えに基づく．
日英それぞれの句に含まれる語数を素性とした．

素性~(3a)(3b) は，対訳関係となっている表現は内容語に関してはその構成比
率について両言語間に相関関係があるという考えに基づく．
日英それぞれについて句の語数に対する内容語の出現数の割合を素性とする．
なお，日本語の内容語は名詞，動詞，形容詞，形容動詞，副詞とし，英語の内
容語は名詞，動詞，形容詞，副詞とした．

素性~(4a)(4b) は，日英両言語を構成する内容語に素性ベクトルの次元を割り
当て，語が出現すれば対応する次元の値を 1 とし，そうでなければ 0 とする
素性である．

素性~(5a)(5b) は，対訳対候補の近傍に現れた内容語に素性ベクトルの次元を
割り当て，語が出現すれば対応する次元の値を 1 とし，そうでなければ 0 とす
る素性である．

対訳文中に既存の対訳辞書によって辞書引きできない語が多数含まれる場合が
ある．
素性~(4a)(4b)(5a)(5b) はそのような場合に既存の対訳辞書を用いた
素性~(1a)(1b) を補完する目的で導入した．
カーネル関数によって素性~(4a) と (4b) の依存関係，素性~(5a) と (5b) の
依存関係をモデルに組み込むことによって，既存の対訳辞書に現れない対訳単
語対における素性~(1a)(1b) と同じ役割を期待することができる．

\subsection{対訳モデルの学習}
\label{sec:learn}

訓練コーパス中の各対訳文において対訳関係となっている表現とそうでない表
現を人手によって作成する．
対応する日英の両文を構文解析し，得られた両言語の句の組合わせについて，
対訳表現となっているものを正事例とし，そうでないものを負事例とする．
本論文における実験では，組合わせの対象とする句は名詞句と動詞句とした．
また，巨大すぎる句構造は対訳表現としての実用的な価値が少ないと思われる
ことから，句の部分構文木の高さが 5 以下のものを組合わせの対象とした．

本論文における実験では，日本経済新聞社英文ビジネスレター
文例大事典~\cite{nikkei_business_corpus} を対訳コーパスとして用いた．
英文ビジネスレター文例大事典の各対訳文は対訳対となる部分があらかじめマー
クアップされており，対訳表現として抽出すべき句の制約 (部分構文木の高さ
が 5 以下の名詞句，動詞句) を満たす対訳対を正事例とした．
負事例は，
Apple Pie Parser~\footnote{{\tt http://www.cs.nyu.edu/cs/projects/proteus/app/}}
と
KNP~\footnote{{\tt http://www-lab25.kuee.kyoto-u.ac.jp/nl-resource/knp.html}}
によって構文解析した結果から対訳表現として抽出する句の制約を満たすもの
のうち，対訳表現になっていないものを選んだ．
具体的には，各対訳文に 1~対ずつある対訳対 $(p_j, p_e)$ ($p_j$ は日本語
句，$p_e$ は英語句) に対して，日英各文を構文解析することによって得られ
た句 $p_j' (\neq p_j)$ や $p_e' (\neq p_e)$ を用いた $(p_j', p_e)$ や
$(p_j, p_e')$ を負事例とした~\footnote{
  本来であれば，構文解析によって得られた句の全ての組合わせから人手に
  よって正事例と負事例に分割して学習するべきであるが，$p_j$ や $p_e$
  を含む全ての対訳対候補の中から対訳対 $(p_j, p_e)$ を高い精度で抽出で
  きることを示せれば本手法の有効性を示すことができることから上記の実験
  設定で十分であると考える．}．

このようにして得られた全ての事例から \ref{sec:feature}~節で述べた方法
によって素性ベクトルを作成し，
教師信号として正事例には $+1$，負事例には $-1$ を与える．これを訓練デー
タとして \ref{sec:svm}~節で述べた SVM によって対訳モデルの学習を行い，
式~(\ref{eq:hyperplain}) における最適な分離平面 $g$ を得る．

\subsection{対訳対の抽出}
\label{sec:extraction}

まず，抽出の対象となる対訳対の候補を作成する．
対訳文になっている日英両言語の文を構文解析し，得られた日英両言語の句の
組合わせを対訳対候補の集合とする．訓練データと条件を同じにするために，
対象とする句は部分構文木の高さが 5 以下の名詞句と動詞句とした．

生成した対訳対候補から \ref{sec:feature}~節で述べた方法によって素性ベ
クトルを作成する．
それらと \ref{sec:learn}~節で述べた方法によって得た最適な分離平面 $g$
を用いて，その対訳対候補の「対訳対らしさ」を測る．
対訳対候補 $(p_j, p_e)$
に対応する素性ベクトルを $\vec{x}_{p_j p_e}$ とした時，最適な分離平面
$g$ を用いて $(p_j, p_e)$ の「対訳対らしさ」を以下の式によって表す．
\begin{equation}
  \label{eq:sim}
  sim(p_j, p_e) = \frac{1}{1+\exp(-g(\vec{x}_{p_j p_e}))}
\end{equation}
任意の $(p_j, p_e)$ に対して $0 < sim(p_j, p_e) < 1$ であり，
$sim(p_j, p_e)$ が大きいほど $(p_j, p_e)$ が「対訳対らしい」ことを表す．

一つの句が複数の句と対応することはないことから，
以下のようなアルゴリズムによって対訳対の抽出を行う．
\begin{quote}
  \begin{enumerate}
  \item[{\bf 入力 :}]
    閾値 $th \in [0,1]$\\
    対訳文中の対訳対候補の集合 $C$
  \item[{\bf 出力 :}]
    抽出された対訳対の集合 $T$
  \item[{\bf 1.}]
    $T \leftarrow \emptyset$
  \item[{\bf 2.}]
    $th \leq sim(p_j, p_e)$ となる $(p_j, p_e) \in C$ がなければ終了．
  \item[{\bf 3.}]
    $\displaystyle (p_j^*, p_e^*)=\argmax_{(p_j, p_e) \in C} sim(p_j,
    p_e)$ となる $(p_j^*, p_e^*)$ を対訳対として抽出し $T$ に追加する．
  \item[{\bf 4.}]
    $p_j^*$ や $p_e^*$ を含む対訳対候補を $C$ から削除する．
  \item[{\bf 5.}]
    {\bf 2.} へ戻る．
  \end{enumerate}
\end{quote}

$th$ の値によって得られる対訳対の品質を調節することができる．
$th$ の値が $1$ に近い時には抽出数が少なくなる代わりに確信度が高い対訳
対だけを抽出し，逆に $th$ の値が $0$ に近い時には確信度が多少低いもの
も抽出することによって抽出数を優先する．

上記の処理は 1~文単位で行う．そのため \ref{sec:learn}~節によって対訳
モデルを一旦学習してしまえば抽出対象となるコーパスは小規模なものでもよ
く，たとえ 1~文からでもそこに含まれる対訳対を抽出することができる．

\section{実験および考察}
\label{sec:experiment_discussion}


\subsection{実験結果}
\label{sec:results}

\ref{sec:SVMdict}~節において提案した手法の有効性を確認するために，日本
経済新聞社英文ビジネスレター文例大事典~\cite{nikkei_business_corpus}
を対訳コーパスとして用いた実験を行った．
コーパスに含まれる対訳文のうち，訓練コーパスとして 4,000~文，テストコー
パスとして 1,000~文を用い，\ref{sec:learn}~節に従い対訳対候補を生成し
た．
その結果，本論文における実験の対象となった対訳対候補の数は
表~\ref{tab:candidates} の通りとなった．
また，対訳対候補に含まれる形態数の平均値とコーパス中における対訳対候補
の出現頻度の平均値を表~\ref{tab:avg_length_count} に示す．
\begin{table}[tbp]
  \centering
  \caption{対訳対候補の数}
  \label{tab:candidates}
  \begin{small}
    \begin{tabular}{|l|r|r|} \hline
      &
      \multicolumn{1}{c|}{正事例} &
      \multicolumn{1}{c|}{負事例} \\ \hline
      
      訓練コーパス & 4,000 & 59,203 \\
      テストコーパス & 1,000 & 15,048 \\ \hline
    \end{tabular}
  \end{small}
\end{table}
\begin{table}[tbp]
  \centering
  \caption{対訳対候補の平均形態素数と平均頻度}
  \label{tab:avg_length_count}
  \begin{small}
    \begin{tabular}{|l|r|r|r|r||r|r|r|r|} \hline
      &
      \multicolumn{4}{c||}{平均形態素数} &
      \multicolumn{4}{c|}{平均頻度} \\ \cline{2-9}
      &
      \multicolumn{2}{c|}{正事例} &
      \multicolumn{2}{c||}{負事例} &
      \multicolumn{2}{c|}{正事例} &
      \multicolumn{2}{c|}{負事例} \\ \cline{2-9}
      &
      \multicolumn{1}{c|}{\makebox[.9cm]{日本語}} &
      \multicolumn{1}{c|}{\makebox[.9cm]{英語}} &
      \multicolumn{1}{c|}{\makebox[.9cm]{日本語}} &
      \multicolumn{1}{c||}{\makebox[.9cm]{英語}} &
      \multicolumn{1}{c|}{\makebox[.9cm]{日本語}} &
      \multicolumn{1}{c|}{\makebox[.9cm]{英語}} &
      \multicolumn{1}{c|}{\makebox[.9cm]{日本語}} &
      \multicolumn{1}{c|}{\makebox[.9cm]{英語}} \\ \hline
      
      訓練コーパス &
      5.03 & 3.97 & 4.96 & 4.23 &
      1.06 & 1.04 & 1.33 & 1.26 \\
      テストコーパス &
      5.10 & 4.09 & 5.03 & 4.62 &
      1.01 & 1.01 & 1.25 & 1.12 \\ \hline
  \end{tabular}
  \end{small}
\end{table}

得られた事例から \ref{sec:feature}~節に従って素性を生成する．
素性~(1a)(1b) のために使用する対訳辞書として
EDICT~\footnote{\tt http://www.csse.monash.edu.au/$\mbox{}^\sim$jwb/edict.html}
に含まれる対訳単語対のうち，訓練コーパス中に出現した 2,879 個を用いた．
素性~(4a)(4b)(5a)(5b) のために使用する語として，訓練コーパス中に 3 回
以上出現する語を用いた．
その結果，用意した素性の個数は表~\ref{tab:features} の通りとなった．

対訳モデルの学習では，カーネル関数を用いない場合 (linear) と 2~次，
3~次，4~次の多項式型カーネル関数 (poly2, poly3, poly4) を用いた場合の
実験を行った．
訓練コーパスから得られた事例を用いて対訳モデルの学習を行い，テストコー
パスから得られた事例から対訳対の抽出を行った．
それぞれの対訳モデルにおいて抽出アルゴリズムの閾値 $th$ の値を 0.1，
0.5，0.7，0.9 と変化させた時の適合率と再現率を図~\ref{fig:result}
\begin{figure}[tbp]
\begin{center}
\epsfile{file=result.eps,width=.6\columnwidth}
  \caption{対訳モデルと抽出精度}
  \label{fig:result}
\end{center}
\end{figure}
に示す．
各点の右に示した数字が閾値 $th$ である．
もっとも良い抽出精度を示した 2~次多項式型カーネル関数を用いた場合の適合率
と再現率を表~\ref{tab:result}
\begin{table}[tbp]
  \centering
  \caption{2~次多項式型カーネル関数による対訳モデルの適合率と再現率}
  \label{tab:result}
  \begin{small}
    \begin{tabular}{|c|r|r|r|r|} \hline
      閾値 $th$ & 出力数 & 正解数 &
      \multicolumn{1}{c|}{適合率 (\%)} &
      \multicolumn{1}{c|}{再現率 (\%)}  \\ \hline
      0.1 & 1,000 & 804 & 80.4 & 80.4 \\ 
      0.5 & 960 & 776 & 80.8 & 77.6 \\ 
      0.7 & 701 & 594 & 84.7 & 59.4 \\
      0.9 & 265 & 229 & 86.4 & 22.9 \\ \hline
    \end{tabular}
  \end{small}
\end{table}
に示す．
また 2~次多項式型カーネル関数を用い，抽出時の閾値 $th=0.5$ の時の対訳対の抽
出例を表~\ref{tab:success}
\begin{table*}[tbp]
  \centering
  \caption{本手法による対訳対の抽出例}
  \label{tab:success}
  {\small
  \begin{tabular}{|l|l|r|} \hline
    \multicolumn{1}{|c|}{日本語句 $p_j$} &
    \multicolumn{1}{c|}{英語句 $p_e$} &
    \multicolumn{1}{c|}{$sim(p_j, p_e)$} \\ \hline

    本部に異動いたします &
    move to the corporate headquarters &
    0.899 \\

    会長として経営を続ける &
    carry on as chairman &
    0.862 \\

    長年の海外勤務 &
    our many years of service overseas &
    0.824 \\

    新しい人事異動 &
    a new assignment &
    0.823 \\

    2年の任期を1期もしくはそれ以上の期間 &
    for one or more two-year terms &
    0.605 \\ \hline
  \end{tabular}}
\end{table*}
に示す．
以上の結果から，本論文で提案した手法によって 1,000~文という比較的小規
模なコーパスから低頻度の対訳対でも高い精度で抽出できることが示された．

\subsection{対訳モデルと抽出精度}
\label{sec:kernel}

SVM は使用するカーネル関数とそれに付随するパラメータに自由度があり，そ
れらは実験的に決定する必要がある．そこで本論文で行った実験においてもカー
ネル関数を使わない場合 (linear) と 2~次，3~次，4~次の多項式型カーネル
関数 (poly2, poly3, poly4) を用いた場合の実験を行った．

linear による抽出精度は多項式型カーネル関数を使用した場合よりも低い．
\ref{sec:feature}~節で述べた素性が，素性同士の依存関係がカーネル関数
によって自動的に学習されることを期待しているためであると考えられる．

多項式型カーネル関数を用いた場合には 2~次 (poly2) がもっとも良い抽出精
度となった．
本論文で行った実験における訓練事例の数や素性の構成では，2~次多項式型カー
ネル関数によって 2~個の素性の依存関係を学習することが最適であることを
示している．
SVM は，より高次元の多項式型カーネル関数を用いることによってより多くの
素性の依存関係を考慮した複雑なモデルを学習することが可能であるが，
あまりに多くの素性の依存関係を学習してしまうと，その中には学習す
る必要のないものも含まれることになり，過学習によってモデルの性能を悪化
させる結果になることが予想される．
本論文における実験でも同様の現象が起こっていると考えられる．

\subsection{訓練コーパスの大きさと抽出精度}
\label{sec:corpus_size}

訓練コーパスの文数が抽出精度に与える影響を調べるために，訓練コーパスの
文数を 200~文から 4,000~文まで 200~文ずつ増やしながら対訳モデルの学習を
行い，テストコーパスからの抽出における適合率と再現率を求める実験を行っ
た結果を図~\ref{fig:size_dict} (左)に示す．
使用したカーネル関数は 2~次多項式型カーネル関数であり，抽出時の閾値 $th$
は 0.5 とした．
適合率，再現率ともに訓練コーパスの文数にほぼ比例して上昇しており，訓練
コーパスの文数が精度に大きな影響を及ぼしていることがわかる．
このため本手法は，対訳モデルの学習において比較的大規模なコーパスを用い
る必要がある．
しかし，抽出時には処理を 1~文単位で行うので，一旦学習が完了してしまえ
ば抽出対象となるコーパスは小規模なものでもよく，たとえ 1~文からでもそ
こに含まれる対訳対を抽出することができる．

\begin{figure}[tbp]
  \centering
  \begin{tabular}{cc}
\epsfile{file=size.eps,width=.47\textwidth} &
\epsfile{file=dict.eps,width=.47\textwidth}
  \end{tabular}
  \caption{訓練コーパスの文数と精度の関係 (左) と
    対訳辞書の大きさと精度の関係 (右)}
  \label{fig:size_dict}
\end{figure}

\subsection{辞書の大きさと抽出精度}
\label{sec:dict_size}

素性~(1a)(1b) で用いている既存の対訳辞書の大きさが抽出精度に与える影響
を調べるために，使用する対訳単語対の数を 0~個から 2,800~個 まで 100~個
ずつ増やしながら対訳モデルの学習を行い，テストコーパスからの抽出におけ
る適合率と再現率を求める実験を行った結果を図~\ref{fig:size_dict} (右)
に示す．
使用したカーネル関数は 2~次多項式型カーネル関数であり，抽出時の閾値 $th$
は 0.5 とした．
適合率，再現率ともに使用する対訳単語対の数にほぼ比例して上昇しており，
本手法において使用する対訳辞書は可能なかぎり多くの対訳単語対を含むもの
を用いた方が良いことがわかる．

\subsection{素性と抽出精度}
\label{sec:important_features}

素性の重要度を調べるために，\ref{sec:feature}~節において述べた素性を
1~種類ずつ削除して対訳モデルの学習を行い，テストコーパスからの抽出にお
ける適合率と再現率の増減を求める実験を行った結果を
表~\ref{tab:important_features}に示す．
使用したカーネル関数は 2~次多項式型カーネル関数であり，抽出時の閾値 $th$
は 0.5 とした．
適合率と再現率における括弧内の値は素性 1~個あたりの増減である．
\begin{table*}[tbp]
  \centering
  \caption{素性を削除した時の適合率と再現率の増減}
  \label{tab:important_features}
  {\small
  \begin{tabular}{|ll|r|rr@{}l|rr@{}l|} \hline
    \multicolumn{2}{|c|}{素性} &
    \multicolumn{1}{c|}{個数} &
    \multicolumn{3}{c|}{適合率 (\%)} &
    \multicolumn{3}{c|}{再現率 (\%)} \\ \hline\hline

    & 辞書による素性 & 3,966 &
    $-13.1$ & $(-3.3$ & $\times 10^{-3})$ &
    $-13.9$ & $(-3.5$ & $\times 10^{-3})$\\ \hline
    (1a) & 対訳対内対訳単語対 & 1,558 &
    $-9.0$ & $(-5.8$ & $\times 10^{-3})$ &
    $-9.3$ & $(-6.0$ & $\times 10^{-3})$ \\
    (1b) & 対訳対外対訳単語対 & 2,408 &
    $-3.2$ & $(-1.3$ & $\times 10^{-3})$ &
    $-4.0$ & $(-1.7$ & $\times 10^{-3})$ \\ \hline\hline

    & 語数による素性 & 2 &
    $-4.2$ & $(-2.1$ & $\times 10^{\pm 0})$ &
    $-3.0$ & $(-1.5$ & $\times 10^{\pm 0})$ \\ \hline
    (2a) & 日本語語数 & 1 &
    $-2.2$ & $(-2.2$ & $\times 10^{\pm 0})$ &
    $-2.1$ & $(-2.1$ & $\times 10^{\pm 0})$ \\
    (2b) & 英語語数 & 1 &
    $-2.5$ & $(-2.5$ & $\times 10^{\pm 0})$ &
    $-2.6$ & $(-2.6$ & $\times 10^{\pm 0})$ \\ \hline\hline

    & 品詞による素性 & 9 &
    $-4.0$ & $(-4.4$ & $\times 10^{-1})$ &
    $-4.2$ & $(-4.7$ & $\times 10^{-1})$ \\ \hline
    (3a) & 日本語品詞 & 5 &
    $-1.0$ & $(-2.0$ & $\times 10^{-1})$ &
    $-1.3$ & $(-2.6$ & $\times 10^{-1})$ \\
    (3b) & 英語品詞 & 4 &
    $-2.4$ & $(-6.0$ & $\times 10^{-1})$ &
    $-2.6$ & $(-6.5$ & $\times 10^{-1})$ \\ \hline\hline

    & 構成語による素性 & 5,660 &
    $-3.1$ & $(-5.5$ & $\times 10^{-4})$ &
    $-4.0$ & $(-7.1$ & $\times 10^{-4})$ \\ \hline
    (4a) & 日本語構成語 & 3,006 &
    $-2.2$ & $(-7.4$ & $\times 10^{-4})$ &
    $-3.9$ & $(-1.3$ & $\times 10^{-3})$ \\
    (4b) & 英語構成語 & 2,654 &
    $-2.4$ & $(-9.0$ & $\times 10^{-4})$ &
    $-3.4$ & $(-1.3$ & $\times 10^{-3})$ \\ \hline\hline

    & 近傍語による素性 & 6,125 &
    $-2.2$ & $(-3.7$ & $\times 10^{-4})$ &
    $-2.5$ & $(-4.1$ & $\times 10^{-4})$ \\ \hline
    (5a) & 日本語近傍語 & 3,266 &
    $-1.3$ & $(-4.1$ & $\times 10^{-4})$ &
    $-1.6$ & $(-4.9$ & $\times 10^{-4})$ \\
    (5b) & 英語近傍語 & 2,859 &
    $-2.0$ & $(-6.9$ & $\times 10^{-4})$ &
    $-2.2$ & $(-7.7$ & $\times 10^{-4})$ \\ \hline\hline

    & 全ての素性を使用 & 15,762 &
    80.8 & & & 77.6 & &\\ \hline
  \end{tabular}}
\end{table*}

素性 1~個あたりの精度の増減では，語数による素性~(2a)(2b) と
品詞による素性~(3a)(3b) を削除した時の下落が特に大きい．
(2a)(2b) に属する素性は全ての事例に存在し，
(3a)(3b) に属する素性も他の素性に比べるとはるかに多くの事例に存在する
素性である．
したがって，モデル構築におけるこれらの役割は大きく，ゆえに削除した時の
精度の下落が大きくなると考えられる．

その他では，対訳辞書による素性~(1a)(1b) を削除した時の下落が大きい．
素性~(1a) は日英両言語の句の中で既存の対訳辞書によって辞書引きできるも 
のがあるかどうかを表しており，この情報が句の対訳関係を推定する際には極
めて重要であるという我々の直感と合致する．
また素性~(1b) の仮定である「対訳関係にある表現は近傍に出現している語の
出現文脈も (言語の違いこそあれ) 似ている」という考えが対訳モデルの構築
において効果が大きいことが示された．

その他の素性を削除した時も抽出精度の下落を引き起こしており，対訳モデル
の構成において有効であることが示された．

\subsection{認識誤りと素性}
\label{sec:mistake}

認識誤りの原因を調べるために，テストコーパスにおいて正しく認識された事
例と正しく認識されなかった事例における素性の出現個数 (素性値が 0 以外
となる要素の個数) の平均値を計算した (表~\ref{tab:avg_features}) ．
使用したカーネル関数は 2~次多項式型カーネル関数であり，抽出時の閾値 $th$
は 0.5 とした．
出力と記された行において $+1$ と記されている列はシステムが対訳対である
と認識した事例を表し，
$-1$ と記されている列は対訳対でないと認識した事例を表す．
素性~(1b) の行に注目すると，対訳対でないと識別された負事例に対して，対
訳対として識別されてしまった負事例における素性~(1b) の出現個数の平均値
がかなり大きく，正事例の場合の値とあまり差のない値となっている．
このことは，対訳対として識別されてしまった負事例の近傍に対訳単語対がよ
く現れていることを表している．
本論文における実験では，同一文に現れる語を近傍とし，素性~(1b) は辞書中
の対訳単語対が近傍に出現するか否かを表しているので，特に頻出する対訳単
語対に関する素性~(1b) の出現個数は増えやすく，それが認識誤りを招いてい
ると考えられる．
したがって近傍の定義を「同一文内」ではなく，「日本語句・英語句から
$n$~語以内」のように近傍の範囲を狭くしたり「日本語句・英語句と係り受け
関係にある」のようにより関連性が強いものだけを素性にすることによってこ
のような誤りは減らすことができると思われる．
しかし，表~\ref{tab:important_features} からわかるように，近傍の範囲を
狭くすることによって素性~(1b) が減りすぎると精度が下落するので，今回の
実験では近傍を「同一文内」とした．
\begin{table}[tbp]
  \centering
  \caption{一事例あたりの素性の出現個数の平均値}
  \label{tab:avg_features}
  \begin{small}
    \begin{tabular}{|l|r|r|r|r|} \hline
      \multicolumn{1}{|c|}{事例} &
      \multicolumn{2}{c|}{正事例} &
      \multicolumn{2}{c|}{負事例} \\ \hline
      \multicolumn{1}{|c|}{出力} &
      \multicolumn{1}{c|}{$+1$} &
      \multicolumn{1}{c|}{$-1$} &
      \multicolumn{1}{c|}{$+1$} &
      \multicolumn{1}{c|}{$-1$} \\ \hline\hline
      \multicolumn{1}{|c|}{事例数} &
      \makebox[.9cm][r]{776} &
      \makebox[.9cm][r]{224} &
      \makebox[.9cm][r]{184} &
      \makebox[.9cm][r]{14,864} \\ \hline\hline

      (1a) 対訳対内対訳単語対 &
      1.07& 0.83 & 0.13 & 0.04 \\ 

      (1b) 対訳対外対訳単語対 & 
      3.75 & 3.88 & 3.51 & 2.75 \\ 

      (2a) 日本語語数 &
      1.00 & 1.00 & 1.00 & 1.00 \\ 

      (2b) 英語語数 & 
      1.00 & 1.00 & 1.00 & 1.00 \\ 

      (3a) 日本語品詞 &
      1.50 & 1.41 & 1.38 & 1.50 \\ 

      (3b) 英語品詞 &
      1.72 & 1.60 & 1.96 & 1.55 \\ 

      (4a) 日本語構成語 & 
      4.92 & 4.47 & 4.32 & 4.77 \\ 

      (4b) 英語構成語 & 
      3.68 & 3.15 & 4.24 & 3.93 \\ 

      (5a) 日本語近傍語 &
      4.47 & 4.40 & 4.39 & 4.45 \\ 

      (5b) 英語近傍語 & 
      4.54 & 4.59 & 4.54 & 4.53 \\ \hline
    \end{tabular}
  \end{small}
\end{table}

\section{関連研究との比較}
\label{sec:related_works}

本手法と同様に対訳文の文対応が既に付いていることを前提にしている研究
には文献~\cite{melamed_97,kitamura_97,yamamoto_01} などがあげられる．

\cite{melamed_97} は Competitive Linking Algorithm という単語対のリン
ク付け法と 2 つのパラメータに対する山登り法を組み合わせて単語対の対応
度を求める手法を提案した．
しかし Melamed の手法は 1~単語対 1~単語の対応を仮定しており，日本語と
英語のように構造が大きく異なる言語に対して適用するのは困難である．

\cite{kitamura_97} は Dice 係数~\cite{kay_93} を対訳対の出現頻度の対数
によって重み付けする重み付き Dice 係数を提案し，これを対訳対の対応度と
して採用した．
\cite{yamamoto_01} は北村らの手法を改良し，文節の依存関係が対訳表現の
抽出において有効な手がかりであることを示した．
北村らの手法と山本らの手法が対応度として採用している重み付き Dice 係数
は対訳対の出現回数に依存しているので，出現回数が少ない対訳対に対する対
応度はデータスパースネスのために信頼することができず，したがって小規模
な対訳コーパスから対訳対を抽出することは難しい．
それに対して本手法は対訳表現の抽出を統計的機械学習のアプローチで捉えて
おり，対訳モデルの学習において対訳対の出現回数に依存しない素性を用いて
対訳対を特徴づける．
したがって，本手法は訓練コーパスによって対訳モデルをあらかじめ学習する
必要がある反面，一旦モデルを学習してしまえば訓練コーパスにおいて出現回
数が少ない対訳対あるいは出現しなかった対訳対でさえもデータスパースネス
に陥ることなく抽出することができるという特徴がある．

本手法と同様に対訳対の抽出を統計的機械学習の枠組みで捉えている研究とし
て文献~\cite{satoken_NLP02} があげられる．
佐藤らは最大エントロピー法
 (Maximum Entropy Method，以下 ME~法) ~\cite{berger_96} を用いて文対応
付き対訳コーパス上に対訳単語対の確率モデルを推定・抽出する手法を提案し
た． 単語の共起情報と品詞情報を使用した素性約 12,000~個を用い，
推定確率 0.1 以上の単語対に対して行った抽出では適合率 73.64\,\%，
再現率 21.79\,\% を実現した．
この手法は，一旦モデルを学習してしまえば未知語を含むコーパスに対して学
習し直す必要がないという点において本論文で提案した手法と共通点がある．

\cite{satoken_NLP02} における報告とは使用しているコーパス・素性や抽出
対象が異なるので，本論文で行った実験において使用したコーパス・素性を用
いて ME~法によって抽出する実験を本手法との比較のために行った．
対訳対候補 $(p_j,p_e)$ に対応する素性ベクトル $\vec{x}_{p_j p_e}$ が正
事例である確率 $P(\vec{x}_{p_j p_e} \in X_{+1}|\vec{x}_{p_j p_e})$ を
ME~法によって推定し，これを式~(\ref{eq:sim}) の代わりに用いて対訳表現
の抽出を行った．
その結果，$th=0.5$ において適合率 69.2\,\%，再現率 63.6\,\% となった．
これはカーネル関数を用いない対訳モデル (linear) とほぼ同じ精度である．
本論文で使用した素性は，素性同士の依存関係がカーネル関数によって自動的
に学習されることを期待しているためであると考えられる．
一般に素性同士には依存関係があるので，ME~法では素性同士の依存関係を表
す素性を新たに作成する必要がある．
その結果，素性の総数が非常に多くなってしまい，過学習を起こす危険がある
ため，ヒューリスティックによって有効な素性だけを選別したり，貪欲戦略に
基づく素性選択アルゴリズム~\cite{berger_96} を使用して素性の総数を減ら
す手法を用いることが多い．
しかし，前者は選別の基準が難しく，後者は計算量が膨大になるという欠点が
ある．
例えば本論文で用いた素性 15,762~個を用いて 2~つの素性の依存関係を表す
素性を生成し，これらを用いて ME~法によって確率モデルを推定しようとする
と，素性の総数がおよそ 250~万個となり，現実的な時間で計算することは困
難である．
一方，SVM では多項式型カーネル関数を用いることによって計算量をほとんど
増やすことなく素性同士の依存関係を自動的に学習することができる．

一方，本手法と異なり，対訳文の文対応が付いていることを前提としない研究
には文献~\cite{tanaka_96,kaji_01} などがあげられる．
これらの手法は「一方の言語で共起する単語の訳語は他方の言語でも共起する」
ということを仮定している．
\cite{tanaka_96} は各言語に出現する語の共起確率行列の距離が
小さくなるように確率翻訳行列を最適化することによって対訳関係を得る手法
を提案した．
\cite{kaji_01} は既存の辞書に含まれる単語との対訳対中に含まれる語の共
起集合の共通部分の大きさによって対応度を計算している．
本論文で提案した手法においても「一方の言語で共起する単語の訳語は他方の
言語でも共起する」という仮定を素性~(1b) に用いている点においてこれらの
手法と共通点がある．
現状では文対応付き対訳コーパスはあまり多くないため，文対応を前提と
しないこれらの手法は適用できる範囲は広いが，文対応付き対訳コーパスを用
いた手法よりも精度が劣る．一方，本手法の前提となって
いる文対応付き対訳コーパスは，原文に忠実に翻訳した対訳コーパスであれば，
\cite{kay_93,utsuro_94,sukehiro_95} などで提案されている手法によって作
成することができる．対応する文がなかったり，1 つの文が複数の文に対応し
ている場合には人手による後編集が必要になるが，その労力は全て人手による
対応付けに比べて比較にならないほど少ないと考えられる．

\section{おわりに}
\label{sec:conclusion}

本論文では，SVM を用いて文対応付き対訳コーパスから対訳表現を抽出する手
法を提案した．
対訳モデルの素性として，対訳辞書による素性，語数による素性，品詞による
素性，構成語による素性，近傍に出現する語による素性を使用し，SVM に基づ
く対訳表現の対応度を用いて対訳表現を抽出する．
既存の手法は対訳表現の対応度の計算に単語の共起関係を利用しているため
にデータスパースネスに陥りやすく，小規模なコーパスからの対訳表現の抽
出は困難である．
それに対して本手法は，訓練コーパスによって対訳モデルをあらかじめ学習す
る必要があるが，一旦モデルを学習してしまえば，訓練コーパスにおいて出現
回数が少ない対訳表現あるいは訓練コーパスにおいて出現しなかった対訳表現
でさえも抽出することができる．
したがってある程度大規模な対訳コーパスから優れた対訳モデルを学習してお
けば，サポートベクタマシンの高い汎化能力によって低頻度の対訳表現でも抽
出が可能であるという特徴を持つ．
本手法の有効性を示すために日英対訳コーパスを用いた対訳表現の抽出実験を
行った．対訳モデルの学習に 2~次多項式型カーネル関数を使用し，抽出時の
閾値 $th=0.5$ とした時には， 1,000 文という比較的小規模なコーパスから
適合率 80.8\,\%，再現率 77.6\,\% の精度で抽出できることを示した．

また素性の重要度を調べる実験では，語数による素性，品詞による素性，対訳
辞書による素性が精度向上に大きく貢献していることがわかった．
しかし，対訳対候補の近傍に現れる語を対訳辞書によって辞書引きして得た素
性において近傍の範囲を「同一文内」としていることが認識誤りを増やす原因
となっている．近傍の範囲を「対訳対候補から $n$ 語以内」や「対訳対候
補と係り受け関係にある」とすることで改善できると思われる．


\acknowledgment

日経英文ビジネスレター文例大事典の研究利用許諾を頂いた日本経済新聞社に
感謝致します．
元慶應義塾大学教授の故中西正和先生に深く感謝し，ご冥福をお祈り致します．


\bibliographystyle{jnlpbbl}
\bibliography{422}

\nocite{satoken_coling2002}
\nocite{satoken_sci2002}

\begin{biography}
\biotitle{略歴}
  \bioauthor{佐藤 健吾}{
    平成 7 年慶應義塾大学理工学部数理科学科卒業．平成 15 年同大学大学院理
    工学研究科博士課程開放環境科学専攻修了．博士(工学)．
    現在，同大学理工学部生命情報学科助手．
    バイオインフォマティクス，自然言語処理，統計的機械学習などに興味を持つ．
    情報処理学会，言語処理学会各会員．}
  \bioauthor{斎藤 博昭}{
    昭和 58 年慶應義塾大学工学部数理工学科卒業．現在同大理工学部情報工学
    科専任講師．工学博士．昭和 59 年よりカーネギーメロン大学に訪問研究員
    として滞在し，機械翻訳および音声認識の研究に従事．情報処理学会，言語
    処理学会，ACL各会員．}

\bioreceived{受付}
\biorevised{再受付}
\bioaccepted{採録}

\end{biography}

\end{document}

