



\documentstyle[epsf,jnlpbbl]{jnlp_j_b5}

\setcounter{page}{115}
\setcounter{巻数}{10}
\setcounter{号数}{3}
\setcounter{年}{2003}
\setcounter{月}{4}
\受付{2002}{4}{30}
\再受付{2002}{7}{15}
\採録{2002}{7}{29}

\setcounter{secnumdepth}{2}

\title{SENSEVAL2J 辞書タスクでの CRL の取り組み\\ --- 日本語単語の多義性解消における種々の機械学習\\手法と素性の比較 ---}
\author{村田 真樹\affiref{CRL} \and 内山 将夫\affiref{CRL} \and 内元 清貴\affiref{CRL} \and 馬 青\affiref{RK} \and 井佐原 均\affiref{CRL}}

\headauthor{村田，内山，内元，馬，井佐原}
\headtitle{SENSEVAL2J 辞書タスクでの CRL の取り組み}

\affilabel{CRL}{通信総合研究所}{Communications Research Laboratory}
\affilabel{RK}{龍谷大学理工学部}{Faculty of Science and Technology, Ryukoku University}

\thispagestyle{empty}
\jabstract{
本稿では，2001年に行なわれたSENSEVAL2 コンテストの
日本語辞書タスクでのわれわれの取り組みについて述べる．
われわれは機械学習手法を用いるアプローチを採用した．
この研究では数多くの機械学習手法と素性を比較検討し用いている．
コンテストには，我々は，サポートベクトルマシン法，シンプルベイズ法，
またそれらの組み合わせのシステム
二つの合計4システムを提出し，組合わせシステムが
参加システム中もっとも高い精度(0.786)を得た．
コンテストの後，シンプルベイズ法で用いていたパラメータを調節したところさらに
高い精度を得た．
現在もっとも性能の高いシステムは
二つのシンプルベイズ法を組み合わせたシステムであり，
その精度は 0.793 である．
また，本稿では素性を変更した実験もいくつか追加で行ない，
各素性の有効性，特徴を調査した．
その調査結果では文字列素性のみを用いても比較的高い精度が
得られるなどの興味深い知見が得られている．
また，関連文献も紹介し，今後の多義解消の研究のための
有益な情報を提供した．
}

\jkeywords{多義解消，シンプルベイズ法，日本語}

\etitle{CRL at Japanese dictionary-based task of\\ SENSEVAL-2\\ --- Comparison of various types of machine\\ learning methods
and features in Japanese\\ word sense disambiguation ---}
\eauthor{Masaki Murata\affiref{CRL} \and Masao Utiyama\affiref{CRL} \and Kiyotaka Uchimoto\affiref{CRL} \and Qing Ma\affiref{RK} \and Hitoshi Isahara\affiref{CRL}}

\eabstract{
This paper describes our work for 
the Japanese dictionary-based lexical-sample task of Senseval-2. 
In this work, 
we compared various types of machine learning methods and features. 
For the contest, 
we submitted four systems to the Japanese dictionary-based lexical-sample task of Senseval-2. 
They were i) a support vector machine method, 
ii) a simple Bayes method, 
iii) a method combining a support vector machine and simple Bayes method, and 
iv) a method combining two kinds of a support vector machine method and 
two kinds of a simple Bayes method. 
The combined methods produced the best precision (0.786) among all the systems 
submitted to the contest. 
After the contest, we tuned the parameter used in the simple Bayes method, and 
it obtained higher precision. 
The system which achieved the best precision now 
was the method combining the two simple Bayes methods 
and its precision was 0.793. 
In this paper, we discussed the results of experiments changing 
the features used and investigated 
the effectiveness and the characteristics of each feature. 
From these results, we obtained 
an interesting conclusion that 
we could obtained good precision 
when we only used string features, which 
are strings of 1-gram to 3-gram just before/after  
the analyzed morpheme. 
We also showed some related works 
that are useful for future work on word sense disambiguation. 
}

\ekeywords{Word Sense Disambiguation, Simple Bayes Method, Japanese Language}

\begin{document}
\maketitle


\section{はじめに}

われわれは2001年に行なわれた SENSEVAL2\cite{senseval2} の日本語辞書タスクのコンテストに参加した．
このコンテストでは，
日本語多義性の解消の問題を扱っており，
高い精度で日本語多義性の解消を実現するほどよいとされる．
われわれは機械学習手法を用いるアプローチを採用した．
機械学習手法としては多くのものを調査した方がよいと考え，
予備調査として先行研究\cite{murata_nlc2001_wsd}において
シンプルベイズ法，決定リスト法，サポートベクトルマシン法などの
手法を比較検討した．
その結果，シンプルベイズ法とサポートベクトルマシン法が
比較的よい精度を出したのでその二つの機械学習手法を
基本とすることにした．また，学習に用いる素性は，
豊富なほどよいと考え，文字列素性，形態素素性，
構文素性，共起素性，UDC素性(図書館などで用いられる国際十進分類を利用した素性)と，
非常に多くの素性を利用した．
コンテストには，
シンプルベイズ法，サポートベクトルマシン法，
またそれらの組み合わせのシステム二つの合計四つのシステムを
コンテストに提出した．
その結果，組合わせシステムが
参加システム中もっとも高い精度(0.786)を得た．
コンテストの後，
シンプルベイズ法で用いていたパラメータを調節したところさらに
高い精度を得た．
また，解析に用いる情報(素性)を変更する追加実験も行ない，
各素性の有効性，特徴を調査した．
本稿では，これらのシステムの説明と結果を述べる．

以降，\ref{sec:imp}節で多義解消の重要性を述べ，
\ref{sec:mondai_settei}節で本コンテストの問題設定を述べる．
\ref{sec:ml_method}節でわれわれが利用した機械学習手法について述べ，
\ref{sec:sosei}節でその機械学習手法で用いる素性について述べ，
\ref{sec:experiment}節でその機械学習手法と素性を用いた実験と
その考察について述べる．
\ref{ref:kanren}節では関連文献について述べる．

\section{多義性解消の重要性}
\label{sec:imp}

本節では多義性解消の重要性を例を用いながら説明する．

例えば，最近重要視されつつある
質問応答\cite{murata2000_1_nl,qac_hp}の問題を考えてみる．
ここで，質問応答の質問として以下のようなものがあったとしよう．
\begin{quote}
  「トラは天王寺動物園にはどのくらいいますか。」
\end{quote}
ここで質問応答システムの知識源として，
\begin{quote}
  「天王寺動物園にはトラが11頭飼育されており…」
\end{quote}
があったとする．この場合システムは「どのくらい」
などの表現から数量表現が解とわかるのでこの知識源から
「11頭」を解として正しく抽出することができる．
しかし，質問が以下のようであったとしよう．
\begin{quote}
  「虎は天王寺動物園にはどのくらいいますか。」
\end{quote}
この質問では先の質問の「トラ」が「虎」に変わっている．
この場合，「トラ」と「虎」が同一であることを計算機に
認識させなければならないが，
これをするためには計算機に単語の意味に関する情報を与えて
おかなければならない．
ここで例えば EDR 辞書\cite{edr}を利用すると，
「虎」の同義語としては
「トラ」「酒酔」「酒酔い」「酔客」「酔狂人」「酔人」
が得られる．
これは「虎」には
「虎という動物」と「酒に酔った人」の二つの意味があり，
この後ろの「酒に酔った人」の意味の場合の同義語が
得られて「酒酔い」などの不要な同義語が得られるのである．
この不要な同義語を使って解の抽出を試みる場合，
例えば
\begin{quote}
  \mbox{「昨夜未明，天王寺動物園で5人の酒酔い客が暴れだし…」}
\end{quote}
という文が知識中にある場合，
誤って「５人」を解として取り出してしまう可能性がある．
ここで多義性の解消をし，ここでの「虎」の意味は
「虎という動物」と認識し同義語は「トラ」だけであると
してから，解の抽出をした方が誤る可能性は減るのである．
「虎」の場合はまだ意味が二つであったからよいが，
高頻度に用いられる平易な語ほど語義の数が多く，
この問題は深刻なものとなる．
このことから，多義性の解消の重要性がわかる．

ここでは質問応答の場合の例をあげたが
各解析システムにおいて多義性の解消は同様に重要なものと
なろう．例えば，
照応解析\cite{murata_deno_nlp}においても，ある語Aが「人間」と「物」の
二つの意味をもっていて，
物しか指示しない「それ」という指示詞が出現した場合，
語Ａの多義性を解消し，もし「人間」であるということが
わかれば，この「それ」の指示先は語Aではありえないと
わかり他の指示先の候補を探せばよいとわかる．
また，機械翻訳でも
訳し分けが必要な語は多義性を解消しなければ
正しい翻訳をすることができない．
このように多義性の解消は種々の場面で役に立つものである．

\section{問題設定}
\label{sec:mondai_settei}

本節では SENSEVAL2 の日本語辞書タスクの問題設定について説明する．

SENSEVAL2 の日本語辞書タスクでは，
評価用のデータとしては
100 単語(このうち50単語が名詞で50単語が動詞)について
それぞれ 100 事例が与えられ，
合計 10000 事例が与えられた．
学習用のデータとしては，
RWC コーパス\cite{shirai_nl2001}が与えられた．
このコーパスは
毎日新聞の 1994 年の 3,000 個の記事を用いたもので，
コーパス中の主要な名詞，動詞，形容詞(総数：約15万個)に対して，
岩波国語辞典に基づいて定義された語義がふられている．
このタスクの目的は，
この語義をその単語のまわりの情報などを用いて
推定することである．
また，精度の評価には，SENSEVAL2 のホームページ\footnote{http://www.sle.sharp.co.uk/senseval2}より
取得できる scorer2 
という評価用プログラムによって算出される， mixed-grained score という
値が用いられた\footnote{具体的には ``scorer2 結果のファイル 正解のファイル sense-map -g mixed'' という
コマンドを打ち込むことにより算出される．}．

\section{機械学習手法}
\label{sec:ml_method}

一般に，単語多義性解消問題の場合，各単語にふられる語義は，
単語ごとにかわるので，機械学習手法による実験は各単語ごとに
逐次的に行なわれる．つまり，学習器は
単語の異なり数の分だけ作成する．
しかし本タスクでは，あらかじめ 50個が名詞で，
 50個が動詞であるとわかっている．このため，
システムは，単語ごとだけでなく，単語と品詞の組に対して
個々に作成した．本コンテストの場合，50個の名詞と
50個の動詞が対象であったので，合計 100 個の学習器を作ることになる．

本稿では学習器のために用いる機械学習手法としては，以下の方法を利用した\footnote{機械学習
手法としては，他に C4.5\cite{c4.5j}などの決定木学習を利用する方法があるが，
本稿では，種々の問題で決定木学習手法が他の手法に比べて劣っている
こと\cite{murata:nlken98,murata_haihan_rule_ipsj,taira_svm}，
また，本稿で扱う問題は属性の種類の数が多く C4.5 が走るまで
属性の数を減らすと精度が落ちるであろうことの二つの理由により，
用いていない．また，最大エントロピー法も有力な手法であるが，
システムの都合で動かない単語があったこと，また先行研究\cite{murata_nlc2001_wsd}において
最大エントロピー法が他の手法よりも精度が低かったことにより本稿では
用いていない．}．
\begin{itemize}
\item 
  シンプルベイズ法
\item 
  決定リスト法
\item 
  サポートベクトルマシン法
\end{itemize}
本節ではこれらの個々の機械学習手法の説明と，
これらの機械学習手法のいくつかを組み合わせる融合手法について説明する．

\subsection{シンプルベイズ法}

シンプルベイズ法は，ベイズの定理に基づいて
各分類になる確率を推定し，
その確率値が最も大きい分類を求める分類とする方法であり，
多義性解消の研究における基本的な方法である．
文脈 $b$ で分類 $a$ を出力する確率は
以下の式で与えられる．
{
\begin{eqnarray}
  p(a|b)  & = & \frac{p(a)}{p(b)}p(b|a)\\ 
  \label{eq:simple_bayes}
  & \simeq & \frac{\tilde{p}(a)}{p(b)} \prod_i \tilde{p}(f_i|a)
\end{eqnarray}
}
ただし，ここで文脈 $b$ は，
あらかじめ設定しておいた素性 $f_j (\in F, 1\leq j\leq k)$ 
の集合である．
$p(b)$ は，文脈 $b$ の出現確率で，
今回の場合分類 a に非依存で定数のため，計算しない．
$\tilde{p}(a)$ と $\tilde{p}(f_i|a)$ は，
それぞれ学習データから推定された確率で，
分類 a の出現の確率，
分類 a のときに素性 $f_i$ を持つ確率を意味する．
$\tilde{p}(f_i|a)$ として最尤推定し求めた値を用いると，
しばしば値がゼロになり，
式(\ref{eq:simple_bayes}) の値がゼロになり
分類先を決めるのが難しい場合が多い．
このため，スムージングがなされるが，
本稿では以下のスムージングをしたものを用いる．
{
\begin{eqnarray}
  \label{eq:simple_bayes2}
  \tilde{p}(f_i|a) = \frac{freq(f_i,a)+\epsilon*freq(a)}{freq(a)+\epsilon*freq(a)}
\end{eqnarray}
}
ただし，$freq(f_i,a)$ と $freq(a)$ は，それぞれ，
素性 $f_i$ を持ちかつ分類が $a$ である事例の個数，
分類が $a$ である事例の個数を意味する．
$\epsilon$ は実験で定める定数である．
本稿では，$\epsilon$ としては 0.01 と 0.0001 を用いた\footnote{\label{fn:bayes_epsilon} SENSEVAL2 のコンテストでは，
われわれは $\epsilon$ としては 0.01 を用いていた．
コンテストの終了後，$\epsilon$ として 0.1 から 0.00000001 まで 1/10 の倍率でいくつか
試してみた．その結果，学習用データでの10分割のクロスバリデーションの結果では
$\epsilon = 0.0001$ のときの値がもっともよかった．}．

\subsection{決定リスト法}

これは，素性$f_i$と分類先$a$の組を規則とし，
それらをあらかじめ定めた優先順序でリストに蓄えておき，
リストで優先順位の高いところから，
入力と素性が一致する規則を利用して分類先を求める方法である\cite{Yarowsky:ACL94}．
本稿では優先順序としては以下のものを用いる\footnote{$\tilde{p}(a|f_i)$ の値が
等しい場合は，出現頻度の多い規則，
すなわち，素性$f_j$で分類$a$である事例の多い規則を優先するように
している．}$^,$\footnote{Yarowsky の研究など一般に用いられている
決定リストでは，対数尤度比，
さらには，その尤度比をスムージングしたものが用いられている．
ところで最近では，
本手法と同じ確率$\tilde{p}(a|f_i)$の式の上で
ベイズ推定に基づくスムージングをした式を用いることで，
従来の尤度比を用いる方法よりも高い精度を得たという
報告\cite{tsuruoka_nlp2002}がある．
本稿ではこのあたりはそれほど注意深く検討していない．
種々のスムージングをすることで今よりもよい精度を得る可能性はある．}．
{
\begin{eqnarray}
  \label{eq:decision_list_order}
  \tilde{p}(a|f_i)
\end{eqnarray}
}
この方法は，以下の式で与えられる，
ある文脈 $b$ での分類 $a$ を出力する確率 $p(a|b)$ がもっとも高い
分類 $a$ を解とする方法と等価であり，
本稿では実際にはこの方法を用いて分類先を特定する．
{
\begin{eqnarray}
  \label{eq:decision_list}
  p(a|b) = \tilde{p}(a|f_{max})
\end{eqnarray}
}
ただし，$f_{max}$ は以下の式によって与えられる．
{
\begin{eqnarray}
  \label{eq:decision_list2}
  f_{max} = argmax_{f_j\in F} \ max_{a_i\in A} \ \tilde{p}(a_i|f_j)
\end{eqnarray}
}
また，$\tilde{p}(a_i|f_j)$ は学習データで素性$f_j$を
文脈に持つ場合の分類$a_i$の出現の割合である．

\begin{figure}[t]
      \begin{center}
      \epsfile{file=margin.eps,height=4cm,width=8cm} 
      \end{center}
    \caption{マージン最大化}
    \label{fig:margin}
\end{figure}

\subsection{サポートベクトルマシン法}

サポートベクトルマシン法は，
空間を超平面で分割することにより
2つの分類からなるデータを分類する手法である．
このとき，2つの分類が正例と負例からなるものとすると，
学習データにおける正例と負例の間隔(マージン)が
大きいもの(図\ref{fig:margin}参照\footnote{図の
白丸，黒丸は，正例，負例を意味し，
実線は空間を分割する超平面を意味し，
破線はマージン領域の境界を表す面を意味する．})ほど
オープンデータで誤った分類をする可能性が低いと考えられ，
このマージンを最大にする超平面を求め
それを用いて分類を行なう．
基本的には上記のとおりであるが，通常，
学習データにおいてマージンの内部領域に
少数の事例が含まれてもよいとする手法の拡張や，
超平面の線形の部分を非線型にする拡張(カーネル関数の導入)がなされたものが
用いられる．
この拡張された方法は，以下の識別関数を用いて分類することと等価であり，
その識別関数の出力値が正か負かによって
二つの分類を判別することができる\cite{SVM,kudoh_svm}．
{
\begin{eqnarray}
  \label{eq:svm1}
  f({\bf x}) & = & sgn \left( \sum^{l}_{i=1} \alpha_i y_i K({\bf x}_i,{\bf x}) + b \right)\\
  b & = & -\frac{max_{i,y_i=-1}b_i + min_{i,y_i=1}b_i}{2}\nonumber\\
  b_i & = & \sum^l_{j=1} \alpha_j y_j K({\bf x}_j,{\bf x}_i) \nonumber
\end{eqnarray}
}
ただし，${\bf x}$ は識別したい事例の文脈(素性の集合)を，
${\bf x}_{i}$と$y_i(i=1,...,l, y_i\in\{1,-1\})$は
学習データの文脈と分類先を意味し，関数 $sgn$ は，
{
\begin{eqnarray}
  \label{eq:svm2}
  sgn(x) \, = & 1 & (x \geq 0)\\
          & -1 & (otherwise) \nonumber
\end{eqnarray}
}
であり，また，各$\alpha_i$は式(\ref{eq:svm5})と式(\ref{eq:svm6})の制約のもと
式(\ref{eq:svm4})の$L(\alpha )$を最大にする場合のものである．
{
\begin{eqnarray}
  \label{eq:svm4}
  L({\alpha}) & = & \sum^l_{i=1} \alpha_i - \frac{1}{2} \sum^l_{i,j=1} \alpha_i \alpha_j y_i y_j K({\bf x_i},{\bf x_j})
\end{eqnarray}
}
{
\begin{eqnarray}
  \label{eq:svm5}
  0 \leq \alpha_i \leq C \, \, (i=1,...,l)
\end{eqnarray}
}
{
\begin{eqnarray}
  \label{eq:svm6}
  \sum^l_{i=1} \alpha_i y_i = 0 
\end{eqnarray}
}
また，関数$K$ はカーネル関数と呼ばれ，様々なものが
用いられるが本稿では以下の多項式のものを用いる．
{
\begin{eqnarray}
  \label{eq:svm3}
  K({\bf x},{\bf y}) & = ({\bf x}\cdot{\bf y} + 1)^d
\end{eqnarray}
}
$C,d$は実験的に設定される定数である．
本稿ではすべての実験を通して$C$,$d$はそれぞれ1と2に固定した．
ここで，$\alpha_i > 0$ となる ${\bf x}_i$ は，
サポートベクトルと呼ばれ，通常，式(\ref{eq:svm1})の和をとっている部分は
この事例のみを用いて計算される．
つまり，実際の解析には学習データのうちサポートベクトルと
呼ばれる事例のみしか用いられない．

サポートベクトルマシン法は
分類の数が2個のデータを扱うもので，通常これに
ペアワイズ手法を組み合わせて用いることで，
分類の数が3個以上のデータを扱うことになる\cite{kudoh_chunk_nl2000}．

ペアワイズ手法とは，N個の分類を持つデータの場合，
異なる二つの分類先のあらゆるペア(N(N-1)/2 個)を作り，
各ペアごとにどちらがよいかを2値分類器(ここでは
サポートベクトルマシン法\footnote{本稿の2値分類器としての
サポートベクトルマシンは，工藤氏が作成した TinySVM\cite{kudoh_svm} を利用している．})で求め，
最終的にN(N-1)/2個の2値分類器の分類先の多数決により，
分類先を求める方法である．

本稿のサポートベクトルマシン法は，
上記のようにサポートベクトルマシン法とペアワイズ手法を
組み合わせることによって実現される．

\subsection{融合手法}
\label{sec:yuugou}

本節では，いくつかの機械学習を組み合わせて用いる融合手法について
説明する．
われわれの融合手法では，
それぞれの単語ごとに用いる機械学習手法を変更する．
(厳密には，本稿の場合は単語と名詞の組に対して学習器を作成しているので，
この融合手法は
それぞれの単語と名詞の組ごとに機械学習手法を変更することになる．)
各単語ごとに用いられる機械学習手法は，
融合する機械学習手法のうち
学習データでの10分割のクロスバリデーションの精度がもっともよかったものとする．

われわれは融合手法としては以下の五つの手法を試した．
\begin{itemize}
\item 
  融合手法1 

  シンプルベイズ($\epsilon$=0.01)とサポートベクトルマシンの組み合わせ

\item 
  融合手法2

  二種類のシンプルベイズ($\epsilon$=0.01)と
  二種類のサポートベクトルマシンの合計四種類のシステムの組み合わせ

  ただし，ここでいう二種類は\ref{sec:sosei}節で後述する素性(解析に用いる情報)を
  すべて用いた場合と，KNP構文素性のみを削除した場合の二つの場合を意味する\footnote{KNP構文素性のみを削除した場合の
    ものを融合の一つに用いたのは，先行研究\cite{murata_nlc2001_wsd}において
    KNP構文素性のみを削除した場合の方が精度が高かった場合があったことによる．}．

\item 
  融合手法3

  シンプルベイズ($\epsilon$=0.0001)とサポートベクトルマシンの組み合わせ

\item 
  融合手法4

  二種類のシンプルベイズ($\epsilon$=0.0001)と
  二種類のサポートベクトルマシンの合計四種類のシステムの組み合わせ

  ただし，ここでいう二種類は\ref{sec:sosei}節で後述する素性(解析に用いる情報)を
  すべて用いた場合と，KNP構文素性のみを削除した場合の二つの場合を意味する．

\item 
  融合手法5

  $\epsilon$=0.01のシンプルベイズと
  $\epsilon$=0.0001のシンプルベイズの組み合わせ

\end{itemize}

\section{素性(解析に用いる情報)}
\label{sec:sosei}

前節で種々の機械学習の説明を述べたが，
それぞれの手法ともに素性(解析に用いる情報)を定義しなければ，
その手法を用いることができない．
本節ではその素性の説明を行なう．

\ref{sec:mondai_settei}節の問題設定で述べたように，
本稿の問題設定では，
日本語文の入力を与えられたときに，
その入力中の語義タグがふられていた各形態素に対して，
その語義の分類を推定して出力することになっている．
このため，解析に用いる情報，すなわち，素性は
入力される日本語文から取り出すことになる．

本稿では素性としては以下のものを定義する．
\begin{itemize}
\item 
  {\bf 文字列素性}\footnote{日本語の研究で機械学習手法の素性に
    文字列素性を用いた研究としては文献\cite{NLP98_nakano,inui:nlken98,murata_nlc2001}などがある．}

  \begin{itemize}
  \item 
    解析する形態素自身の文字列 

  \item 
    解析する形態素の直前の 1 〜 3 gram の文字列

  \item 
    解析する形態素の直後の 1 〜 3 gram の文字列

  \end{itemize}

\item 
  {\bf RWC形態素素性}

  \begin{itemize}
  \item 
    解析する形態素自身のRWCコーパスの品詞情報，品詞細分類情報，品詞細細分類情報\footnote{ここで，
      品詞情報，品詞細分類情報，品詞細細分類情報は，
      RWC コーパスでの 3, 4, 5 カラム目のものを意味する．}

  \item 
    解析する形態素の直前の形態素の単語自身，
    その単語の分類語彙表\cite{bgh}の 5桁，
    その単語の分類語彙表の 3桁，
    品詞情報，品詞細分類情報，品詞細細分類情報

  \item 
    解析する形態素の直後の形態素の単語自身，
    その単語の分類語彙表の 5桁，
    その単語の分類語彙表の 3桁，
    品詞情報，品詞細分類情報，品詞細細分類情報

  \end{itemize}

\item 
  {\bf JUMAN形態素素性}

  コーパスを JUMAN\cite{JUMAN3.6} で形態素解析し，
  その結果を素性として利用する．

  \begin{itemize}
  \item 
    解析する形態素自身の JUMAN の解析結果の品詞情報，品詞細分類情報，品詞活用形情報

  \item 
    解析する形態素の直前の形態素の単語自身，
    その単語の分類語彙表の 5桁，
    その単語の分類語彙表の 3桁，
    品詞情報，品詞細分類情報，品詞活用形情報

  \item 
    解析する形態素の直後の形態素の単語自身，
    その単語の分類語彙表の 5桁，
    その単語の分類語彙表の 3桁，
    品詞情報，品詞細分類情報，品詞活用形情報

  \end{itemize}

\item 
  {\bf 構文素性}

  コーパスを KNP\cite{KNP2.0b6} で構文解析し，
  その結果を素性として利用する．

  \begin{itemize}
  \item 
    解析する形態素を含む文節自身，また，
    その文節が体言かいなか，付属語の品詞，品詞細分類，活用情報

  \item 
    解析する形態素を含む文節の係り先の文節の自立語，
    その単語の分類語彙表の 5桁，
    その単語の分類語彙表の 3桁，
    品詞情報，品詞細分類情報，品詞活用形情報
    
  \item 
    解析する形態素を含む文節の係り元の文節の自立語，
    その単語の分類語彙表の 5桁，
    その単語の分類語彙表の 3桁，
    品詞情報，品詞細分類情報，品詞活用形情報．

    ただし，すべての場合において，付属語の情報を併用する．
    
  \end{itemize}

\item 
  {\bf 同一文内共起素性}

  コーパスを JUMAN で形態素解析し，
  その解析結果の形態素列を素性として利用する．

  \begin{itemize}
  \item 
    同一文中の各形態素，
    また，その単語の分類語彙表の 5桁，
    その単語の分類語彙表の 3桁．
    
  \end{itemize}

\item 
  {\bf UDC 素性}

  RWCコーパスには，各記事ごとに
  図書館などで書籍の分類に用いられる，
  国際十進分類(UDC)がふられている．
  この情報は各記事がどういう分野のものかを
  示している．

  \begin{itemize}
  \item 
    解析対象とする形態素を含む記事の UDC コード
    の最初の1桁，2桁，3桁．
    
  \end{itemize}
\end{itemize}

\section{実験}
\label{sec:experiment}

本節ではまず\ref{sec:experiment1}節で，
コンテストに提出したシステム，また，
コンテストの後に改良したシステムについて記述し，
その後\ref{sec:experiment2}節で，
素性を変更した実験について記述する．

\subsection{コンテストの実験結果(一部コンテスト後のシステムも含む)}
\label{sec:experiment1}

われわれは SENEVAL2 のコンテストに四つのシステム(CRL1 から CRL4)，
すなわち，
サポートベクトルマシン，シンプルベイズ($\epsilon=0.01$)，
融合手法1，融合手法2 を提出した．
また，コンテストの後，$\epsilon=0.0001$ を用いるシンプルベイズと
決定リスト法，融合手法3,4,5 を用いて実験を行なった．
これらの結果を表\ref{tab:result}に示す．
「ベースライン」は学習データで最も頻度の大きかった分類を
常に正解として選ぶ手法を意味する．
「コンテストの最高システム」は
コンテストに参加した団体によって提出された
すべてのシステムにおいてもっとも高いシステムを意味し，
実際にはそれは融合手法2であった．
表の値は，\ref{sec:mondai_settei}節でも述べた
scorer2 から算出される
mixed-grained score の値である．
値の分子が整数でないのは複数の語義が正解となる場合も考慮し
部分点も与えるためである\footnote{われわれのシステムでは
複数の解を出力することは基本的にできない．
このため，複数の解が答えになる場合はその複数の解を一つの組に
してその組を個々の解の分類とは別に新たに一つの分類として扱うことで，複数の解が答えになる場合を扱っている．}．
また，scorer2 のプログラムでは適合率(精度)と再現率を出力するが，
われわれのシステムではすべての事例に対して解を出すため
適合率(精度)と再現率の値は常に一致する．このため，表では
それらを精度として統一して記述している．



\begin{table*}[t]
\caption{実験結果}
\label{tab:result}
  \begin{center}
\small\renewcommand{\arraystretch}{}
\hspace*{-1cm}
\begin{tabular}[c]{|l|cc|}\hline
\multicolumn{1}{|c|}{方法}  & \multicolumn{2}{|c|}{精度} \\\hline
ベースライン & 0.726  & (7260.00/10000) \\
サポートベクトルマシン (CRL1)            & 0.783 & (7829.75/10000) \\
シンプルベイズ $\epsilon=0.01$ (CRL2)            & 0.778  & (7775.82/10000)\\
シンプルベイズ $\epsilon=0.0001$   & 0.790  & (7902.53/10000)\\
決定リスト            & 0.760 & (7603.70/10000) \\
融合手法 1 (CRL3)            & 0.786  & (7864.50/10000) \\
融合手法 2 (CRL4) (コンテストでの最高システム)   & 0.786  & (7864.83/10000) \\
融合手法 3               & 0.787  & (7873.33/10000) \\
融合手法 4            & 0.787  & (7869.00/10000) \\
融合手法 5              & 0.793  & (7933.37/10000)\\\hline
\end{tabular}
\end{center}
\end{table*}


表\ref{tab:result}の結果より以下のことがわかる．
\begin{itemize}
\item 
  すべてのシステムがベースラインよりも高い精度を出した．
  このことから，それぞれの手法ともにそれなりにまともな
  解析をしていることがわかる．

\item 
  提出した4システム(CRL1からCRL4)の中では，
  CRL4 の融合手法2 が最も高い精度 (0.786) をえた．

\item 
  また，コンテストの後，$\epsilon=0.0001$ を用いるシンプルベイズを
  用いたところ，さらに高い精度 (0.790) をえた．
  (この $\epsilon=0.0001$ の設定は脚注\ref{fn:bayes_epsilon}でのべたように学習データのみを用いて
  定めた値なので恣意的ではない．)
  この結果から，この問題ではシンプルベイズ法が有効であることがわかる．
  最近の機械学習手法を用いた種々の研究\cite{taira_svm,kudoh_chunk_nl2000,murata_nlc2001}では
  たいていの場合サポートベクトルマシン法がよいようだが，
  日本語多義解消ではそうではないことがあることがわかる\footnote{われわれの先行研究\cite{murata_nlc2001_wsd}では日本語多義性解消で
    サポートベクトルマシンがもっともよい精度としていたが，
    これはシンプルベイズで$\epsilon=0.01$で実験した場合であって，
    $\epsilon=0.0001$の場合はシンプルベイズの方がよいこともあることに注意して欲しい．}．
  しかし，シンプルベイズとサポートベクトルマシンの精度差は
  それほど大きいものではない．

\item 
  融合手法としては
  シンプルベイズ法でのパラメータ調整が適切でない場合つまり，
  $\epsilon=0.01$ のとき，
  サポートベクトルマシンが 0.783 でシンプルベイズが 0.778 でそれらの
  融合が 0.786であったので，精度向上を実現できる場合があることがわかる．
  しかし，
  シンプルベイズ法でのパラメータ調整を適切にした場合つまり，
  $\epsilon=0.0001$ のとき，
  サポートベクトルマシンが 0.783 でシンプルベイズが 0.790 でそれらの
  融合が 0.787であったので，精度向上を実現できていない．
  このことから，システムの融合がいつでも効果があるわけではなく
  効果がないことがあることもわかる．

\item 
  融合手法としては，
  $\epsilon=0.01$ のシンプルベイズと$\epsilon=0.0001$ のシンプルベイズを
  融合する手法も用いたが，この精度は 0.793 となり精度向上を実現した．
  これは単語ごとに最適な $\epsilon$ が異なることを示唆する．
  本稿のシンプルベイズ法では単純なスムージング法を採用しているが，
  本稿のスムージング法があまりよくないために単語ごとに最適なパラメータがかわっている
  可能性もある．今後は他のもう少し高度なスムージング法も試してみる必要があると思われる．
  ところで，融合手法としては同じシンプルベイズでもパラメータをかえたものを組み合わせることで
  精度向上ができる場合があることがわかった\footnote{$\epsilon$ を
    0.1 から 0.00000001 まで 0.1 倍ずつ変えたシステム 8個を本稿の融合手法で
    組み合わせたシステムでも実験してみたが，その精度は 0.01 と 0.0001 の融合手法と
    まったく同じ精度であった．}．

\end{itemize}

\begin{table}[t]
\caption{融合手法において用いられている学習手法の内訳}
\label{tab:yuugou_wariai}

\vspace*{.5cm}

\hspace*{1.5cm}
\begin{minipage}[h]{20cm}

(a) 融合手法1

\begin{tabular}[c]{|l|c|}\hline
\multicolumn{1}{|c|}{手法} & \multicolumn{1}{|c|}{用いられた単語の数}  \\\hline
サポートベクトルマシン  &  69 単語\\
シンプルベイズ ($\epsilon$=0.01)  &  31 単語\\\hline
\end{tabular}

\vspace{0.5cm}

(b) 融合手法2

\begin{tabular}[c]{|l|c|}\hline
\multicolumn{1}{|c|}{手法} & \multicolumn{1}{|c|}{用いられた単語の数}  \\\hline
サポートベクトルマシン  &  48 単語\\
シンプルベイズ ($\epsilon$=0.01)  &  24 単語\\
サポートベクトルマシン (KNP構文素性削除) &  19 単語\\
シンプルベイズ (KNP構文素性削除,$\epsilon$=0.01)  &  9 単語\\\hline
\end{tabular}

\vspace{0.5cm}

(c) 融合手法3

\begin{tabular}[c]{|l|c|}\hline
\multicolumn{1}{|c|}{手法} & \multicolumn{1}{|c|}{用いられた単語の数}  \\\hline
サポートベクトルマシン  &  37 単語\\
シンプルベイズ ($\epsilon$=0.0001)  &  63 単語\\\hline
\end{tabular}

\vspace{0.5cm}

(d) 融合手法4

\begin{tabular}[c]{|l|c|}\hline
\multicolumn{1}{|c|}{手法} & \multicolumn{1}{|c|}{用いられた単語の数}  \\\hline
サポートベクトルマシン  &  26 単語\\
シンプルベイズ ($\epsilon$=0.0001)  &  46 単語\\
サポートベクトルマシン (KNP構文素性削除) &  17 単語\\
シンプルベイズ (KNP構文素性削除,$\epsilon$=0.0001)  &  11 単語\\\hline
\end{tabular}

\vspace{0.5cm}

(e) 融合手法5

\begin{tabular}[c]{|l|c|}\hline
\multicolumn{1}{|c|}{手法} & \multicolumn{1}{|c|}{用いられた単語の数}  \\\hline
シンプルベイズ ($\epsilon$=0.01)  &  33 単語\\
シンプルベイズ ($\epsilon$=0.0001)  &  67 単語\\\hline
\end{tabular}

\end{minipage}


\end{table}


本研究では，融合手法としては\ref{sec:yuugou}節で述べたように
5つのものを用いている．
ここで，実際に実験対象である100単語のうち，どのくらいの割合で
融合される個々の機械学習手法が用いられていたかを示しておきたい．
この用いられた機械学習手法の内訳を表\ref{tab:yuugou_wariai}に示す．
ただし，融合手法において用いる学習手法は，
学習データでの10分割のクロスバリデーションの精度が
もっともよかったものとしているが，
この精度が同点であった場合は，
その同点であった学習手法のうち
表に示した順序で最初に出現した手法を用いている．
表\ref{tab:yuugou_wariai}からわかるように精度の高い手法
ほど，融合手法において多く利用されていることがわかる．
例えば，「サポートベクトルマシン」の精度は 0.783 で，
「シンプルベイズ($\epsilon$=0.01)」の精度は 0.778 で，
「サポートベクトルマシン」の方が精度が高いが，
実際に表\ref{tab:yuugou_wariai}(a)では，
「サポートベクトルマシン」の方が多く用いられている．

\begin{table*}[t]
\caption{素性を変更した場合の精度(全課題100単語)}
\label{tab:sosei_change}
  \begin{center}
\small\renewcommand{\arraystretch}{}
\begin{tabular}[c]{|l@{ }|c@{ }c@{ }|c@{ }c@{ }|c@{ }c@{ }|}\hline
\multicolumn{1}{|c|}{素性}  & \multicolumn{2}{|c|}{シンプルベイズ} & \multicolumn{2}{|c|}{サポートベクトル} & \multicolumn{2}{|c|}{決定リスト}  \\\hline
全素性利用          & {\bf 0.790}  & (7902.53/10000) & 0.783  & (7829.25/10000) & 0.760  & (7603.70/10000) \\\hline
文字列素性なし      & 0.786  & (7863.03/10000) & 0.781  & (7810.08/10000) & 0.757  & (7574.03/10000) \\
RWC形態素素性なし   & 0.787  & (7869.87/10000) & 0.780  & (7800.58/10000) & 0.759  & (7592.70/10000) \\
JUMAN形態素素性なし & 0.787  & (7872.70/10000) & 0.783  & (7826.75/10000) & 0.760  & (7598.20/10000) \\
KNP構文素性なし     & 0.784  & (7838.37/10000) & 0.779  & (7788.70/10000) & 0.757  & (7568.52/10000) \\
文内共起素性なし    & 0.778  & (7782.30/10000) & 0.770  & (7696.67/10000) & 0.766  & (7658.35/10000) \\
UDC 素性なし        & 0.788  & (7883.12/10000) & {\bf 0.784}  & (7842.58/10000) & 0.759  & (7589.70/10000) \\\hline
文字列素性のみ      & 0.760  & (7600.33/10000) & 0.777  & (7774.93/10000) & {\bf 0.773}  & (7731.63/10000) \\
RWC形態素素性のみ   & 0.710  & (7104.92/10000) & ------ & (---------/------) & 0.759  & (7589.30/10000)  \\
JUMAN形態素素性のみ & 0.723  & (7231.01/10000) & ------ & (---------/------) & 0.758  & (7579.07/10000) \\
KNP構文素性のみ     & 0.749  & (7487.18/10000) & 0.738  & (7379.62/10000) & -----  & (------/------) \\
文内共起素性のみ    & 0.754  & (7539.53/10000) & ------ & (---------/------) & 0.741  & (7410.77/10000) \\
UDC 素性のみ        & ------ & (---------/------) & ------ & (---------/------) & -----  & (------/------) \\\hline
\end{tabular}
\end{center}
\end{table*}

\begin{table*}[t]
\caption{素性を変更した場合の精度(名詞50単語)}
\label{tab:sosei_change_noun}
  \begin{center}
\small\renewcommand{\arraystretch}{}
\begin{tabular}[c]{|l@{ }|c@{ }c@{ }|c@{ }c@{ }|c@{ }c@{ }|}\hline
\multicolumn{1}{|c|}{素性}  & \multicolumn{2}{|c|}{シンプルベイズ} & \multicolumn{2}{|c|}{サポートベクトル} & \multicolumn{2}{|c|}{決定リスト}  \\\hline
全素性利用          & {\bf 0.782}  & (3911.50/5000)  & 0.780  & (3899.42/5000)  & 0.749  & (3745.58/5000) \\\hline
文字列素性なし      & 0.779  & (3897.00/5000)  & 0.779  & (3896.08/5000)  & 0.746  & (3729.92/5000) \\
RWC形態素素性なし   & 0.777  & (3886.67/5000)  & 0.775  & (3872.58/5000)  & 0.748  & (3737.58/5000) \\
JUMAN形態素素性なし & 0.778  & (3887.67/5000)  & 0.779  & (3895.75/5000)  & 0.748  & (3740.08/5000) \\
KNP構文素性なし     & 0.777  & (3883.83/5000)  & 0.772  & (3862.50/5000)  & 0.745  & (3725.25/5000) \\
文内共起素性なし    & 0.766  & (3828.08/5000)  & 0.768  & (3840.33/5000)  & 0.753  & (3762.92/5000) \\
UDC 素性なし        & 0.781  & (3906.83/5000)  & {\bf 0.785}  & (3922.58/5000)  & 0.747  & (3736.08/5000) \\\hline
文字列素性のみ      & 0.758  & (3790.08/5000)  & 0.767  & (3835.00/5000)  & 0.764  & (3819.58/5000) \\
RWC形態素素性のみ   & 0.745  & (3723.25/5000)  & ------ & (---------/------) & {\bf 0.771}  & (3852.67/5000)  \\
JUMAN形態素素性のみ & 0.737  & (3685.25/5000)  & ------ & (---------/------) & 0.766  & (3831.33/5000) \\
KNP構文素性のみ     & 0.721  & (3605.58/5000)  & 0.739  & (3695.08/5000)  & ------ & (---------/------) \\
文内共起素性のみ    & 0.739  & (3693.75/5000)  & ------ & (---------/------) & 0.726  & (3629.25/5000) \\
UDC 素性のみ        & ------ & (---------/------) & ------ & (---------/------) & ------ & (---------/------) \\\hline
\end{tabular}
\end{center}
\end{table*}

\begin{table*}[t]
\caption{素性を変更した場合の精度(動詞50単語)}
\label{tab:sosei_change_verb}
  \begin{center}
\small\renewcommand{\arraystretch}{}
\begin{tabular}[c]{|l@{ }|c@{ }c@{ }|c@{ }c@{ }|c@{ }c@{ }|}\hline
\multicolumn{1}{|c|}{素性}  & \multicolumn{2}{|c|}{シンプルベイズ} & \multicolumn{2}{|c|}{サポートベクトル} & \multicolumn{2}{|c|}{決定リスト}  \\\hline
全素性利用          & {\bf 0.798}  & (3991.03/5000)  & 0.786  & (3930.33/5000)  & 0.772  & (3858.12/5000) \\\hline
文字列素性なし      & 0.793  & (3966.03/5000)  & 0.783  & (3914.00/5000)  & 0.769  & (3844.12/5000) \\
RWC形態素素性なし   & 0.797  & (3983.20/5000)  & 0.786  & (3928.00/5000)  & 0.771  & (3855.12/5000) \\
JUMAN形態素素性なし & 0.797  & (3985.03/5000)  & 0.786  & (3931.00/5000)  & 0.772  & (3858.12/5000) \\
KNP構文素性なし     & 0.791  & (3954.53/5000)  & 0.785  & (3926.20/5000)  & 0.769  & (3844.27/5000) \\
文内共起素性なし    & 0.791  & (3954.22/5000)  & 0.771  & (3856.33/5000)  & 0.779  & (3895.43/5000) \\
UDC 素性なし        & 0.795  & (3976.28/5000)  & 0.784  & (3920.00/5000)  & 0.771  & (3853.62/5000) \\\hline
文字列素性のみ      & 0.762  & (3810.25/5000)  & {\bf 0.788}  & (3939.93/5000)  & {\bf 0.782}  & (3912.05/5000) \\
RWC形態素素性のみ   & 0.676  & (3381.66/5000)  & ------ & (---------/------) & 0.747  & (3736.63/5000)  \\
JUMAN形態素素性のみ & 0.709  & (3545.76/5000)  & ------ & (---------/------) & 0.750  & (3747.73/5000) \\
KNP構文素性のみ     & 0.776  & (3881.60/5000)  & 0.737  & (3684.53/5000)  & ------ & (---------/------) \\
文内共起素性のみ    & 0.769  & (3845.78/5000)  & ------ & (---------/------) & 0.756  & (3781.52/5000) \\
UDC 素性のみ        & ------ & (---------/------) & ------ & (---------/------) & ------ & (---------/------) \\\hline
\end{tabular}
\end{center}
\end{table*}

\subsection{素性を変更した実験}
\label{sec:experiment2}

次に解析に用いる情報，すなわち，素性を変更した場合の実験を行なった．
この実験では融合手法は用いず，シンプルベイズ($\epsilon=0.0001$)，サポートベクトルマシン，決定リストで行なった．
この結果を表\ref{tab:sosei_change}から表\ref{tab:sosei_change_verb}までに示す．
表\ref{tab:sosei_change}は課題の100単語すべてでの結果であり，
表\ref{tab:sosei_change_noun}と表\ref{tab:sosei_change_verb}は
名詞50単語，動詞50単語に対する結果である．
各表とも，まず全素性を用いる方法の精度を示し，
次に各素性を省いた場合のもの，最後に各素性のみを用いた場合のものを示している．
各手法で最も大きいときの精度を太字にしている．
また，表で ``------'' としている部分については，
システムおよび素性の設定の都合でシステムが問題を解けなかった単語が一部あった
ため精度が算出できなかったものを示す．

\begin{itemize}
\item 
  全課題での実験において精度がもっとも高かったのは，
  全素性を用いるシンプルベイズ法であった．
  シンプルベイズ法では，名詞50単語，動詞50単語でも，
  全素性を用いる方法が安定して最も高い精度を獲得した．

\item 
  サポートベクトルマシン法では，
  名詞では UDC 素性を用いない方法が最もよく，
  動詞では 文字列素性のみを用いる方法がもっともよかった．

\item 
  決定リスト法では名詞ではRWC形態素素性のみを用いる方法がもっともよく，
  動詞では文字列素性のみを用いる方法がもっともよかった．

\item 
  名詞50単語の精度では，
  UDC素性を用いないサポートベクトルマシンがもっとも精度がよい．
  このことから，常にシンプルベイズ法がよいわけではなく，
  他の方法の方がよい場合があることがわかる．

\item 
  表\ref{tab:sosei_change}の文字列素性のみを用いる場合の
  各手法の精度を見て欲しい．
  このときは，サポートベクトルマシン法がもっとも精度がよく
  0.777 であった．また，決定リスト法では 0.773 であった．
  これらの数字は最高精度に比較してもそれほど悪くなく，
  単なる文字列の素性だけでも高い精度を獲得できることを
  示している．
  また，特に動詞の場合は，サポートベクトルマシン，決定リスト法ともに
  文字列素性のみを用いた場合がもっとも精度が高い．

\end{itemize}

これらの結果から現状で日本語多義解消を行なう場合，
簡便性を考えると以下の二通りのアプローチがあると思われる．
\begin{itemize}
\item 
  手法としては簡便なシンプルベイズを用いるが，
  素性としてはさまざまなものを用意して高い精度を目指す．
  
\item 
  手法としては，サポートベクトルマシンや決定リストなどの
  強力な機械学習手法を用いるが，
  文字列素性などの少数の有用な素性のみを用いるもの．
  このアプローチでは前者に比べて素性が簡便であってもよい．
  
\end{itemize}

\section{関連文献}
\label{ref:kanren}

本節では関連文献について説明する．

英語単語の多義語の曖昧性解消に
機械学習手法を用いた研究は極めて
多数存在する\cite{Fuji98a,sense1,fukumoto_ipsj2001}が，
日本語単語の多義語の曖昧性解消に
機械学習手法を用いた研究は，
SENSEVAL2 以前はほとんどなかった\cite{shinou98}．
例えば，新納の研究では語の多義の解消ではなく，
同音異義語の判別を扱っていた．
その意味で，日本語多義解消の問題で「シンプルベイズ法」
「決定リスト法」「サポートベクトルマシン法」の三つの
機械学習手法，さらには，
素性を変化させた場合の実験結果を示している本稿は，
今後の日本語単語の多義語の曖昧性解消の問題を
考えるための資料として非常に役に立つものと思われる．

次にいくつかの関連研究を紹介したい．

まず，SENSEVAL2 で英語を含む数多くの言語で優秀な成績をとっていた
Yarowsky らのシステム\cite{yarowsky_s2}について説明する．
Yarowsky らのシステムは，シンプルベイス法と決定リストの組み合わせであり，
決定リストで求まる確信度が高いところでは決定リストの手法の解を
用い，それ以外の場合は種々の手法の多数決の結果を解とする手法である．
確信度を用い決定リストで確実に求まるところだけを
別個に扱っているところが興味深い．

次に，SENSEVAL2 の日本語辞書タスクに参加していた八木らの
システム\cite{yagi_nlc2001}について説明する．
八木らのシステムは決定リスト法を機械学習手法として
用いており，学習用のデータとして，
RWC コーパス以外に
岩波国語辞典の例文のデータを用いていることを特徴としている．
RWC コーパスでの語義の定義は
岩波国語辞典を用いているため，
岩波国語辞典の例文のデータも語義解析のための学習データとして
利用できるのである．
八木らの研究ではこの例文データも利用した場合の方が
利用しない場合よりも精度が高かったとしている．
この結果は，
われわれの研究でもこのデータを追加で用いることで精度を向上できる
可能性を示唆するものであり，興味深い．

次に，高村らの素性空間を再構成する手法\cite{takamura2001_NL}について説明する．
この研究は英語を対象に行なわれており，
機械学習手法としてはサポートベクトルマシン法を利用している．
この手法は学習に用いる素性を構成し直すところに特徴がある．
普通に抽出した素性だけでなく，その素性の分布に対して
独立成分分析や主成分分析を行ない，元々の素性よりも
一段抽象化したような素性を新たに作り出し，これも素性と
して追加で用いる方法である．
この素性を再構成する方法を含めた複数のシステムの多数決を
用いる方法で，単純なサポートベクトルマシン法の性能を
上回ったとしている．独立成分分析などにより素性の情報を
抽象化することでデータスパースネス対策などに役立っていると
思われ，興味深い．

最後に，中野らの AdaBoost を用いた手法\cite{nakano_ada}を説明する．
この研究では SENSEVAL2 日本語タスクのデータを対象としており，
AdaBoost を機械学習手法として利用している．
AdaBoost は正しく分類された事例の重みを下げ，
誤って分類された事例の重みを上げて，再学習をする手法である．
中野らの報告では，AdaBoost を利用することで
決定リスト法，シンプルベイズ法よりも高い精度を得たと
報告している．ただし，その最高精度は 79.1\,\% であり，
われわれの最高精度の 79.3\,\% より若干低い．
しかし，この結果はわれわれのシステムの素性の情報が豊かであるためで
ある可能性があり，
われわれのシステムの素性の情報で AdaBoost を利用すると
さらによい精度を得ることができる可能性がある．
しかし，中野らはわれわれのシステムでは用いていない
日本語語彙体系\cite{ntt}の辞書の情報を用いており，
中野らの方が素性の情報が少ないとは言い切れないので，
われわれのシステムの素性の情報で AdaBoost を利用すると
さらによい精度を得ることができるかどうかはわからない．
また，われわれのシステムの素性の情報の方が豊富であっても，
われわれのシステムの素性で AdaBoost が本当によい精度を出せるか
どうかはわからない．

次に，多義性の研究に直接は関係はないが，
複数の機械学習の方法を組み合わせるのに，スタッキングを
用いる手法\cite{Halteren_cl2001}について説明したい．
スタッキングを用いる手法とは，もともとの素性の他に，
複数の機械学習の結果を素性として追加し，その追加された素性を
用いて機械学習を行なう方法である．
従来は複数の機械学習の方法の融合には多数決が
多く用いられていたが，
スタッキングの方法ではどの機械学習の方法がよいかを学習することに
なっており
たいていの場合で多数決の方法よりも精度が高くなると思われる．
このスタッキングを利用する研究としては，
形態素解析のもの\cite{Halteren_cl2001}や固有名詞表現抽出のもの\cite{Utsuro_ne}などがある．本稿でのシステムの融合では，
各単語でもっとも精度の高い手法を利用していた．
このわれわれの融合手法は各単語ごとに用いる手法を
最尤推定で求めるものになっていて少々は
融合に学習を用いていることにはなっている．
しかし，手法の組み合わせにおいても強力な学習手法を用いた方が
精度はよいと思われるので，われわれの手法でもスタッキングを
利用することを考えた方がよい．

以上，種々の有力な関連研究を紹介した．
それぞれの手法ともに特徴的な要素を持っており，
\ref{sec:experiment2}節の最後に述べた考察と含めて
それらを総合的に考察し，
それぞれの手法の良い面を組み合わせることで，
さらによりよい多義解消を行なえると思われる．

\section{おわりに}

本稿では，2001年に行なわれたSENSEVAL2 コンテストの
日本語辞書タスクでのわれわれの取り組みについて述べた．
我々は，サポートベクトルマシン法，シンプルベイズ法，
またそれらの組み合わせのシステム
二つの合計4システムを提出し，組合わせシステムが
参加システム中もっとも高い精度(0.786)を得た．
コンテストの後，シンプルベイズ法で用いていたパラメータを
調節したところさらに高い精度を得た．
現在もっとも性能の高いシステムは
二つのシンプルベイズ法を組み合わせたシステムであり，
その精度は 0.793 である．
また，本稿では素性を変更した実験もいくつか追加で行ない，
各素性の有効性，特徴を調査した．
その調査結果では文字列素性のみを用いても比較的高い精度が
得られるなどの興味深い知見が得られている．
また，関連文献も紹介し，今後の多義解消の研究のための
有益な情報を提供した．

\acknowledgment

SENSEVAL2 の運営，および，本特集号に尽力
してくださいました，東大黒橋助教授と北陸先端大白井助教授を
はじめとする方々に感謝いたします．



\begin{thebibliography}{}

\bibitem[\protect\BCAY{Cristianini \BBA\ Shawe-Taylor}{Cristianini \BBA\
  Shawe-Taylor}{2000}]{SVM}
Cristianini, N.\BBACOMMA\  \BBA\ Shawe-Taylor, J. \BBOP 2000\BBCP.
\newblock {\Bem An Introduction to Support Vector Machines and Other
  Kernel-based Learning Methods}.
\newblock Cambridge University Press.


\bibitem[\protect\BCAY{藤井}{藤井}{1998}]{Fuji98a}
藤井敦 \BBOP 1998\BBCP.
\newblock \JBOQ コーパスに基づく多義性解消\JBCQ\
\newblock \Jem{人工知能学会}, {\Bbf 13}  (6), pp.~904--911.

\bibitem[\protect\BCAY{福本}{福本}{2002}]{fukumoto_ipsj2001}
福本文代 \BBOP 2002\BBCP.
\newblock \JBOQ 語義の曖昧性解消のための最適な属性選択\JBCQ\
\newblock \Jem{情報処理学会論文誌}, {\Bbf 43}  (1).

\bibitem[\protect\BCAY{Halteren, van Zavrel, \BBA\ Daelemans}{Halteren
  et~al.}{2001}]{Halteren_cl2001}
Halteren, V. H., Zavrel, J., \BBA\ Daelemans, W. \BBOP 2001\BBCP.
\newblock \BBOQ Improving Accuracy in Word Class Tagging through the
  Combination of Machine Learning Systems\BBCQ\
\newblock {\Bem Computational Linguistics}, {\Bbf 27}  (2), pp.~199--229.

\bibitem[\protect\BCAY{Ide \BBA\ Mylonas}{Ide \BBA\ Mylonas}{2000}]{sense1}
Ide, N.\BBACOMMA\  \BBA\ Mylonas, E. \BBOP 2000\BBCP.
\newblock {\Bem Computers and the humanities, Special Issue on SENSEVAL}.
\newblock Kluwer Academic Publishers.

\bibitem[\protect\BCAY{池原，宮崎，白井，横尾，中岩，小倉，大山，林}{池原\Jetal
  }{1997}]{ntt}
池原悟，宮崎正弘，白井諭，横尾昭男，中岩浩巳，小倉健太郎，大山芳史，林良彦
  \BBOP 1997\BBCP.
\newblock \Jem{日本語語彙体系}.
\newblock 岩波書店.

\bibitem[\protect\BCAY{乾，内元，村田，井佐原}{乾\Jetal }{1998}]{inui:nlken98}
乾裕子，内元清貴，村田真樹，井佐原均 \BBOP 1998\BBCP.
\newblock \JBOQ 文末表現に着目した自由回答アンケートの分類\JBCQ\
\newblock \Jem{情報処理学会 自然言語処理研究会 NL128-25}.

\bibitem[\protect\BCAY{キンラン}{キンラン}{1995}]{c4.5j}
J.R.キンラン \BBOP 1995\BBCP.
\newblock \Jem{AIによるデータ解析}.
\newblock トッパン.

\bibitem[\protect\BCAY{国立国語研究所}{国立国語研究所}{1964}]{bgh}
国立国語研究所 \BBOP 1964\BBCP.
\newblock \Jem{分類語彙表}.
\newblock 秀英出版.

\bibitem[\protect\BCAY{工藤，松本}{工藤，松本}{2000}]{kudoh_chunk_nl2000}
工藤拓，松本裕治 \BBOP 2000\BBCP.
\newblock \JBOQ Support Vector Machine を用いた Chunk 同定\JBCQ\
\newblock \Jem{自然言語処理研究会 2000-NL-140}.

\bibitem[\protect\BCAY{Kudoh}{Kudoh}{2000}]{kudoh_svm}
Kudoh, T. \BBOP 2000\BBCP.
\newblock \BBOQ {TinySVM: Support Vector Machines}\BBCQ\ \\
\newblock {http://cl.aist-nara.ac.jp/\verb+~+taku-ku/software/TinySVM/index.html}.

\bibitem[\protect\BCAY{黒橋，長尾}{黒橋，長尾}{1998}]{JUMAN3.6}
黒橋禎夫，長尾真 \BBOP 1998\BBCP.
\newblock \Jem{日本語形態素解析システム {JUMAN} 使用説明書 version 3.6}.
\newblock 京都大学大学院工学研究科.

\bibitem[\protect\BCAY{黒橋}{黒橋}{1998}]{KNP2.0b6}
黒橋禎夫 \BBOP 1998\BBCP.
\newblock \Jem{日本語構文解析システム {KNP} 使用説明書 version 2.0b6}.
\newblock 京都大学大学院情報学研究科.

\bibitem[\protect\BCAY{村田，長尾}{村田，長尾}{1997}]{murata_deno_nlp}
村田真樹，長尾真 \BBOP 1997\BBCP.
\newblock \JBOQ
  用例や表層表現を用いた日本語文章中の指示詞・代名詞・ゼロ代名詞の指示対象の推
定\JBCQ\
\newblock \Jem{言語処理学会誌}, {\Bbf 4}  (1).

\bibitem[\protect\BCAY{村田，内元，馬，井佐原}{村田\Jetal
  }{1998}]{murata:nlken98}
村田真樹，内元清貴，馬青，井佐原均 \BBOP 1998\BBCP.
\newblock \JBOQ 学習による文節まとめあげ ---
  決定木学習，最大エントロピー法，用例ベースによる手法と排反な規則を用いる新手
法の比較 ---\JBCQ\
\newblock \Jem{情報処理学会 自然言語処理研究会 NL128-4}.

\bibitem[\protect\BCAY{村田，内元，馬，井佐原}{村田\Jetal
  }{2000a}]{murata_haihan_rule_ipsj}
村田真樹，内元清貴，馬青，井佐原均 \BBOP 2000a\BBCP.
\newblock \JBOQ 排反な規則を用いた文節まとめあげ\JBCQ\
\newblock \Jem{情報処理学会論文誌}, {\Bbf 41}  (1), pp.~59--69.

\bibitem[\protect\BCAY{村田，内山，井佐原}{村田\Jetal
  }{2000b}]{murata2000_1_nl}
村田真樹，内山将夫，井佐原均 \BBOP 2000b\BBCP.
\newblock \JBOQ 類似度に基づく推論を用いた質問応答システム\JBCQ\
\newblock \Jem{自然言語処理研究会 2000-NL-135}, pp.~181--188.

\bibitem[\protect\BCAY{村田，馬，内元，井佐原}{村田\Jetal
  }{2001a}]{murata_nlc2001}
村田真樹，馬青，内元清貴，井佐原均 \BBOP 2001a\BBCP.
\newblock \JBOQ
  サポートベクトルマシンを用いたテンス・アスペクト・モダリティの日英翻訳\JBCQ\
\newblock \Jem{電子情報通信学会 言語理解とコミュニケーション研究会 NLC2000-78}.

\bibitem[\protect\BCAY{村田，内山，内元，馬，井佐原}{村田\Jetal
  }{2001b}]{murata_nlc2001_wsd}
村田真樹，内山将夫，内元清貴，馬青，井佐原均 \BBOP 2001b\BBCP.
\newblock \JBOQ 種々の機械学習手法を用いた多義解消実験\JBCQ\
\newblock \Jem{電子情報通信学会 言語理解とコミュニケーション研究会 NLC2001-2}.

\bibitem[\protect\BCAY{中野，村田，長尾}{中野\Jetal }{1998}]{NLP98_nakano}
中野貴之，村田真樹，長尾真 \BBOP 1998\BBCP.
\newblock \JBOQ ネットニュース記事のタイプ分類\JBCQ\
\newblock \Jem{言語処理学会 第４回年次大会}.

\bibitem[\protect\BCAY{中野，平井}{中野，平井}{2002}]{nakano_ada}
中野桂吾，平井有三 \BBOP 2002\BBCP.
\newblock \JBOQ {AdaBoost} を用いた語義の曖昧性解消\JBCQ\
\newblock \Jem{言語処理学会 第８回年次大会}.

\bibitem[\protect\BCAY{日本電子化辞書研究所}{日本電子化辞書研究所}{1993}]{edr}
日本電子化辞書研究所 \BBOP 1993\BBCP.
\newblock \JBOQ {EDR}電子化辞書仕様説明書\JBCQ.

\bibitem[\protect\BCAY{{NTCIR-3 QAC task committee}}{{NTCIR-3 QAC task
  committee}}{2001}]{qac_hp}
{NTCIR-3 QAC task committee} \BBOP 2001\BBCP.
\newblock
\newblock \BBOQ Question Answering Challenge (QAC) Home Page\BBCQ. \\
\newblock {http://www.nlp.cs.ritsumei.ac.jp/qac/}.

\bibitem[\protect\BCAY{新納}{新納}{1998}]{shinou98}
新納浩幸 \BBOP 1998\BBCP.
\newblock \JBOQ 複合語からの証拠に重みをつけた決定リストによる同音異
  義語判別\JBCQ\
\newblock \Jem{情報処理学会論文誌}, {\Bbf 39}  (12).

\bibitem[\protect\BCAY{白井，柏野，橋本，徳永，有田，井佐原，荻野，小船，高橋，長尾，橋田，村田}{白井\Jetal }{2001}]{shirai_nl2001}
白井清昭，柏野和佳子，橋本三奈子，徳永健伸，有田英一，井佐原均，荻野紫穂，小船隆一，高橋裕信，長尾確，橋田浩一，村田真樹 \BBOP 2001\BBCP.
\newblock \JBOQ 岩波国語辞典を利用した語義タグ付きテキストデータベー
  スの作成\JBCQ\
\newblock \Jem{自然言語処理研究会 2001-NL-141}.

\bibitem[\protect\BCAY{平，春野}{平，春野}{2000}]{taira_svm}
平博順，春野雅彦 \BBOP 2000\BBCP.
\newblock \JBOQ Support Vector Machine によるテキスト分類に
  おける属性選択\JBCQ\
\newblock \Jem{情報処理学会論文誌}, {\Bbf 41}  (4), pp.~1113--1123.

\bibitem[\protect\BCAY{高村，山田，工藤，山本，松本}{高村\Jetal
  }{2001}]{takamura2001_NL}
高村大也，山田寛康，工藤拓，山本薫，松本裕治 \BBOP 2001\BBCP.
\newblock \JBOQ 素性空間再構成による Word-Sense Disambiguation\JBCQ\
\newblock \Jem{情報処理学会 自然言語処理研究会 2001-NL-144}.

\bibitem[\protect\BCAY{鶴岡，近山}{鶴岡，近山}{2002}]{tsuruoka_nlp2002}
鶴岡慶雅，近山隆 \BBOP 2002\BBCP.
\newblock \JBOQ ベイズ統計の手法を利用した決定リストのルール信頼度推定法\JBCQ\
\newblock \Jem{言語処理学会誌}, {\Bbf 9}  (3).

\bibitem[\protect\BCAY{宇津呂，颯々野，内元}{宇津呂\Jetal }{2002}]{Utsuro_ne}
宇津呂武仁，颯々野学，内元清貴 \BBOP 2002\BBCP.
\newblock \JBOQ 正誤判別規則学習を用いた複数の日本語固有表
  現抽出システムの出力の混合\JBCQ\
\newblock \Jem{言語処理学会誌}, {\Bbf 9}  (1).


\bibitem[\protect\BCAY{八木，野呂，白井，徳永，田中}{八木\Jetal
  }{2001}]{yagi_nlc2001}
八木豊，野呂智哉，白井清昭，徳永健伸，田中穂積 \BBOP 2001\BBCP.
\newblock \JBOQ 決定リストを用いた語義曖昧性解消\JBCQ\
\newblock \Jem{電子情報通信学会 言語理解とコミュニケーション研究会 NLC2001-42}.

\bibitem[\protect\BCAY{Yarowsky}{Yarowsky}{1994}]{Yarowsky:ACL94}
Yarowsky, D. \BBOP 1994\BBCP.
\newblock \BBOQ Decision Lists For Lexical Ambiguity Resolution: Application to
  Accent Restoration in {Spanish} and {French}\BBCQ\
\newblock In {\Bem 32rd Annual Meeting of the Association of the Computational
  Linguistics}, \BPGS\ pp.~88--95.

\bibitem[\protect\BCAY{Yarowsky}{Yarowsky}{2001}]{senseval2}
Yarowsky, D. \BBOP 2001\BBCP.
\newblock {\Bem Proceedings of SENSEVAL-2}.

\bibitem[\protect\BCAY{Yarowsky, Cucerzan, Florian, Schafer, \BBA\
  Wicentowski}{Yarowsky et~al.}{2001}]{yarowsky_s2}
Yarowsky, D., Cucerzan, S., Florian, R., Schafer, C., \BBA\ Wicentowski, R.
  \BBOP 2001\BBCP.
\newblock \BBOQ The Johns Hopkins SENSEVAL-2 System Descriptions\BBCQ\
\newblock In {\Bem Proceedings of SENSEVAL-2}.

\end{thebibliography}


\begin{biography}
\biotitle{略歴}
\bioauthor{村田 真樹}{
1993年京都大学工学部卒業．
1995年同大学院修士課程修了．
1997年同大学院博士課程修了，博士（工学）．
同年，京都大学にて日本学術振興会リサーチ・アソシエイト．
1998年郵政省通信総合研究所入所．
現在，独立行政法人通信総合研究所主任研究員．
自然言語処理，機械翻訳，情報検索の研究に従事．
言語処理学会，情報処理学会，人工知能学会，電子情報通信学会，ACL，各会員．}
\bioauthor{内山 将夫}{
1992年筑波大学第三学群情報学類卒業．
1997年筑波大学大学院工学研究科博士課程修了．博士（工学）．
1997年信州大学工学部電気電子工学科助手．
1999年郵政省通信総合研究所非常勤職員．
2001年独立行政法人通信総合研究所任期付き研究員．
自然言語処理の研究に従事．
言語処理学会，情報処理学会，人工知能学会，日本音響学会，ACL，各会員．}
\bioauthor{内元 清貴}{
1994年京都大学工学部卒業．
1996年同大学院修士課程修了．
同年郵政省通信総合研究所入所．現在，独立行政法人通信総合研究所研究員．
自然言語処理の研究に従事．
言語処理学会，情報処理学会，ACL，各会員．}
\bioauthor{馬 青}{
1983年北京航空航天大学自動制御学部卒業．
1987年筑波大学大学院理工学研究科修士課程修了．
1990年同大学院工学研究科博士課程修了．工学博士．
1990 $\sim$ 93年株式会社小野測器勤務．
1993年郵政省通信総合研究所入所．
1994年同所主任研究官．
2003年龍谷大学理工学部教授．
人工神経回路網モデル，知識表現，自然言語処理の研究に従事． 
日本神経回路学会，言語処理学会，電子情報通信学会，各会員．}
\bioauthor{井佐原 均}{
1978年京都大学工学部電気工学第二学科卒業．
1980年同大学院修士課程修了．博士（工学）．
同年通商産業省電子技術総合研究所入所．
1995年郵政省通信総合研究所．
現在，独立行政法人通信総合研究所けいはんな情報通信融合研究センター自然言語グループリーダー．
自然言語処理，機械翻訳の研究に従事．
言語処理学会，情報処理学会，人工知能学会，日本認知科学会，各会員．}

\bioreceived{受付}
\biorevised{再受付}
\bioaccepted{採録}

\end{biography}

\end{document}

