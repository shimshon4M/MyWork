<?xml version="1.0" ?>
<root>
  <addtolength>-0.5pt</addtolength>
  <addtolength>-0.5pt</addtolength>
  <section title="Introduction">DiscoveringimplicitinformationfromnaturallanguagediscourseisessentialtoawiderangeofNLPtasks,suchasquestionanswering,informationextractionandrecognizingtextualentailment(RTE).SeveralNLPcomponentsforprocessingavarietyofdiscoursephenomena(e.g.,anaphoraresolution)areexploitedwheninferringimplicitinformation.Inthefieldofcomputationallinguistics,eachNLPcomponenthasbeenstudiedextensivelyinrecentdecades;however,lessattentionhasbeenpaidtohowtointegratethemintoasingleinferenceframework.Inthispaper,weexploreabduction-baseddiscourseprocessingasaframeworkforintegratingNLPcomponents.Abduction,definedhereasinferencetothebestexplanation,haslongbeenstudiedinawiderangeofcontexts.Forexample,abductionhasbeenviewedasapromisingframeworkfordescribingthemechanismofhumanperception.Theideaisthatthedeclarativenatureofabductionenablesustoinferthemostplausible,implicitlystatedinformationcombiningseveraltypesofinferenceandpiecesofexplicitlyobservedinformation,ashumansdo.Abduction-baseddiscourseprocessingwasstudiedintensivelyinthe1980sand1990s;Hobbsetal.(1993)Hobbs93showthatthelowest-costabductiveproofprovidesthesolutiontoabroadrangeofnaturallanguageunderstandingproblems,suchaswordsensedisambiguation,anaphora,andmetonymyresolution.AsdetailedinSec.~,thekeyadvantageofusingabductionfordiscourseprocessingistwofold:abduction-baseddiscourseprocessingmodelsinterdependenciesbetweenNLPtasksandidentifiesthemostcoherentinterpretation;itdiscoversplausiblenewinformationbycombiningheterogeneousinferencerulesandpiecesofinformationobservedfromtexts.Whilethelackofworldknowledgeresourceshamperedapplyingabductiontoreal-lifeproblemsinthe1980sand1990s,anumberoftechniqueshavebeendevelopedinthelastdecade.Consequently,severalresearchersstartedapplyingabductiontoreal-lifeproblems,exploitinglargeknowledgebases.Forinstance,inspiredbyHobbsetal.(1993)Hobbs93,proposedanabduction-basedNLPframeworkusingfortythousandaxiomsextractedfromthepopularontologicalresourcesWordNetandFrameNet.Theyevaluatetheirapproachonthereal-lifenaturallanguageprocessingtaskofRTE.However,inordertoapplyabductiontoreal-lifeproblemswithalarge-scaleknowledgebase,westillneedtoaddressthefollowingissue:howtosearchforthebestexplanationefficiently.Inthispaper,weadoptfirst-orderlogic-based,cost-basedabduction(henceforthfirst-ordercost-basedabduction),whereweusefunction-freefirst-orderlogic(FOL)asarepresentationlanguage.Infirst-ordercost-basedabduction,anexplanationisrepresentedbyasetofliterals,andtheplausibilityoftheexplanationisevaluatedthroughthesumofthecostsdefinedoneachliteral.Thebestexplanationisdefinedasthelowest-costexplanation.Findingthelowest-costexplanationcanbereducedtoaconstrainedcombinatorialoptimizationproblemwithrespecttothecostfunction,whichisanNP-hardproblem;thishamperstheapplicationofabductionwithlargeknowledgeresourcestoreal-lifeproblems.Infact,Ovchinnikovaetal.(2011)Ovch11reportthattheMini-TACITUScost-basedabductionsystemcouldnotsearchtheentiresearchspaceofexplanationswithin30mininmostoftheRTEproblemsintheirexperiments.Intheliterature,manyresearchershavetriedtoovercomecost-basedabduction'sinefficiencyusingarangeofmethodsfromapproximationtoexactinference~.Forexample,Santos(1994)formulatedcost-basedabductioninpropositionallogicusingintegerlinearprogramming(ILP),andshoweditsefficiency.However,tothebestofourknowledge,mostoftheproposedmethodsareoptimizedforpropositionallogic.Inordertoemploythesemethodsforfirst-ordercost-basedabduction,weneedtotransformknowledgebasesandobservationstopropositionallogic(henceforth,wecallthetransformationgrounding).Theprocessofgroundinggeneratesahugesearchspaceanddoesnotscaletolargerproblems,asdiscussedinSec.~.Inthispaper,weprovideascalablesolutiontofirst-ordercost-basedabductionwiththefollowingcontributions:weproposeanefficientsearchtechniqueforFOLabduction,whichavoidsexpandingfirst-orderlogicalformulaetopropositionallevel;weformulatethebest-explanationfindingproblemasanILPoptimizationproblemtomakeourframeworkextensible,supportingdefiniteclausesinbackgroundknowledge;wedescribehowcuttingplaneinference(CPI),aniterativeoptimizationstrategydevelopedinoperationsresearch,canbeexploitedformakingFOLabductiontractable,showingitsefficiencybyprovidingevaluationonalarge,real-lifedataset;theabductiveinferenceenginepresentedinthispaperismadepubliclyavailable.Ourpaperisstructuredasfollows.Westartwithabriefintroductionofabduction-baseddiscourseprocessing,takingHobbs'sinterpretationasabductionframeworkasamotivatingexample(Sec.~).WethendescribehowtoperformefficientsearchesinFOLabduction,andformulatethesearchproblemasanILPoptimizationproblem(Secs.~and).WethenshowhowCPIenablesustoapplyFOLabductionwithlargeknowledgebases(Sec.~).Finally,weevaluatetheefficiencyofourCPI-basedframeworkonalarge,real-lifeproblemofNLP,RTE(Sec.~),andgiveacomparisonofourworktopriorimplementationsofcost-basedabduction(Sec.~).</section>
  <section title="Background"/>
  <subsection title="Interpretation as Abduction">Hobbsetal.(1993)Hobbs93pioneeredanabduction-basedapproachfornaturallanguageunderstanding.Thekeyideaisthat``interpretingsentencesistoprovethelogicalformsofsentences,allowingassumptions,mergingredundancieswherenecessary.''TheydemonstratedthatawiderangeofNLPtasksinvolvedindiscourseinterpretation,includinganaphoraresolutionanddiscourserelationrecognition,canbecastastheproblemoffindinganexplanationtothepiecesofinformationobservedfromthediscourse.Figure~showsanexampletakenfromHobbsetal.1993Hobbs93.Inthisexample,wesolvethreetypesofNLPtasks:(i)coreferenceresolution,e.g.,thecoreferencerelationbetweenJohnandhe(x1=y1);(ii)intentrecognition,e.g.,theintentionofJohn(backwardinferenceongo(e1,x1,x2)toloan(y2));and(iii)wordsensedisambiguation,e.g.,themeaningofbankisnotariverbank,butafinancialinstitution(backwardinferenceonbank(x2)tofinancial_inst(x2),wheretheinferencerulemeansthat``afinancialinstitutionisexpressedasbankinatext'').Whilethelackofknowledgewasaseriousprobleminapplyingabductiontoreal-lifeproblemsinthe1980sand1990s,thesituationhaschanged.Anumberoftechniquesthatacquirelexicalresourcesusefulforlanguageprocessinghavebeendeveloped.Giventherecentadvancesintechniques,severalresearchersstartedpursuingabduction-basedapproachesfor``real-life''problems,usinglargeknowledgebases.Forinstance,Ovchinnikovaetal.(2011)Ovch11tookonthepopular,knowledge-intensiveNLPtaskofRTE.Aseriesofstudiesinmachinereadingprojects,whichdiscoverimplicitinformationfromtexts,canalsobeviewedasanabductiveinterpretationproblem.</subsection>
  <subsection title="Cost-based abduction">Abductionisinferencetothebestexplanation.Weusefunction-freefirst-orderlogicasthemeaningrepresentationofabductioninthispaper.Formally,first-orderlogicalabductionisdefinedasfollows:.Whileinductionfindsasetofplausiblerulesfromobservations,abductionfindsasetofplausiblefacts.Given:BackgroundknowledgeBandobservationsO,whereBisasetoffirst-orderlogicalformulaeandOisasetofliteralsorequalities.Find:Anexplanation(orhypothesis)HsuchthatHBO,HB,whereHisasetofliteralsorequalities.EachelementinHiscalledanelementalexplanation.Letusdefinesometerminologies.Wedefineequalitytobetheformx=y(positiveequality)orx=y(negativeequality),wherexandyareeithervariablesorconstants.Theequalityx=ymeansthatreferentsofxandyarethesame(i.e.,p(x),p(y),x=yhasthesamemeaningasp(x)).WesaythattheliteralpisthelogicalconsequenceofSifSp;pis(explicitly)hypothesizedw.r.t.HifpH;pisimplicitlyhypothesizedifHBpw.r.t.HandB(i.e.,pisalogicalconsequenceofHw.r.t.B);pisexplainedifHBpp;otherwisepisassumed.WerefertotheoperationthatweunifytwoormoreliteralsinsetSofliterals,andapplytheunifiertoSasfactoringofS.Inthispaper,weassumethatallvariablesoccurringinalogicalformofbackgroundknowledgeareuniversallyquantifiedwiththewidestpossiblescope,unlessitisexplicitlystatedasexistentiallyquantified.Ontheotherhand,weassumethatvariablesoccurringinanexplanationandobservationareimplicitlyexistentiallyquantified.Weassumethatthebackgroundknowledgehasnocyclicdependenciesbetweenanexplainingandanexplainedliteral(e.g.,B=P(x)Q(x),Q(x)P(x)hasacyclicdependency).Wecallthisassumptionknowledgerecursion-freeassumption.Typically,severalexplanationsHexplainingOexist.Wecalleachofthemacandidateexplanation,andrepresentthesetofcandidateexplanationsofOgivenBas_,.Thegoalofabductionistofindthebestexplanationamongcandidateexplanationsbyaspecificevaluationmeasure.Inthispaper,weformulateabductionasthetaskoffindingtheminimum-costexplanationamong_,.Henceforth,werefertoabductionbasedontheminimum-costexplanationfindingascost-basedabduction(CBA).Formally,wefind=_H_,cost(H),wherecostisafunction_,R,whichiscalledthecostfunction.Letusdescribethetaskofabductionwithatoyexample.GivenB=p(x,y)q(x)r(x),s(x)r(x),O=r(z),wehavefourcandidateexplanations:H_1=r(z),H_2=p(z,w),q(z),H_3=s(z),andH_4=p(z,w),q(z),s(z).Thetaskofabductionistoselectthebestexplanationamongthemintermsofcost.Supposecost(H_1)=5.5,cost(H_2)=12.25,cost(H_3)=10.8,andcost(H_4)=7.13.Then,thecorrectpredictionisthenH_1.Itiscrucialtodiscussthespecificityofexplanations.WesaythatanexplanationHismorespecificthananotherexplanationH'ifHBH'.AsdiscussedinHobbsetal.1993Hobbs93,wewanttodecidetheappropriatespecificityofanexplanationbecausethereisoftenlittleevidence(i.e.,observation)tosupportspecificexplanations.Traditionally,twoextrememodesofabductionhavebeenconsidered.Thefirstismost-specificabduction.Inmost-specificabduction,whatwecanexplainfrombackgroundknowledgeisalldetailed,whichissuitablefordiagnosticsystems.Indiagnosticsystems,usersmightwanttoknow,asmuchaspossible,aboutwhathascausedthecurrentsituation.Somecost-basedandprobabilisticapproachesfallintothisgroup~.Thesecondisleast-specificabduction.Literally,inthismode,anexplanationisobtainedbyjustassumingobservations.Usingonlyleast-specificabductionhaslittlepurpose,butasdescribedbelow,itmakessenseifitiscombinedwithmost-specificabduction.Innaturallanguageunderstandingsystems,weneedbothmodesatthesametime.Adoptingonlyoneoftheselevelsisproblematic.Forexample,ifweadoptmost-specificabduction,thesystemyieldstoospecificanexplanation,suchas``Bobtookagunbecausehewouldrobbankusingamachinegunwhichhehadboughtthreedaysago.''Conversely,ifweadoptleast-specificabduction,thesystemassumesjustobservation,asin``Bobtookagunbecauseheagun.''Wethuswanttodeterminethesuitablespecificityduringinference.Tothebestofourknowledge,theweightedabductionofHobbsetal.(1993)Hobbs93istheonlyframeworkthatconcernstheappropriatenessofexplanationspecificity.Thecostfunctionofweightedabductionhandlesthisbypropagatingcostsofpropositionsandunificationasdescribedin.~.</subsection>
  <subsection title="Cost function">Intheliterature,severaltypesofcostfunctionshavebeenproposed,includingcost-andprobability-basedfunctions.Inthispaper,weadoptthecostfunctionproposedbyHobbsetal.(1993)Hobbs93.ThecostfunctionassumesthateachelementalexplanationpHhasthenon-negativecostofhypothesizingp(intuitively,theplausibilityofpbeinganexplanationforgivenobservations),andsumsupthecostsofassumedelementalexplanations.Henceforth,wewriteP(x)^ctodenoteP(x)havingacostc.DuringtheconstructionofH,onecanfactorHtogenerateanewexplanationatanytime.WhenHisfactored,thefollowingthingshappen:(i)theliteralthathasthesmallestcostamongasetofunifiedliteralsremainsinH,and(ii)fortheunifierx_i/y_i_i=1^n,asetofelementalexplanationsx_i=y_i^0_i=1^nisaddedtoH.Forexample,onecanfactorH=R(a)^20,R(b)^10,Q(a)^20withtheunifiera/btogetH'=R(b)^10,a=b^0,Q(b)^20,wherethesmallercost10isassignedtoR(b).Formally,thecostfunctionisdefinedasfollows:whereA(H)isasetofassumedliteralsinH.InHobbsetal.(1993),eachcostisdeterminedbytwofactors:(i)weights,theparametersassignedtoaxiomsinbackgroundknowledge;and(ii)thecostsofobservationsinitiallyassigned.Inthispaper,wedonotgointodetail,andjustassumethateachcostisgiveninsomeway.</subsection>
  <section title="ILP-based inference for cost-based abduction">Wenowdescribeourstrategytofindthebestexplanationinfirst-orderCBA.Thekeyideaisthatwesolvefirst-orderCBAproblemsusingtheliftedinferencetechnique,whereeachinferenceoperationisdirectlyperformedonafirst-orderlevel,likeresolution.Inprinciple,thiswayofproblemformulationgivesusthreebenefits.First,wecanreducethesearchspaceofcandidateexplanationsincomparisontoagroundingapproach,becausewecanavoidinstantiatingFOLformulaewithallpossibleconstants.Second,thebestexplanationfindingproblemcanbereducedtotheconstrainedcombinatorialoptimizationproblemoffirst-orderliteralsand/orequalities,meaningthatwecanexploitseveralchoicesofcombinatorialoptimizationtechnologydevelopedinoperationsresearch.Specifically,ouroptimizationproblemcanbenaturallyformulatedasanILPproblem,whichcanbeefficientlysolvedbyexistingILPsolvers.Third,theresultingframeworkishighlyextensible;forexample,wecaneasilyincorporatelinguisticallymotivatedheuristicsbysimplyaddingsomeILPvariablesand/orconstraintstoanoptimizationproblem,keepingtheoverallframeworkunchanged.Intherestofthissection,wefirstformalizethebestexplanationfindinginfirst-orderCBAusingtheliftedinferencetechnique,andthendescribehowtosolveitasanILPoptimizationproblem.</section>
  <subsection title="Lifted first-order inference for CBA">First-orderlogicinheritsallthetheoreticalpropertiesofpropositionallogic,andhenceasoundandcompleteinferencecanbeperformedonpropositionallogic.However,performingfirst-orderlogicalinferenceonapropositionallevelhassevereoverhead,becauseweneedgrounding,whichgeneratesthegroundinstancesoffirst-orderlogicalformulaeinknowledgebasesandobservations(i.e.,instantiatingthemwithallpossibleconstants).Thegroundingproceduregeneratesalotofformulaewhenadomainislarge.Inthispaper,wethusproposetoperformcost-basedabductiononafirst-orderlevel.Theapproachisinthespiritofresolution,butisappliedtothebestexplanationfindingproblems.Intherestofthissection,weshowhowtosolvetheabductiveinferenceproblemonfirst-orderlevel.Figure~summarizesourapproach.Inprinciple,ourapproachtakestwosteps:(i)Step1:search-spacegeneration,and(ii)Step2:best-explanationsearch.Insearch-spacegeneration,wefirstconstructasetofallpossibleliteralsand/orequalitiesthatarepotentiallyincludedinH.Forexample,giventhetoyprobleminSec.~,weconstructthefollowingset:r(z),p(z,w),q(z),s(z).Inthebest-explanationsearchstep,wefindthebestexplanationforObyfindingthebestcombinationofliteralsorequalitiesamongthesetofliteralsconstructedinthesearch-spacegenerationstep,accordingtothecostfunction.Theproblemissolvedintheformofconstrainedbooleanoptimizationproblem,whichistheproblemoffindingthetruthassignmenttobooleanvariablesthatmaximizesorminimizesanobjectivefunctionsatisfyingthegivenconstraints.Nowwemoveontothedetailofourapproach,inwhichweusethefollowingformatforbackgroundknowledge,observations,andexplanation:Backgroundknowledge:asetoffirst-orderdefiniteclauses(i.e.,p_1p_2p_nq,wherep_1,p_2,,p_n,andqareatoms);Observations:asetofpositiveliteralsorpositiveequalities;Explanation:asetofpositiveliteralsorpositiveequalities.Henceforth,wecalleachclauseinbackgroundknowledgeanaxiom,theright-handsidethehead,andtheleft-handsidethebody.[b](Backgroundknowledge,Observation,Costfunctioncost)algorithmWegivetheoverallalgorithminAlgorithm~.GivenbackgroundknowledgeBandobservationsO,wefirstcreatethesetofliteralsorequalitiesthatarepotentiallyincludedasconstituentsofthebestexplanationofO(line2--10).Werefertotheliteralorequalitypasthepotentialelementalexplanation,whichweenumeratebyfirstinitializingwithO.Wetheniterativelyapplybackwardinferencetoeachp(line2--6).Algorithm~depictsthebackward-inferenceoperationindetail(lines5--8).Wedefinebackwardinferenceasthefollowingoperation:Input:aclauseintheformp_1p_2p_nqandtheliterall,wheretheremustexistthemostgeneralunifiersuchthatl=q.Output:p_1,p_2,,p_n,wherethevariablesthatarenotsubstitutedby(notSubstitutedVars(p_1,p_2,,p_n,)inAlgorithm~)arereplacedwithexistentiallyquantifiedvariablesnotappearinginsofar.*-1example,giventheaxiomp(x,y)q(x,y,z)r(x)andr(a),itderivesp(a,u_1),q(a,u_1,u_2),whereu_1andu_2areexistentiallyquantifiedvariablesnotappearingin.Notethatisnotequivalenttoasetofresolventsthataregeneratedbyaparticularproofprocedure.Thegoalofproofprocedureistocheckwhetheralogicalformulaisimpliedbyasetoflogicalformulae.Therefore,thederivedproofmightnotcontainasetofallliteralsthatcanexplainobservations.Forexample,SLDresolution,isabackward-inference-basedproofprocedurethatworksondefiniteclauses,whereliteralsresolveduponareselectedbyaparticularcomputationrule(e.g.,leftmost),andtheresolutionprocedureterminateswhentheproofisfoundtobeafailureorasuccess.However,whatwewanttoenumerateisthesetofallliteralsthatcanexplainobservations.Sincewehavetheknowledge-recursion-freeassumption,lines2--11terminateinfinitetime(i.e.,untilnomorebackwardinferencecanbeapplied).[b](Backgroundknowledge,setSofalgorithmInlines6--10,wesearchforthepairsofunifiableliteralsininordertorepresenttheapplicationoffactoringoperationtoH.Foreachpairofunifiableliterals,weaddtheequalitiesthatarepotentiallyhypothesizedbytheunifier(seeSec.~).Wedonotunifysuchliteralsinherebecausewewanttoallowthatthefactoringoperationsarealsodefeasible,``possibly''trueoperations.Weusethecostfunctiontodeterminewhethertheyshouldbefactored.Inline11,wefindthebestexplanation.Given,theproblemofbestexplanationfindingcanbereducedtoaconstrainedcombinatorialoptimizationproblem.Notethatthenumberofcandidateexplanationsgrowsexponentially(i.e.,O(2^||)),becauseeachexplanationisrepresentedbythecombinationofpotentialelementalexplanations.Weimmediatelyseethatasimpleapproachthatfindsaminimalexplanationbyevaluatingallcandidateexplanationsisintractable.Toimproveefficiency,weformulatethebestexplanationfindingasthe0-1ILPoptimizationproblemtoexploitthestate-of-the-artsearchstrategyofcombinatorialoptimizationproblems.Theformulationisdescribedinthenextsection.</subsection>
  <subsection title="ILP Formulation">Weformulatethebest-explanationfindingproblemasanILPoptimizationproblemwherethesearchspaceisrepresentedasILPvariablesandconstraints,andthecostfunctionisusedastheILPobjective.Intuitively,foreachp,weintroducesome0-1statevariablethatrepresentswhetherthepotentialelementalexplanationpis(explicitlyorimplicitly)hypothesized.Then,everypossibleHH_,canbeexpressedasacombinationofvalueassignmentstothesestatevariables.WeelaborateontwotypesofILPvariablesandILPconstraintsintheoptimizationproblem:(i)forrepresentingthesearchspaceofcandidateexplanations,and(ii)forimplementingthecostfunction.</subsection>
  <subsubsection title="Formulation for CBA search space: ">Torepresentwhethertheliteralorequalitypishypothesized(includingimplicitlyhypothesized),weintroduceanILPvariableh0,1asfollows:[foreachp:h_p=1&amp;iffHBp;0&amp;otherwise.cases]Forexample,H_2inFigure~holdsh_r(x)=1,wherer(x)ishypothesizedinH_2.Wealsousehtorepresentequalities.InH_3,thevariableh_x=Aissetto1becausex=Aisassumed.Notethathvariablesdonotrepresentthetruthvaluesofp(i.e.,h_p=0doesnotmeanHBp).Onceavalueassignmenttohisdetermined,weconstructHonthebasisoftheassignmentasfollows:Givenaparticularvalueassignmenttohvariables,wegenerateanexplanationHasfollows:pHh_p=1foreachp;pHh_p=0foreachp.mydefThatis,alllogicalconsequencesofHBareconsideredtobeanexplanation(i.e.,thegeneratedexplanationincludesimplicitly,aswellasexplicitly,hypothesizedliterals).NotethatnotallvalueassignmentstoILPvariableshareallowed.BythedefinitionofcandidateexplanationinSec.~,forexample,itisnotallowedtooutputtheassignmentthatthereexistspOs.t.HBp.Toensurethatthesearchspaceincludesonlyvalidcandidateexplanations(i.e.,HsatisfiesHBOandHB),weimposeseveralconstraintsonthevalueassignmentsofh.WedenoteTtorepresentasetoflogicalatomictermsin.Inthisformulation,wegenerateO(n^3)ILPconstraintsforConstraint2,wherenisthenumberoflogicalatomictermsappearingin.AsthereaderwillseeinSec.~,thismakesinferenceintractableinlarge-scaleprocessing.WeshowhowthisdrawbackcanbeovercomebyexploitingCPIinSec.~.</subsubsection>
  <subsubsection title="Formulation for implementing the cost function: ">AsmentionedinSec.~,weadoptthecostfunctionproposedbyHobbsetal.(1993)Hobbs93.Forconvenience,werepeatthecostfunction:whereA(H)isasetofassumedliteralsinH.ThismeansthatthecostofHiscalculatedfromthesubsetofhypothesizedliterals.Torepresentthesetofliteralscountedinthecostfunction,wefirstintroduceILPvariablesc0,1asfollows:[foreachp:c_p=1&amp;ifp0&amp;otherwise.cases]InFigure~,c_s(x)issetto0inH_2sinces(x)doesnotpaythecost(i.e.,s(x)isexplainedbyr(x)).Usingcvariables,theobjectivefunctionoftheILPproblemisgivenbyNotethatitiseasytoincorporateothercriteriaintothecostfunction.Forinstance,onecanconsidertheplausibilityofcoreferencerelationsbetweentwomentionsinatext.Assumingthatmentionsarerepresentedbyvariables(e.g.,cat(x)meansthatmentionx,whoselinguisticexpressioniscat,appearsinatext),onecanadd_x,yTcost(x,y,O)h_x=y,wherethecostiscalculatedbytheinformationmentionedinO.Forexample,onecoulddesignthecostfunctionthatreturnsahighercostiftwocontradictorypropertiesarementionedinO(e.g.,cat(x)anddog(y)occurinO).Again,fromthedefinitionofcvariables,notallvalueassignmentstocareallowed.weintroduceseveralconstraintsoncasfollows.Finally,weimposeaconstraintonu_p_1,p_2sothatthevalueofu_p_1,p_2isallowedtobe1onlyif(i)thereexistequalitiesthatmakep_1andp_2equivalentinH,and(ii)p_1andp_2arehypothesized.AsmentionedinSec.~1,themostsimilarworktooursisSantosSantos94'sILP-basedformulationofpropositionallogic-basedCBA(1994),butourapproachisdifferentintwoways.First,wearecapableofevaluatingthespecificityofexplanations,whichisanimportantfeatureforabduction-basedNLP,asdiscussedinSec.~.TheSantosapproachamountstoperformingmost-specificabduction,andhefindsatruthassignmenttoallpropositions.Letusdescribehowtheappropriatelevelofspecificityiscontrolledinourapproach.SupposeO=p(a),q(a)andB=r(x)p(x).Wethenhavetwocandidateexplanations.ThefirstexplanationisH_1=p(a),q(a),whichsimplyassumesobservations,andthecostiscost(p(a))+cost(q(a))(i.e.,c_p(a)=1,c_q(a)=1).Backwardchainingonp(a)yieldsthesecondexplanationH_2=q(a),r(a),whichismorespecificthanH_1.ThecostofH_2iscost(q(a))+cost(r(a))(c_p(a)=0,c_q(a)=1,c_r(a)=1).Notethatwedonotcountp(a)becausep(a)isnotassumedanymore.Therefore,forthisproblem,ifcost(r(a))&lt;cost(p(a)),thenamorespecificexplanationH_1isselectedasthebestexplanation;otherwise,thelessspecificexplanationH_2isselected.ThisiscontrolledbytheILPvariablescandConstraints5and6,whicharenotintroducedintheSantosapproach.Tosummarize,ourapproachcandecidewhichspecificityofexplanationisappropriateforthecurrentobservationandknowledgebase,onthebasisofhowwelltheexplanationissupportedbyobservations.Second,ourapproachdirectlymodelsfirst-orderCBA,whileSantosapproachformulatespropositional-logicabduction.Wecouldemployhisapproachforfirst-orderCBAsinceitiswellknownthatFOLformulaecanberepresentedbypropositionallogicformulaethroughtheapplicationofgroundingprocedure(i.e.,generatelogicalformulae,replacingvariableswithallpossibleconstants).However,abductiveinferenceoverpropositionallevelwillmakeinferenceintractablewhenexistentiallyquantifiedvariablesareincludedinobservationsorbackgroundknowledge.Forexample,supposethatB=q(x,y)p(x,y),r(x,y,z)q(x,y),O=p(x,y)andallpossibleconstantsareC=C_1,C_2,,C_n.Togroundthisobservation,weneedtogenerateadisjunctiveclauseforp(x,y),replacingxandywithallpossiblecombinationsfromC,i.e.,p(C_1,C_1)p(C_1,C_2)p(C_n,C_n).Theextensionoftheexpressivityofobservationisnotdifficult,buttheproblemarisesinthesearch-spacegenerationprocess:wegetO(n^2)potentialelementalexplanations(i.e.,q(C_i,C_j)foralli,j1,2,,n)toexplaineachdisjunctwiththeaxiomq(x,y)p(x,y).Inaddition,backchainingoneachq(C_i,C_j)withr(x,y,z)q(x,y)yieldsO(n)potentialelementalexplanations(i.e.,r(C_i,C_j,C_k)forallk1,2,,n).Incontrast,thesearch-spacegenerationinourapproachyieldsp(x,y),q(x,y),r(x,y,u).Asthereaderscansee,ourapproachseemstobemorerobusttothesizeofdomain.Indiscourseprocessing,thisrobustnessisimportantbecauseliteralsusuallyhavemorethantwoorthreeargumentstorepresentaneventwithitsparticipants.</subsubsection>
  <subsection title="CPI for CBA">OnemajordrawbackofILPformulationisthatitneedstogenerateO(n^3)transitivityconstraints,wherenisthenumberoflogicalatomicterms,becauseweperforminferenceoverFOL-basedrepresentation.Thatmakesinferenceintractable(seeSec.~forempiricalevidence)becauseitgeneratesanILPoptimizationproblemthathasquitealargenumberofconstraints.Howdoweovercomethisdrawback?Theideaisthat``allthetransitivityconstraintsmaynotbeviolatedallatonce;sowegraduallyoptimizeandaddtransitivityconstraintsifviolatedinaniterativemanner.''Moreformally,weproposeapplyingCPItotheCBAproblemsthatwasoriginallydevelopedforsolvinglargelinearprogramming(LP)problemsinoperationsresearch.CPIhasbeensuccessfullyappliedtoawiderangeofconstrainedoptimizationproblemswhereconstraintsareverylarge,fromprobabilisticdeductiveinferenceproblemstomachinelearningproblems.Tothebestofourknowledge,however,ourworkisthefirstsuccessfulapplicationofCPItoabductiveinferencetasks.Inprinciple,CPIsolvesoptimizationprobleminaniterativemannerasfollows:itsolvesanoptimizationproblemwithoutconstraints,andthenaddsviolatedconstraintstotheoptimizationproblem.Whentheiterationterminates,itguaranteessolutionstobeoptimal.Theproposedalgorithm,calledCPI4CBA,isalsoanexactinferenceframework.HowdoweapplythetechniqueofCPItocost-basedabductionproblems?Intuitively,weiteratethefollowingtwosteps:(i)solvinganabductionproblemwithoutenforcingtransitivityonlogicalatomicterms,and(ii)generatingtransitivityconstraintsdynamicallywhentransitivenessofunificationisviolated(e.g.,HBx=yy=z,andHBx=z).Theiterationterminatesifthereisnoviolatedunificationtransitivity.ThepseudocodeisgiveninAlgorithm~.Inline1,wefirstcreateanILPoptimizationproblemdescribedinSec.~,butwithouttransitivityconstraints(i.e.,Constraint2),wheredenotesasetofILPvariables,andIdenotesasetofILPconstraints.Inlines2--13,werepeatcheckingconsistencyofunificationtransitiveness,addingconstraintsforviolatedtransitiveness,andre-optimizing.Inline3,wefindthesolutionsolforthecurrentILPoptimizationproblem.Then,foreachpair(x,y)oflogicalatomictermsunifiedinthesolutionsol(line4),wefindthelogicaltermzwhichisunifiablewithxandy(line5).Ifthetransitiverelationx,ywithrespecttozisviolated(i.e.,h_x=z=0h_y=z=1orh_x=z=1h_y=z=0),thenwegenerateconstraintsforpreventingthisviolation,andkeepitinsetVofconstraints(lines6--9).Finally,weagainperformanILPoptimizationwithnewlygeneratedconstraints(lines12and3).Theiterationendswhenthereisnoviolatedtransitiveness(line13).[b](BackgroundKnowledgeB,ObservationO)algorithmThekeyadvantageofCPI4CBAisthatitcanreducethetimeofsearch-spacegeneration,anditisalsoexpectedtoreducethetimeofILPoptimization.CPI4CBAdoesnotgeneratealltransitivityconstraintsbeforeoptimization,whichsavessearch-spacegenerationtime.Inaddition,optimizationproblemsthatwesolvewouldbecomesmallerthantheoriginalprobleminmostcases,becausenotalltransitivityconstraintsneedtobeconsidered.Intheworstcase,weneedtosolvetheoptimizationproblemthatissameastheoriginalone;however,inmostcases,wefoundthisunnecessary.Wewillshowitsempiricalevidencethroughlarge-scaleevaluationinSec.~.</subsection>
  <section title="Runtime Evaluation">HowmuchdoesCPIimprovetheruntimeofanILP-basedreasoner?DoesCPIscaletolargerreal-lifeproblems?Toanswerthesequestions,weevaluatedtheCPI4CBAalgorithmintwosettings:(i)STORY,thetaskofplanrecognition;and(ii)RTE,thepopular,knowledge-intensive,real-lifenaturallanguageprocessingtaskofRTE.Whilemostoftheexistingabductiveinferencesystemsareevaluatedonrathersmall,and/orartificialdatasets,ourevaluationtakesreal-life,muchlargerdatasets(seeSec.~).Inourexperiments,wecompareoursystemwithsystemsbasedonMarkovlogicnetworks(MLNs).Forourexperiments,wehaveuseda12-CoreOpteron6174(2.2~GHz)128~GBRAMmachine,andassigned8CPUcoresforeachrun.ForanILPsolver,weusedaGurobioptimizer.Itiscommercial,butanacademiclicenseisfreelyavailable.Inourexperiments,weuseConstraint6withoutenumeratingpotentiallogicalconsequences(i.e.,Algorithm~isused).Theempiricalevaluationwiththeenumerationofpotentiallogicalconsequencesisourfuturework.</section>
  <subsection title="Settings">STORY:Forthissetting,wehaveusedNgandMooneyNg92'sstoryunderstandingdataset(1992),whichiswidelyusedfortheevaluationofabductiveplanrecognitionsystems.Inthistask,weneedtoabductivelyinferthetop-levelplansofcharactersfromactions.WefollowSinglaandMooneySingla11'ssettingtodefinetop-levelplanpredicates.Theseinclude10typesofliterals,suchasshopping.,andpartying.Thedatasetconsistsof50planrecognitionproblemsrepresentedbyasetofgroundatoms(e.g.,getting_off(Getoff16),agent_get_off(Getoff16,Fred16),name(Fred16,Fred))and107backgrounddefiniteclauses(e.g.,go_step(r,g)going(g)robbing(r)).Thedatasetcontainsonaverage12.6literalsinthelogicalformsofactions.Weadditionallygenerated73ILPconstraintstopreventonevariable/constantfromhavingdistincttop-levelplanpredicatesinahypothesis(e.g.H=robbing(R),shopping(R)).Forexample,wegeneratedh_robbing(x)+h_shopping(y)+h_x=y2torepresentthatrobbingandshoppingcannotbehypothesizedforthesamevariable/constant.NotethatH=robbing(R),shopping(S)isstillpossible.Toassignacosttoeachliteral(i.e.,cost(h)inequation()),wefollowedHobbsetal.Hobbs93'sweightedabductiontheory~.Inthetheory,asmentionedinSec.~,eachliteralintheleft-handsideoftheaxiomshasasetofweights,whichisexpressedasp_1^w_1_2^w_2...p_n^w_nq.Duringbackwardchaining,eachweightismultipliedwiththecostofliteralthatisbackchainedon.Forexample,givenp(x)^0.6(x)^0.6r(x)andr(a)^10,thetheoryderivesp(a)^6,q(a)^6.BecausethebackgroundknowledgeofNgandMooney'sdatasetdoesnothaveweights,weassignedweightstotheaxiomssothatthesumoftheweightsis1.2(e.g.,p^0.4q^0.4r^0.4s).Thisassignmentmeansthatbackwardinferencealwaysincreasesthecostofexplanation,andunificationistheonlywaytoreducethecost.Thatis,itisalmostequivalenttoperformingpurelogic-basedabduction,wherethenumberofliteralsinanexplanationisusedastheplausibilityofexplanation.RTE:Forobservations(input),weemployedthesecondchallengeoftheRTEdataset.Inthis,weneedtodeterminecorrectlywhetheronetext(calledtext,orT)entailsanother(calledhypothesis,orH).Thedatasetconsistsofadevelopmentsetandatestset,eachofwhichincludes800naturallanguagetexthypothesispairs.Weusedall800textsfromthetestset.WeconvertedtextsintologicalformspresentedinHobbs(1985)Hobbs85usingtheBoxersemanticparser.Thenumberofliteralsinobservationsis29.6literalsonaverage.Forbackgroundknowledge,weextracted289,655axiomsfromWordNet3.0,and7,558axiomsfromFrameNet1.5,followingOvchinnikovaetal.(2011)Ovch11.Inprinciple,theWordNetknowledgebasecontainsvariouslexicalrelationsbetweenwords,suchasIS-A,ontologicalrelations(e.g.,dog(x)animal(x)).FrameNetknowledgebasescontainlexeme-to-framemappings,frame-framerelations,etc.Forexample,themappingfromsurfacerealization``x_1givex_2x_3''toaframe``Giving''isgivenbyGiving(e_1,x_1,x_2,x_3)donor(e_1,x_1)recipient(e_1,x_2)theme(e_1,x_3)give(e_1,x_1,x_2,x_3).WeagainfollowedHobbs'sweightedabductiontheoryforcalculatingthecostofexplanation.WeassignedtheweightstoaxiomsbyfollowingOvchinnikovaetal.(2011)Ovch11inthissetting.</subsection>
  <subsection title="Results and discussion">Thereasonerwasgivena2-mintimelimitforeachinferencestep(i.e.,search-spacegenerationandbest-explanationsearch).InTable~,weshowtheresultsofeachsettingfortwoinferencemethods:(i)IAICBA:theinferencemethodwithoutCPI,and(ii)CPI4CBA:inferencemethodwithCPI.Inordertoinvestigatetherelationshipbetweenthesizeofsearchspaceandtheruntime,weshowtheresultsforeachdepth,whichweusedforlimitingthelengthofbackwardchaining.Inthe``Generation''column,weshowtheruntime,whichistakenforsearch-spacegenerationinseconds,averagedoverallproblemswhosesearch-spacegenerationisfinishedwithin2~min.Intheparenthesis,weshowthepercentageofthoseproblemswhosesearch-spacegenerationisfinishedwithin2~min.Inthecolumn``ILPinf,''weshowtheruntimeofILPoptimizationaveragedononlyproblemssuchthatbothsearch-spacegenerationandILPoptimizationarefinishedwithin2~minaswellasthepercentageofthoseproblems(e.g.,80%means``for80%ofalltheproblems,search-spacegeneration,aswellasILPinference,wasfinishedwithin2min.'').Inthe``#ofILPcnstr''column,weshowtheaveragednumberofgeneratedILPconstraints.ConcerningCPI4CBA,thenumberdenotestheaveragednumberofconstraintsconsideredintheend,includingtheconstraintsaddedbyCPI.ThenumbermarkedbyindicatestheaveragednumberofconstraintsthatareaddedduringCPI(i.e.,howmanytimesaretheconstraintsaddedbyline7or9inAlgorithm~).Overall,theruntimesinbothsearch-spacegenerationandILPinferencearedramaticallyimprovedfromIAICBAtoCPI4CBAinbothsettings,asshowninTable~.Inaddition,CPI4CBAcanfindoptimalsolutionsinILPinferenceformorethan90%oftheproblems,evenfordepth.ThisindicatesthatCPI4CBAscalestolargerproblems.TheresultsofIAICBAinRTEsettingsindicatethesignificantbottleneckofIAICBAinlarge-scalereasoning:thetimeofsearch-spacegeneration.Search-spacegenerationcouldbeperformedwithin2~minforonly90.7%oftheproblems.CPI4CBAsuccessfullyovercomesthisbottleneck.CPI4CBAisclearlyadvantageousinsearch-spacegenerationbecauseitisnotnecessarytogeneratetransitivityconstraints,anoperationthatgrowscubicallybeforeoptimization.Inaddition,CPI4CBAalsoreducesthetimeofILPinferencesignificantly.InILPinference,CPIdidnotguaranteethereductionofinferencetimeintheory;however,asshowninTable~,wefoundthatthenumberofILPconstraintsactuallyusedismuchlessthantheoriginalproblem.Therefore,CPI4CBAsuccessfullyreducesthecomplexityoftheILPoptimizationproblemsinpractice.ThisisalsosupportedbythefactthatCPI4CBAkeeps76.9%in``ILPinf''forDepth=becauseitsolvesverylargeILPoptimizationproblemsthatfailtobegeneratedinIAICBA.InordertoseehowCPIcontributestotheimprovementinILPinferencetime,weshowhowtheruntimeofIAICBAisaffectedbytheCPI4CBAmethodforeachprobleminFigure~.EachdatapointcorrespondstooneprobleminSTORYandRTEsettings.WeshowthedatapointsforproblemsforwhichwefoundoptimalsolutionsinILPinferenceforDepth=.Overall,theruntimeofCPI4CBAissmallerthanthatofIAICBAinmostproblems.Inparticular,CPI4CBAsuccessfullyreducesthetimeofILPinferenceforlargerproblemsbyexploitingtheiterativeoptimizationtechnique.InthelargerdomainofRTEsetting,wefoundthatperformancewasimprovedin81.7%oftheproblems.Finally,wecompareCPI4CBAwiththeexistingMLN-basedsystems.Insummary,oursystemiscomparableorslightlylessefficientintheSTORYsetting;however,oursystemismoreefficientintheRTEsetting.FortheSTORYsetting,SinglaandMooney(2011)Singla11reportedtheaveragedinferencetimeoftwoexistingMLN-basedsystemsusingCPIonthetestset:(i)KateandMooney(2009)Kate09'sapproach:2.93~s,and(ii)SinglaandMooney(2011)Singla11'sapproach:0.93s.Tomakethecomparisonfair,weevaluatedourapproachwithasingleCPUcoreonthetestset.Ittook2.36sonaveragetoprocessalltheproblems(optimalsolutionswerefoundforallofthem).MLN-basedapproachesseemtobereasonablyefficientforsmalldatasets.However,itdoesnotscaletolargerproblems;fortheRTEsetting,Blytheetal.(2011)Blythe11reportedthatonly28from100selectedRTE-2problemscouldberuntocompletionwithonlytheFrameNetknowledgebases.Theprocessingtimewas7.5~minonaverage(personalcommunicationThedatasetcontains65backgroundaxiomsand14abductiveinterpretationproblemsfromHobbsetal.(1993).Hobbs93AsreportedinBlytheetal.(2011)Blythe11,ittook5.6~sfortheirsystemtoprocess12problems(2problemscannotbesolved),whileittook1.0~sforoursystemtoprocessalltheproblemswithasingleCPUcore(Depth=8).).However,ourmethodsolves76.9%ofalltheproblems,withsuboptimalsolutionsstillavailablefortheremaining21.5%,andittakesonly0.84~sforsearch-spacegeneration,and11.73~sforILPinference.AsmentionedinSec.,ourframeworkismorescalablebecauseitdoesnotneedtogeneratetheaxiomsexplicitlytoemulateanexplainingawayeffect(i.e.,inferringonecausemakesanothercauselessprobable)andneedsnogrounding(Sec.).</subsection>
  <section title="Related work">Thecomputationalaspectofabductionhasbeenstudiedextensivelyinthecontextsoflogicprogrammingandstatisticalrelationallearning.Inthecontextoflogicprogramming,abductionhasbeenintroducedastheextensionoflogicprogramming,wheretheextendedframeworkisoftencalledabductivelogicprogramming(ALP).Sinceabductionandinductionsharethebasicframework(seeSec.~fordetail),abductionhasalsobeenstudiedintheareaofinductivelogicprogramming,thelogicprogrammingframeworkforinduction.InthecontextofALP,Stickel(1991)Stickel91showedhowtoformulateminimum-costexplanationfindinginProlog,apopularimplementationoflogicprogramming.StickelallowedthesystemtoassumeliteralsduringSLDresolutionwhendefiniteclauserulesorfactsunifiablewiththetargetedliteralarenotfound.Inthissystem,thecostofexplanationiscalculatedbythesumofthecostsofelementalexplanations,andthecostsofaxiomsusedforconstructingtheproof.However,Stickeldidnotshowhowtoimplementitefficiently.Inthefollowingyears,anumberofmethodsattemptingtofindtheminimum-costexplanationefficientlywereproposed;forexample,Santos(1994)formulatedcost-basedabductioninpropositionallogicusingILP,andshoweditsefficiency.However,mostofthemfocusonimprovingtheefficiencyofpropositionallogic-basedabduction.AsdiscussedinSec.~,onecouldusesuchaframeworkthroughpropositionalizationtechniquesforfirst-orderCBA;however,thepropositionalizationwillproduceahugenumberofgroundinstancesofbackgroundknowledgeaxiomsandliteralsinobservation.Hence,theywouldnotscaletolargerproblemswithsizeableknowledgebases.Inthecontextofstatisticalrelationallearning,abductionhasalsobeenwidelystudied.OneoftheprominentformalismsisPRISM,whichisagenerallogic-basedprobabilisticmodelinglanguage.Inthepasttwodecades,anumberoftechniquesforefficientinferenceorlearninghavebeenstudiedextensively(seeSatoandKameya(2008)Sato08foroverview).Concerninginference,inprinciplePRISMachievesthebestexplanationfindinginapolynomialtimethroughatabledsearchtechniqueforlogicprograms.However,thistechniqueexploitsthelocalinformationcomputedsofar,andhenceisincompatiblewiththefactoringofexplanation,whichisaglobaloperation(personalcommunication).Itisanontrivialissuetoincorporatethefactoringprocessintothesearchwithoutlossofefficiency.Anotherimportantstreamistheseriesofstudies,whereabductionhasbeenemulatedthroughMLNs,aprobabilisticdeductiveinferenceframework.MLNsprovidefullsupportoffirst-orderpredicatelogicandsoftwarepackagesofinferenceandlearning;however,MLN-basedapproacheshavesevereoverheadsofinference:(i)theyrequirespecialprocedurestoconvertabductionproblemsintodeductionproblemsbecauseofthedeductivenatureofMLNs,and(ii)theyneedgroundingforinference.Toemulateabductioninthedeductiveframework,thepioneeringworkofMLN-basedabductionexploitsthereverseimplicationoftheoriginalaxioms,andusestheadditionalaxiomstoemulatetheexplainingawayeffect(i.e.,inferringonecausemakesanothercauselessprobable).Forexample,supposeB=p_1q,p_2q,p_3q.Then,BisnotusedinMLNbackgroundknowledgebaseasitis:Bisconvertedintothefollowingsetoflogicalformulae:qp_1p_2p_3,qp_1p_2,qp_1p_3.Asthereaderscanimagine,anMLN-basedapproachsuffersfromtheinefficiencyofinferencebecauseoftheincreaseinconvertedaxioms.Inaddition,tothebestofourknowledge,mostoftheexistingapproachesformaximum-a-posterior(MAP)inferenceforMLNneed(partial)groundingofaxioms,whichmakesinferenceprohibitivelyslow.Intermsoftheapplications,therearemultipleresearchesthatexploitabductioninmanyfields.Forexample,insystemsbiology,abductionisusedfordiscoveringscientificknowledge,suchascausalrelationshipsfromgenotypetophenotype,ormodelinginhibitioninmetabolicnetworks.</section>
  <section title="Conclusion">WehaveproposedanILP-basedliftedformulationforcost-basedabductiononFOL.AlthoughabductivereasoningonFOLiscomputationallyexpensive,wedemonstratedthattheliftedinferenceandCPItechniquesbringusasignificantboosttotheefficiencyofFOL-basedreasoning.Wehaveevaluatedourmethodontwodatasets,includingreal-lifeproblems(i.e.,RTEdatasetwithaxiomsgeneratedfromWordNetandFrameNet).OurevaluationrevealedthatourinferencemethodCPI4CBAwasmoreefficientthanotherexistingsystemsonalargeKB.Theabductiveinferenceenginepresentedinthispaperismadepubliclyavailable.Infuturework,weplantoapplyCPItobothsearch-spacegenerationandILPinference,repeatingthegenerationofpotentialelementalexplanationsandILPoptimizationinteractively,asinCuttingPlaneMAPinferenceinMLNs.Wealsoplantoextendtheexpressivityofourapproach,i.e.,tosupportnegationsinbackgroundknowledge.Thecapabilityofhandlingnegationsiscrucialforawiderangeofabductiveinferencesystems.Forexample,inabduction-basednaturallanguageinterpretation,onecaneasilyimaginethatitneedstohandlenegatedexpressions,suchas``Idon'tlikeicecream'',or``Tweetyisnotabird.''Ourfuturedirectionalsoincludesprovidingtheformalproofofcompletenessandsoundnessofourapproach.thankthereviewersofthispaperfortheirhelpfulcomments.ThisworkwaspartiallysupportedbyGrant-in-AidforJSPSFellows(22-9719)andGrant-in-AidforScientificResearch(23240018).TheauthorswouldliketothankEnago(www.enago.jp)fortheEnglishlanguagereview.document</section>
</root>
