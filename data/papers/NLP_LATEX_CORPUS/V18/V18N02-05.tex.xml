<?xml version="1.0" ?>
<root>
  <subsection title="F_1 and the annotation cost">WecomparetheannotationcostandF_1oftheproposedframeworkandthebaselinesinFig.~andTable.With=0.99,[Proposed]requiresslightlyfewernumbersofmanuallylabeledtokensthan[Wanvarie10],butachievesthesupervisedF_1.Withsmallerthreshold,=0.90,[Proposed]requiresmuchlessnumberofmanuallytokens,andstillachievesthesupervisedF_1.AlthoughtheachievedF_1ofthethreesystemsarenotthesame,thedifferenceisnotstatisticallysignificant.[Proposed]with=0.90requirescomparablecomputationaltimeto[FuSAL],butrequiresmuchlesslabeledtokensinordertoachievethesupervisedF_1.Thetrainingtimeofasingleiterationdependsontwofactors;thedatasizeandthecomplexityoflabeldistribution.Thedatasizeisthenumberoftokensinthetrainingset,bothlabeledandunlabeledtokens.ThedatasizedoesmattersincetheoptimizationofCRFswillverifythecurrentparametersettingsoneachsampleinthedataset.Hence,alargetrainingsetrequiresmoretrainingtimethanasmalltrainingset.Withthesamedatasize,thetrainingsetwithcomplexlabeldistributiontendstorequiremoretimethanthesimpletrainingset.Thecomplexityofthedistributiondependsonthenumberofmanuallylabeledtokens.Therefore,thetrainingsetwithfullylabeledsequencesrequiresmoretrainingtimethanthepartiallylabeledtrainingset.In[Proposed],wehavetolabelsomesequencesmorethanonce,i.e.labelingmorethanatokeninthesequence.With=0.90,51.75%ofthetrainingsequencesareinformative.Amongtheinformativesequences,61.57%ofsequenceshaveonlyasinglelabeledtoken.Theothersequencesareuninformative,andneverbeselected.Weconcludethatourstrategycanefficientlyselectinformativetokenstoreducetheannotationcost,whilemaintainingthesupervisedF_1.</subsection>
  <section title="Introduction">Sequencelabelinghasmanyapplicationsinnaturallanguageprocessingsuchaspart-of-speechtagging,shallowparsing,semanticrolelabeling.Sequencelabelingisakindoftheclassificationtasks.Aclassifier,oratagger,willpredicttheoutputofeachtokeninthegivensequence.Sequencelabelingisnottrivialduetothedependencyamongtheoutputlabelsinthesequence.Thedependencyalsomakestheannotationdifficult,andisalsothereasonofhighannotationcost.Wecandivideasequenceintosmallersubsequencesconsistingoftokens,andlabeleachsubsequence.Inmanycases,labelingasubsequenceiseasierthanlabelingthewholesequence.Forexample,wecanexploitthekeywordlinkinWikipediatextforwordboundaries.proposedanannotationstyleforinformationextraction(IE),whichallowsuserstopartiallylabeltheentityinthedocument.Ifwecantrainataggerusingpartiallylabeledsequencesinsteadofthefullylabeledsequences,wecanreducethehumanannotationeffort.Thereareseveralapproachestotrainataggerusingpartiallylabeledsequences.Supposethatwehaveatagger,wecangivelabelstotheunlabeledpartsusingthemodelprediction.However,thepredictionisnotasaccurateasthemanualannotation.Moreover,addingmis-labeledtokenstothetrainingsetusuallydecreasestheaccuracyofthetagger.Instead,weproposetoautomaticallyestimatethelabelofunlabeledtokenswithoutexplicitlylabelingthem.Thetaggersinearlyiterationsareusuallynotaccurate,andmayincorrectlypredicttheoutputlabels.Theseimplicitlyincorrectlylabeledtokensmaybecorrectedwhenthetaggersbecomemoreaccurateinlateriterations.Activelearningisanotherapproachtoreducetheannotationcostfromtheconventionalpassivelearningbyelaboratelyselectingtheinformativesamplesfortraining.Theconventionalactivelearningtreatsasequenceasasampleandsearchfortheinformativesequence.Wearguethatwearerequiredtolabelsomeunnecessarytokensusingthissequencesamplingstrategy.Sinceweproposetopartiallylabelasequence,weproposeasubsequencesamplingstrategywhichismoreefficientthanthesequencesamplingforourframework.Theorganizationoftherestofpaperisasfollows.Westartwiththediscussiononrelatedworktotheproposedframeworkinsection.SectionisdevotedtotheparameterestimationofCRFs.Insection,wedescribeourframeworkindetail.Sectioncontainstheexperiments,discussionandanalysisoftheresult.Finally,weconcludeourcontributionanddiscussthefutureworkinsection.</section>
  <section title="Related work">SeveralalgorithmsareproposedtosolvethesequencelabelingtasksuchasHiddenMarkovModels(HMMs),ConditionalRandomFields(CRFs),Perceptron,SupportVectorMachine(SVM),andensembleapproacheswhichcombinetheresultoftwoormoreapproachestogether.Eachofthementionedalgorithmsisprimarilyproposedasasupervisedlearningalgorithm,whoseperformancereliesonalargeamountoflabeleddata.Thecostofdatalabelingisthemaindisadvantageofsupervisedlearning,especiallyinthetasksuchassequencelabelingwhosedatalabelingisveryexpensive.Semi-supervisedlearningisalearningparadigmwhichcanbenefitfromboththehighly-accurate-but-costlylabeleddata,andnoisy-but-cheapunlabeleddata.Ononehand,wecanboosttheaccuracyofthesupervisedsystemusingadditionallargeamountofunlabeleddata.Ontheotherhand,wecanreducetheannotationcostofanewcorpusbylabelingonlyasmallpartofthecorpus,whilemaintainingthecomparableaccuracytothesupervisedapproacheswhichrequiresfullannotation.Thereareseveralapproachesinthesemi-supervisedlearningparadigm.Self-trainingandtransductiveSVMareexamplesofsemi-supervisedlearningwhichassumethattheunlabeleddatahasthesamestructureasthelabeleddata.Hence,wecanpropagatetheknowledgefromthelabeledsettotheunlabeledset.Self-taughtlearningstartslearningthegeneralconceptfromunlabeleddata,andrefinetheconcepttothespecifictaskusinglabeleddata.separatedthemodelparametersforlabeledandunlabeleddata,andestimatebothtypesofparameterssimultaneously.proposedtolabelonlyambiguoussamplesandautomaticallylabeleasysamples.Usingpartiallylabeleddataisanotherkindofsemi-supervisedlearningapproaches.Partiallylabeleddataisobtainedeasierthanfullylabeleddatainmanysituations.Wecantrainaclassifieronpartiallylabeledsequencesbypredictingthelabelsofunlabeledtokens,propagatingthelabelinformationfromlabeledtounlabeledtokens,orimplicitlyestimatingthelabelsinthetrainingprocess.Anotherapproachistosimplifythetrainingintothepoint-wiselearning,byneglectingthedependenciesamongoutputlabels.Apartfromthesemi-supervisedlearning,activelearningparadigmisproposedtoreducetheannotationcostfromthepassivelearningbyelaboratelyselectinghighlyinformativesamplesforthetraining.havecomparedseveralactivelearningstrategiesforfullysupervisedsequencelabelingtasks.Activelearningisalsousedincombinationwithsemi-supervisedlearningmethodsinordertoreducetheannotationcost.</section>
  <section title="Conditional random fields (CRFs)"/>
  <subsection title="Conventional conditional random fields">TheobjectiveofthesequencelabelingtaskistofindanoutputlabelsequenceyY:(y_1,...,y_T)ofthegiveninputsequencexX:(x_1,...,x_T).XandYisthesetofallpossibleinputandthecorrespondingoutputsequences,respectively.Tisthelengthofthesequence.Weemploytheconditionalrandomfields(Laffertyetal.2001)tomodeltheprobabilityofy,givenxbyWehavediscussedthatanoutputlabel,y,dependsonboththeinputandtheoutputtokens.Hence,ourfeaturefunction(x,y):XYisamappingfrombothinputandoutputtoarealvalue.Supposethatwehavedfeatures,and=(_1,...,_d)isourfeaturevector.isthemodelparameters,ortheweightvector,estimatedinthetrainingprocess.Finally,Z_,x,Yisthenormalizingfactordefinedby[	Z_,x,Y=_yYe^(x,y).]Let_jbetheprobabilityoftheprefixsequencetopositionj,calledtheforwardprobability:	_j(y')&amp;=_y''(_j-1(y'')p_t(y'',y')p_s(y')),	_1(y')&amp;=p_s(y').align*Let_jbetheprobabilityofthesuffixsequencefrompositionj,calledthebackwardprobability:	_j(y')&amp;=_y''(p_t(y',y'')p_s(y'')_j+1(y'')),	_T(y')&amp;=1.align*p_t(y',y'')isthetransitionprobabilityfromlabely'tolabely''.p_s(y')istheoutputprobabilityoflabely'.Aftercalculatingall_j,_j,wecanefficientlycomputeZ_,x,Yby[	Z_,x,Y=_y'Y_1_1(y')_1(y'),]whereY_1isallpossiblelabelsofy_1.Inordertoestimate,wewillmaximizethefollowingloglikelihoodfunction:Nisthenumberofsequencesinthetrainingset.x^(n)andy^(n)arethen^thinputsequenceandtheViterbioutputsequenceofx^(n),respectively.WecanapplystandardoptimizationtechniquessuchasL-BFGSorSGDtotheobjectivefunctionin().</subsection>
  <subsection title="Conditional random fields for partially labeled sequences">Theconventionalmethoddescribedinsectionrequiresthewholesequencetobelabeledinordertocomputethesequenceprobabilityusing().Weneedtore-definethesequenceprobabilityifthesequenceisonlypartiallylabeled.GivenapartiallyorambiguouslylabeledsequenceL,letY_LbethesetofallpossibleoutputsequencesconsistentwithL.follow(BellareandMcCallum2007;Tsuboietal.2008)toestimatetheprobabilityofY_LgivenxbyUsing(),theloglikelihoodin()ismodifiedtoLL()&amp;=_n=1^NP_(Y_L^(n)|x^(n))	&amp;=_n=1^N(_yY_L^(n)e^(x^(n),y)		_yYe^(x^(n),y))	&amp;=_n=1^N(_yY_L^(n)(e^(x^(n),y))-_yY(e^(x^(n),y)))	&amp;=_n=1^N(Z_,x^(n),Y_L^(n)-Z_,x^(n),Y).alignY_L^(n)isthesetofallpossibleoutputsequencesofx^(n).Z_,x^(n),Y_L^(n)canbecomputedbythesamealgorithmusedforZ_,x,Y.Wethenapplythestandardoptimizationtechniquesto()asdoneto().</subsection>
  <section title="Proposed framework">OuractivelearningframeworkisshowninFig.~.frameworkconsistsof2mainparts,thequerystrategyandthestoppingcriteria.Wedenoteourframeworkaspartiallylabelingwithtokensampling([Proposed]).</section>
  <subsection title="Query strategy">Thekeyofactivelearningisthequerystrategywhichshouldselectgoodsamplesfortraining.Firstly,wewilldefinethesampleanditsinformativeness.Wehavediscussedintheintroductionthatwecanpartiallylabelasequence.Sinceatokenisthesmallestsubsequenceunit,wewilltreateachtokenasoursample.Then,wewillsearchforinformativetokens,incontrasttotheconventionalactivelearningwhichtreatsthewholesequenceasasampleandsearchforinformativesequences.Ineachiteration,ataggercanpredicttheoutputofeachtokenwithdifferentconfidencelevel.Wedefinethepredictionconfidenceofeachtokenbyitsmarginalprobability:whereandaretheforwardandbackwardprobabilitiesdefinedinsection.Sinceasequencecanhaveseveralpossibleoutputsequences,themarginalprobabilityofatokendependsonthechoiceoftheoutputsequence.Inordertocomparethepredictionconfidenceoftokensbetweensequences,wechoosethepredictionconfidencebasedonthemostlikelyoutputsequencesdefinedbytheViterbisequence:Whenthepredictionconfidenceofatokenislessthanthepredefinedthreshold,itindicatesthatthetokenmaycontaincrucialinformationwhichisnotpreviouslyknowntothetagger.Hence,wedefinesuchatokentobeinformative,andaskahumanannotatortolabelthetoken.Ineachiteration,wesearchforqtokenswhicharethemostinformative,andupdatetheirlabelswithhumanannotation.Althoughwesearchfortokens,weprovidetheinformativetokenswiththeircontext,i.e.thewholesequences,toahumanannotatorsincecontextisnecessaryforthelabeling.Asequencemaybeselectedseveraltimes.Suchasequencewillhaveseverallabeledtokens.WetrainthetaggerineachiterationusingCRFsdefinedinsection.Thetrainingsetcontainsbothfullylabeled,partiallylabeled,andunlabeledsequences.However,ourCRFsparameterestimationcannotbenefitfromtheunlabeledsequences.Onthecontrary,addingunlabeledsequenceswillslowtheoptimizationprocess.Hence,wejusttrainthetaggerusingthefullyandpartiallylabeledsequences.</subsection>
  <subsection title="Stopping criteria">Sincewesearchforinformativetokens,wecanstopthelearningifwecannotfindmoreinformativetokensinthetrainingset.However,addingfewnewlabeledtokenstothetrainingsetdoesnothelpimprovingtheF_1butjustwastingthetrainingtimeofthetagger.Inthequeryprocess,weusethetaggertopredicttheoutputofalltokensinthewholetrainingset.LetthepredictedoutputofthewholetrainingsetatiterationtbeS_t.Iftherearefewdifferencesbetweenthepredictedlabelsoftheconsecutiveiterations(i.e.S_tandS_t+1),wecanstopthelearning.WemeasurethedifferenceofpredictedsetsusingKappastatistics.Fromourexperiment,standardthresholdat0.99makesthelearningstopsoearlythatthesystemcannotachievehighF_1.Inthispaper,weregardthedatasetsassimilarwhen(S_t,S_t+1)&gt;0.9999.However,thedifferenceofthepredictedtrainingsetsmaybesmallifthequerysizeissmall.Wealsoaddtheconditionthatthenumberofinformativetokensmustbelessthanthequerysizeinordertostopthelearning.</subsection>
  <section title="Experiments and results"/>
  <subsection title="Data">WeconductedexperimentsonchunkingtaskusingCoNLL-2000dataset,andnamedentityrecognitiontaskusingCoNLL-2003dataset.WesummarizethestatisticsofeachdatasetinTable.Bothtasksareformulatedasasequencelabelingtask.Atokeninasequencereferstoaword,whilethesequenceitselfreferstoasentence.ThelabelofeachtokeniseitherBegin-Chunk,Inside-ChunkorOutsidechunk,e.g.B-NPisthebeginningtokenofanNPchunk.Forthenamedentityrecognitiontask,achunkisanamedentitychunk,e.g.PERSON,ORGANIZATION.WeanalyzedtheresultusingCoNLL-2000chunkingdatasetbutsimplifiedthetasktonounphrase(NP)chunking.FortheNPchunkingtask,wehave3typesoflabel,B-NP,I-NP,andO.Fig.~showsanexampleofNPchunkingformulatedasasequencelabelingtask.ThefeaturetemplatesforeachdatasetareshowninTable.Eachtemplateisshowninabracket.w_iandlw_iarethewordanditslowercaseatpositioniinasentence.p_iisthepart-of-speech(POS)ofw_i,whichisprovidedinthedataset.c_iisachunktype,e.g.NPchunk.wtpisthewordtypedescribedinTable.pw[c]_iandsw[c]_iarec-characterprefixandsuffixofwordw_i,e.g.3-characterprefixofthewordAmericanisAme.y_iisanoutputlabelofw_i.</subsection>
  <subsection title="Evalution">OurgoalistoachievethesupervisedF_1atthesmallestannotationcost.F_1iscalculatedusingCoNLLevaluation.WeclaimthatweachievethesupervisedF_1ifthepredictedoutputisnotsignificantlydifferentfromthesupervisedpredictioninMcNemartest.Wemeasuretheannotationcostin2aspects;thenumberofmanuallylabeledtokens,andthetimerequiredinthewholetraining(excludingthehumanannotation).</subsection>
  <subsection title="Initial set">Theconventionalactivelearningusuallyvalidatestheresultamongseveralrandominitialsetssincethetrainingissensitivetotheinitialset.Wecomparedtheinitialsetofshortsequences,longsequencesandrandomsequencesinFig.~.Eachinitialsetconsistsof50sequences.Theconfidencethresholdissetto0.99.Alltokenswiththepredictionconfidencelessthanthethresholdareinformative.Thetop500informativetokensareselectedandmanuallylabeledineachiterationofactivelearning.TheresultinFig.~showsthattheinitialsetwithlongsequencesclearlyconvergeswithfewerlabeledtokensthantheothersets,whilerequiressimilartrainingtime.Wediscussthatlongsequencescontainmoreinformationthanshortsequences.Themodeltrainedontheselongsequencesismoreaccurateandmorereliablethanthemodeltrainedonshortsequences.Althoughalongandcomplexsequencemayrequirelongannotationtime,theheuristicisappliedonlyintheinitialstep.Thesamplingintheotherstepsisaccordingtothequerystrategyofactivelearning,andthewholenumberoflabeledtokensofusinglonginitialsetislessthanwhenusingtheotherinitialsets.Hence,wewilluse50longestsequencesasourinitialsetinallofthefollowingexperiments.Notethattheremaybemoresophisticatedmodelswhicharemoreefficientthantheheuristictestedinthiswork.Weleavethetasktofindthebestinitialsettofuturework.</subsection>
  <subsection title="Baseline systems">Inthissection,wewillcompareseveraliterativeandnon-iterativebaselines.[Sup]:Thefirstbaselineisasimplesupervisedsystemtrainedonthewholetrainingset.TheF_1of[Sup]istheupper-boundofallsystems.[Pointwise]:proposedapoint-wisetrainingframeworkinthedomainadaptationtask,whichneglectsthedependenciesamongtheoutputlabels.Sincewewillcompareeachmethodonasingledomain,wetrainapoint-wisesysteminsupervisedmanner.Specifically,weuseallfeaturesinTableexceptthetransitionfeature.Thesupervisedpoint-wisesystemistheupperboundaccuracyofapoint-wiseactivelearningsystem.[Partial]:Thelastnon-iterativebaselineisthesystemtrainedonpartiallylabeleddata.Wecalculatetheconfidencescoreofeachunlabeledtokensusingthemodeltrainedontheinitialset.Alltokenswithconfidencelessthanaremanuallylabeled.Theothertokensareleftunlabeled.Thecomparisonbetweennon-iterativebaselinesisshowninthefirsthalfofTable.Neither[Pointwise]nor[Partial]achievesthesupervisedF_1.Althoughthetrainingof[Pointwise]isfast,theF_1ofthesystemiscompetitivebutstillsignificantlyworsethanthesupervisedupperbound.ThelowF_1achievementof[Pointwise]indicatesthatthedependenciesbetweenoutputlabelsareimportant.SincetheupperboundF_1ofthesupervisedpoint-wisesystemisnothigh,wewillnotapplytheiterativetrainingtopoint-wisefeatures.[Partial]achieveshigherF_1than[Pointwise]eventhoughitrequireslessnumberoflabeledtokens,butitsbestF_1isstillsignificantlylowerthanthesupervisedupperbound.Thefollowingexperimentisthecomparisonamongtheiterativebaselines.Alliterativesystemssample500sequenceswithlowViterbiscore(definedin())ineachiteration.Theseiterativebaselinesaresequence-basedsamplingincontrasttothetoken-basedsamplingoftheproposedframework.Thetokenineachsampledsequenceisinformativeifitsscore(definedin())islessthantheconfidencethreshold,.Wesetto0.99.[FuSAL]:Thefirstiterativebaselineisafullysupervisedactivelearningsystem,whosealltrainingtokensaremanuallylabeled.[FuSAL]becomesalmostexactlythesameas[Sup]afterwelabelthewholetrainingset.Theonlydifferencebetween[FuSAL]and[Sup]isthesentenceorderinginthetrainingset.[Bootstrap]:Bootstrappingisasemi-supervisedlearningapproach.Onlytokensintheinitialsetaremanuallylabeled.Theothertokensareeitherlabeledbythemodelpredictioniftheirpredictionconfidenceishigh,orleftunlabeledotherwise.[Tomanek09]:Thesecondsemi-supervisedactivelearningbaselineisthesystemproposedin.Ineachsampledsentence,thelowconfidencetokensaremanuallylabeled,whilethehighconfidencetokensareautomaticallylabeledbythemodelprediction.[Wanvarie10]:Thelastsemi-supervisedactivelearningbaselineisproposedin.Thesystemissimilarto[Tomanek09]exceptthatthehighconfidencetokensareleftunlabeledinsteadofbeinglabeledbythemodelprediction.Theyimplicitlyestimatethoselabelsinthetrainingsetusingthemethoddescribedin(Tsuboietal.2008).Figureshowsthelearningcurveofeachtrainingstrategy.NotethattheF_1of[Sup]doesnotvarysincewetrainthetaggeroncewiththewholetrainingset.WejustdrawastraightF_1lineof[Sup]forreference.Similarly,thenumberofmanuallylabeledtokensof[Bootstrap]isalsofixedtothenumberoftokensintheinitialset.However,theF_1of[Bootstrap]variessincewegraduallyaddtokenstothetrainingset.WedrawastraightlineoffinalF_1of[Bootstrap]vs.numberoflabeledtokens.Thebottomlineofthebootstrappingsystem([Bootstrap])indicatesthatwecanlearnlittlenewinformationfromunlabeledtokens.Hence,theF_1of[Bootstrap]isfarworsethantheotherbaselines.AmongthethreelearningcurvesinthemiddleofthegraphontheleftofFig.~,thebottomcurveof[FuSAL]hastheslowestlearningrate.TheF_1of[FuSAL]graduallyincreasedandreachedthesupervisedlevelwhenapproximately150~koftokensarelabeled.Duetothelimitedspace,wecutthelearningcurvetothefirst15~klabeledtokens.Theothertwocurvesrepresentlearningrateof[Tomanek09]and[Wanvarie10].Although[Tomanek09]achievesmuchhigherF_1than[Bootstrap]with4%additionallabeledtokens,itsbestF_1isstillfarworsethanthesupervisedF_1.Ontheotherhand,[Wanvarie10]achievesslightlyhigherF_1than[Tomanek09]whensimilarnumberoflabeledtokensareavailable,andfinallyachievesthesupervisedF_1aftermoretokensarelabeled.Since[Wanvarie10]didnotexplicitlylabelhighconfidencetokensasdonein[Tomanek09],thepredictionerrorsinearlyiterationsarerecoveredwhenthetaggerbecomesmorepreciseinlateriterations.Asaresult,[Wanvarie10]achieveshigherF_1than[Tomanek09].TheerrorrecoveryisalsothereasonthattheF_1ofactivesamplingissuperiortotheF_1ofthebatchsampling,sincethebatchsamplingreliesontheaccuracyoftheinitialmodelwhichisusuallypoor.ThegraphontherightofFig.~showsthetrainingtimeofeachbaseline.Thetrainingwithfewlabeledtokenstendstorequirelesstrainingtime.Wewilldiscussonthetrainingtimelaterinsection.Finally,wecomparetheachievementofactiveandbatchsamplingbaselinesinTable.Inthetermsofmanuallylabeledtokens,activesamplingmethodsclearlyrequirefewlabeledtokensandachievehigherF_1.Sincethemodelbecomesmorecertaininlateiterations,weneedtolabelfewnewtokensbutstillachievehighF_1.However,theiterativemethodshaveadrawbackinthetrainingtimesincewehavetore-estimatethemodelparametersineveryiteration.Duetothelimitedspace,weselectonly[Sup]and[Wanvarie10]tocomparewiththeproposedmethod([Proposed])intherestoftheexperiments.</subsection>
  <subsection title="Effect of the query size">Wecansavetheannotationcostintermsoflabeledtokensbyselectingonlyfewhighlyinformativetokensineachiteration.However,acertainnumberoftokensarenecessarytoachievethecertainlevelofF_1.Ifthequerysizeissmall,wehavetodoseveraliterationstoobtainsufficientnumberoflabeledtokens,andwehavetospendmoretimeonthetrainingofthetaggers.SincetheCRFstrainingisthemosttimeconsumingprocessintheframework,itisnotpracticaltore-trainthetaggersseveraltimes.Thequerysizesettingcontrolsthetrade-offbetweenthenumberofmanuallylabeledtokensandthetimerequiredinthetraining.Wecomparethequerysizesettingsvariedfrom50,500,to1000sequencesperiteration.Theconfidencethresholdissetto0.99.TheresultisshowninFig.~.Thelargequerysizesettingclearlyreducesthewholetrainingtime,butslightlyrequiresmorelabeledtokens.Notethatwedidnotcounttheactualtimespentbytheannotator.Largequerysettingusuallyrequiresmoretimeforanannotatortolabelthequeriedset.Nevertheless,webelievethatlabelingasingletokenwillnottakelongtime.Wesetthequerysizeto500tokensperiterationinallofthefollowingexperiments.Inotherwords,atmost500sequencesarepresentedtotheannotatorateachiteration.</subsection>
  <subsection title="Effect of the confidence threshold">Withhighconfidencethresholdsettings,themodelbecomesmorereliablewiththetrade-offofthehumanannotationcost.FromtheresultinFig.~,thesystemwithlowthreshold,=0.60,requiresfewmanuallylabeledtokensbutstopsearlybeforeachievingthesupervisedF_1.Wearguethatthenumberoflabeledtokensisinsufficient,andthemodeltrainedonthistrainingsetproducesmanyerrors.Incontrast,withthehighthreshold,=0.90and0.99,thesystemsrequiremorelabeledtokensthanthesystemwithlowthresholdsetting,butachievethesupervisedF_1level.</subsection>
  <subsection title="Result on chunking and named entity recognition">Finally,weconductedexperimentsonthechunkingandthenamedentityrecognitiontasks.Fig.~andFig.~showthattheproposedalgorithmconsistentlyoutperformsallbaselinesinbothdatasets.TheF_1oftheproposedframeworkalsoreachesthesupervisedF_1withlessmanuallylabeledtokens.AlthoughthefinalF_1of[Proposed]withthresholdat0.99isslightlyhigherthantheF_1ofthesamemethodwiththresholdat0.90,thepredictionresultsfrombothsystemsarenotstatisticallysignificantlydifferentfromthesupervisedF_1.TablesummarizestheannotationcostandF_1achievementoftheproposedframeworkandthebaselinesystems.Wecanreducetheannotationcostintermsoflabeledtokenstoonly6.98%inCoNLLchunking,and7.01%inCoNLLnamedentityrecognitiontask.SimilartotheresultonNPchunkingtask,notallofthesequencesareinformative,andhalfoftheseinformativesequenceshaveonlyasinglelabeledtoken.Theresultconfirmsourintuitionthatwedonotneedtolabelthewholesequences.However,theproposedframeworkneedstwicemoretrainingtimethan[FuSAL]systems.IncontrasttotheresultinNPchunkingtask,chunkingandnamedentityrecognitiontaskshavemuchmoretypesoflabels,whichmakesthelabeldistributioncomplex.Thus,bothtasksrequiremorecomputationaltimethanNPchunking.</subsection>
  <section title="Conclusion and future work">Wehaveproposedasemi-supervisedactivelearningframeworkwhichrequireslessmanuallylabeledtokenstoachievethesupervisedF_1.Thekeyofourframeworkisthetokensamplingandlabelingwhichcanbothreducetheannotationcostandrecoverthepredictionerrorsfrompreviousiterations.Sincetheactualannotationcostofalltokensmaynotbeequal,wecandirectlyincorporateamorerealisticcostmodelsuchas,whichcanrepresenttheactualhumanannotationtime,toourquerystrategy.Ourframeworkisnotlimitedtothesequencelabelingtask.Weareplanningtoextendourworktoageneralstructuredpredictiontask.Moreover,wecanalsoemployotherlearningalgorithmsapartfromtheCRFstagger.However,weneedtore-definethepredictionconfidence.thankparticipantsintheNLPspecialinterestgroupofIPSJforvaluablecomments.ThefirstauthorissupportedbytheHigherEducationalStrategicScholarshipsforFrontierResearchNetwork,Thailand.document</section>
</root>
