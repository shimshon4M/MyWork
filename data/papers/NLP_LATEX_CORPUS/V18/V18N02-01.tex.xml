<?xml version="1.0" ?>
<root>
  <jtitle>確率的タグ付与コーパスからの言語モデル構築</jtitle>
  <jauthor>森信介笹田鉄郎NeubigGraham</jauthor>
  <jabstract>確率的言語モデルは，仮名漢字変換や音声認識などに広く用いられている．パラメータは，コーパスの既存のツールによる処理結果から推定される．精度の高い読み推定ツールは存在しないため，結果として，言語モデルの単位を単語（と品詞の組）とし，仮名漢字モデルを比較的小さい読み付与済みコーパスから推定したり，単語の発音の確率を推定せずに一定値としている．これは，単語の読みの確率を文脈と独立であると仮定していることになり，この仮定に起因する精度低下がある．このような問題を解決するために，本論文では，まず，仮名漢字変換において，単語と読みの組を単位とする言語モデルを利用することを提案する．単語と読みの組を単位とする言語モデルのパラメータは，自動単語分割および自動読み推定の結果から推定される．この処理過程で発生する誤りの問題を回避するために，本論文では，確率的タグ付与を提案する．これらの提案を採用するか否かに応じて複数の仮名漢字変換器を構築し，テストコーパスにおける変換精度を比較した結果，単語と読みの組を言語モデルの単位とし，そのパラメータを確率的に単語分割し，さらに確率的読みを付与したコーパスから推定することで最も高い変換精度となることが分かった．したがって，本論文で提案する単語と読みの組を単位とする言語モデルと，確率的タグ付与コーパスの概念は有用であると結論できる．</jabstract>
  <jkeywords>確率的言語モデル，仮名漢字変換，確率的単語分割，確率的タグ付与</jkeywords>
  <section title="はじめに">確率的言語モデルは，統計的手法による仮名漢字変換や音声認識などに広く用いられている．確率的言語モデルは，ある単語列がある言語でどの程度自然であるかを出現確率としてモデル化する．仮名漢字変換においては，確率的言語モデルに加えて，仮名漢字モデルが用いられる．仮名漢字モデルは，入力記号列と単語の対応を記述する．音声認識では，仮名漢字モデルの代わりに，発音と単語の対応を記述する発音辞書と音響モデルが用いられる．確率的言語モデルの推定のためには，システムを適応する分野の大量のテキストが必要で，その文は単語に分割されている必要がある．このため，日本語を対象とする場合には，自動単語分割や形態素解析が必要であるが，ある程度汎用性のあるツールが公開されており，辞書の追加などで一般的な分野の言語モデルが構築可能となっている．仮名漢字モデルや発音辞書における確率の推定には，実際の使用における単語の読みの頻度を計数する必要がある．しかしながら，読み推定をある程度の汎用性と精度で行うツールは存在しない．したがって，仮名漢字モデルを比較的小さい読み付与済みコーパスから推定したり，後処理によって，一部の高頻度語にのみ文脈に応じた発音を付与し，他の単語に関しては，各発音の確率を推定せずに一定値としているのが現状である．一方で，単語（表記）を言語モデルの単位とすることには弊害がある．例えば，「…するや，…した」という発声が，「…する夜，…した」と書き起こされることがある．この書き起こし結果の「夜」は，この文脈では必ず「よる」と発音されるので，「夜」と書き起こすのは不適切である．この問題は，単語を言語モデルの単位とする仮名漢字変換においても同様に起こる．これは，単語の読みの確率を文脈と独立であると仮定して推定（あるいは一定値に固定）していることに起因する．このような問題を解決するために，本論文では，まず，すべての単語を読みで細分化し，単語と読みの組を単位とする言語モデルを利用することを提案する．仮名漢字変換や音声認識において，単語と品詞の組を言語モデルの単位とすることや，一部の高頻度語を読みで細分化することが行われている．提案手法は，品詞ではなく読みですべての単語を細分化することとみなすこともできるので，提案手法は既存手法から容易に類推可能であろう．しかしながら，提案手法を実現するためには，文脈に応じた正確な読みを様々な分野のテキストに対してある程度の精度で推定できる必要がある．このため，提案手法を実現したという報告はない．単語を単位とする言語モデルのパラメータは，自動単語分割の結果から推定される．自動単語分割の精度は十分高いとはいえ，一定の割合の誤りは避けられない．この問題による悪影響を避けるために，確率的単語分割という考えが提案されている．この方法では，各文字の間に単語境界が存在する確率を付与し，その確率を参照して計算される単語n-gramの期待頻度を用いて言語モデルを構築する．計算コストの削減のために，実際には，各文字間に対してその都度発生させた乱数と単語境界確率の比較結果から単語境界か否かを決定することで得られる擬似確率的単語分割コーパスから従来法と同様に言語モデルが構築される．単語と読みの組を単位とする言語モデルのパラメータは，自動単語分割および自動読み推定の結果から推定される．自動単語分割と同様に，自動読み推定の精度は十分高いとしても，一定の割合の誤りは避けられず，言語モデルのパラメータ推定に悪影響がある．これを回避するために，確率的タグ付与とその近似である擬似確率的タグ付与を提案する．実験では，タグとして入力記号列を採用し，単語と入力記号列の組を単位とする言語モデルを用いる仮名漢字変換器を構築し，単語を単位とする言語モデルを用いる場合や，決定的な単語分割や入力記号付与などの既存手法に対する提案手法の優位性を示す．</section>
  <section title="統計的仮名漢字変換">統計的手法による仮名漢字変換は，キーボードから直接入力可能な入力記号Yの正閉包yY^+を入力として，日本語の文字Xの正閉包である変換候補(x_1,;x_2,;)を確率値P(x|y)の降順に提示する=Ａ,Ｂ,,Ｚ,０,１,,９,ぁ,あ,,ん,ヴ,ヵ,ヶ,ー,＝,¥,｀,「,」,；,’,、,。,！,＠,＃,＄,％,＾,＆,＊,（,）,＿,＋,｜,〜,｛,｝,：,”,＜,＞,？,・である．．文献確率的モデルによる仮名漢字変換では文を単語列w=whとみなし，これを単語wX^+を単位とする言語モデルと仮名漢字モデルに分解して実現する方法を提案している．本節では，まずこれについて説明し，次に単語と読みを組とする言語モデルによる方法を提案し定式化する．</section>
  <subsection title="従来手法">文献確率的モデルによる仮名漢字変換では，変換候補をP(w|y)で順序付けすることを提案しており，これを次の式が示すように，単語を単位とする言語モデルと仮名漢字モデルに分解する．ここで，後述するパラメータ推定のために，単語と入力記号列との対応関係は各単語において独立であるとの仮定をおく．さらに，分母P(y)は出力に依らないので，分子だけを以下のようにモデル化する．P(y|w)P(w)=_i=1^hP(y_i|w_i)P(w_i|w_i-n+1^i-1)	P(y_i|w_i)P(w_i|w_i-n+1^i-1)=P(w_i|w_i-n+1^i-1)P(y_i|w_i)&amp;	ifw_iWP(|w_i-n+1^i-1)M_y,n(y_i)&amp;ifw_iWcasesgatherここでWは確率的言語モデルの語彙を表す．簡単のために，この式の中のw_i;(i0)は，文頭に対応する特別な記号であり，これは文末w_h+1も表す．この式のP(w_i|w_i-n+1^i-1)とP(|w_i-n+1^i-1)は，語彙にと未知語記号を加えたW,上のn-gramモデルである．パラメータは，単語に分割されたコーパスから以下の式を用いて最尤推定する．この式中のf(e)は，事象eのコーパスにおける頻度を表す．figure:LMAが示すように，この学習コーパスには自動単語分割の結果であることが多いが，自動単語分割器の学習に用いたタグ付きコーパスが利用可能な場合にはこれを加えることもある．eqnarray:KKConv1のP(y_i|w_i)は，単語単位の仮名漢字モデルであり，パラメータは，単語に分割されかつ各単語に入力記号列が付与されたコーパスから以下の式を用いて最尤推定する．eqnarray:KKConv1から分かるように，単語単位の仮名漢字モデルでは，単語と入力記号列との対応関係が各単語において独立であると仮定している．この仮定により，比較的少量の入力記号列付与済みコーパスからある程度信頼できるパラメータを推定することができる．eqnarray:KKConv1のM_y,n(y_i)は，未知語モデルであり，入力記号の集合に単語の両端を表す記号を加えたY上のn-gramモデルで実現されるによれば，あるテストコーパスにおいて未知語を構成する文字の33.0%が片仮名であった．入力記号集合は主に平仮名からなるが，この先行研究と同様に，出力においてはこれらを片仮名とする．．このパラメータは低頻度の単語に対応する入力記号列から推定する．</subsection>
  <subsection title="提案手法">本論文では，言語モデルの単位を単語と入力記号列の組u=とすることを提案する．その上で，以下の式のようにP(w|y)をモデル化する．[P(w|y)=P(w,y)P(y)=P(u)P(y)]分母P(y)は出力に依らないので，分子だけを以下のようにモデル化する．P(u)=_i=1^hP(u_i|u_i-n+1^i-1)	P(u_i|u_i-n+1^i-1)=P(u_i|u_i-n+1^i-1)&amp;ifu_iUP(|u_i-n+1^i-1)M_u,n(u_i)&amp;ifu_iUcasesgatherここでUは言語モデルの語彙（単語と入力記号列の組の集合）を表す．この式の中のu_i;(i0)とu_h+1は，単語を単位とする場合と同様に，文頭と文末に対応する記号である．または未知の組を表す記号である．eqnarray:KKConv2のM_u,n(u)=M_u,n()は未知語モデルである．従来手法と同様に，大きな学習コーパスを用いれば実際の使用における未知語率は極めて低く，また未知語に対する正確な仮名漢字変換は困難であると考えて，アルファベットU上の未知語モデルの代わりにアルファベットY上の未知語モデルM_y,n(y)を用いることとする．これは，eqnarray:KKConv1と共通である．以上から，提案手法の仮名漢字変換は，以下の式のようになる．ここでy_i=y(u_i)はu_i=1iの入力記号列である．なお，M_u,n(u)の代わりにM_y,n(y)を用いることは以下の式で与えられる近似であり，YXであるので，入力記号列のみからなる文字列を未知語として出力することになる．[M_u,n(u)=M_u,n()M_y,n(y)&amp;ifwY^+0&amp;ifwY^+cases]この式のM_y,n(y)のパラメータは，学習コーパスにおける語彙Uに含まれない表記と入力記号列の組の入力記号列から推定する．これは，学習コーパスにおける未知の組の単語を入力記号列に置き換えた結果からM_u,n(u)を推定しているのと同じである．eqnarray:KKConv3のP(u_i|u_i-n+1^i-1)とP(|u_i-n+1^i-1)は，語彙にとを加えたU,上のn-gramモデルである．パラメータは，単語に分割されかつ入力記号列が付与されたコーパスから以下の式を用いて最尤推定する．figure:LMBが示すように，この学習コーパスには自動単語分割・読み付与の結果を用いることができる．さらに自動単語分割器や読み付与の学習に用いたタグ付きコーパスが利用可能な場合にはこれを加えることもできる（figure:LMBの点線）．単語を単位とする従来手法と同程度の信頼性となるパラメータを推定するために，従来手法においてパラメータ推定に用いられる単語に分割されたコーパスと同程度の量の単語に分割されかつ入力記号列が付与されたコーパスが必要である．換言すれば，自動単語分割と同程度の精度で入力記号列を推定するシステムが必要である．これまで，各単語に対する入力記号列（読み）をその文脈に応じて十分な精度で推定する研究やフリーウェアがなかったために，提案手法は現実的ではなかったと思われるがあるが，公開されていない．また，読み推定ツールとしてKAKASI（http://kakasi.namazu.org/,2010年5月）があるが，様々な分野において十分な精度とはいえない．．次節では，この方法を説明し，さらに入力記号列を確率的に付与することで，入力記号列の推定誤りの影響を緩和する方法を提案する．</subsection>
  <section title="仮名漢字変換のための言語資源とその処理">仮名漢字変換や音声認識のための言語モデルは，単語分割済みコーパスと生コーパスの自動単語分割結果から構築される．この節では，まずこの過程を概説する．次に，前節で提案したモデルのパラメータをより正確に推定するために，単語に入力記号列や発音などのタグを確率的に付与することを提案する．</section>
  <subsection title="コーパス">仮名漢字変換や音声認識のための単語を単位とする言語モデル作成においては，これらを適用する分野のコーパスが必須である．一般に，コーパスには単語境界情報がないので，自動単語分割器や形態素解析器を用いて文を言語モデルの単位に分割し，その結果に対して単語n-gram頻度を計数する（figure:LMA参照）．なお，これら自動単語分割器や形態素解析器などの自然言語処理システムは，単語分割済みあるいは品詞付与済みのコーパスから学習することが多い．その場合には，これら自然言語処理システムの学習コーパスも言語モデルの学習コーパスに加えることができる（figure:LMAの点線）が，実際には，これら自然言語処理システムはツールとして配布され，辞書追加程度の適応しかなされず，自然言語処理システムの学習コーパスが言語モデルの学習に利用されることは少ない．</subsection>
  <subsection title="形態素解析と自動単語分割">形態素解析は，日本語の自然言語処理の第一段階として研究され，ルールに基づく方法が一定の成果を上げた．同じ頃，統計的手法が提案され，アルゴリズムとデータの分離に成功した．統計的手法は，ルールに基づく方法と同等かそれ以上の精度を達成しており，現在では主流になっている．さらに，フリーソフトとして公開され，容易に利用可能となっている．このような背景から，仮名漢字変換や音声認識のための言語モデル作成のために，形態素解析が用いられている．結果的に，単語（表記）と品詞の組を言語モデルの単位とすることが多い．しかしながら，仮名統計的漢字変換や音声認識等の実現には品詞情報は不要であり，形態素解析器の学習コーパス作成のコストを不必要に増大させるのみである．また，英語等の単語間に空白を置く言語の音声認識においては，言語モデルの単位として当然単語が用いられる．日本語においても単語を言語モデルの単位とする音声認識の取り組みがあり，十分な認識精度を報告している．以上の考察から，本論文では，単語と品詞の組を言語モデルの単位とする手法は，単語を単位とする手法に含まれるとして，以下の議論を展開する．言語モデルの構築においては，適応対象の分野の大量のテキストに対する統計をとることが非常に有用である．このため，形態素解析や自動単語分割等の自動処理が必須であるが，自動処理の結果は一定量の誤りを含む．この単語分割誤りによる悪影響を緩和するために，確率的に単語に分割することが提案されている．この手法では，自動単語分割器によって各文字の間に単語境界がある確率を付与し，その確率を参照して計算される単語n-gramの期待頻度を用いて言語モデルが構築される．実用上は，モンテカルロシミュレーションのように，各文字間に対して都度発生させた乱数と単語境界確率の比較結果から単語境界か否かを決定することで得られる擬似確率的単語分割コーパスから従来法と同様に言語モデルが構築される．</subsection>
  <subsection title="自動読み推定">前節で，仮名漢字変換のための言語モデルの単位として単語と入力記号列の組を用いることを提案した．この考え自体は特に新規ではなく，以前から存在している．実際，音声認識において，数詞のあとの「本」など一部の高頻度語に文脈に応じた発音を付与する後処理が行われている．また，発音レベルでの書き起こしが得られる場合に，単語と発音の対応を推定し，単語と品詞と発音の組を単位をとする言語モデルを構築する研究もある．しかしながら，この考えを一般的な場合において実現するためには，高精度の自動読み推定システムが必要である．前述の形態素解析の研究とその成果であるフリーソフトにおいては，読みの推定は軽視されており，文脈に応じた読みを高い精度で出力する研究やフリーソフトはなかった．このため，単語と入力記号列の組や単語と発音の組を単位とする言語モデルは一般的な意味で実現されていなかった．前節で提案した単語と入力記号列の組を単位とする言語モデルの構築においては，コーパスを単語に分割し，文脈に応じた読みを付与することができるKyTea（京都テキスト解析ツールキット）を用いて適応対象の分野のテキストを自動的に単語と入力記号列の組の列に変換する（figure:LMB参照）．その結果からequation:UMを用いて単語と入力記号列の組のn-gram確率を推定する．KyTeaの詳細はappe:kyteaに記述した．</subsection>
  <subsection title="確率的タグ付与">自動読み推定の結果は，形態素解析や自動単語分割等の自動処理の場合と同様に，一定量の誤りを含む．学習コーパスに含まれる読み推定誤りは，言語モデルや仮名漢字モデル，あるいは発音辞書に悪影響を及ぼす．特に，ある単語に対して至る所で同じ誤った読みを付与する場合には，非常に重大な問題となる．この問題を回避するために，確率的単語分割と同様に，単語に対する入力記号列付与や発音付与を確率的に行うことを提案する．すなわち，読み推定においては，ある単語に対する読みを決定的に推定するのではなく，可能な読みとその確率値を返すようにする．より一般的には，単語に対する読みや品詞などのタグ付与を，ある基準で最適となる唯一のタグを出力する処理ではなく，タグtと確率値pの組の列(t_1p_1,t_2p_2,)を出力する処理へと一般化する．この際，タグの確率値は，周辺の他の単語のタグと独立であるとの仮定をおく．この結果得られるコーパスを確率的タグ付与コーパスと呼ぶ．確率的タグ付与コーパスの文whは，以下のように，各単語に可能なタグと確率値の組の列が付与されている．w_1,(t_1,1p_1,1,t_1,2p_1,2,,t_1,k_1p_1,k_1)w_2,(t_2,1p_2,1,t_2,2p_2,2,,t_2,k_2p_2,k_2)w_h,(t_h,1p_h,1,t_h,2p_h,2,,t_h,k_hp_h,k_h)gather*ここで，t_i,jとp_i,jはそれぞれ，i番目の単語のj番目のタグとその確率を表す．このような確率的タグ付与コーパスにおける単語とタグの組のn-gramの1回の出現あたりの頻度f_1(u)は，以下の式で計算される期待頻度として定義される．この値をコーパスにおけるすべての出現箇所に渡って合計した結果が単語とタグの組の列uの期待頻度である．単語とタグの組のn-gram確率は，この期待頻度の相対値として定義される．仮名漢字変換のための言語モデル構築では，タグとして単語に対応する入力記号列を用いる．確率的入力記号列付与のためのモデルは，単語ごとに入力記号列が付与されたコーパスからロジスティック回帰などの点予測器を推定しておくことで実現できる．</subsection>
  <subsection title="擬似確率的タグ付与">確率的単語分割の場合と同様に，確率的タグ付与コーパスに対する単語とタグの組の列の頻度の計算は，決定的タグ付与コーパスに対する頻度計算と比べてはるかに多い計算を要する．実際，対象となる組の列としての頻度がF回とすると，equation:STCFreqによる期待頻度の計算は，各出現箇所における(n-1)回の浮動小数点の積を実行し（F(n-1)回の乗算），その結果の総和を(F-1)回の加算により算出することになる．通常の決定的タグ付与コーパスに対する頻度の計算は，F回のインクリメントで済むことを考えると，非常に大きな計算コストが必要である．また，非常に小さい期待頻度の単語とタグの組の列が多数生成され，これによる計算コストの増大も起こる．このような計算コストの問題は，次に述べる擬似確率的タグ付与コーパスによって近似的に解決される．擬似確率的タグ付与コーパスは，各単語に対して都度発生させた乱数とタグの確率の比較結果から当該単語のタグを唯一に決定することで得られる単語とタグの組の列である．この手続きを複数回繰り返して得られるコーパスに対して頻度を計数することで確率的タグ付与コーパスの期待頻度の近似値が得られる．このときの繰り返し回数を倍率と呼ぶ．擬似確率的タグ付与コーパスは，確率的単語分割コーパスと同様に一種のモンテカルロ法となっており，近似誤差に関しては以下の議論が同様に可能である．モンテカルロ法によるd次元の単位立方体[0,1]^d上の定積分I=_[0,1]^df(x)dxの数値計算法では，単位立方体[0,1]^d上の一様乱数xNを発生させてI_N=_i=1^Nf(x_i)とする．このとき，誤差|I_N-I|は次元dによらずに1/Nに比例する程度の速さで減少することが知られている．擬似確率的タグ付与コーパスにおける単語とタグの組のn-gram頻度の計算はこの特殊な場合である．すなわち，equation:STCFreqの値は，n次元の単位立方体中の矩形の部分領域（i番目の軸方向の長さがp_i,j_i）の体積である．したがって，誤差はnの値によらずに1/FNに比例する程度の速さで減少する．</subsection>
  <section title="評価">提案手法の評価のために，学習コーパスの作成の方法と言語モデルの単位が異なる仮名漢字変換を構築し，テストコーパスに対する変換精度を測定した．この節では，その結果を提示し提案手法の評価を行う．</section>
  <subsection title="実験条件">実験に用いたコーパスの諸元をtable:corpusに掲げる．学習コーパスは，LとRの2種類である．学習コーパスLは，現代日本語書き言葉均衡コーパス2009年モニター版と日常会話の辞書の例文と新聞記事からなり，人手による単語分割と入力記号付与がなされている．学習コーパスRは新聞記事からなり，単語境界や入力記号などの付加情報はない．単語境界や入力記号の推定は，京都テキスト解析ツールキットKyTeaによって行った．テストコーパスTは，学習コーパスRと同じ新聞の別の記事であり，変換精度の計算のために入力記号が付与されてある．</subsection>
  <subsection title="評価基準">仮名漢字変換の評価基準は，各入力文の一括変換結果と正解との最長共通部分列(LCS;longestcommonsubsequence)の文字数に基づく再現率と適合率である（figure:criteria参照）．正解コーパスに含まれる文字数をN_CORとし，一括変換の結果に含まれる文字数をN_SYSとし，これらの最長共通部分列の文字数をN_LCSとすると，再現率はN_LCS/N_CORと定義され，適合率はN_LCS/N_SYSと定義される．figure:criteriaの例では，これらは以下のようになる．これらに加えて，文正解率も計算した．これは，変換結果が文全体に渡って一致している文の割合を表す．</subsection>
  <section title="おわりに">本論文では，単語分割済みコーパスの各単語に対して，確率的にタグを付与することを提案した．具体的なタグとして単語の読みを採用し，ある単語がある読みになる確率を読みが付与されていないコーパスから推定することを実現した．さらに，単語分割済みコーパスから自動読み推定を用いて表記と読みの組を単位とする確率的言語モデルを推定し，仮名漢字変換に用いることを提案した．実験では，単語分割や読み推定が決定的にあるいは確率的に行われているコーパスから，単語を単位とする言語モデルと，単語と読みの組を単位とする言語モデルを推定し，仮名漢字器を構築した．これら複数の仮名漢字器の変換精度を比較した結果，単語と読みの組を言語モデルの単位とし，そのパラメータを確率的に単語分割されかつ確率的に読み付与されたコーパスから推定することで最も高い変換精度となることが分かった．したがって，本論文で提案する単語と読みの組を単位とする言語モデルと，確率的タグ付与コーパスの概念は有用であると結論できる．命題証明</section>
</root>
