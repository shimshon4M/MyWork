    \documentclass[japanese]{jnlp_JS2.0}
\usepackage{jnlpbbl_1.3}
\usepackage[dvips]{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{hangcaption_jnlp}
\usepackage{udline}
\setulminsep{1.2ex}{0.2ex}
\let\underline

\def\dg{}



\def\BT{}                                 

\def\UW{}                                 

\def\Dder{}                            

\def\Ider{}              

\def\Nsum#1{}                      

    \def\Bdma#1{}                

\def\Stri#1#2{} 

\def\Conc#1#2{}        

\def\argmax{}           

\def\argmin{}           

\def\MLE{}    

\def\QED{}             

\def\because{}



\def\MC#1#2#3{}
\def\lineB#1#2{}
\def\lineC#1#2#3{}



\def\ecaption#1{}

\def\tabref#1{}

\def\figref#1{}

\def\equref#1{}

\def\secref#1{}

\def\subref#1{}

\def\appref#1{}


\def\KKC#1{}

\def\Cite#1{}

\def\UU{}
\def\unit{}
\def\unitA1#1{}
\def\pair#1#2{}

\Volume{18}
\Number{2}
\Month{June}
\Year{2011}

\received{2010}{5}{31}
\revised{2010}{11}{7}
\rerevised{2011}{1}{3}
\accepted{2011}{4}{5}

\setcounter{page}{71}

\jtitle{確率的タグ付与コーパスからの言語モデル構築}
\jauthor{森　　信介\affiref{ACCMS} \and 笹田　鉄郎\affiref{KU} \and Neubig Graham\affiref{KU}}
\jabstract{
  確率的言語モデルは，仮名漢字変換や音声認識などに広く用いられている．パラメータは，
  コーパスの既存のツールによる処理結果から推定される．精度の高い読み推定ツールは存在
  しないため，結果として，言語モデルの単位を単語（と品詞の組）とし，仮名漢字モデルを比
  較的小さい読み付与済みコーパスから推定したり，単語の発音の確率を推定せずに一定値と
  している．これは，単語の読みの確率を文脈と独立であると仮定していることになり，この
  仮定に起因する精度低下がある．
  このような問題を解決するために，本論文では，まず，仮名漢字変換において，単語と読み
  の組を単位とする言語モデルを利用することを提案する．単語と読みの組を単位とする言語
  モデルのパラメータは，自動単語分割および自動読み推定の結果から推定される．この処理
  過程で発生する誤りの問題を回避するために，本論文では，確率的タグ付与を提案する．こ
  れらの提案を採用するか否かに応じて複数の仮名漢字変換器を構築し，テストコーパスにお
  ける変換精度を比較した結果，単語と読みの組を言語モデルの単位とし，そのパラメータを
  確率的に単語分割し，さらに確率的読みを付与したコーパスから推定することで最も高い変
  換精度となることが分かった．したがって，本論文で提案する単語と読みの組を単位とする
  言語モデルと，確率的タグ付与コーパスの概念は有用であると結論できる．
}
\jkeywords{確率的言語モデル，仮名漢字変換，確率的単語分割，確率的タグ付与}

\etitle{Language Model Estimation \\
	from a Stochastically Tagged Corpus}
\eauthor{Shinsuke \textsc{Mori}\affiref{ACCMS} \and Tetsuro \textsc{Sasada}\affiref{KU} \and Graham \textsc{Neubig}\affiref{KU}} 
\eabstract{
  In this paper, first we propose a language model based on pairs of word and input
  sequence. Then we propose the notion of a stochastically tagged corpus to cope
  with tag estimation errors. The experimental results we conducted using \textit{kana-kanji} converters showed that our ideas, the language model based on pairs of
  word and input sequence and the notion of a stochastically tagged corpus, both
  improved the accuracy. Therefore we conclude that the language model based on
  pairs and the notion of a stochastically tagged corpus are effective in language
  modeling for the \textit{kana-kanji} conversion task.
}
\ekeywords{stochastic language model, Kana-kanji conversion, stochastic segmentation, 
  stochastic tagging}

\headauthor{森，笹田，Neubig}
\headtitle{確率的タグ付与コーパスからの言語モデル構築}

\affilabel{ACCMS}{京都大学学術情報メディアセンター}{Academic Center for Computing and Media Studies, Kyoto University}
\affilabel{KU}{京都大学情報学研究科}{School of Informatics, Kyoto University}



\begin{document}
\maketitle

\section{はじめに}

確率的言語モデルは，統計的手法による仮名漢字変換
\cite{確率的モデルによる仮名漢字変換} \cite{Google.IME} 
\cite{漢字かなのＴＲＩＧＲＡＭをもちいたかな漢字変換方法}
や音声認識\cite{音声認識システム} \cite{Self-Organized.Language.Modeling.for.Speech.Recognition}などに広く用いられてい
る．確率的言語モデルは，ある単語列がある言語でどの程度自然であるかを出現確率としてモ
デル化する\footnote{単語の定義に関しては様々な立場がある．本論文では，英語などの音声
認識の言語モデル\cite{Self-Organized.Language.Modeling.for.Speech.Recognition}と同様
に，ある言語においてなんらかの方法で認定される文字列と定義する．}．仮名漢字変換におい
ては，確率的言語モデルに加えて，仮名漢字モデルが用いられる．仮名漢字モデルは，入力記
号列と単語の対応を記述する．音声認識では，仮名漢字モデルの代わりに，発音と単語の対応
を記述する発音辞書と音響モデルが用いられる．

確率的言語モデルの推定のためには，システムを適応する分野の大量のテキストが必要で，そ
の文は単語に分割されている必要がある．このため，日本語を対象とする場合には，自動単語
分割や形態素解析が必要であるが，ある程度汎用性のあるツールが公開されており，辞書の追
加などで一般的な分野の言語モデルが構築可能となっている．

仮名漢字モデルや発音辞書における確率の推定には，実際の使用における単語の読みの頻度を
計数する必要がある．しかしながら，読み推定をある程度の汎用性と精度で行うツールは存在
しない\footnote{音声認識では発音が必要で，仮名漢字変換では入力記号列が必要である．こ
れらは微妙に異なる．本論文では，この違いを明確にせず両方を意味する場合に「読み」とい
う用語を用いる．}．したがって，仮名漢字モデルを比較的小さい読み付与済みコーパスから推
定したり\cite{確率的モデルによる仮名漢字変換}，後処理によって，一部の高頻度語にの
み文脈に応じた発音を付与し，他の単語に関しては，各発音の確率を推定せずに一定値として
いる\cite{音声認識システム}のが現状である．



一方で，単語（表記）を言語モデルの単位とすることには弊害がある．例えば，「…するや，…
した」という発声が，「…する夜，…した」と書き起こされることがある．この書き起こし結
果の「夜」は，この文脈では必ず「よる」と発音されるので，「夜」と書き起こすのは不適切
である．この問題は，単語を言語モデルの単位とする仮名漢字変換においても同様に起こる．
これは，単語の読みの確率を文脈と独立であると仮定して推定（あるいは一定値に固定）してい
ることに起因する．

このような問題を解決するために，本論文では，まず，すべての単語を読みで細分化し，単語
と読みの組を単位とする言語モデルを利用することを提案する．仮名漢字変換や音声認識にお
いて，単語と品詞の組を言語モデルの単位とすることや，一部の高頻度語を読みで細分化する
ことが行われている\cite{確率的モデルによる仮名漢字変換} \cite{音声認識システム}．提案
手法は，品詞ではなく読みですべての単語を細分化することとみなすこともできるので，提案
手法は既存手法から容易に類推可能であろう．しかしながら，提案手法を実現するためには，
文脈に応じた正確な読みを様々な分野のテキストに対してある程度の精度で推定できる必要が
ある．このため，提案手法を実現したという報告はない．



単語を単位とする言語モデルのパラメータは，自動単語分割の結果から推定される．自動単語
分割の精度は十分高いとはいえ，一定の割合の誤りは避けられない．この問題による悪影響を
避けるために，確率的単語分割\cite{確率的単語分割コーパスからの単語N-gram確率の計算}と
いう考えが提案されている．この方法では，各文字の間に単語境界が存在する確率を付与し，
その確率を参照して計算される単語$n$-gramの期待頻度を用いて言語モデルを構築する．計算
コストの削減のために，実際には，各文字間に対してその都度発生させた乱数と単語境界確率
の比較結果から単語境界か否かを決定することで得られる擬似確率的単語分割コーパスから従
来法と同様に言語モデルが構築される\cite{擬似確率的単語分割コーパスによる言語モデルの
改良} ．

単語と読みの組を単位とする言語モデルのパラメータは，自動単語分割および自動読み推定の
結果から推定される．自動単語分割と同様に，自動読み推定の精度は十分高いとしても，一定
の割合の誤りは避けられず，言語モデルのパラメータ推定に悪影響がある．これを回避するた
めに，確率的タグ付与とその近似である擬似確率的タグ付与を提案する．

実験では，タグとして入力記号列を採用し，単語と入力記号列の組を単位とする言語モデルを
用いる仮名漢字変換器を構築し，単語を単位とする言語モデルを用いる場合や，決定的な単語
分割や入力記号付与などの既存手法に対する提案手法の優位性を示す．

\section{統計的仮名漢字変換}
\label{section:KKC}

統計的手法による仮名漢字変換\cite{確率的モデルによる仮名漢字変換}は，キーボードから直
接入力可能な入力記号$\cal Y$の正閉包$\Bdma{y} \in {\cal Y}^{+}$を入力として，日本語の
文字$\cal X$の正閉包である変換候補$(\Bdma{x}_{1},\;\Bdma{x}_{2},\;\cdots)$を確率値
$P(\Bdma{x}|\Bdma{y})$の降順に提示する\footnote{一般的な仮名漢字変換フロントエンドと
同様に，ローマ字から（主に）平仮名への変換が行われると仮定している．したがって，入力記
号は${\cal Y} = \{Ａ, Ｂ, \cdots, Ｚ, ０, １, \cdots, ９, ぁ, あ, \cdots, ん, ヴ, ヵ,
ヶ, ー, ＝, ¥, ｀, 「, 」, ；, ’, 、, 。, ！, ＠, ＃, ＄, ％, ＾, ＆, ＊, （, ）,
＿, ＋, ｜, 〜, ｛, ｝, ：, ”, ＜, ＞, ？, ・\}$である．}．文献
\Cite{確率的モデルによる仮名漢字変換}では文を単語列$\Bdma{w} = \Conc{w}{h}$とみなし，
これを単語$w \in {\cal X}^{+}$を単位とする言語モデルと仮名漢字モデルに分解して実現す
る方法を提案している．本節では，まずこれについて説明し，次に単語と読みを組とする言語
モデルによる方法を提案し定式化する．



\subsection{従来手法}

文献\Cite{確率的モデルによる仮名漢字変換}では，変換候補を$P(\Bdma{w}|\Bdma{y})$で順序
付けすることを提案しており，これを次の式が示すように，単語を単位とする言語モデルと仮名
漢字モデルに分解する\footnote{正確には，単語と品詞の組を単位とする言語モデルを提案して
いる．}．
\begin{equation}
  \label{equation:M1}
  P(\Bdma{w}|\Bdma{y}) = \frac{P(\Bdma{y}|\Bdma{w})P(\Bdma{w})}{P(\Bdma{y})}
\end{equation}
ここで，後述するパラメータ推定のために，単語と入力記号列との対応関係は各単語において独
立であるとの仮定をおく．さらに，分母$P(\Bdma{y})$は出力に依らないので，分子だけを以下
のようにモデル化する．
\begin{gather}
 P(\Bdma{y}|\Bdma{w})P(\Bdma{w})
  = \prod_{i=1}^{h} P(\Bdma{y}_{i}|w_{i})P(w_{i}|\Bdma{w}_{i-n+1}^{i-1})
	\nonumber \\
 P(\Bdma{y}_{i}|w_{i})P(w_{i}|\Bdma{w}_{i-n+1}^{i-1})
  = \begin{cases}
   P(w_{i}|\Bdma{w}_{i-n+1}^{i-1})P(\Bdma{y}_{i}|w_{i}) &
	\text{if}\ w_{i} \in {\cal W} \\
   P(\UW|\Bdma{w}_{i-n+1}^{i-1})M_{y,n}(\Bdma{y}_{i}) &
    \text{if}\ w_{i} \not \in {\cal W} 
    \end{cases}
 \label{eqnarray:KKConv1}
\end{gather}
ここで${\cal W}$は確率的言語モデルの語彙を表す．簡単のために，この式の中の$w_{i}\;(i
\leq 0)$は，文頭に対応する特別な記号 \BT であり，これは文末$w_{h+1}$も表す．この式の
$P(w_{i}|\Bdma{w}_{i-n+1} ^{i-1})$と$P(\UW|\Bdma{w}_{i-n+1}^{i-1})$は，語彙に \BT と未
知語記号 \UW を加えた${\cal W}\cup\{\BT,\UW\}$上の$n$-gramモデルである．パラメータは，
単語に分割されたコーパスから以下の式を用いて最尤推定する．
\begin{equation}
  \label{equation:LM}
  P(w_{i}| \Bdma{w}_{i-n+1}^{i-1})
  = \frac{f(\Bdma{w}_{i-n+1}^{i})}{f(\Bdma{w}_{i-n+1}^{i-1})}
\end{equation}
この式中の$f(e)$は，事象$e$のコーパスにおける頻度を表す．\figref{figure:LMA}が示すよ
うに，この学習コーパスには自動単語分割の結果であることが多いが，自動単語分割器の学習
に用いたタグ付きコーパスが利用可能な場合にはこれを加えることもある．

\begin{figure}[tb]
  \begin{center}
  \includegraphics{18-2ia1f1.eps}
  \end{center}
  \caption{単語を単位とする言語モデルの作成の手順}
  \label{figure:LMA}
\end{figure}

\equref{eqnarray:KKConv1}の$P(\Bdma{y}_{i}|w_{i})$は，単語単位の仮名漢字モデルであり，
パラメータは，単語に分割されかつ各単語に入力記号列が付与されたコーパスから以下の式を
用いて最尤推定する．
\begin{equation}
  \label{equation:PM}
  P(\Bdma{y}_{i}|w_{i}) = \frac{f(\Bdma{y}_{i},\;w_{i})}{f(w_{i})}
\end{equation}
\equref{eqnarray:KKConv1}から分かるように，単語単位の仮名漢字モデルでは，単語と入力記
号列との対応関係が各単語において独立であると仮定している．この仮定により，比較的少量
の入力記号列付与済みコーパスからある程度信頼できるパラメータを推定することができる．

\equref{eqnarray:KKConv1}の$M_{y,n}(\Bdma{y}_{i})$は，未知語モデルであり，入力記号の
集合に単語の両端を表す記号を加えた${\cal Y} \cup \{\BT\}$上の$n$-gramモデルで実現され
る\footnote{文献\Cite{確率的モデルによる仮名漢字変換}によれば，あるテストコーパスにお
いて未知語を構成する文字の33.0\%が片仮名であった．入力記号集合は主に平仮名からなるが，
この先行研究と同様に，出力においてはこれらを片仮名とする．}．このパラメータは低頻度の
単語に対応する入力記号列から推定する．



\subsection{提案手法}

本論文では，言語モデルの単位を単語と入力記号列の組$u = \unit$とすることを提案する．そ
の上で，以下の式のように$P(\Bdma{w}|\Bdma{y})$をモデル化する．
\[
  P(\Bdma{w}|\Bdma{y}) = \frac{P(\Bdma{w},\Bdma{y})}{P(\Bdma{y})}
                       = \frac{P(\Bdma{u})}{P(\Bdma{y})}
\]
分母$P(\Bdma{y})$は出力に依らないので，分子だけを以下のようにモデル化する．
\begin{gather}
  P(\Bdma{u}) = \prod_{i=1}^{h} P(u_{i}|\Bdma{u}_{i-n+1}^{i-1})
	\nonumber\\
  P(u_{i}|\Bdma{u}_{i-n+1}^{i-1})
   = \begin{cases}
     P(u_{i}|\Bdma{u}_{i-n+1}^{i-1})  & \text{if} \ u_{i} \in {\cal U} \\
     P(\UU|\Bdma{u}_{i-n+1}^{i-1})M_{u,n}(u_{i})
                                      & \text{if} \ u_{i} \not \in {\cal U}
     \end{cases}
  \label{eqnarray:KKConv2}
\end{gather}
ここで${\cal U}$は言語モデルの語彙（単語と入力記号列の組の集合）を表す．この式の中の
$u_{i}\;(i \leq 0)$と$u_{h+1}$は，単語を単位とする場合と同様に，文頭と文末に対応する
記号 \BT である．また \UU は未知の組を表す記号である．

\equref{eqnarray:KKConv2}の$M_{u,n}(u) = M_{u,n}(\unit)$は未知語モデルである．従来手
法と同様に，大きな学習コーパスを用いれば実際の使用における未知語率は極めて低く，また
未知語に対する正確な仮名漢字変換は困難であると考えて，アルファベット${\cal U}$上の未
知語モデルの代わりにアルファベット${\cal Y}$上の未知語モデル$M_{y,n}(\Bdma{y})$を用い
ることとする．これは，\equref{eqnarray:KKConv1}と共通である．以上から，提案手法の仮名
漢字変換は，以下の式のようになる．
\begin{equation}
  \label{eqnarray:KKConv3}
  P(u_{i}|\Bdma{u}_{i-n+1}^{i-1})
   = \begin{cases}
     P(u_{i}|\Bdma{u}_{i-n+1}^{i-1}) & \text{if} \ u_{i} \in {\cal U} \\
     P(\UU|\Bdma{u}_{i-n+1}^{i-1})M_{y,n}(\Bdma{y}_{i})
                                     & \text{if} \ u_{i} \not \in {\cal U}
     \end{cases}
\end{equation}
ここで$\Bdma{y}_{i} = y(u_{i})$は$u_{i} = \unitA1{i}$の入力記号列である．なお，
$M_{u, n}(u)$の代わりに$M_{y,n}(\Bdma{y})$を用いることは以下の式で与えられる近似であ
り，${\cal Y} \subsetneq {\cal X}$であるので，入力記号列のみからなる文字列を未知語と
して出力することになる．
\[
  M_{u,n}(u) = M_{u,n}(\unit) \approx
  \begin{cases}
    M_{y,n}(\Bdma{y}) & \text{if} \ w \in {\cal Y}^{+} \\
    0                 & \text{if} \ w \not \in {\cal Y}^{+}
  \end{cases}
\]
この式の$M_{y,n}(\Bdma{y})$のパラメータは，学習コーパスにおける語彙${\cal U}$に含まれ
ない表記と入力記号列の組の入力記号列から推定する．これは，学習コーパスにおける未知の
組の単語を入力記号列に置き換えた結果から$M_{u,n}(u)$を推定しているのと同じである．

\begin{figure}[t]
\begin{center}
\includegraphics{18-2ia1f2.eps}
\end{center}
\caption{単語と読みの組を単位とする言語モデルの作成の手順}
\label{figure:LMB}
\end{figure}

\equref{eqnarray:KKConv3}の$P(u_{i}|\Bdma{u}_{i-n+1}^{i-1})$と$P(\UU|\Bdma{u}_{i-n+1}
^{i-1})$は，語彙に\BT と\UU を加えた${\cal U}\cup\{\BT,\UU\}$上の$n$-gramモデルであ
る．パラメータは，単語に分割されかつ入力記号列が付与されたコーパスから以下の式を用い
て最尤推定する．
\begin{equation}
  \label{equation:UM}
  P(u_{i}| \Bdma{u}_{i-n+1}^{i-1})
  = \frac{f(\Bdma{u}_{i-n+1}^{i})}{f(\Bdma{u}_{i-n+1}^{i-1})}
\end{equation}
\figref{figure:LMB}が示すように，この学習コーパスには自動単語分割・読み付与の結果を用
いることができる．さらに自動単語分割器や読み付与の学習に用いたタグ付きコーパスが利用
可能な場合にはこれを加えることもできる（\figref{figure:LMB}の点線）．

単語を単位とする従来手法と同程度の信頼性となるパラメータを推定するために，従来手法に
おいてパラメータ推定に用いられる単語に分割されたコーパスと同程度の量の単語に分割され
かつ入力記号列が付与されたコーパスが必要である．換言すれば，自動単語分割と同程度の精
度で入力記号列を推定するシステムが必要である．

これまで，各単語に対する入力記号列（読み）をその文脈に応じて十分な精度で推定する研究や
フリーウェアがなかったために，提案手法は現実的ではなかったと思われる\footnote{研究と
しては文献\Cite{N-gramモデルを用いた音声合成のための読み及びアクセントの同時推定}があ
るが，公開されていない．また，読み推定ツールとして KAKASI （http://kakasi.namazu.org/ , 2010年5月）があるが，様々な分野において十分な精度とはいえ
ない．}．次節では，この方法を説明し，さらに入力記号列を確率的に付与することで，入力記
号列の推定誤りの影響を緩和する方法を提案する．

\section{仮名漢字変換のための言語資源とその処理}
\label{section:LRS}

仮名漢字変換や音声認識のための言語モデルは，単語分割済みコーパスと生コーパスの自動単
語分割結果から構築される．この節では，まずこの過程を概説する．次に，前節で提案したモ
デルのパラメータをより正確に推定するために，単語に入力記号列や発音などのタグを確率的
に付与することを提案する．



\subsection{コーパス}

仮名漢字変換や音声認識のための単語を単位とする言語モデル作成においては，これらを適用
する分野のコーパスが必須である．一般に，コーパスには単語境界情報がないので，自動単語
分割器\cite{点推定と能動学習を用いた自動単語分割器の分野適応}や形態素解析器
\cite{日本語形態素解析システムJUMAN使用説明書.version.1.0,
確率的形態素解析,
統計的言語モデルとN-best探索を用いた日本語形態素解析法,
形態素クラスタリングによる形態素解析精度の向上,
Conditional.Random.Fields.を用いた日本語形態素解析}を用いて文を言語モデルの単位に分割
し，その結果に対して単語$n$-gram頻度を計数する（\figref{figure:LMA}参照）\inhibitglue\footnote{形態
素解析を利用する場合は，品詞も付与されるので，単語と品詞の組を単位とすることもあるが，
仮名漢字変換や音声認識の出力に品詞は不要なので，実質的には品詞の異なる同音異義語の識
別程度の効果しかない．さらに，茶筌などを利用すると「読み」も付与されるが，これらは文
脈に依存しないので実質的には付与していないのと同じである．}．なお，これら自動単語分割
器や形態素解析器などの自然言語処理システムは，単語分割済みあるいは品詞付与済みのコー
パスから学習することが多い．その場合には，これら自然言語処理システムの学習コーパスも
言語モデルの学習コーパスに加えることができる（\figref{figure:LMA}の点線）が，実際には，
これら自然言語処理システムはツールとして配布され，辞書追加程度の適応しかなされず，自
然言語処理システムの学習コーパスが言語モデルの学習に利用されることは少ない．



\subsection{形態素解析と自動単語分割}

形態素解析は，日本語の自然言語処理の第一段階として研究され，ルールに基づく方法が一定
の成果を上げた\cite{日本語形態素解析システムJUMAN使用説明書.version.1.0}．同じ頃，統
計的手法\cite{確率的形態素解析,統計的言語モデルとN-best探索を用いた日本語形態素解析
法}が提案され，アルゴリズムとデータの分離に成功した．統計的手法は，ルールに基づく方法
と同等かそれ以上の精度を達成しており，現在では主流になっている．さらに，フリーソフト
として公開され，容易に利用可能となっている．

このような背景から，仮名漢字変換や音声認識のための言語モデル作成のために，形態素解析
が用いられている．結果的に，単語（表記）と品詞の組を言語モデルの単位とすることが多い
\footnote{音声認識\cite{音声認識システム}ではすべての可能な読みも付加しているが，文脈
に応じた読みは付与されず，同音異義語の峻別には用いられていない．}．しかしながら，仮名
統計的漢字変換や音声認識等の実現には品詞情報は不要であり，形態素解析器の学習コーパス
作成のコストを不必要に増大させるのみである．また，英語等の単語間に空白を置く言語の音
声認識においては，言語モデルの単位として当然単語が用いられる．日本語においても単語を
言語モデルの単位とする音声認識の取り組みがあり，十分な認識精度を報告している\cite{単
語を認識単位とした日本語の大語彙連続音声認識}．以上の考察から，本論文では，単語と品詞
の組を言語モデルの単位とする手法は，単語を単位とする手法に含まれるとして，以下の議論
を展開する．

言語モデルの構築においては，適応対象の分野の大量のテキストに対する統計をとることが非
常に有用である．このため，形態素解析や自動単語分割等の自動処理が必須であるが，自動処
理の結果は一定量の誤りを含む．この単語分割誤りによる悪影響を緩和するために，確率的に
単語に分割することが提案されている\cite{確率的単語分割コーパスからの単語N-gram確率の
計算}．この手法では，自動単語分割器によって各文字の間に単語境界がある確率を付与し，そ
の確率を参照して計算される単語$n$-gramの期待頻度を用いて言語モデルが構築される．実用
上は，モンテカルロシミュレーションのように，各文字間に対して都度発生させた乱数と単語
境界確率の比較結果から単語境界か否かを決定することで得られる擬似確率的単語分割コーパ
スから従来法と同様に言語モデルが構築される\cite{擬似確率的単語分割コーパスによる言語
モデルの改良}．



\subsection{自動読み推定}

前節で，仮名漢字変換のための言語モデルの単位として単語と入力記号列の組を用いることを
提案した．この考え自体は特に新規ではなく，以前から存在している．実際，音声認識におい
て，数詞のあとの「本」など一部の高頻度語に文脈に応じた発音を付与する後処理が行われて
いる\cite{音声認識システム}．また，発音レベルでの書き起こしが得られる場合に，単語と発
音の対応を推定し，単語と品詞と発音の組を単位をとする言語モデルを構築する研究もある
\cite{講演音声認識のための音響・言語モデルの検討}．しかしながら，この考えを一般的な場
合において実現するためには，高精度の自動読み推定システムが必要である．前述の形態素解
析の研究とその成果であるフリーソフトにおいては，読みの推定は軽視されており，文脈に応
じた読みを高い精度で出力する研究やフリーソフトはなかった．このため，単語と入力記号列
の組や単語と発音の組を単位とする言語モデルは一般的な意味で実現されていなかった．

前節で提案した単語と入力記号列の組を単位とする言語モデルの構築においては，コーパスを
単語に分割し，文脈に応じた読みを付与することができるKyTea（京都テキスト解析ツールキッ
ト）\cite{仮名漢字変換ログの活用による言語処理精度の自動向上}を用いて適応対象の分野の
テキストを自動的に単語と入力記号列の組の列に変換する（\figref{figure:LMB}参照）．その結
果から\equref{equation:UM}を用いて単語と入力記号列の組の$n$-gram確率を推定する．
KyTeaの詳細は\appref{appe:kytea}に記述した．



\subsection{確率的タグ付与}

自動読み推定の結果は，形態素解析や自動単語分割等の自動処理の場合と同様に，一定量の誤
りを含む．学習コーパスに含まれる読み推定誤りは，言語モデルや仮名漢字モデル，あるいは
発音辞書に悪影響を及ぼす．特に，ある単語に対して至る所で同じ誤った読みを付与する場合
には，非常に重大な問題となる．この問題を回避するために，確率的単語分割と同様に，単語
に対する入力記号列付与や発音付与を確率的に行うことを提案する．すなわち，読み推定にお
いては，ある単語に対する読みを決定的に推定するのではなく，可能な読みとその確率値を返
すようにする．より一般的には，単語に対する読みや品詞などのタグ付与を，ある基準で最適
となる唯一のタグを出力する処理ではなく，タグ$t$と確率値$p$の組の列$(\pair{t_{1}}{p
_{1}},\pair{t_{2}}{p_{2}},\cdots)$を出力する処理へと一般化する．この際，タグの確率値
は，周辺の他の単語のタグと独立であるとの仮定をおく．この結果得られるコーパスを確率的
タグ付与コーパスと呼ぶ．確率的タグ付与コーパスの文$\Conc{w}{h}$は，以下のように，各単
語に可能なタグと確率値の組の列が付与されている．
\begin{gather*}
 \langle w_{1},(\pair{t_{1,1}}{p_{1,1}},\pair{t_{1,2}}{p_{1,2}},
    \cdots,\pair{t_{1,k_1}}{p_{1,k_1}}) \rangle \\
 \langle w_{2},(\pair{t_{2,1}}{p_{2,1}},\pair{t_{2,2}}{p_{2,2}},
    \cdots,\pair{t_{2,k_2}}{p_{2,k_2}}) \rangle \\
 \vdots \\
 \langle w_{h},(\pair{t_{h,1}}{p_{h,1}},\pair{t_{h,2}}{p_{h,2}},
    \cdots,\pair{t_{h,k_h}}{p_{h,k_h}}) \rangle
\end{gather*}
ここで，$t_{i,j}$と$p_{i,j}$はそれぞれ，$i$番目の単語の$j$番目のタグとその確率を表す．
このような確率的タグ付与コーパスにおける単語とタグの組の$n$-gramの1回の出現あたりの頻
度$f_{1}(\Bdma{u})$は，以下の式で計算される期待頻度として定義される．
\begin{equation}
  \label{equation:STCFreq}
  f_{1}(\Bdma{u})
  = f_{1}(\pair{w_{1}}{t_{1,j_1}}\pair{w_{2}}{t_{2,j_2}}\cdots\pair{w_{n}}{t_{n,j_n}})
  = \prod_{i=1}^{n} p_{i,j_i}
\end{equation}
この値をコーパスにおけるすべての出現箇所に渡って合計した結果が
\pagebreak
単語とタグの組の列
$\Bdma{u}$の期待頻度である．単語とタグの組の$n$-gram確率は，この期待頻度の相対値とし
て定義される．仮名漢字変換のための言語モデル構築では，タグとして単語に対応する入力記
号列を用いる．

確率的入力記号列付与のためのモデルは，単語ごとに入力記号列が付与されたコーパスからロ
ジスティック回帰などの点予測器を推定しておくことで実現できる．



\subsection{擬似確率的タグ付与}
\label{subsec:pseudo}

確率的単語分割の場合と同様に，確率的タグ付与コーパスに対する単語とタグの組の列の頻度
の計算は，決定的タグ付与コーパスに対する頻度計算と比べてはるかに多い計算を要する．実
際，対象となる組の列としての頻度が$F$回とすると，\equref{equation:STCFreq}による期待
頻度の計算は，各出現箇所における$(n-1)$回の浮動小数点の積を実行し（$F(n-1)$回の乗算），
その結果の総和を$(F-1)$回の加算により算出することになる．通常の決定的タグ付与コーパス
に対する頻度の計算は，$F$回のインクリメントで済むことを考えると，非常に大きな計算コス
トが必要である．また，非常に小さい期待頻度の単語とタグの組の列が多数生成され，これに
よる計算コストの増大も起こる．このような計算コストの問題は，次に述べる擬似確率的タグ
付与コーパスによって近似的に解決される．

擬似確率的タグ付与コーパスは，各単語に対して都度発生させた乱数とタグの確率の比較結果
から当該単語のタグを唯一に決定することで得られる単語とタグの組の列である．この手続き
を複数回繰り返して得られるコーパスに対して頻度を計数することで確率的タグ付与コーパス
の期待頻度の近似値が得られる．このときの繰り返し回数を倍率と呼ぶ．

擬似確率的タグ付与コーパスは，確率的単語分割コーパス\cite{擬似確率的単語分割コーパス
による言語モデルの改良}と同様に一種のモンテカルロ法となっており，近似誤差に関しては以
下の議論が同様に可能である．モンテカルロ法による$d$次元の単位立方体$[0,1]^{d}$上の定
積分$I = \int_{[0,1]^{d}}f(x)dx$の数値計算法では，単位立方体$[0,1]^{d}$上の一様乱数
$\Stri{x}{N}$を発生させて$I_{N} = \sum_{i=1} ^{N} f(x_{i})$とする．このとき，誤差
$|I_{N}-I|$は次元$d$によらずに$1/\sqrt{N}$に比例する程度の速さで減少することが知られ
ている．擬似確率的タグ付与コーパスにおける単語とタグの組の$n$-gram頻度の計算はこの特
殊な場合である．すなわち，\equref{equation:STCFreq}の値は，$n$次元の単位立方体中の矩
形の部分領域（$i$番目の軸方向の長さが$p_{i,j_{i}}$）の体積である．したがって，誤差は
$n$の値によらずに$1/\sqrt{FN}$に比例する程度の速さで減少する．


\section{評価}
\label{section:評価}

提案手法の評価のために，学習コーパスの作成の方法と言語モデルの単位が異なる仮名漢字変換
を構築し，テストコーパスに対する変換精度を測定した．この節では，その結果を提示し提案手
法の評価を行う．



\subsection{実験条件}

実験に用いたコーパスの諸元を\tabref{table:corpus}に掲げる．学習コーパスは，$L$と$R$の
2種類である．学習コーパス$L$は，現代日本語書き言葉均衡コーパス2009年モニター版
\cite{Balanced.Corpus.of.Contemporary.Written.Japanese}と日常会話の辞書の例文と新聞記
事からなり，人手による単語分割と入力記号付与がなされている．学習コーパス$R$は新聞記事
からなり，単語境界や入力記号などの付加情報はない．単語境界や入力記号の推定は，京都テ
キスト解析ツールキットKyTea \cite{点推定と能動学習を用いた自動単語分割器の分野適応}
\footnote{Version 0.1.0, http://www.phontron.com/kytea/（2010年10月）．}によって行っ
た．テストコーパス$T$は，学習コーパス$R$と同じ新聞の別の記事であり，変換精度の計算の
ために入力記号が付与されてある．

\begin{table}[tb]
\caption{コーパス}
\input{01table01.txt}
  \label{table:corpus}
\vspace{1\baselineskip}
\end{table}


\subsection{評価基準}

\begin{figure}[tb]
\begin{center}
\includegraphics{18-2ia1f3.eps}
\end{center}
  \caption{評価基準}
  \label{figure:criteria}
\end{figure}

仮名漢字変換の評価基準は，各入力文の一括変換結果と正解との最長共通部分列(LCS;
longest common subsequence) \cite{文字列中のパターン照合のためのアルゴリズム}の文字数
に基づく再現率と適合率である（\figref{figure:criteria}参照）．正解コーパスに含まれる文
字数を$N_{COR}$とし，一括変換の結果に含まれる文字数を$N_{SYS}$とし，これらの最長共通
部分列の文字数を$N_{LCS}$とすると，再現率は$N_{LCS}/N_{COR}$と定義され，適合率は
$N_{LCS}/N_{SYS}$と定義される．\figref{figure:criteria}の例では，これらは以下のように
なる．
\begin{description}
\item[\ 再現率：] $N_{LCS}/N_{COR} =  5/8$
\item[\ 適合率：] $N_{LCS}/N_{SYS} = 5/11$
\end{description}
これらに加えて，文正解率も計算した．これは，変換結果が文全体に渡って一致している文の
割合を表す．



\subsection{評価}

学習コーパスの作成の方法と言語モデルの単位による仮名漢字変換精度の差を調べるために，
以下の3通りの方法で作成された学習コーパスのそれぞれから，単語を言語モデルの単位とする
仮名漢字変換（\equref{eqnarray:KKConv1}参照）と単語と入力記号列の組を言語モデルの単位と
する仮名漢字変換（\equref{eqnarray:KKConv3}参照）を作成した．言語モデルはすべて2-gramモデ
ルである\footnote{音声認識で一般的な3-gramモデルを用いなかったのは，仮名漢字変換の先
行研究\cite{確率的モデルによる仮名漢字変換}とその実用化の例\cite{Google.IME}が2-gram
モデルを用いていること，仮名漢字変換での入力記号列は比較的短い傾向があり（第1著者の場
合約2.2単語分）長い履歴が実際にはほとんど有効ではないこと，3-gramモデルは必要となる記
憶域が増大し処理速度が低下するなど実用化に向かないことである．}．
\par
\KKC{DD}:　決定的に単語分割し，決定的に入力記号列を付与する．\par
\KKC{DS}:　決定的に単語分割し，確率的に入力記号列を付与する．\par
\KKC{SS}:　確率的に単語分割し，確率的に入力記号列を付与する．\par
\noindent
ここで，「確率的」は擬似確率的単語分割および疑似確率的入力記号付与を意味し，全て倍率
は1とした．

文献\cite{擬似確率的単語分割コーパスによる言語モデルの改良}では，1,890,041文字の生コー
パスに対して1〜256の倍率による擬似確率的単語分割コーパスを評価している．その結果，倍
率が8〜32程度で確率的単語分割コーパスと同程度の性能となっている．前後数単語の単語分割
の可能性は16〜32通り程度（その出現にも偏りがある）なので高頻度の単語（候補）の高頻度の文
脈はある程度大きいコーパスであれば，倍率が1の擬似確率的単語分割コーパスでも十分に真の
分布に近い推定値が得られると考えられる．本実験での生コーパスの文字数は，この文献での
実験の約27.9倍であり，ある程度の頻度の組の列$\Bdma{u}$の出現頻度
（\subref{subsec:pseudo}の$F$）は約27.9倍となっていることが期待される．したがって，倍率
（第3.5項の$N$）が1であっても，上述の文献における実験での倍率27.9に相当し，確率的タグ付与
コーパス($N \rightarrow \infty$)に近い性能が期待される．

3つの学習コーパスの作成の方法と2つの言語モデルの単位のすべての組み合わせによる仮名漢
字変換の精度を\tabref{table:result}に示す．表中のIDの最初の2文字は学習コーパスの作成
の方法を表し，次の1文字は言語モデルの単位を表す．文献\Cite{確率的モデルによる仮名漢字
変換} は，単語と品詞の組を言語モデルの単位とし，生コーパスの形態素解析結果を学習コー
パスに利用していないが，生コーパスの利用による精度向上は広く一般に知られているので，
単語を言語モデルの単位とし，生コーパスの決定的な単語分割と入力記号列付与結果を利用す
る\KKC{DDw}が既存手法に対応するとし，これをベースラインとする．

\begin{table}[t]
\caption{仮名漢字変換の精度 2-gram}
\input{01table02.txt}
\label{table:result}
\end{table}

まず，\tabref{table:result}中の\KKC{DDw}と\KKC{DSw}と\KKC{SSw}の比較についてである．
これらは，すべて単語を言語モデルの単位とする．自動分割と入力記号付与の両方を決定的に
行った結果から言語モデルを推定するベースライン\KKC{DDw}に対して，入力記号付与を確率的
に行う\KKC{DSw}はより高い変換精度となっている．これにより，入力記号付与を確率的に行う
ことが有効であることが分かる．\KKC{DDw}と\KKC{DSw}の言語モデルは共通で，違いは仮名漢
字モデルのみである．このことから，入力記号付与を確率的に行うことで，仮名漢字モデルが
より適切に推定できることが分かる．さらに，単語分割も確率的に行う\KKC{SSw}の精度は，入
力記号付与のみを確率的に行う\KKC{DSw}よりも高くなっている．このことから，確率的入力記
号付与は，確率的単語分割\cite{擬似確率的単語分割コーパスによる言語モデルの改良}と協調
して精度向上に寄与することがわかる．

次に，\tabref{table:result}中の\KKC{DDu}と\KKC{DSu}と\KKC{SSu}の比較についてである．こ
れらは，すべて単語と入力記号列の組を言語モデルの単位とする．この場合も，確率的に入力記
号を付与することで精度が向上し，単語分割も確率的に行うことでさらに精度が向上しているこ
とが分かる．

さらに，言語モデルの単位の差異についてである．\tabref{table:result}から，\KKC{DDw}と
\KKC{DDu}，\KKC{DSw}と\KKC{DSu}，\KKC{SSw}と\KKC{SSu}のいずれの組の比較においても，言
語モデルの単位を単語から単語と入力記号列の組に変更することで変換精度が向上しているこ
とが分かる．

\begin{figure}[tb]
\begin{center}
\includegraphics{18-2ia1f4.eps}
\end{center}
  \caption{疑似確率的単語分割と疑似タグ付与の合計の倍率($m^{2}$)と仮名漢字変換精度の関係}
  \label{figure:graph}
\end{figure}

最後に，提案手法\KKC{SSu}における倍率と精度の関係についてである．これを調べるために，
$m$倍の疑似確率的単語分割の各結果に対する$m$倍の疑似確率的タグ付与の結果（合計$m^2$,
$m \in \{1,\,2,\,4\}$）を用いた場合の精度を計算した．\figref{figure:graph}は，倍率と精
度の関係である（$m = 1$は，\tabref{table:result}の\KKC{SSu}と同じ）．この結果から，倍率
を上げることで，少しではあるが精度が向上することがわかる．一方で，それぞれの場合の語
彙（表記と読みの組）のサイズは順に，123,078組，181,800組，295,801組であり，単語分割とタ
グ付与を決定的に行う\KKC{DDu}の99,210組との差は，倍率が大きくなるに従って非常に大きく
なる．\figref{figure:graph}から精度の差は大きくないので，倍率は$1^2$か$2^2$程度が現実
的であろう．

以上のことから，仮名漢字変換の言語モデルを単語から単語と入力記号列の組とし，入力記号
を確率的に付与したコーパスからこれを推定することが有効であると言える．さらに，確率的
単語分割と組み合わせることでさらなる精度向上が実現できると結論できる．

\section{おわりに}

本論文では，単語分割済みコーパスの各単語に対して，確率的にタグを付与することを提案し
た．具体的なタグとして単語の読みを採用し，ある単語がある読みになる確率を読みが付与さ
れていないコーパスから推定することを実現した．さらに，単語分割済みコーパスから自動読
み推定を用いて表記と読みの組を単位とする確率的言語モデルを推定し，仮名漢字変換に用い
ることを提案した．

実験では，単語分割や読み推定が決定的にあるいは確率的に行われているコーパスから，単語
を単位とする言語モデルと，単語と読みの組を単位とする言語モデルを推定し，仮名漢字器を
構築した．これら複数の仮名漢字器の変換精度を比較した結果，単語と読みの組を言語モデル
の単位とし，そのパラメータを確率的に単語分割されかつ確率的に読み付与されたコーパスか
ら推定することで最も高い変換精度となることが分かった．したがって，本論文で提案する単
語と読みの組を単位とする言語モデルと，確率的タグ付与コーパスの概念は有用であると結論
できる．

\appendix

\newtheorem{命題}{}
\newtheorem{証明}{}


\section{自動読み推定}
\label{appe:kytea}

本論文で用いた自動読み推定\cite{仮名漢字変換ログの活用による言語処理精度の自動向上}
は，コーパスに基づく方法であり，単語に分割された文を入力とし，単語毎に独立に以下の分
類に基づいて読み推定が行われる．
\begin{description}
\item[Q$_1$] 学習コーパスに出現しているか
\item[\ ] はい
  \begin{description}
  \item[Q$_2$] 読みが唯一か複数か
  \item[\ ] 複数 $\Rightarrow$ ロジスティック回帰
    \cite{LIBLINEAR:.A.Library.for.Large.Linear.Classication}を用いて読みを選択
  \item[\ ] 唯一 $\Rightarrow$ その読みを選択
  \end{description}
\item[\ ] いいえ
  \begin{description}
  \item[Q$_2^{'}$] 辞書に入っているか
  \item[\ ] はい $\Rightarrow$ 最初の項目の読みを選択
  \item[\ ] いいえ $\Rightarrow$ 文字と読みの2-gramモデルによる最尤の読みを選択
  \end{description}
\end{description}
複数の読みが可能でその確率が必要な場合には，ロジスティック回帰の出力確率や文字と読み
の2-gramモデルによる生成確率を正規化した値を利用する．

分類器の学習に用いたコーパスは，現代日本語書き言葉均衡コーパス
\cite{Balanced.Corpus.of.Contemporary.Written.Japanese}であり，辞書はUniDic
\cite{コーパス日本語学のための言語資源：形態素解析用電子化辞書の開発とその応用}
である．

学習コーパスとして33,147文（899,025単語，1,292,249文字）を用い，テストコーパスとして同
一分野の3,681文（98,634単語，141,655文字）を用いた場合の読み推定精度を測定した．評価基
準は，入力記号単位の適合率と再現率である．その結果，一般的な手法である単語と読みを組
とする3-gramモデル
\Cite{N-gramモデルを用いた音声合成のための読み及びアクセントの同時推定}の適合率と再現
率はそれぞれ99.07\%と99.12\%であり，本論文で用いた自動読み推定の適合率と再現率はそれ
ぞれ99.19\%と99.26\%であった．この結果から，本論文で用いた自動読み手法は，既存手法と
同程度の精度となっていることがわかる．


\acknowledgment

本研究の一部は，科学研究費補助金・若手 A（課題番号：08090047）により行われた．

    \bibliographystyle{jnlpbbl_1.5}
\begin{thebibliography}{}

\bibitem[\protect\BCAY{Aho}{Aho}{1990}]{文字列中のパターン照合のためのアルゴリ
ズム}
Aho, A.~V. \BBOP 1990\BBCP.
\newblock \JBOQ 文字列中のパターン照合のためのアルゴリズム\JBCQ\
\newblock \Jem{コンピュータ基礎理論ハンドブック}. I: 形式的モデルと意味論\JVOL,
  \mbox{\BPGS\ 263--304}. Elsevier Science Publishers.

\bibitem[\protect\BCAY{伝\JBA 小木曽\JBA 小椋\JBA 山田\JBA 峯松\JBA 内元\JBA
  小磯}{伝 \Jetal
  }{2007}]{コーパス日本語学のための言語資源：形態素解析用電子化辞書の開発とそ
の応用}
伝康晴\JBA 小木曽智信\JBA 小椋秀樹\JBA 山田篤\JBA 峯松信明\JBA 内元清貴\JBA
  小磯花絵 \BBOP 2007\BBCP.
\newblock コーパス日本語学のための言語資源：形態素解析用電子化辞書の開発とその応用. 
\newblock \Jem{日本語科学}, {\Bbf 22}, \mbox{\BPGS\ 101--122}.

\bibitem[\protect\BCAY{Fan, Chang, Hsieh, Wang, \BBA\ Lin}{Fan
  et~al.}{2008}]{LIBLINEAR:.A.Library.for.Large.Linear.Classication}
Fan, R.-E., Chang, K.-W., Hsieh, C.-J., Wang, X.-R., \BBA\ Lin, C.-J. \BBOP
  2008\BBCP.
\newblock \BBOQ LIBLINEAR: A Library for Large Linear Classication.'' 
\newblock {\Bem Journal of Machine Learning Research}, {\Bbf 9}, \mbox{\BPGS\
  1871--1874}.

\bibitem[\protect\BCAY{Google}{Google}{2010}]{Google.IME}
Google \BBOP 2010\BBCP.
\newblock
\newblock \BBOQ Google IME.\BBCQ
\newblock http://www.google.com/intl/ja/ime/ (2010年10月).

\bibitem[\protect\BCAY{Jelinek}{Jelinek}{1985}]{Self-Organized.Language.Modeli
ng.for.Speech.Recognition}
Jelinek, F. \BBOP 1985\BBCP.
\newblock \BBOQ Self-Organized Language Modeling for Speech Recognition.\BBCQ\
\newblock Tech. rep., IBM T. J. Watson Research Center.

\bibitem[\protect\BCAY{工藤\JBA 山本\JBA 松本}{工藤 \Jetal
  }{2004}]{Conditional.Random.Fields.を用いた日本語形態素解析}
工藤拓\JBA 山本薫\JBA 松本裕治 \BBOP 2004\BBCP.
\newblock Conditional Random Fields を用いた日本語形態素解析. 
\newblock \Jem{情報処理学会研究報告}, NL161\JVOL.

\bibitem[\protect\BCAY{Maekawa}{Maekawa}{2008}]{Balanced.Corpus.of.Contemporar
y.Written.Japanese}
Maekawa, K. \BBOP 2008\BBCP.
\newblock \BBOQ Balanced Corpus of Contemporary Written Japanese.\BBCQ\
\newblock In {\Bem Proceedings of the 6th Workshop on Asian Language
  Resources}, \mbox{\BPGS\ 101--102}.

\bibitem[\protect\BCAY{丸山\JBA 荻野\JBA 渡辺}{丸山 \Jetal
  }{1991}]{確率的形態素解析}
丸山宏\JBA 荻野紫穂\JBA 渡辺日出雄 \BBOP 1991\BBCP.
\newblock 確率的形態素解析. 
\newblock \Jem{日本ソフトウェア科学会第8回大会論文集}, \mbox{\BPGS\ 177--180}.

\bibitem[\protect\BCAY{松本\JBA 黒橋\JBA 宇津呂\JBA 妙木\JBA 長尾}{松本 \Jetal
  }{1993}]{日本語形態素解析システムJUMAN使用説明書.version.1.0}
松本裕治\JBA 黒橋禎夫\JBA 宇津呂武仁\JBA 妙木裕\JBA 長尾眞 \BBOP 1993\BBCP.
\newblock \Jem{日本語形態素解析システムJUMAN使用説明書 version 1.0}.
\newblock 京都大学工学部長尾研究室.

\bibitem[\protect\BCAY{森\JBA 長尾}{森\JBA
  長尾}{1998}]{形態素クラスタリングによる形態素解析精度の向上}
森信介\JBA 長尾眞 \BBOP 1998\BBCP.
\newblock 形態素クラスタリングによる形態素解析精度の向上. 
\newblock \Jem{自然言語処理}, {\Bbf 5}  (2), \mbox{\BPGS\ 75--103}.

\bibitem[\protect\BCAY{森\JBA Neubig}{森\JBA
  Neubig}{2010}]{仮名漢字変換ログの活用による言語処理精度の自動向上}
森信介\JBA Neubig Graham  \BBOP 2010\BBCP.
\newblock 仮名漢字変換ログの活用による言語処理精度の自動向上. 
\newblock \Jem{言語処理学会年次大会}.

\bibitem[\protect\BCAY{森\JBA 小田}{森\JBA
  小田}{2009}]{擬似確率的単語分割コーパスによる言語モデルの改良}
森信介\JBA 小田裕樹 \BBOP 2009\BBCP.
\newblock 擬似確率的単語分割コーパスによる言語モデルの改良. 
\newblock \Jem{自然言語処理}, {\Bbf 16}  (5), \mbox{\BPGS\ 7--21}.

\bibitem[\protect\BCAY{森\JBA 宅間\JBA 倉田}{森 \Jetal
  }{2007}]{確率的単語分割コーパスからの単語N-gram確率の計算}
森信介\JBA 宅間大介\JBA 倉田岳人 \BBOP 2007\BBCP.
\newblock 確率的単語分割コーパスからの単語N-gram確率の計算. 
\newblock \Jem{情報処理学会論文誌}, {\Bbf 48}  (2), \mbox{\BPGS\ 892--899}.

\bibitem[\protect\BCAY{森\JBA 土屋\JBA 山地\JBA 長尾}{森 \Jetal
  }{1999}]{確率的モデルによる仮名漢字変換}
森信介\JBA 土屋雅稔\JBA 山地治\JBA 長尾真 \BBOP 1999\BBCP.
\newblock 確率的モデルによる仮名漢字変換. 
\newblock \Jem{情報処理学会論文誌}, {\Bbf 40}  (7), \mbox{\BPGS\ 2946--2953}.

\bibitem[\protect\BCAY{村上}{村上}{1991}]{漢字かなのＴＲＩＧＲＡＭをもちいたか
な漢字変換方法}
村上仁一 \BBOP 1991\BBCP.
\newblock 漢字かなのTRIGRAMをもちいたかな漢字変換方法. 
\newblock \Jem{情報処理学会第43回全国大会}, 3\JVOL, \mbox{\BPGS\ 287--288}.

\bibitem[\protect\BCAY{長野\JBA 森\JBA 西村}{長野 \Jetal
  }{2006}]{N-gramモデルを用いた音声合成のための読み及びアクセントの同時推定}
長野徹\JBA 森信介\JBA 西村雅史 \BBOP 2006\BBCP.
\newblock N-gramモデルを用いた音声合成のための読み及びアクセントの同時推定. 
\newblock \Jem{情報処理学会論文誌}, {\Bbf 47}  (6), \mbox{\BPGS\ 1793--1801}.

\bibitem[\protect\BCAY{永田}{永田}{1999}]{統計的言語モデルとN-best探索を用いた
日本語形態素解析法}
永田昌明 \BBOP 1999\BBCP.
\newblock 統計的言語モデルとN-best探索を用いた日本語形態素解析法. 
\newblock \Jem{情報処理学会論文誌}, {\Bbf 40}  (9), \mbox{\BPGS\ 3420--3431}.

\bibitem[\protect\BCAY{Neubig\JBA 中田\JBA 森}{Neubig \Jetal
  }{2010}]{点推定と能動学習を用いた自動単語分割器の分野適応}
Neubig, Graham, 中田陽介\JBA 森信介 \BBOP 2010\BBCP.
\newblock 点推定と能動学習を用いた自動単語分割器の分野適応. 
\newblock \Jem{言語処理学会年次大会}.

\bibitem[\protect\BCAY{西村\JBA 伊東\JBA 山崎}{西村 \Jetal
  }{1999}]{単語を認識単位とした日本語の大語彙連続音声認識}
西村雅史\JBA 伊東伸泰\JBA 山崎一孝 \BBOP 1999\BBCP.
\newblock 単語を認識単位とした日本語の大語彙連続音声認識. 
\newblock \Jem{情報処理学会論文誌}, {\Bbf 40}  (4), \mbox{\BPGS\ 1395--1403}.

\bibitem[\protect\BCAY{鹿野\JBA 伊藤\JBA 河原\JBA 武田\JBA 山本}{鹿野 \Jetal
  }{2001}]{音声認識システム}
鹿野清宏\JBA 伊藤克亘\JBA 河原達也\JBA 武田一哉\JBA 山本幹雄 \BBOP 2001\BBCP.
\newblock \Jem{音声認識システム}.
\newblock オーム社.

\bibitem[\protect\BCAY{堤\JBA 加藤\JBA 小坂\JBA 好田}{堤 \Jetal
  }{2002}]{講演音声認識のための音響・言語モデルの検討}
堤怜介\JBA 加藤正治\JBA 小坂哲夫\JBA 好田正紀 \BBOP 2002\BBCP.
\newblock 講演音声認識のための音響・言語モデルの検討. 
\newblock \Jem{電子情報通信学会技術研究会報告}, \mbox{\BPGS\ 117--122}.

\end{thebibliography}

\begin{biography}
  \bioauthor{森　　信介}{
    1998年京都大学大学院工学研究科電子通信工学専攻博士後期課程修了．
    同年日本アイ・ビー・エム（株）入社．
    2007年より京都大学学術情報メディアセンター准教授．
    現在に至る．
    自然言語処理ならびに計算言語学の研究に従事．
    工学博士．
    1997年情報処理学会山下記念研究賞受賞．
    2010年情報処理学会論文賞受賞．
    2010年第58回電気科学技術奨励賞．
    情報処理学会会員．
    }
  \bioauthor{笹田　鉄郎}{
    2007年京都大学工学部電気電子工学科卒業．
    2009年同大学院情報学研究科修士課程修了．
    同年同大学院博士後期課程に進学．
    現在に至る．
  }
  \bioauthor[:]{Neubig Graham}{
    2005年米国イリノイ大学アーバナ・シャンペーン校工学部コンピュータ・サイエンス専攻
    卒業．
    2010年京都大学大学院情報学研究科修士課程修了．
    同年同大学院博士後期課程に入学．
    現在に至る．
    自然言語処理に関する研究に従事．
  }
\end{biography}

\biodate

\end{document}

