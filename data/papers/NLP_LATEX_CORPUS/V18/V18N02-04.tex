    \documentclass[japanese]{jnlp_JS2.0}
\usepackage{jnlpbbl_1.3}
\usepackage[dvips]{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{hangcaption_jnlp}
\usepackage{udline}
\setulminsep{1.2ex}{0.2ex}
\let\underline

\def\dg{}

\def\BT{}                                 

\def\UW{}                                 

\def\Dder{}                            

\def\Ider{}              

\def\Nsum#1{}                      

\def\Bdma#1{}                

\def\Stri#1#2{} 

\def\Conc#1#2{}        

\def\argmax{}           

\def\argmin{}           

\def\MLE{}    

\def\QED{}             

\def\because{}



\def\MC#1#2#3{}



\def\ecaption#1{}

\def\tabref#1{}

\def\figref#1{}

\def\equref#1{}

\def\secref#1{}

\def\subref#1{}

\def\appref#1{}


\def\AWS#1{}

\def\BT{}
\def\UW{}

\def\lineB#1#2{}

\def\MC#1#2#3{}           

\def\pprime{}

\def\dg{}



\Volume{18}
\Number{2}
\Month{June}
\Year{2011}

\received{2010}{8}{10}
\revised{2010}{11}{22}
\accepted{2011}{1}{23}


\setcounter{page}{139}

\jtitle{3 種類の辞書による自動単語分割の精度向上}
\jauthor{森　　信介\affiref{ACCMS}  \and 小田　裕樹}
\jabstract{
  本論文では，日本語の文の自動単語分割をある分野に適用する現実的な状況において，精度
  向上を図るための新しい方法を提案する．提案手法の最大の特徴は，複合語を参照すること
  が可能な点である．複合語とは，内部の単語境界情報がなく，その両端も自動分割器の学習
  コーパスの作成に用いられた単語分割基準と必ずしも合致しない文字列である．このような
  複合語は，自然言語処理をある分野に適用する多くの場合に，利用可能な数少ない言語資源
  である．提案する自動単語分割器は，複合語に加えて単語や単語列を参照することも可能で
  ある．これにより，少ない人的コストでさらなる精度向上を図ることが可能である．

  実験では，これらの辞書を参照する自動単語分割システムを最大エントロピー法を用いて構
  築し，それぞれの辞書を参照する場合の自動単語分割の精度を比較した．実験の結果，本論
  文で提案する自動単語分割器は，複合語や単語列を参照することにより，対象分野において
  より高い分割精度を実現することが確認された．
}
\jkeywords{単語分割，辞書，単語列，複合語}

\etitle{Automatic Word Segmentation using Three Types of Dictionaries}
\eauthor{Shinsuke Mori\affiref{ACCMS} \and Hiroki Oda} 
\eabstract{
  In this paper we propose a new method for automatically segmenting a sentence in
  Japanese into a word sequence. The main advantage of our method is that the
  segmenter is, by using a maximum entropy framework, capable of referring to a list
  of compound words, i.e. word sequences without boundary information. This allows
  for a higher segmentation accuracy in many real situations where only some
  electronic dictionaries, whose entries are not consistent with the word
  segmentation standard, are available. Our method is also capable of exploiting a
  list of word sequences. It allows us to obtain a far greater accuracy gain with
  low manual annotation cost.

  We prepared segmented corpora, a compound word list, and a word sequence
  list. Then we conducted experiments to compare automatic word segmenters referring
  to various types of dictionaries. The results showed that the word segmenter we
  proposed is capable of exploiting a list of compound words and word sequences to
  yield a higher accuracy under realistic situations.
}
\ekeywords{word segmentation, dictionary, word sequence, compound word}

\headauthor{森，小田}
\headtitle{3 種類の辞書による自動単語分割の精度向上}

\affilabel{ACCMS}{京都大学学術情報メディアセンター}
	{Kyoto University, Academic Center for Computing and Media Studies}



\begin{document}
\maketitle



\section{はじめに}

日本語や中国語のように，明示的な単語境界がない言語においては，自動単語分割は自然言語
処理の最初のタスクである．ほとんどの自然言語処理システムは，単語単位に依存しており，
自動単語分割器はこれらの言語に対して非常に重要である．このような背景の下，人手による
単語分割がなされた文からなるコーパスを構築する努力
\cite{京都大学テキストコーパス・プロジェクト,Balanced.Corpus.of.Contemporary.Written.Japanese}とともに，経験的手法による自動単語分
割器や同時に品詞を付与する形態素解析器の構築
\cite{統計的言語モデルとN-best探索を用いた日本語形態素解析法,形態素クラスタリングによる形態素解析精度の向上,文字クラスモデルによる日本語単語分割,A.Stochastic.Finite-State.Word-Segmentation.Algorithm.for.Chinese,最大エントロピーモデルに基づく形態素解析.--未知語の問題の解決策--,Conditional.Random.Fields.を用いた日本語形態素解析}が試みられてきた．



近年，自然言語処理が様々な分野に適用されている．特許開示書の自動翻訳，裁判記録の自動
作成のための音声認識用の言語モデル作成，医療文章からの情報抽出などである．これらの応
用では品詞は不要なので，本論文では品詞を付与しない単語分割を扱う．単語分割では，コー
パス作成の労力を単語境界付与に集中することができるので，品詞付与が必要となる形態素解
析を前提とするよりもより実用的であることが多い．現在の自動単語分割器は，一般的な分野
のコーパスから構築されており，上述のような様々な分野の文を高い精度で単語分割できない．
とりわけ，対象分野特有の単語や表現の周辺での精度の低下が著しい．これらの対象分野に特
有の単語や表現は，処理すべき文において重要な情報を保持しているので，この問題は深刻で
ある．



このような問題を解決するためには，対象分野での単語分割精度の向上を図る必要がある．理
想的方法は，ある程度の量の対象分野の文を，一般分野のコーパス作成と同じ単語分割基準に
沿って人手で単語分割し，自動単語分割器を再学習することである．しかしながら，多くの実
際の状況では，人による利用を想定した辞書が対象分野の唯一の追加的言語資源である．これ
らの見出し語は，単語分割基準とは無関係に選定されており，単語分割基準に照らすと単語で
はないことが多い．本論文では，これらの見出し語のように，内部の単語分割情報が与えられ
ておらず，かつ両端が単語境界であるという保証がない文字列を複合語と呼ぶ．本論文では，
単語分割済みコーパスに加えて，複合語辞書を参照する自動分割器を提案する．ほとんどの複
合語は両端が単語境界であり，内部に単語分割基準に従って単語境界情報を付与することで単
語列に変換することが可能である．このために必要な人的コストは，適用分野の単語分割済み
コーパスの作成に比べて非常に少ない．本論文ではさらに，単語列辞書を参照し精度向上を図
る自動単語分割器を提案する．提案手法を用いることにより，一般に販売されている辞書（複合
語辞書）を参照することで，付加的な人的コストなしに，ある分野における自動単語分割の精度
を向上させることができる．また，単語列辞書を参照する機能により，コーパスを準備するよ
りもはるかに低い人的コストでさらなる精度向上を実現することが可能になる．

\section{単語分割のための言語資源}
\label{section:LRS}

この節では，まず，日本語を例に単語境界を明示しない言語の文を単語に分割する問題につい
て説明する．次に，入力文を自動的に単語列に分割する自動単語分割器を構築するために利用
可能な言語資源について述べる．


\subsection{単語分割問題}


日本語や中国語のように，単語境界を明示しない言語は多数ある．これらの言語における自然
言語処理の第一歩は，文に単語境界情報を付与することである．以下では，次の文を入力の例
として，単語分割問題を説明する．
\begin{description}
\item[入力：] 畜産物価格安定法を施行
\end{description}
単語分割問題は，入力の文字列の全ての隣接する2文字間に単語境界を置くか置かないかを決定
する2値分類問題である．注目している2文字が異なる単語に属する場合には空白を，同じ単語
に属する場合には何も置かないとすると，上記の例文を適切に単語に分割した結果は以下のよ
うになる．
\begin{description}
\item[出力：] 畜産 物 価格 安定 法 を 施行
\end{description}
このように，入力文字列を単語に分割することで，単語を単位とした自然言語処理技術を適用
することが可能となる．ただし，単語分割における誤りは，後続する自然言語処理の精度を低
下させる点に注意しなければならない．


\subsection{単語分割済みコーパス}
\label{section:WSC}

自動単語分割器に関する研究は多数あり，そのほとんどがデータに基づく方法を採用している
\cite{統計的言語モデルとN-best探索を用いた日本語形態素解析法,形態素クラスタリングによる形態素解析精度の向上,A.Stochastic.Finite-State.Word-Segmentation.Algorithm.for.Chinese,最大エントロピーモデルに基づく形態素解析.--未知語の問題の解決策--,Conditional.Random.Fields.を用いた日本語形態素解析}\footnote{単語分割と同時に品詞を
付与する形態素解析器は，品詞を無視することで自動単語分割器として用いることができるの
で，自動単語分割器の研究に含めている．}．これらの自動単語分割器は，単語分割基準に従っ
て人手で単語に分割されたコーパスからパラメータを推定する．したがって，学習コーパスに
おける誤りは自動分割の結果に波及し，後続する自然言語処理のアプリケーションの精度を損
なう．それゆえ，単語分割済みコーパスの質は，非常に重要である．高い分割精度を確保する
ためにも，後続する自然言語処理を適用する対象分野の単語分割済み文を自動単語分割システ
ムの学習コーパスとすることが望ましい．しかしながら，単語分割基準に従って正確に単語分
割されたコーパスを用意するコストは非常に高い．というのも，作業者は，対象分野の用語と
単語分割基準を熟知している必要があるからである．実際，コンピューター操作に熟練し単語
分割基準を熟知した作業者が，ある分野の5,000文を非常に高い精度で単語分割するのに2週間
（8時間$\times$10日）を要したという事実もある\footnote{これは著者の実際の経験に基づいて
いる．その際の作業においては，高機能のエディターを高度に利用し，必要に応じてプログラ
ムを記述・実行し，自動分割器を適宜再構築することで単語分割誤りを非常に効率良く修正し
た．}．したがって，低いコストで準備できる言語資源のみを用いて対象分野の文を高い精度で
分割する自動単語分割器の実現方法は非常に有用である．


\subsection{3種類の辞書}

\begin{figure}[t]
  \begin{center}
\includegraphics{18-2ia4f1.eps}
  \end{center}
  \caption{単語境界の3値表現}
  \label{figure:3-valued}
\vspace{-0.5\baselineskip}
\end{figure}

単語分割基準が所与とすれば，単語分割問題に用いることができる辞書は，3つに分類できる．
以下では，これら3種類の辞書について，\figref{figure:3-valued}に示した3値表現を用いて
詳述する．
    \begin{itemize}
\item \textbf{単語辞書：}この辞書は，単語分割基準に従う単語からなる．つま
  り，この辞書に含まれる文字列は，ある文脈で最左の文字の左と最右の文字の右に単語境界
  があり，さらにその内部には単語境界がない．例えば，3値表現された以下の文字列は単語で
  ある．
  \begin{quote}
    \verb*+|言-語|+
  \end{quote}

\item \textbf{単語列辞書：}この辞書は，単語の列からなる．つまり，この辞書
  に含まれる文字列は，ある文脈で最左の文字の左と最右の文字の右に単語境界があり，さら
  に，各文字列のすべての文字間に単語分割基準に従って単語境界情報が付与されている．例
  えば，3値表現された以下の文字列は単語列である．
  \begin{quote}
    \verb*+|計-算|言-語|学|+
  \end{quote}

\item \textbf{複合語辞書：}この辞書は，単語の連接である文字列からなる．つ
  まり，この辞書に含まれる文字列は，ある文脈でその左右両端に単語境界があるが，文字列
  内部の単語境界情報は不明である．例えば，3値表現された以下の文字列は複合語である．
  \begin{quote}
    \verb*+|計 算 言 語 学|+
  \end{quote}
    \end{itemize}

人が利用することを想定した商用・非商用の機械可読辞書は多数ある．実際，様々な対象分野
における専門用語や固有名詞を多数含む辞書がある
\cite{CD-科学技術45万語対訳辞典.英和／和英}．また，仮名漢字変換のための辞書が様々な分
野において公開されている\cite{無料ライフサイエンス辞書の活用と効能}．これらの辞書の見
出し語は，自動単語分割器が学習に用いるコーパスの単語分割基準とは無関係に選定されてお
り，多くの自動単語分割器において，これを精度向上に直接利用することはできない．

単語分割基準に照らすと，人が利用することを想定した辞書の見出し語の多くは，
\pagebreak
上記の分類
では複合語である．左右両端は単語境界であることがかなりの確度で期待できるが，文字列の
内部の単語境界情報がない．複合語は，人的コストをかけて単語列に変換することができる．
この際に必要な作業は，左右の両端が単語境界であることのチェックと，文字列内のすべての
文字境界に単語境界情報を付与することである．このコストは，対象分野の単語分割済みコー
パスの作成に要するコストに比べて非常に少ない．以上の議論から，複合語や単語列を参照す
ることで精度が向上する自動単語分割器を構築することは実用的意義が非常に大きい．


\section{単語分割法}
\label{section:AWS}

本節では，前節の3種類の辞書を学習データとする日本語単語分割法について述べる．提案手法
は，3種類の辞書と単語分割済みコーパス（部分的にアノテーションされていれば良い）を学習デー
タとする．


\subsection{最大エントロピーモデルによる点予測単語分割}
\label{subsection:ME}

日本語の単語分割の問題は，入力文の各文字間に単語境界が発生するか否かを予測する問題と
みなせる\cite{文字クラスモデルによる日本語単語分割,日本語単語分割を題材としたサポートベクタマシンの能動学習の実験的研究,教師なし隠れマルコフモデルを利用した最大エントロピータグ付けモデル,Training.Conditional.Random.Fields.Using.Incomplete.Annotations}．つまり，文
$\Bdma{x} = \Conc{x}{m}$に対して，文字$x_{i}$と$x_{i+1}$の間が単語境界であるか否かを
表すタグ$t_{i}$を付与する問題とみなせる．

付与するタグは，単語境界であることを表すタグ{\bf E} （``\verb*+|+''に相当）と，非単語境
界であることを表すタグ{\bf N} （``\verb*+-+''に相当）の2つのタグからなる．各文字間のタ
グを単語境界が明示されたコーパスから学習された最大エントロピーモデル(ME model; maximum entropy model)により推定する\footnote{文献
\cite{日本語単語分割の分野適応のための部分的アノテーションを用いた条件付き確率場の学習}
のようにCRF (conditional random fields)により推定することもできるが，計算コストと記憶
領域が大きくなる．これらの差は，スパースな部分的アノテーションコーパスからの学習にお
いて顕著となる．つまり，CRFのように系列としてモデル化する方法では，アノテーションのな
い部分も考慮する必要があるのに対して，点推定の最大エントロピーモデルでは，アノテーショ
ンのある部分のみを考慮すればよい．このような考察から，本論文では計算コストの少ない最
大エントロピーモデルを用いる．}．その結果，より高い確率を与えられたタグをその文字間
のタグとし，単語境界を決定する．すなわち，以下の式が示すように，最大エントロピーモデ
ルにより，単語境界と推定される確率が非単語境界と推定される確率より高い文字間を単語境
界とする．
\begin{displaymath}
  P_{ME}({\bf E}|i,\Bdma{x}) > P_{ME}({\bf N}|i,\Bdma{x})
\end{displaymath}
これにより，入力文を単語に分割することができる．

最大エントロピーモデルによる単語分割法では，単語境界情報が付与された $\Bdma{x} =
\Conc{x}{m}$の各文字間を，タグ$t_{i}$と素性の組み合わせ$S$とみなして，学習と確率推定を行
う．
\begin{displaymath}
  S = \{(t_{i},\,f_{i,1}(\Bdma{x}),\,f_{i,2}(\Bdma{x}),\,\cdots)
  \;|\;1 \leq \forall i \leq m-1\},
\end{displaymath}
素性$f_{i,j}(\Bdma{x})$の詳細は次項で述べる．


\subsection{参照する素性}


文字$x_{i}$と$x_{i+1}$の間に注目する際の最大エントロピーモデルの素性としては，文字列
$x_{i-1}^{i+2}$に含まれるの部分文字列である文字$n$-gramおよび字種$n$-gram ($n = 1,\,
2,\,3$)をすべて用いる\footnote{字種は，漢字，ひらがな，カタカナ，アルファベット，数
  字，記号の6つとした．}．ただし，以下の点を考慮している．
\begin{itemize}

\item 素性として利用する$n$-gramは，先頭文字の字種がその前の文字の字種と同じか否か，
  および，末尾文字の字種がその次の文字の字種と同じか否かの情報を付加して参照する
  \footnote{パラメータ数の急激な増加を抑えつつ素性の情報量を増加させることを意図して
  いる．これにより，参照範囲を前後1文字拡張して$x_{i-2}^{i+3}$の範囲の$n$-gram ($n =
  3,\,4,\,5$)が参照されることになる．}．

\item 素性には注目する文字間の位置情報を付加する．

\end{itemize}


\subsection{辞書の利用}

さらに，前述した3種類の辞書を参照し，以下の素性を用いることを提案する．
\begin{itemize}

\item 文字列$x_{i-1}^{i+2}$に含まれる文字$n$-gram ($n = 1,\,2,\,3$)が，単語辞書中の単
  語と一致する文字列であるか否かを表す9素性（9つの位置の文字$n$-gramについて判定）

\item 注目する文字境界（$x_{i}x_{i+1}$の間）の辞書中の位置を表す以下の4素性
  \begin{itemize}
  \item 単語の開始位置の``\verb*+|+''に該当するか否かを表す素性（単語，単語列，複合語
    のいずれかが$x_{i+1}x_{i+2}\cdots$に前方一致するか否か）
  \item 単語の終了位置の``\verb*+|+''に該当するか否かを表す素性（単語，単語列，複合語
    のいずれかが$\cdots x_{i-1}x_{i}$に後方一致するか否か）
  \item 単語列の単語境界の``\verb*+|+''に該当するか否かを表す素性（$\cdots x_{i}
    x_{i+1} \cdots$の位置にいずれかの単語列が出現し，かつ$x_{i}$と$x_{i+1}$の間が
    単語列中の``\verb*+|+''に該当するか否か）
  \item 単語や単語列の``\verb*+-+''に該当するか否かを表す素性（$\cdots x_{i}x_{i+1}
    \cdots$ の位置にいずれかの単語か単語列が出現し，かつ$x_{i}$と$x_{i+1}$の間が
    ``\verb*+-+''に該当するか否か）
  \end{itemize}
\end{itemize}


\section{評価}
\label{section:評価}
提案手法の評価のために，様々な自動単語分割器を構築し，テストコーパスに対する分割精度
を測定した．この節では，その結果を提示し提案手法の評価を行う．


\subsection{実験条件}

\begin{table}[t]
  \caption{単語分割済みコーパス}
  \label{table:corpus}
\input{04table01.txt}
\end{table}

\begin{table}[t]
  \caption{辞書}
  \label{table:dictionary}
\input{04table02.txt}
\end{table}

まず，対象分野のテストコーパスを日本経済新聞\cite{日経全文記事データベース[4紙版]}の
記事とした．
\pagebreak
学習コーパスは一般分野の単語分割済みコーパスである．評価のために，学習コ
ーパスと同じ分野のテストコーパスも用いた（\tabref{table:corpus}参照）．一般分野コーパス
は，現代日本語書き言葉均衡コーパス
\cite{Balanced.Corpus.of.Contemporary.Written.Japanese}（13,181文）と，日常会話のための
辞書の例文\cite{会話作文英語表現辞典}（14,754文）である．すべての文は，人手により適切に
単語に分割されている．実験では，9-foldの交差検定を行った．つまり，テストコーパスを9つ
の部分に分割し，8つの部分を複合語や単語列の選定に用い，残りの1つを自動分割のテストと
して用いることを，9通りに渡って行った．実験に際して，\secref{section:LRS}で述べた3 種
類の辞書を用意した．1つ目は単語辞書(UniDic-1.3.9)で，語彙サイズが非常に大きいことに加
えて，その見出し語が学習コーパスと同じ単語分割基準に従っていることが注意深くチェック
されている\cite{コーパス日本語学のための言語資源：形態素解析用電子化辞書の開発とその
応用}．2つ目は複合語辞書で，その見出し語は機械可読の商用辞書\cite{CD-科学技術45万語対
訳辞典.英和／和英}から得た．その多くは，専門用語と固有名詞である．実験では，テストと
して用いられる1つの部分コーパス以外の8つの部分コーパスに文字列として出現する複合語を
用いた．これらの複合語は，両端は単語分割基準と一致していることが期待されるが，その保
証はない．3つ目は単語列辞書である．単語列辞書は，複合語辞書の見出し語を単語分割基準に
従って人手により分割することで得られる．単語列は，結果的に1単語である場合もある．実験
では，8つの部分コーパスに文字列として出現する単語列を用いた．複合語の左右どちらかの端
が単語分割基準と一致していない場合は単語列辞書から除外した．したがって，単語列の数は
複合語の数よりも少なくなっている．

\tabref{table:dictionary}にこれらの辞書の諸元を示す．この表から複合語と単語列の平均文
字数はそれぞれ3.04文字と3.12文字であることが分かる．これらは，学習コーパスの単語長
（1.40文字）や新聞コーパスの平均単語長（1.50文字）より長い．単語列は平均1.61単語からなる．
自動単語分割器のパラメータは，学習コーパスとこれらの辞書を同時に参照し推定される．



\subsection{評価基準}

自動単語分割の評価に用いた基準は，適合率，再現率，境界推定精度及び文正解率である．こ
れらの計算方法を以下のような例を用いて説明する．ここで，自動単語分割の結果をAWS，正解
の単語列をCORとしている．
\begin{description}
\item[　AWS:] \underline{畜産} 物価 格安 定法 \underline{を} \underline{施行}
\item[　COR:] \underline{畜産} 物 価格 安定 法 \underline{を} \underline{施行}
\end{description}
境界推定精度は，単語境界情報が正しく推定された文字間の割合である．上記の例では，文字
数は11あるので，単語境界の推定対象となる文字間の数は10である．この内，単語境界情報が
正しく推定された文字間は5であるので境界推定精度は5/10となる．文正解率は，すべての文字
間において単語境界情報が正しく推定された文の割合である．適合率と再現率は，以下のよう
に計算される．まず，正解の単語列に含まれる単語数を$N_{COR}$，自動単語分割の結果に含ま
れる単語数を$N_{AWS}$とし，さらに正解の単語列と自動単語分割の結果の最長部分一致単語列
に含まれる単語数を$N_{LCS}$とする．この定義の下，適合率は$N_{LCS}/N_{AWS}$で与えられ，
再現率は$N_{LCS}/N_{COR}$で与えられる．上述の例では，最長部分一致単語列は下線を引いた
単語列であり，その単語数から$N_{LCS} = 3$である．正解の単語列の単語数から$N_{COR}=7$
であり，自動単語分割の結果の単語数から$N_{AWS}=6$である．したがって，再現率は$N_{LCS}
/N_{COR} = 3/7$であり，適合率は$N_{LCS}/N_{AWS} = 3/6$である．



\subsection{評価}

参照する辞書による単語分割精度の差を調べるために，辞書を参照せずコーパスのみから学習
する自動単語分割器\AWS{B}（ベースライン）を構築し，さらに以下の4つの自動単語分割器を構
築した．

\begin{tabular}{rl}
  \AWS{W1:} & コーパスに加えて単語辞書\AWS{w1}を参照 \\
  \AWS{W2:} & コーパスに加えて単語辞書\AWS{w2}を参照 \\
  \AWS{S:}  & コーパスに加えて単語列辞書\AWS{s}を参照 \\
  \AWS{C:}  & コーパスに加えて複合語辞書\AWS{c}を参照
\end{tabular} \\
これらの自動単語分割器による一般分野における分割精度を\tabref{table:result1}に，対象
分野における分割精度を\tabref{table:result2}に示す．

\begin{table}[tb]
  \caption{一般分野における自動単語分割の精度}
  \label{table:result1}
\input{04table03.txt}
\end{table}

\begin{table}[tb]
  \caption{適用分野における自動単語分割の精度}
  \label{table:result2}
\input{04table04.txt}
\end{table}

\tabref{table:result1}から，ベースラインである自動単語分割器\AWS{B}の一般分野における
分割精度は十分高いが，\tabref{table:result2}から，対象分野においては分割精度が著しく
低下することがわかる．自動単語分割器\AWS{C}の結果から，複合語辞書を用いることで対象分
野における分割精度が向上することが分かる．複合語辞書としては，多くの分野において利用
可能な人のための辞書を直接用いることができるので，付加的な人的コストを必要としない．
このことから，複合語辞書を参照することで自動単語分割器の精度向上が実現できることは非
常に有用であるといえる．

自動単語分割器\AWS{C}の分割精度は，両分野において，コーパスと同じ基準で単語に分割され
た単語辞書\AWS{w1}を参照する自動単語分割器\AWS{W1}の分割精度より低い．これは，単語辞
書\AWS{w1}の見出し語の数が，約14.5万語と非常に大きいことと，本実験での適応対象である
新聞記事が，単語辞書の想定分野となっていることによる．このように，大きな単語辞書を，
様々な分野に対して準備することは現実的ではないであろう．実際，単語辞書に含まれる単語
の合計の文字数は430,797文字（\tabref{table:dictionary}参照）と非常に大きく，これは，対
象分野の約9,879文（\tabref{table:corpus}参照）に相当する．第\subref{section:WSC}で述べた
ように，非常に熟練した作業者の単語分割速度が1日500文程度であるから，この量のコーパス
を作成するには，対象分野の表現と単語分割基準を熟知した作業者が約20日間作業にあたる必
要があると考えられる．

単語列辞書\AWS{s}と同じ単語数の単語辞書\AWS{w2}を用いる自動単語分割器\AWS{W2}の分割精
度を自動単語分割器\AWS{C}と比べると，\AWS{C}の精度は\AWS{W2}よりも高い．複合語辞書は，
処理すべき適用分野のテキストと共に提供されることが多い．これらのことから，ある分野に
自動単語分割器を適用する場合，一般的な分野の辞書を整備するのではなく，その分野の複合
語辞書を利用するのがよい戦略であるといえる．

単語列辞書を参照する自動単語分割器\AWS{S}の対象分野における単語分割精度は，複合語辞書
を参照する自動単語分割器\AWS{C}よりも高い．これは，単語列辞書が複合語辞書に単語境界情
報を付与した結果であることを考えると当然である．このことから，対象分野のテキストに出
現する複合語に単語境界情報を付与することで，さらなる精度向上を実現できることが分かる．

文字列「共同宣言」を複合語としてまたは単語列として辞書に追加することにより分割精度が
向上した例を次に示す．
\begin{description}
\item[　\AWS{B}:] …\verb*+|日-米|安-保|共|同|宣-言-取|り|ま-と-め|+…
\item[　\AWS{C}:] …\verb*+|日-米|安-保|共|同|宣-言|取|り|ま-と-め|+…
\item[　\AWS{S}:] …\verb*+|日-米|安-保|共-同|宣-言|取|り|ま-と-め|+…
\end{description}
自動単語分割器\AWS{B}では「宣言取」を単語であると出力しているが，\AWS{C}では，複合語
「\verb*+|共 同 宣 言|+」を参照することで文字列「共同宣言」の後に単語境界があることが
分かり，正しく「宣言」と「取」に分割されている．さらに\AWS{S}では，単語列
「\verb*+|共-同|宣-言|+」を参照することですべての箇所で正しく単語に分割されている．な
お，分野特有であると思われる表現でも，実験に利用した商用辞書に含まれていない単語列に
関しては，辞書の追加による精度向上はみられなかった．

自動単語分割器\AWS{S}による対象分野の分割精度は，大規模な単語辞書を参照する自動単語分
割器\AWS{W1}による分割精度よりも高い．複合語辞書に含まれる合計の文字数は，59,868文字
（\tabref{table:dictionary}参照）で，これは，対象分野の1,373文に相当する．アノテーショ
ンに必要な人的コストは，単語辞書の構築に必要な人的コストと比べて非常に少ない．さらに，
複合語の多くが専門用語と固有名詞であるので，作業者に要求される技能は，分野知識と名詞
に関する単語分割基準の熟知である．したがって，作業者の確保はコーパスの準備に比べては
るかに容易である．

以上のような考察から，現実的な状況において，既存の辞書のカバー率が高くないある対象分
野の自動単語分割器を構築する最良の戦略は，
\begin{enumerate}
\item 対象分野のテキストに出現する複合語辞書の見出し語を収集し，提案する自動単語分割
  器を用いる
\item 作業者が確保できる場合には，さらにこれらの複合語に単語境界情報を付与し，提案す
  る自動単語分割器を用いる
\end{enumerate}
であると結論できる．


\section{関連研究}

自動単語分割の問題をある文字の次に単語境界があるかの予測として定式化することは
かなり
以前から行われている\cite{日本語における単語の造語モデルとその評価,品詞・区切り情報を含む拡張文字の連鎖確率を用いた日本語形態素解析,文字クラスモデルによる日本語単語分割}．これらの研究では，文字に単語境界情報を付与して
予測単位としている．文字間に単語境界があるかを識別学習により決定することも提案されて
いる\cite{日本語単語分割を題材としたサポートベクタマシンの能動学習の実験的研究}．この
研究の主眼は能動学習の調査・分析である．辞書の利用に関する記述はなく，また分野適応に
ついても述べられていない．

統計的手法による日本語の文の自動単語分割に関する初期の研究は，丸山ら
\cite{確率的形態素解析}による単語$n$-gramモデルを用いる方法がある．また，永田
\cite{統計的言語モデルとN-best探索を用いた日本語形態素解析法}による品詞$n$-gramモデ
ルによる形態素解析\footnote{ここでは，自動単語分割に加えてそれぞれの単語の品詞を同時
に推定する処理を形態素解析と呼んでいる．}もある．森ら
\cite{形態素クラスタリングによる形態素解析精度の向上}は，すべての品詞を語彙化した形態
素$n$-gramモデルを用いることによる精度向上を報告し，さらに単語辞書の参照を可能にする
方法を提案し，それによる精度向上を報告している．内元ら
\cite{最大エントロピーモデルに基づく形態素解析.--未知語の問題の解決策--}は，最大エン
トロピーモデルを用いる形態素解析において単語辞書を参照する方法を提案している．このよ
うに，単語分割基準に沿った単語辞書を参照する方法はすでにあるが，複合語や単語列を参照
し精度向上を実現した自動単語分割器の報告は，我々の知る限りない．

なお，提案手法は，形態素解析にも拡張可能である．提案する自動単語分割器は音声認識や仮
名漢字変換の言語モデル作成に用いることを想定しているので，品詞を推定する必要はないと
考えている．品詞も推定すべきか，どの程度の粒度の品詞体系にすべきか，などの問題は後続
する自然言語処理と準備すべきデータの作成コストを含む全体の問題であり，本論文の議論を
超える．

複合語や単語列を参照し精度向上を図る取り組みは，完全に単語に分割された文に加えて，そ
れ以外の断片的な情報を用いて精度向上を図る取り組みの一つである．坪井ら
\cite{日本語単語分割の分野適応のための部分的アノテーションを用いた条件付き確率場の学習}
は，学習コーパスの文の単語境界情報が部分的であるような不完全なアノテーションからも条
件付き確率場による自動単語分割器や形態素解析器を学習できる枠組みを提案している．本論
文で提案する自動単語分割器は点予測を用いているので，単語分割に関してはこの問題は解決
されているといえる．したがって，提案する自動単語分割器は，単語境界情報が部分的に付与
されたコーパスと複合語や単語列のすべてを同時に参照することができる．

自動単語分割は，中国語においても提案されている
\cite{A.Stochastic.Finite-State.Word-Segmentation.Algorithm.for.Chinese}．自動単語分
割器は，空白で区切られる単位が大きい韓国語やフィンランド語などにおいても有用で，言語
モデルの作成
\cite{Korean.large.vocabulary.continuous.speech.recognition.with.morpheme-based.recognition.units,Unlimited.vocabulary.speech.recognition.with.morph.language.models.applied.to.Finnish}
に用いられている．提案手法は，これらの言語に対する言語処理においても有用である．


\section{おわりに}

本論文では，日本語の文の自動単語分割器をある分野に適用する現実的な状況において，精度
向上を図るための新しい方法を提案した．提案手法の最大の特徴は，複合語を参照することが
可能な点である．本論文で言う複合語とは，内部の単語境界情報がない文字列であり，人の利
用を想定した辞書の見出し語の多くが複合語である．この機能により，一般に販売されている
辞書を参照し，付加的な人的コストなしに，ある分野における自動単語分割の精度を向上させ
ることができる．提案する自動単語分割器は，内部に単語境界情報をもつ単語列を参照するこ
とも可能である．この機能により，コーパスを準備するよりも低い人的コストでさらなる精度
向上を実現することが可能になる．

実験では，これらの辞書を参照する自動単語分割器を最大エントロピー法を用いて構築し，こ
れらのさまざまな辞書を参照する場合の自動単語分割の精度を比較した．実験の結果，本論文
で提案する自動単語分割器は，複合語を参照することにより，より高い分割精度を人的コスト
なしに実現することが確認された．また，単語列を参照することにより，少ない人的コストで
さらなる精度向上が実現されることが示された．したがって，本論文で提案する自動単語分割
器は，自然言語処理をある分野に適用する場合に非常に有用である．

\acknowledgment

本研究の一部は，科学研究費補助金・若手 A（課題番号：08090047）により行われた．


\bibliographystyle{jnlpbbl_1.5}
\begin{thebibliography}{}

\bibitem[\protect\BCAY{伝\JBA 小木曽\JBA 小椋\JBA 山田\JBA 峯松\JBA 内元\JBA
  小磯}{伝 \Jetal
  }{2007}]{コーパス日本語学のための言語資源：形態素解析用電子化辞書の開発とそ
の応用} 
伝康晴\JBA 小木曽智信\JBA 小椋秀樹\JBA 山田篤\JBA 峯松信明\JBA 内元清貴\JBA
  小磯花絵 \BBOP 2007\BBCP.
\newblock \JBOQ
  コーパス日本語学のための言語資源：形態素解析用電子化辞書の開発とその応用.\JBCQ\
\newblock \Jem{日本語科学}, {\Bbf 22}, \mbox{\BPGS\ 101--122}.

\bibitem[\protect\BCAY{ドナルド\JBA 羽鳥\JBA 山田\JBA 伊良部}{ドナルド \Jetal
  }{1992}]{会話作文英語表現辞典}
ドナルドキーン\JBA 羽鳥博愛\JBA 山田晴子\JBA 伊良部祥子 \BBOP 1992\BBCP.
\newblock \JBOQ 会話作文英語表現辞典\JBCQ\
\newblock 朝日出版社.

\bibitem[\protect\BCAY{颯々野}{颯々野}{2006}]{日本語単語分割を題材としたサポー
トベクタマシンの能動学習の実験的研究}
颯々野学 \BBOP 2006\BBCP.
\newblock \JBOQ
  日本語単語分割を題材としたサポートベクタマシンの能動学習の実験的研究.\JBCQ\
\newblock \Jem{自然言語処理}, {\Bbf 13}  (2), \mbox{\BPGS\ 27--41}.

\bibitem[\protect\BCAY{Hirsim{\"{a}}ki, Creutz, Siivola, Kurimo, Virpioja,
  \BBA\ Pylkk{\"{o}}nen}{Hirsim{\"{a}}ki
  et~al.}{2006}]{Unlimited.vocabulary.speech.recognition.with.morph.language.m
odels.applied.to.Finnish}
Hirsim{\"{a}}ki, T., Creutz, M., Siivola, V., Kurimo, M., Virpioja, S., \BBA\
  Pylkk{\"{o}}nen, J. \BBOP 2006\BBCP.
\newblock \BBOQ Unlimited vocabulary speech recognition with morph language
  models applied to Finnish.\BBCQ\
\newblock {\Bem Computer Speech and Language}, {\Bbf 20}, \mbox{\BPGS\
  515--541}.

\bibitem[\protect\BCAY{金子}{金子}{2003}]{無料ライフサイエンス辞書の活用と効能}
金子周司 \BBOP 2003\BBCP.
\newblock \JBOQ 無料ライフサイエンス辞書の活用と効能\JBCQ\
\newblock \Jem{ファルマシア}, {\Bbf 42}  (5), \mbox{\BPGS\ 463--467}.

\bibitem[\protect\BCAY{風間\JBA 宮尾\JBA 辻井}{風間 \Jetal
  }{2004}]{教師なし隠れマルコフモデルを利用した最大エントロピータグ付けモデル}
風間淳一\JBA 宮尾祐介\JBA 辻井潤一 \BBOP 2004\BBCP.
\newblock \JBOQ
  教師なし隠れマルコフモデルを利用した最大エントロピータグ付けモデル.\JBCQ\
\newblock \Jem{自然言語処理}, {\Bbf 11}  (4), \mbox{\BPGS\ 3--24}.

\bibitem[\protect\BCAY{工藤\JBA 山本\JBA 松本}{工藤 \Jetal
  }{2004}]{Conditional.Random.Fields.を用いた日本語形態素解析}
工藤拓\JBA 山本薫\JBA 松本裕治 \BBOP 2004\BBCP.
\newblock \JBOQ Conditional Random Fields を用いた日本語形態素解析.\JBCQ\
\newblock \Jem{情報処理学会研究報告}, NL161\JVOL.

\bibitem[\protect\BCAY{黒橋\JBA 長尾}{黒橋\JBA
  長尾}{1997}]{京都大学テキストコーパス・プロジェクト}
黒橋禎夫\JBA 長尾眞 \BBOP 1997\BBCP.
\newblock \JBOQ 京都大学テキストコーパス・プロジェクト\JBCQ\
\newblock \Jem{言語処理学会第3回年次大会発表論文集}, \mbox{\BPGS\ 115--118}.

\bibitem[\protect\BCAY{Kwon \BBA\ Park}{Kwon \BBA\
  Park}{2003}]{Korean.large.vocabulary.continuous.speech.recognition.with.morp
heme-based.recognition.units}
Kwon, O.-W.\BBACOMMA\ \BBA\ Park, J. \BBOP 2003\BBCP.
\newblock \BBOQ Korean large vocabulary continuous speech recognition with
  morpheme-based recognition units.\BBCQ\
\newblock {\Bem Speech Communication}, {\Bbf 39}, \mbox{\BPGS\ 287--300}.

\bibitem[\protect\BCAY{Maekawa}{Maekawa}{2008}]{Balanced.Corpus.of.Contemporar
y.Written.Japanese}
Maekawa, K. \BBOP 2008\BBCP.
\newblock \BBOQ Balanced Corpus of Contemporary Written Japanese.\BBCQ\
\newblock In {\Bem Proceedings of the 6th Workshop on Asian Language
  Resources}, \mbox{\BPGS\ 101--102}.

\bibitem[\protect\BCAY{丸山\JBA 荻野\JBA 渡辺}{丸山 \Jetal
  }{1991}]{確率的形態素解析}
丸山宏\JBA 荻野紫穂\JBA 渡辺日出雄 \BBOP 1991\BBCP.
\newblock \JBOQ 確率的形態素解析.\JBCQ\
\newblock \Jem{日本ソフトウェア科学会第8回大会論文集}, \mbox{\BPGS\ 177--180}.

\bibitem[\protect\BCAY{森\JBA 長尾}{森\JBA
  長尾}{1998}]{形態素クラスタリングによる形態素解析精度の向上}
森信介\JBA 長尾眞 \BBOP 1998\BBCP.
\newblock \JBOQ 形態素クラスタリングによる形態素解析精度の向上.\JBCQ\
\newblock \Jem{自然言語処理}, {\Bbf 5}  (2), \mbox{\BPGS\ 75--103}.

\bibitem[\protect\BCAY{永井\JBA 日高}{永井\JBA
  日高}{1993}]{日本語における単語の造語モデルとその評価}
永井秀利\JBA 日高達 \BBOP 1993\BBCP.
\newblock \JBOQ 日本語における単語の造語モデルとその評価.\JBCQ\
\newblock \Jem{情報処理学会論文誌}, {\Bbf 34}  (9), \mbox{\BPGS\ 1944--1955}.

\bibitem[\protect\BCAY{永田}{永田}{1999}]{統計的言語モデルとN-best探索を用いた
日本語形態素解析法}
永田昌明 \BBOP 1999\BBCP.
\newblock \JBOQ 統計的言語モデルとN-best探索を用いた日本語形態素解析法.\JBCQ\
\newblock \Jem{情報処理学会論文誌}, {\Bbf 40}  (9), \mbox{\BPGS\ 3420--3431}.

\bibitem[\protect\BCAY{日本経済新聞社}{日本経済新聞社}{2001}]{日経全文記事デー
タベース[4紙版]}
日本経済新聞社 \BBOP 2001\BBCP.
\newblock 日経全文記事データベース（4紙版）\inhibitglue.

\bibitem[\protect\BCAY{日外アソシエーツ}{日外アソシエーツ}{2001}]{CD-科学技術4
5万語対訳辞典.英和／和英}
日外アソシエーツ \BBOP 2001\BBCP.
\newblock \JBOQ CD-科学技術45万語対訳辞典 英和／和英\JBCQ.

\bibitem[\protect\BCAY{小田\JBA 森\JBA 北}{小田 \Jetal
  }{1999}]{文字クラスモデルによる日本語単語分割}
小田裕樹\JBA 森信介\JBA 北研二 \BBOP 1999\BBCP.
\newblock \JBOQ 文字クラスモデルによる日本語単語分割.\JBCQ\
\newblock \Jem{自然言語処理}, {\Bbf 6}  (7), \mbox{\BPGS\ 93--108}.

\bibitem[\protect\BCAY{Sproat \BBA\ Chang}{Sproat \BBA\
  Chang}{1996}]{A.Stochastic.Finite-State.Word-Segmentation.Algorithm.for.Chin
ese}
Sproat, R., Shih, C., Gale, W., and Chang, N. (1996).
\newblock \BBOQ A stochastic finite-state word-segmentation algorithm for
  Chinese.\BBCQ\
\newblock {\Bem Computational Linguistics}, {\Bbf 22}  (3), \mbox{\BPGS\
  377--404}.

\bibitem[\protect\BCAY{Tsuboi, Kashima, Mori, Oda, \BBA\ Matsumoto}{Tsuboi
  et~al.}{2008}]{Training.Conditional.Random.Fields.Using.Incomplete.Annotatio
ns}
Tsuboi, Y., Kashima, H., Mori, S., Oda, H., \BBA\ Matsumoto, Y. \BBOP
  2008\BBCP.
\newblock \BBOQ Training Conditional Random Fields Using Incomplete
  Annotations.\BBCQ\
\newblock In {\Bem Proceedings of the 22th International Conference on
  Computational Linguistics}.

\bibitem[\protect\BCAY{坪井\JBA 森\JBA 鹿島\JBA 小田\JBA 松本}{坪井 \Jetal
  }{2009}]{日本語単語分割の分野適応のための部分的アノテーションを用いた条件付
き確率場の学習}
坪井祐太\JBA 森信介\JBA 鹿島久嗣\JBA 小田裕樹\JBA 松本裕治 \BBOP 2009\BBCP.
\newblock \JBOQ
  日本語単語分割の分野適応のための部分的アノテーションを用いた条件付き確率場の
学習.\JBCQ\
\newblock \Jem{情報処理学会論文誌}, {\Bbf 50}  (6), \mbox{\BPGS\ 1622--1635}.

\bibitem[\protect\BCAY{内元\JBA 関根\JBA 井佐原}{内元 \Jetal
  }{2001}]{最大エントロピーモデルに基づく形態素解析.--未知語の問題の解決策--}
内元清貴\JBA 関根聡\JBA 井佐原均 \BBOP 2001\BBCP.
\newblock \JBOQ 最大エントロピーモデルに基づく形態素解析—未知語の問題の解決策—.\JBCQ\
\newblock \Jem{自然言語処理}, {\Bbf 8}  (1), \mbox{\BPGS\ 127--141}.

\bibitem[\protect\BCAY{山本\JBA 増山}{山本\JBA
  増山}{1997}]{品詞・区切り情報を含む拡張文字の連鎖確率を用いた日本語形態素解
析}
山本幹雄\JBA 増山正和 \BBOP 1997\BBCP.
\newblock \JBOQ
  品詞・区切り情報を含む拡張文字の連鎖確率を用いた日本語形態素解析.\JBCQ\
\newblock \Jem{言語処理学会第3回年次大会発表論文集}, \mbox{\BPGS\ 421--424}.


\end{thebibliography}



\begin{biography}
  \bioauthor{森　信介}{
    1998年京都大学大学院工学研究科電子通信工学専攻博士後期課程修了．
    同年日本アイ・ビー・エム（株）入社．
    2007年より京都大学学術情報メディアセンター准教授．
    現在に至る．
    自然言語処理ならびに計算言語学の研究に従事．
    工学博士．
    1997年情報処理学会山下記念研究賞受賞．
    2010年情報処理学会論文賞受賞．
    2010年第58回電気科学技術奨励賞．
    情報処理学会会員．
    }
  \bioauthor{小田　裕樹}{
    1999年徳島大学大学院工学研究科博士前期課程知能情報工学専攻修了．
    同年NTTソフトウェア（株）入社．
    言語処理・情報検索システム等の開発,コンサルティング業務に従事．
    確率・統計的自然言語処理およびその応用に興味を持つ．
    工学博士．
    2010年情報処理学会論文賞受賞．
    情報処理学会会員．
  }
\end{biography}

\biodate

\end{document}

