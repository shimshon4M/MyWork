<?xml version="1.0" ?>
<root>
  <section title="Introduction">WordSenseDisambiguation(WSD)isaprocessbywhichoneidentifiesthesenseofaword'soccurrence.Thisprocessaimstoalleviatetheproblemofsemanticambiguityunderlyingmanyapplicationsofnaturallanguageprocessing(NLP)suchasmachinetranslation(MT)andinformationretrieval(IR).Forexample,ifanIRsystemreceivesanambiguousquerysuchas``Orange'',betterresultscouldbeobtainedifthesystemcouldidentifythesenseoforange(orange/fruit,orange/color,orange/telecom-company,etc.)bothinthequeryandinthedocuments.generalapproachtoWSDistoexploitelements(e.g.wordsandsyntacticstructure)inthecontextoftheambiguousword,whichwecalltargetwordinthispaper,inordertobuildacontextmodel.Theintuitionisthatawordwithaparticularsensehasahigherchancetoco-occurwithcertaincontextwordsorappearwithinsomespecificsyntacticstructures.Forinstance,orangemeaningthefruitismorelikelytohave``juice''initssurroundingsortobeplural,orangemeaningthecolorismorelikelytohaveadjectiveasitspart-of-speech(POS),andorangemeaningthetelecom-companyismorelikelytohaveanuppercase``O''.Suchcontextualelementscanbeusedasfeaturestohelpdeterminethesenseofthetargetword.FeaturesemployedinWSDsystemsareofvariousnature.CommonfeaturesincludePOS-tags,bag-of-wordsfeatures,localcollocations,syntacticrelationsandtopicalfeatures.State-of-the-artapproachesmayalsouseexternalknowledgesourcessuchasdictionariesandthesauriinordertoobtainadditionalfeatures.Allthesefeatureshavebeenfoundusefulinpreviousstudies.However,manyofthesefeaturesrequiresophisticatedtoolsandresources,whicharenotalwaysavailableormaybeinefficientforlarge-scaleapplications.Therefore,alegitimatequestioniswhethertheapproachesproposedintheliteraturecanreallyhelprealworldapplications.Inthisstudy,ouraimistodevelopamethodofWSDthatissufficientlyefficienttobeusedinlargescaleapplicationssuchasIR.Wethusturntoasimplertypeoffeature---co-occurringcontextwords.ThistypeoffeatureisusedinalmostallapproachestoWSD.Thebasicassumptionisthatthemeaningofawordcanbedefinedbythewordsthataccompanyitinthetext.Thisassumptionsoundsreasonable.However,inmanypreviousstudies,thecontextwordsarecollectedwithinafixedsizetextwindowaroundthetargetword.Usually,alltheoccurrencesofcontextwordswithinthewindowareattributedthesameimportance.Thatis,awordatadistanceof5fromthetargetwordisconsideredtobeasimportantasanothercontextwordatadistanceof1.Thisimplicitlyassumesthatthecontextwordswithinthewindowhavethesamedisambiguationpowerforthetargetword,whichiscounterintuitive.Onewouldratherthinkthatacontextwordclosertothetargetwordismoreimportantthanamoredistantcontextword.Forexample,inthesentenceThegovernmentcreatedaspecialfundtosupportthepeopleaffectedbythecollapseofMississippiriverbank,thewordsMississippiandriverhaveamuchlargerimpactonthemeaningofbankthantheotherwordsinthesentence.Ifthedistancefactorisignored,andallthewordsinthesentenceareattributedequalimportance,thewordbankcouldbemisclassifiedasafinancialinstitutionasaneffectofthecontextwordsfundandgovernment.Ingeneral,theimpactofacontextwordonthetargetwordcouldbedefinedasadecayingfunction---amonotonicallydecreasingfunctionofthedistancefromthetargetword.Therearemanysuchfunctions:linear,exponential,logarithmic,etc.Itisdifficulttoselecttherightfunctiontocapturetheimpactofacontextwordonthetargetword.ThereisindeednoextensiveinvestigationonwhatdecayingfunctionshouldbeusedforWSD,leavingthisimportantquestionwideopen.Inthispaper,wearguethattheappropriatedecayingfunctionisdependentonthetextcollectionandonthelanguage.Ontheonehand,theappropriatedecayingfunctionforonelanguagemaybedifferentfromthatofanotherlanguage,forreasonssuchasspecificlanguagestructures.Ontheotherhand,atextcollectioninonearea(e.g.medicine)couldalsorequireadifferentdecayingfunctionfromthatofanotherarea(e.g.novels).Therefore,itseemslikelythatthereisnouniversaldecayingfunction.Whatweproposeinthispaperistolearntheoptimaldecayingfunctionforthespecifictextcollectioninanunsupervisedmanner.Insodoing,wehopetoremovetheburdenofmanuallydeterminingsuchafunction.Themainideabehindourapproachisthat,forthesametargetword,twocontextmodelsconstructedfromdifferentrandomsamplesshouldbesimilar,becausethesamewordshouldhaveroughlythesamemeaning(s)inthetwosamples.Therefore,thedecayingfunctionthatmaximizesthesimilarityofthetwocontextmodelsshouldbeselected.Followingthisidea,weproposetouseamachinelearningmethod(gradientdescent)todeterminetheweightsofcontextwordssoastomaximizethesimilarityofthetwocontextmodels.Oneshouldnotethatwedonotassumethatacontextmodelforatargetwordcorrespondstoauniquewordsense.Thecontextualmodelmayinvolveambiguity;i.e.,thecontextsforitsmultiplesensesareallencompassedwithinthesamemodel.Thisdoesnotaffectourbasicassumption,asbothcontextmodelscreatedfromthetwosampleswouldbeequallyambiguous,andtheirglobalsimilarityshouldstillbehigh.Theaboveassumptionwillallowustofindtheoptimaldecayingfunctionforatextcollection.Theresultingweightscanthenbeusedtobuildacontextmodelforeachtargetwordoccurrence,whichwethenuseinNa&quot;veBayesclassifiers.WetestedtheresultingsystemsagainstseveralmanuallydefineddecayingfunctionsontwoWSDdatasets:theSemeval-2007EnglishLexicalSample(ELS),andtheSemeval-2010JapaneseWSD(JWSD)task.Ourmethodprovedtobeveryeffectiveonbothdatasetsdespitethesimplefeaturesused;theresultswerenotsignificantlydifferentfromthebestsysteminELS,andoutperformedthebestsystemintheJWSDtask.Thisshowsthatsimplecontextwords,whenusedappropriately,canbeaseffectiveasmoresophisticatedfeaturesusedinotherapproaches.Theremainderofthispaperisorganizedasfollows:WediscussthestateoftheartofWSDclassifiersinSectionandpresentourcontextmodelsandourapproachtolearntheweightingfunctioninSection.OurNa&quot;veBayesclassifiersarepresentedinSection.ExperimentsonEnglishandJapanesearepresentedinSection,andSection.Sectionsandarethediscussionandconclusion.</section>
  <section title="The state of the art of WSD">WSDhasbeenstudiedforalongtime,andvariousmethodshavebeenproposedforit.Wedonotintendtomakeacompletesurveyofthesemethods,butwillratherdiscusssomeofthemostrelatedstudies.Interestedreaderscanrefertoorforcompletesurveys.</section>
  <subsection title="WSD using linguistic features">Manypreviousstudiestrytoexploitdifferenttypesoflinguisticfeature.NUS-MLisatypicalsupervisedsysteminthiscategory.Wewilldescribethisparticularsystemindetailbecauseitusesmostofthefeaturesusedinotherstudiesanditwasthetop-performingsystemintheSemeval-2007EnglishWSDtasks.NUS-MLisbasedonaprevioussystemby,ontowhichtopicalmodelingfeatureswereadded.Inpreviousexperiments,SVMhasbeenshowntobeabetterclassifierwiththesefeatures.However,sincetheintegrationofthetopicalfeatureswithintheframeworkofSVMisnotstraightforward,Na&quot;veBayesclassifierswereusedinstead.Thewholesetoffeaturesitusesisasfollows:Onecannoticeseveralpotentialproblemswiththeuseoftheabovefeatures.First,thewordpairsusedintheNUS-MLsystemareintendedtocoverallpossiblecollocationswithinadistanceof3.However,manyofthepairsdonotformmeaningfulcollocations.Theinclusionofthesepairswouldintroducenoisyfeaturesthatwouldnotbeeasytodiscardduringthedisambiguation(orclassification)process.Second,thedistancebetweenthewordscomposingeachpairisnottakenintoaccount.Intuitively,acloserpairofwordswouldhaveagreaterchanceofformingatruecollocationthanamoredistantpairofwords.Thisistheaspectthatourmethodconsiders.AgeneralobservationontheuseofsophisticatedlinguistictoolsandresourcesforWSDisthattheyarenotalwaysavailableinpractice(e.g.POS-taggersareunavailableformanylanguages).Moreover,evenwhentheyareavailable,theiraccuracy,coverageandreliabilitycouldbequestionable.Inaddition,theprocessingtimeisoftentoolongforrealapplications.Forexample,theconstructionoftopicmodelswithLDAisverydemandingoncomputingtime,andmoreproblematicallyinmemory.Unfortunately,theirusedoesnotscalewellforlarge-scaleapplications.Therefore,inthisstudy,wewillturntoamuchsimplerapproach,whichusesonlycontextwordsthatco-occurwiththetargetword.Inthenextsubsection,wewilldescribesomeoftheapproachesrelatedtoit.</subsection>
  <subsection title="Weighted context models">Thegeneralideabehindcontextmodelsistousethewordsthatco-occurwiththetargetwordtoresolveitssense.Onceacontextmodelisconstructedforatargetwordtobedisambiguated,onecancompareitwiththecontextmodelofeachsenseinordertoselecttheonethatistheclosest.Behindtheabovegeneralideaistheimportantissueofhowthecontextmodelisconstructed.Aswestatedearlier,thebasicmethodistobuildacontextmodel(orvector)usingthecontextwordsinafixed-sizewindowaroundthetargetword.Generally,eachwordoccurrenceinsuchawindowhasacountvalueof1.Inotherwords,allthecontextwordsinthewindowareassumedtobeequallyimportantforthemeaningofthetargetword.Aswediscussedearlier,thisiscounterintuitive.Inreality,wewouldexpectthatclosercontextwordshavemoreimpactonthemeaningofthetargetwordthanfartherones.Thisobservationmotivatesapproachesthatuseadecayingfunctiontodecreasetheimportance(weight)ofcontextwordsfartherfromthetargetword.SeveraldecayingweightingfunctionshavebeenproposedindifferentNLPtasks.proposestheuseofanexponentialdecayfunctiontomodelthestrengthofdependencybetweenwordsinIRexperiments.Thestrengthofdependencebetweentwowordsxandyisdefinedase^-(avgDist(x,y)-1)PMI(x,y).Inthisequation,avgDist(x,y)istheaveragedistancebetweenthetwowordsinthecorpus,andPMI(x,y)isthepointwisemutualinformationbetweenthetwowords,basedonfixedwindowco-occurrencestatistics.Thismeasureisusedtomaximizethecohesionofasetoftranslationwordsforaqueryincross-lingualIR(CLIR).proposestorepresentonesensebythecentroidofthecontextvectorsofsensetaggedinstances.Theusedcontextvectorsarelimitedtoco-occurrencesstatisticsofcontextwords.Inordertoimprovethequalityoftherepresentation,thetermfrequenciesaremultipliedbyseveralfactors,oneofwhichistodividethetermfrequencybythesquarerootoftheaveragedistanceofoccurrences.Theuseofthisfactorresultedina3%improvementindisambiguationaccuracy.Sincewecouldexpectthattherewillbeonlyoneinstanceofacontextwordinonesampleinmostcases,thisbasicallymeansthatthefactorappliesapowerlawd^wheredisthedistanceand=-12controlsthedecayrate.However,aswewillseeinourexperiments,theparameterstronglydependsonthelanguageandthetextcollection.Settingthisparameteratafixedvalueisnotthebestchoice.usesthehyperspaceanalogtolanguage(HAL),whichusesafixed-sizeslidingwindowtodeterminewordco-occurrencestatistics.ThisisequivalenttousingalineardecayfunctionforcontextwordssincetheweightofacontextwordisdefinedasN-d,whereNisthesizeofthewindowanddthedistancefromthecontextwordtothetargetword.Thecontextvectorsdefinedthiswaycanbeusedtoestimatethesimilarityofwords.SeeforitsuseinqueryexpansioninIR.Inamorerecentstudy,proposespositionallanguagemodelsforIR,whichpropagatestheweightofwordoccurrencestotheirsurroundingpositions.Atapositioni,thepropagatedcountofawordxiscalculatedasfollows:c^(x,i)=_jc(x,j)K(|i-j|),withc(x,j)beingthecountforwordxatpositionjinthedocument,andKbeingadecayingkernelfunction.Usingthisvalue,onecandeterminescoresforquerywordsatanypositioninthedocumentandcombinethemheuristically.Itisshownthatsuchaweightingfunctioncanimproveretrievaleffectiveness.Sinceitwasnotclearwhatkernelfunctionwouldworkbest,theauthorsofthatstudyexperimentedwithvariouskernelfunctions:Gaussian(e^-d^2),Linear(0,1-d),Cosine((d)uptod=2),andCircle(1-d^2uptod=^-12)wheredisthedistanceandaparameterdeterminedmanually.Amongthesefunctions,theGaussianperformedthebest.Aswecansee,allthesestudiesusedpredefinedweightingfunctions.However,thesefunctionshavenotbeenthoroughlycomparedonthesametaskanditisstillunclearwhichfunctionperformsthebest.Inparticular,nostudyhascomparedmultiplefunctionsforWSD.Wedonotknowhowmuchofanimprovementmaybebroughtbyusingadecayingfunctioninsteadofasimpleuniformweighting.Inaddition,auniquedecayingfunctioncannotfitvarioustextcollectionsandlanguages.Thiswillbeclearlyshowninourexperiments,inwhichweshowthatthebestdecayingfunctionsforacollectioninEnglishisverydifferentfromtheoneforacollectioninJapanese.Ratherthandefiningsuchafunctionmanuallyandtestalargenumberofpossiblefunctionsforagivencollection,inthisstudy,wetrytolearntheoptimalweightingfunctionfromthedata.Inthenextsectionwewilldescribeourproposedcontextmodels,togetherwiththeunsupervisedmethodtocomputeweightsforcontextwordsbasedontheirdistance.</subsection>
  <section title="Learning the optimal context model"/>
  <subsection title="Context model">Ourcontextmodelsarecomposedofdistributionsofthewordsoccurringaroundthetargetword.Inthisstudy,welimitourselvestothesesimplefeaturesanddiscardmoresophisticatedones,inordertobesufficientlyefficientforlargescaleapplications.However,thisdoesnotmeanthatotherfeaturescannotbecombinedwithinthemodellater.Weassumethatacontextwordatdifferentdistancefromthetargetwordhasadifferentimportance,whichwecapturebymeansofacount---thenumberofoccurrencewithinthewindow.Noticethatthiscountcouldbefractional.Wedefine_iasbeingthecountthatonewordoccurrencetakeswhenitoccursatadistanceifromthetargetword.Thisviewofweightedcontextwordsgeneralizesanyweightsetting.Forinstance,uniformweightingwithinafixedwindowofsizescanbeexpressedas_i=1if1is,0otherwise.Wewilllaterdescribethewaytodefineourfunction_i.Inthisstudy,wewillusethelanguagemodelingapproachtodefinecontextmodels.However,thebasicideacanalsobeimplementedwithinotherframeworks.Oncethefunctionisdefined,wecandefineacontextmodelasfollows.LetWbeasetoftextwindowscontainingthetargetwordt,andc_W,i,xbethenumberofoccurrencesofwordxatdistanceiinW,themaximumlikelihood(ML)contextmodelforWisSuchaprobabilityfunctionwillthenformacontextmodelforthetargetword.Theaboveformulationsuffersfromthezeroprobabilityproblem;toavoidit,weproposetoaddDirichletpriorstothedistribution,totheproportionoftheglobalcollectionprobabilitiesP(x|C).Let_Wbeapseudo-countcontrollingtheprior'scontribution,thesmootheddistributionisasfollows:</subsection>
  <subsection title="Estimation of appropriate Dirichlet priors">WhenusingDirichletpriors,thepseudo-count_Wisusuallyassumedtobeaconstanttobedeterminedmanually.However,sinceitsvaluestronglydependsonthecollectionandthesizeofcontextmodel,wecanexpectbetterresultsbyestimatingitforeachlanguagemodel.Todosoweproposetheuseofcross-validation,maximizingthefollowingleave-one-outlog-likelihood:where[P_Dir,W^-1,k(x)=_i_ic_W,i,x+_WP(x|C)-_k_x^_i_ic_W,i,x^+_W-_k]istheprobabilityofxinthesetofwindowsWwithoneinstanceatdistancekleftout.ThisprobabilityisestimatedusingDirichletsmoothingwithaprior_W.Theprobabilityestimatedaboveisusedtoestimatethelikelihood(L_-1)oftheinstanceleftout.Ourgoalhereistodetermineapseudo-countthatmaximizesthelikelihoodoftheleft-outinstance.TofindtheoptimumweuseNewton'smethodforgradientdescent:[^(0)=1^(i+1)=^(i)-L_-1^(^(i))L_-1^(^(i))|^(i+1)-^(i)|&lt;]whereisaparametercontrollingtheendingoftheprocess.Wecansetittoasmallvaluesuchas=10^-4.Aswecansee,equation()wasadaptedtohandlethenon-uniformweightsofoccurrences.Thisinturnresultsinthefollowingderivatives:LetC_W,x=_j_jc_W,j,x,C_W=_xC_W,x,L_-1^()&amp;=-_x_i_ic_W,i,x[(P(x|C)C_W,x+_WP(x|C)-_i)^2-(1C_W+_W-_i)^2]L_-1^()&amp;=-_x_i_ic_W,i,x[(P(x|C)C_W,x+_WP(x|C)-_i)^2-(1C_W+_W)^2]align*Usingthisestimationmethodremovestheneedforanextraparameter.</subsection>
  <subsection title="Learning the decaying function">Thebasicideabehindourapproachisverysimple.Ifweassumethatthecontextmodelsasdefinedabovearerepresentationsofthemeaningofaword,then,ifwordsaandbaresynonymous,thecontextmodelsmadefromtwosetsofwindowsAandBcontainingsamplesofthetwowordsshouldbesimilar.Startingfromuniformweights,wecouldthentrytomaximizesomedistributionsimilaritymeasureover.LetTbethesetofallwindowsforourtargetwordt,wecandefineAandBasrandompartitionsofTintwosubsets(wewilluseWtodenoteoneofsuchsubsets).Aswementionedearlier,eveniftheactualwordtispolysemous,ifthesamplesarerandomlytakenfromthesamecorpus,themixtureofthesensesshouldbesimilar,andthismixturewouldrepresenttheaggregatedsenseoftheword.So,thetwocontextmodelsshouldstillbesimilar.</subsection>
  <subsubsection title="Evaluating similarity~">Inthecontextoflanguagemodeling,awellknownmeasureistheKullback-Leibler(KL)divergence,alsoknownasrelativeentropy.ItisaninformationtheoreticdivergencemeasureindicatinghowmanyextrabitsitwouldtakeonaveragetousesomeencodingP,giventhattherealdistributionisQ.Itisdefinedasfollows:D_KL(P,Q)&amp;=_xP(x)P(x)Q(x)&amp;=-_xP(x)Q(x)+_xP(x)P(x)&amp;=H(P,Q)-H(P)align*Aswecanseeabove,itisequivalenttothedifferencebetweenthecross-entropyofPandQ(i.e.H(P,Q))andtheentropyofP(i.e.H(P)).ThedistributionQmustbesmoothedbecauseofthelogarithmpresentinthecross-entropyequation.Anotherpopularmeasureisinformationradius(IRad),alsoknownastheJensen-Shannon(JS)divergenceorthetotaldivergencetotheaverage.ItistheaverageoftheKL-divergenceofeachdistributionagainsttheaverageofthetwodistributions:[IRad(P,Q)=12D_KL(P,P+Q2)+12D_KL(Q,P+Q2)]Thismeasurehastheadvantagethatthatitisbounded;itiszerowhenthedistributionsarethesame,andonewhentheydiffercompletely,anditissymmetric.1-IRad(P,Q)behaveslikeacosine,thusthenameinformationradius.Asecondadvantageisthatthismeasureisself-smoothed.However,bothabovemeasureshavesomeproblemsduetothefactthatthebaseentropyisremovedfromit.Bothmeasuresonlyconsiderhowclosetwodistributionsare,withouttakingintoaccounttheirusefulnessforthegivendata.Forinstance,theKL-divergenceisminimal(i.e.0)whenthetwodistributionsareidentical,whateverthesedistributionsare.Thesedistributionscanbetheonethatfitthedata(i.e.atruedistribution)orauniformdistribution.KL-divergenceisunabletomakeadifferencebetweenthem.Fromthepointofviewofcreatingamodeltodescribeadata,thetruedistributionisobviouslypreferred.This,however,cannotbedeterminedbyminimizingKL-divergencealone.Similarproblemoccurswheninformationradiusisused.Theaboveobservationshowsthatthemeasurewehavetomaximizeinourcontextshouldcombinethefollowingtwoaspects:thesimilaritybetweentheresultingmodelsobtainedfromthetwosamples,andthedescriptionpowerofthesemodels.Inotherwords,toKL-divergenceorinformationradius,wehavetoaddapenaltycomponentaccordingtothemodel'sdescriptivepower.Wecandefinethemodel'sdescriptivepowerbyitsentropy:Theloweritsentropy,thebetterthemodel.Therefore,themeasurewehavetominimizecouldbe:[D_KL(P,Q)+H(P)=H(P,Q)]Nowweusetheabovemeasureinbothdirectionslikeinformationradius.Thisleadsusto:[H(P,Q)+H(Q,P)]Aswementionedearlier,themeasureofcrossentropyH(P,Q)requiresustosmooththeseconddistributionduetothezeroprobabilityproblem.So,finally,weproposethefollowinglossfunctiontominimizebetweenthetwomodelsobtainedfromthetwosamples:[l()=H(P_ML,A,P_Dir,B)+H(P_ML,B,P_Dir,A)]whereP_ML,AandP_ML,BarethemaximumlikelihoodmodelsforthesamplesAandBweightedusing,andP_Dir,AandP_Dir,BarethemodelssmoothedusingaDirichletprior.Withthisfunction,themodelswillstillneedtobesimilartoachieveahighcross-entropy,butadditionally,modelswillbepenalizedifthissimilarityisobtainedbyincreasingtheentropiesofthemodels.ThereaderwillnoticethatthisvalueiscompatiblewiththeNa&quot;veBayesWSDapproach(Section),inwhichthelikelihoodofthecontextwordsofonesampleiscomputedaccordingtothesmoothedcontextmodelofoneclass.Definingtheoptimalweightingfunctionaccordingtothisvalueisthuslogical.</subsubsection>
  <subsubsection title="Gradient descent on similarity">Weusegradientdescenttofindtheoptimalweights.Herearethederivativesneededforagradientdescent(WandT-WarerespectivelyAandB,orthereverse):H(P_ML,W,P_Dir,(T-W))_i&amp;=-_xV[P_ML,W(x)_i.P_ML,W(x)_iP_Dir,(T-W)(x)+&amp;-_xV[P_ML,W(x)_i..P_Dir,(T-W)(x)_iP_ML,W(x)P_Dir,(T-W)(x)]P_ML,W(x)_i&amp;=c_W,i,x-P_ML,W(x)c_W,i_j_jc_W,jP_Dir,W(x)_i&amp;=c_W,i,x-P_Dir,W(x)c_W,i_j_jc_W,j+_Walign*Now,asthereadermayalreadynotice,theoptimalvalueofthisfunction(orthoseoftheothersimilaritymeasures)dependsonthequantityofdata(numberofinstances)containedinthemodels.Sometimes,itmaybepossiblethattherearefewinstancesforatargetword.Inthiscase,theabovelearningprocesstendtoresultinstableuniformweights.Incontrast,havingmanyinstanceswillresultinweightsachievingalowercross-entropybyshiftingmostoftheweighttoshorterdistances.Inthemostextremecase---thedegeneratecase,alltheweightswillendonthedistance1.Inordertoreducetheriskofoccurrenceofthedegeneratecase,wedonotuseonlyonetargetword,butrathertheweightsarelearntoverasetoftargetwords.Thedetailedprocesswillbedescribedinthenextsubsection.Furthermore,weproposetouseearlystopping,stoppingtheprocessbeforeitstartstodegenerate.Thekeyquestionisnowtoknowwhentostoptheprocess.Itisunrealistictouseastrategythatreliesonanevaluationonthetestdatatodecidewhethertheprocessshouldstop.Thestrategyweusehereisbasedontheobservationthattheweightsassignedtopositionsatlargedistancesshouldbesmall:whenanyoftheweights(usuallytheweightsoflargedistances)fallsbelowasmallthreshold=0.001,thenweconsiderthattheweightshavebecomereasonable,sothelearningprocesscanstop.Indeed,thisapproachisequivalenttoaformofregularization:weintegrateapenaltyinthelossfunctioncorrespondingtotheweightofcontextwordsatlargedistances.</subsubsection>
  <subsubsection title="Stochastic gradient descent">Earlier,wedescribedtheprocessforonetargetword.Thisprocessisrepeatedforanumberoftargetwords(seeAlgorithm1).Werandomlyselectoneword(ambiguousornot)fromthecorpus,computeitsgradient,makeasmallgradientstep,andtheprocessisrepeated.InAlgorithm1,RandomPartitionsplitsthesetoftargetwordwindowsintwo,MakeMLcombinesthewordoccurrencesandweightfunctionintoamaximumlikelihoodunigramlanguagemodel(Equation),ComputePseudoCountusesNewton'smethodtocomputeapseudo-counttocontrolthesmoothing(Section)andMakeDiraddsthecollectionlanguagemodelpriortothemaximumlikelihood(Equation).Thestopconditioniswhatwedescribedabove:anyweightislowerthan&lt;0.001.0WeappliedtheaboveweightlearningprocesstotheEnglishcollectionandJapanesecollectionwith=0.001.</subsubsection>
  <section title="Classifiers for supervised WSD tasks">WeusedthesamesystemintheEnglishandJapanesetasks,sowewillexplainitbeforegoingintodetailsabouteachtasks.WechosetoemployNa&quot;veBayes(NB)classifiersforthefollowingreasons.First,althoughSupportVectorMachine(SVM)classifiersworkbetterthanNa&quot;veBayesinmanycases,integratingsomestypesoffeaturessuchasthetopicalfeaturesofNUS-ML(Section)isproblematic.Moreover,astheuseofrealvaluedcountsmakesthedataevensparser,smoothingbecomesveryimportant.Unfortunately,smoothingisincompatiblewithhyperplane-basedclassifiers.Second,whendealingwithlargedatasets,theefficiencyoftheclassificationbecomesanimportantfactor.Thisveryefficiencyreasonistheonethatmotivatedourstudy.NBclassifiersareattractiveinsuchregard,sincetheyfunctionwithasimplesparsesetofcounts.Aswewillseeinourexperiments,theNBapproachwithawelldefinedcontextmodelcanrivalapproachesusingotherclassificationmodelsusingmoresophisticatedfeatures.</section>
  <subsection title="Features">Althoughtoolssuchasparsersandtaggers,orresourcessuchasdictionaries,thesauriandontologies,canbeusedtoextractfeatures,theiruseinlarge-scaleapplicationsinevitablyraisesperformanceandcoverageissues.Wethereforeusealternativefeaturesthatcanbeeasilyextracted:thetargetwordform,thecontextcontentwords,andthelocalstopwords.</subsection>
  <subsection title="Combining the features">ThefinalscoreforthesamplewandtheclassScombinesthefeatureswedefinedabove.Itisdefinedasfollows:[Score(w,S)=P(S)P_Tar(w|S)^_TarP_Stop(w|S)^_StopP_Con(w|S)^_Con]withP(S)beingtheclasspriors,andP_Tar,P_Stop,P_Conbeingasdefinedabove.ForNBclassifiers,itisacommonpracticetoapplymoreorlesssmoothingtofeaturesinordertocontroltheirimpacts.ThetraditionaluseofLaplacesmoothing(uniformpriorsaddingtypicallyonetothecounts)forNBclassifiersindeedover-smoothesfeaturesinordertogetaroundfeaturedependenceproblems.Suchover-smoothingis,however,lessthanideal.Forinstance,picturethecaseoftwohighlycorrelatedfeatures.Theimpactofsuchfeaturesshouldbereducedtocompensatetheiroverlap,butitisnotclearhowthisshouldbedonebycontrollingthesmoothing.WethususeautomaticDirichletsmoothing(usingtheestimationmethoddescribedinSection)tofixprobabilities,thenscalethemwiththefactors_Tar,_Stopand_Con.Theseparameterswillbedeterminedsoastomaximizetheperformanceonthetrainingdatausingleave-one-out.Forourtrials,wetriedboththeautomaticestimationoftheprior,andthemanualselectionofapseudo-count,keepingthecaseresultinginthehighestcross-validationaccuracy.Sincetheselectionofamanualpseudo-counthasthesameobjectiveastoscalethelog-likelihood,whenamanualvalueisused,theassociatedscalingfactorissetto1.</subsection>
  <section title="Experiments on Semeval-2007 English Lexical Sample">Semeval(previouslycalledSenseval)isasemanticevaluationworkshopheldeverythreeyears.OurfirstWSDexperimentusesdatafromthe2007EnglishLexicalSample(ELS)task.ThetaskconsistedofbuildingWSDclassifiersmaximizingtheWSDaccuracyofheld-outtestdata.Thedetailsofthedataareasfollows:Thewordsampleswereextractedfromtheonemillion-wordWallStreetJournal(WSJ)Treebank.Onehundredlemmas(65verbsand35nouns)werechosenfortheirpolysemyandnumberofannotatedinstances.TheoriginaldatawasannotatedusingWordNetsenses,butasthissenseinventoryistoofinetogetagoodannotatoragreement,theorganizersfusionedsomesensesusinggroupingsofWordNetsensesdefinedintheOntoNotesproject.Theresultinginventorieshaveanaverageof3.6sensesperword.Tableliststhenumberoftaggedinstances.Theaverageannotatoragreementisclaimedtobeover90%.</section>
  <subsection title="Baselines">Themostfrequentsense(MFS)baselinealwaysassignsthemostprobablesensefoundinthetrainingdata.Itactsasalowerboundforclassificationaccuracy.Achangeinaccuracybetweencross-validationonthetrainingdataandtrialsonthetestdatacanalsohintatdifferencesinthenatureofthetwosets.Forthesakeofcomparison,wealsousedaSVMbaselinewiththesamefeaturesastheonesstatedabove.WeusedtheSVM-lightpackagewithlinearkernelsandusedthegrid-searchscripttoautomaticallyselecttheregularizationparameters.OurtrialsindicatedthattheSVMclassifiershadserousissuesindealingwiththesparsitypresentthecountdata.Wetriedvariousformulationsforthem,suchasnormalizingthecountsforonesample,inputtingthemaslog-of-count,etc.Intheend,thebestresultswereobtainedwithBooleanfeaturesindicatingthepresenceofawordinafixed-sizewindow(ofsize3).ForourNBsystems,wechosevariousbaselinesusingweightingfunctionsproposedinpreviousstudies:OurmethodisnamedLearned:Seeingtheresultingcurvefromtheleaningprocess,wealsoaddthepowerlawfunction,whichisdefinedasfollows:Thisfunctionwasimplicitlysuggestedin,wherethetermfrequenciesweredividedbythesquarerootoftheaveragedistanceofoccurrences.Thisisroughlyequivalenttoapowerlawwithaparameter=-12.Theparametersforthesefunctionswereselectedafterextensivetrials,astomaximizetheaccuracyonleave-one-outcross-validationonthetrainingdata.</subsection>
  <subsection title="Computing the weights">TheSemevaldatawasinsufficienttocomputeweightsforoursystem.Tocomputetheweights,weusedanexternalcorpusfromtheAssociatedPress88--90corpusoftheTRECcollection(CD1&amp;2),containing242,918documents.ThecollectionwasstemmedwiththePorterstemmerandeachstemwasconsideredasatargetwordwhilelearningtheweights.Fullwindowsof100wordswerecollectedforeverystem.Thiswindowsizewasconsideredbigenoughtobuildcontextmodels.Weusedanupperboundof1,000windowsamplespermodelsinceweconsidereditsufficientfortheestimationofthemodels.Astoplistwasmadefromthetop10mostfrequentwords,andplaceholderswereleftinthewindowstopreservethedistances.Consecutivestopwords(ex:``ofthe'')weremergedinoneplaceholdertoken.Thisresultedin32,650targetwordscontaining5,870,604windows.Figureshowsthefirstportionoftheresultingcurve,togetherwiththebestsettingsforthebaselines.Allthecurvesinitiallyhaveroughlythesamedecay,butthelearnedcurvediffersfromtheothersinitslongtail.Thepowercurvebearsastrikingresemblancetothelearnedcurve.Aswewillsee,thepowerlawcurvealsogivesaverygoodaccuracy.</subsection>
  <subsection title="Results">Tableliststheresults(theoriginaltopsystemsareshowntotheright).Thecontextwordswereprocessedthesamewayastheexternalcorpus.Thetargetwordwasnotstemmedbutwasconvertedtolowercase.TheSemevaldatacontainsonlythesmallpassagescontainingthetargetwords,makingwordsrelatedtothemunusuallyfrequent.Therefore,weuseacollectionlanguagemodelbasedontheconcatenationoftheAPcollectionandtheSemevaldata.Theclasspriorsusedanabsolutediscountingof0.5.Tableliststheparametersforeachsystem.Theseparametersweredeterminedsoastomaximizetheaccuracyinleave-one-outtrialsonthetrainingdata.Aswecansee,thebestpowerlawfunctionforthiscollectionhastheparameter=-1.1.Thisisdifferentfrom=-12setmanuallyin.TheresultsshowninTableconfirmedourintuitionthatdecayingweightingfunctionsarebeneficialforWSD:allNBclassifierswithadecayingfunctionworkbeterthantheNBclassifierwithuniformweighting.However,differentdecayingfunctionsleadtodifferentWSDaccuracies.ThisconfirmstheimportanceofchoosinganappropriatedecayingfunctionforWSD.Thelearnedweightingfunctionworkedbetterthanthebaselinefunctionsofthepreviouswork.Theaccuracyofoursystemalsocomparedwelltothecurrentstateoftheart.Allbutthebestoneofthe14originalsubmissionsofthetaskwereoutperformed.Arandomizationtest(one-way,notiesplitting,2Mpasses)betweenPowerNBandNUS-MLgivesap-valueof0.1818.Wecanthereforeconcludethatoursystemiscomparableinaccuracytothebestsystemintheliterature,eventhoughitdoesnotusefeaturessuchaslocalcollocations,positionalwordfeaturesorPOS-tags.Wealsonoticedthatthenegativepowercurveworkedslightlybetterthanthelearnedfunction,althoughthedifferencebetweenthemwasnotstatisticallysignificant.Thisresultclearlyshowsthatthepowerlawistherightfunctiontouseifwehavetochooseamanuallydefinedfunction.</subsection>
  <section title="Experiments on Semeval-2010 Japanese WSD">Lastyear'sSemevalJapaneseWSDtaskisalexicalsampletypetasklikeELS.Itsdetailsareasfollows:ThewordsampleswereextractedfromtheBalancedCorpusofContemporaryWrittenJapanese(BCCWJ).Thiscorpus,muchliketheBrowncorpus,containsvarioustypesofdocumentgenressuchasbooks,magazines,newspapers,whitepapers,transcriptsfromthedietandwebsites(blogsandQAsites).taskdatacontainsasubsetofthese:books,whitepapersandnewspapersareinthetrainingdataandthetestdataalsocontainsQ&amp;Awebpages.Thereare50lemmas(22nouns,23verbsand5adjectives)thatwerechosenfortheentropyoftheirsensedistribution.ThisishalfthenumberchosenfortheELS,buttheproportionmaybebettersinceitgivesfewerlemmastoverbs.ThemiddleleveloftheIwanamiKokugodictionaryentrieswasusedtomakesenseinventories.Thedefinitionsinthisdictionaryhaveatree-likestructure,withonelevelunderthemiddlelevelcontainingprecisedetails.Forthetargetwordstobedisambiguated,thereisanaverageof3.58sensesperword,whichpracticallyisthesameastheELStask.Whennoneoftheinventorysenseswouldfitasample,theannotatorsattachedan``out-of-dictionary''tag,whichisthenseenasanadditionalsenseclass.Thereare50trainingand50testinstancesforeachofthetargetwords.ThisisdifferentfromtheELSintworegards.(1)ThenumberoftaggedsamplesintheELSvariedwitheverytaskwordandfollowedtheirfrequencyinthecorpus.(2)TheELShadaroundfourtimestheamountoftraininginstances.Theinter-annotatoragreementhadakappavalueof0.678.SinceJapanesedoesnotemploywordseparators,theorganizerstreatedthedocumentswiththeChaSensyntacticanalyzerandmanuallycorrectedtheerrors.Thisprovidedparticipantswithgold-standardwordsegmentation,POS-tagging,wordreading,andbaseformreadingforconjugatedwords.TheproportionsoftheinstanceswithregardtogenrearelistedinTable.Aswecansee,thegenredistributionisquitedifferentbetweentestandtrainingdata,themaindifferencebeingtheadditionofQ&amp;Awebpagestothetestdata.</section>
  <subsection title="Treatment of Japanese text">WetriedtogiveasimilartreatmenttotheJapaneseaswedidtotheEnglishdata,butthelanguagesarequitedifferent.Stemmingdoesn'tapplytoJapanese,soweusedlemmatization.Sincewehadparsinginformation,weconvertedcontextwordsintotheirsurfaceformreadingsorbaseformreadingswhenpresent,andappendedthekanjis(Chinesecharacters)inthesurfaceform.ThistreatmentismotivatedbythefollowingaspectsofJapanese:Japanesehasmanyhomophones,sousingonlythereadingwouldresultintoostrongaconflation.Theuseofkanjisisveryrelatedtotheusedsense.Forinstance,someverbshavedifferentcharacterstospecializetheirmeanings.Agoodexampleofthisistheverbtomeet(au),whichcantakedifferentcharactersdependingonwhetheritmeanstomeetwithsomeone,tohaveanencounter,anaccident,abadexperience,ortofittogether.Insomecases,thewordcanbewrittenwithoutkanji(i.e.inhiraganaorkatakana)inordertogiveabroaderormoreabstractsense.Somewordscanbewrittensometimeswithkanjis,sometimeswithkanjisandin-betweenokurigana(``accompanyingletters'').Theseokuriganaactasphoneticcomplementstodisambiguatetheusedreading.Oneexampleofthisisthewordsales(uriage)whichcanbewrittenuri-oru-ri-a-ge(theunderlinemeansthereadingisinsideonekanji).Thepresenceofdifferentreadingsforthesamesurfaceformcanalterthemeaningoftheword.Forinstance,inJapaneselikeinEnglish,thewordmarkethasboththesupermarketandstockmarketsenses.Thetwoarewrittenthesame,buttheyhavedifferentreadings.TheJapanesereading``ichiba''hasthe``stockmarket''sense,andtheChinesereading``shij^o''meansthelocationwhereonecanbuygoods.Appendingthekanjisandthereadinghadthepurposeofconflatingsomeoftheseforms.Asareplacementtoastoplist,weusedtheparsinginformationtostripallconjugations(setsuzoku-to,jod^o-shi,etc.),particles(alljo-shi),symbols(blanks,kig^o,etc.),andnumbers.Notethatthiscouldalsohavebeendonebyusingwordfrequencies.Weusedthetop10mostfrequentwordsasthestopwordfeatures.Thesewereallparticles(jo-shi).</subsection>
  <subsection title="Computing weights">ThedatafortheJWSDtaskdidnotcontainenoughtexttocomputetheweights.Asasupplement,weusedtheMainichi2008corpusofNTCIR-8asanexternalcorpuswiththeChasen+Unidicparserthatwasusedtocreatethetaskdata.Thisislessthanidealsinceitresultsinadifferentparsethanthemanuallycorrectedtaskdata.Theresultingweightsforthefirst10positionsareshowninFigure.AgeneralobservationsisthatthesecurvesaresimilartotheEnglishcase.However,wedoobserveafewdifferences.TheweightsfallmuchfasterinJapanesethaninEnglish.Thedifferencesmaybeexplainedpartlybythedifferenttreatmentsofthetext.Therearealsolinguisticreasons.Forinstance,theorderofthesentencecomponentsismoreflexibleinJapanese.Moreover,thesubjectisoftenomittedinJapanese.AnotherdifferenceisthattheJapaneselanguageisasubject-object-verbtypeoflanguage,whereasEnglishissubject-verb-object.Thatis,verbsinEnglishareusuallybetweentheirsubjectandcomplements,buttheyarealmostalwaysonoppositeendsofthesentenceinJapanese.Thesefactorsmaymakenon-localdependenciesmorefrequentinJapanese.IfweassumethatwordsinEnglishandJapanesehaveonaverageasmanydependencies,ifmorearenon-local,fewerofthemwillbenearby,reducingtherelationofclosecontextwords.Thenon-localdependencieswillbewithinthenoisyfarthercontext,andthustheweightwilldecayfaster.</subsection>
  <section title="Discussion">OurexperimentsshowthatJapaneseWSDismoredifficultthanEnglish,probablybecauseithasmorenon-localdependencies.Inordertocapturesuchdependencieswithouttheuseoflexicalparsers,wewilltrymergingsimplepatternswithourweightlearningmethod.Asourmethodbasicallygroupswordoccurrencesintobins,wedonothavetolimitourselvestothesimplisticwindowdistanceandcandefinemorespecializedbinsthatcanreflecttheaveragerelationsofthefeatures.Also,aswenowhaveanideaofthenatureoftheoptimalweightingfunction,wemayinvestigateoneofthepitfallsofthewindowdistance:theuniformincreaseindistancewithoutregardtothewordsencountered.Webelievethatsomewords,suchasstopwords,shouldaddlessdistancethanothers.Ontheotherhand,wecaneasilyseethattokenssuchascommas,fullstops,parenthesesandparagraphshouldaddmoredistanceincomparisonwithregularwords.Wecouldthereforemakeuseofacongruencescoreforaword,anindicatorshowingonaveragehowmuchwhatcomesbeforeissimilartowhatcomesaftertheword.Oneofthedifficultiesofcomputingsuchvalueisthatwewouldhavetocomparethecontextbeforeandaftersinglesinstances.Althoughthiswouldleadtoaterriblesparsityproblem,webelievethattheuseofdimensionreductionmethodssuchasLDAwouldalleviateit.Theweightingcurveandcontextmodelsresultingfromourmethodcouldbeusedinvariousfields.ThecontextmodelscanbeseenasaconditionalprobabilityP(x|t)ofawordgivenanoccurrenceofthetargetwordt.Suchprobabilitiesareoftenusedinco-occurrence-basedmethods,suchasqueryexpansioninIR.TranslatingawordisataskthathassomesimilaritytoWSD.Indeed,arecentSemevaltaskwastoevaluatedisambiguationaccuracyviaaparrallelcorpus.Thecontextmodelsweproposewouldfitverywellinsuchatask,andthiscouldleadtoimprovementsinstatisticalmachinetranslationandcross-languageIR,sinceinboththesedomainstheuseofcontextinformationislimited.Lastly,theweightsresultingfromourprocesscanbeinterpretedastheexpectedstrengthofrelationbetweentwowordsinadocumentasafunctionoftheirdistancefromeachother.ThewordrelationshipbetweenwordsindocumentsandwordsinqueriesisacurrentresearchtopicinIR.ThenewweightingschemacouldbeeasilyintegratedwithadependencymodelinIR.Weplantoperformsuchanintegrationinthefuture.</section>
  <section title="Conclusion">InWSD,contextwordsareoftenusedtoconstructedabag-of-wordsmodelforthetargetword.However,thecontextwordsareweightedeitheruniformlyoraccordingtoamanuallyselecteddecayingfunction.NostudyhadinvestigatedwhethersuchadecayingfunctionisthemostappropriateforWSD.Inthispaper,wepresentedaprincipledmethodtoweightcontextwordsinabag-of-wordsmodelofwordsense.Toinvestigatewhattheoptimalfunctionwouldbe,weproposedanunsupervisedmethodforfindingoptimalweightsforcontextwordsinfunctionoftheirdistancetothetargetword.Thegeneralideaistofindtheweightsthatbestfitthedata,insuchawaythatthecontextmodelsforthesametargetwordgeneratedfromtworandomsamplesbecomesimilar.OurexperimentsonWSDinEnglishandJapanesesuggestthevalidityoftheprinciple.Inourexperiments,weobservedthatthelearnedfunctionshaveverysimilarshapestopowerlawfunctions.ThisfunctionhasnotbeensuggestedinpreviousstudiesonWSD.Thisfunctioncouldsuggestsomeproprtiesofworddependencywithintextwindowandhelpsusunderstandtruedependenciesbetweenneighboringwords.Wealsoproposedtheuseofalternativesfeaturesasreplacementsforlocalcollocations,syntacticparserandlexicalanalyzerdata.Wefoundthatthelocalstopwordsarehighlyrelatedtothesensedistribution.Thesefeatures,combinedwiththecontextmodels,andsmoothingestimationtechniquesresultsinalightweightandscalableWSDsystemwithstate-of-the-artperformance.TheEnglishandJapaneseexperimentsshowedthatthedecayingfunctionislanguage-dependent,butitroughlyfollowsthesamebackgroundfunction.ForlanguagessuchasJapanese,wherethewritingstylechangesgreatlywiththedocumentgenre,thisfunctioncouldalsobecorpus-dependent.OurapproachhastheadvantageofbeingabletoadapttosuchdifferencesanditcouldbeeasilyadaptedtootherNLPtaskssuchasIRandMT.Thesearesubjectsthatwewillinvestigateinourfutureresearch.workispartiallysupportedbyaJapaneseMEXTGrant-in-AidforScientificResearchonInfo-plosion(#21013046)andtheJapaneseMEXTResearchStudentScholarshipprogram.Theauthorsarethankfultotheanonymousreviewersfortheirconstructivereviews.document</section>
</root>
